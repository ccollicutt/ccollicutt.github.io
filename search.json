[
  
    {
      "title"    : "The Numerous Pains of Programming: Death by a Thousand Cuts",
      "category" : "",
      "tags"     : "",
      "url"      : "/2024/10/25/pain-of-programming.html",
      "date"     : "2024-10-25 00:00:00 -0400",
      "content"  : "I recently started building a small Python application, comprising less than 1000 lines of code. However, it’s been a real challenge to get it to the point where it can be deployed and run in a production environment. I’ve done this before, but it’s always difficult, and each time I’m reminded of how much effort it is, in part because I always have to start anew.As a part-time developer…which of these extra things do I actually have to do? And to what extent? The code, sure, but all the other things?The Pain of ProgrammingFor most applications, writing the code is relatively straightforward, though it can still be challenging. However, there are tons of other things around the code, all that extra work you have to do to make it a quote unquote “real application”.Below I’m not even including things like picking a language or framework.  NOTE: This is not a perfect, comprehensive list–it’s a brainstorm of things I can think of that I’ve had to do, or might have to do to make an application “real” and somewhat professional.  Writing and maintaining comprehensive test suites, unit tests, integration tests, etc.  Setting up testing infrastructure and testing frameworks  Designing and implementing CI/CD pipelines  Creating and maintaining distributable packages  Managing production environment deployment and monitoring  Implementing release management  Writing and maintaining documentation  Creating and updating system architecture diagrams  Managing version control workflows and git complexity  Supporting and responding to user needs and feedback  Implementing basic security best practices  Conducting stress testing and performance optimization  Containerizing the application and creating Kubernetes manifests  Setting up vulnerability scanning  Managing security issues in dependencies  Managing databases, data models, and migrations  Configuring IDE settings and extensions  Setting up development environments (e.g., virtual environments)  Managing dependency trees and version conflicts  Configuring and maintaining linters and formatters  Setting up logging and monitoring infrastructure  Dealing with debugging tools…more than print statements?  Testing with SSL certificates  Deploying on different operating systems and user environments  Dealing with authentication, perhaps RBAC too  Do I need different environments for development, testing, staging, and production?  Keeping track of issues, tickets, and other project management tasks  How to get secrets and configuration into the application safely and securely  Dealing with updating the software, from configuration to removing features and functionality  Understanding resource requirements, memory, cpu, etc. and what to set limits to in production  Threat modelling–how would an attacker get in, what would be the impact, etc.  What license should I use for the software? What does it mean if I choose the wrong one?  What metrics should it output, and how?I think I could keep going, and I’m sure there are many more.The above is all a tremendous amount of work, work that extends well beyond the few hundred or thousand lines of code you write for the core of a simple application. For a small application, it doesn’t seem worth it. There doesn’t seem to be much in the way of easy-to-use automation or other tools to help deal with it all–it’s just a big old mess of tasks that add up to a lot of work.What is a Real Application?I keep using the phrase “real application”, but what does that mean? What makes an application real? I suppose what I mean is an enterprise, production-grade application, of some kind. But that doesn’t mean much–I’m not sure anyone knows what makes an application production-grade. It’s kind of a vague term, and I think we could probably do better. The reality is that programming is more of an art, a dedicated craft, with hundreds of things that need to be done and hundreds many more tools, technologies, and techniques that can be used to do them. Some of this is optional, some of it is required, and some of it is just good practice. Which is which…it’s not always clear.Tedious WorkThis list is a decomposition of what a developer has to do. I find all this extra work absolutely exhausting and, frankly, quite tedious. In every single one of these steps it seems to me like there just has to be a better way. And for some, I’m sure there is, I simply may not know about it yet. But I don’t believe that there is any single tool, technique, or technology that can help with all of it.Ultimately, I’d love to have a magic wand to wave and get all this done, and thus just focus on the code that provides the value, but I don’t think that’s possible. Generative AI isn’t going to help with this all that much, one can imagine it tying this all together somehow, but the problems still exist, they would just be hidden, like a river of lave beneath the surface.Imagine what we could do if we just got rid of all this tedium?An Application Can Never Be Done, but It Has to Be RunI think the reality is that an application can never be complete. There is always more work. It can never be 100% finished–there is always something missing, which is an incredible place to be when you consider that one of humanity’s main outputs at this point in time, one of our main economic drivers, is the writing and running of software. What a bizarre situation to be in!My next post will be regarding some solutions to the pain of programming, some kind of protective armor that can stop the thousand cuts."
    } ,
  
    {
      "title"    : "Incus Installation and Use - Setup Storage Pools and Bridged Networking",
      "category" : "",
      "tags"     : "",
      "url"      : "/2024/10/19/incus-installation-and-use.html",
      "date"     : "2024-10-19 00:00:00 -0400",
      "content"  : "In this post I’ll show you how to install and setup Incus on a physical host running Ubuntu 24.04. I’ll setup a storage pool and a bridge network, then launch a VM. Once this is all done, I’ll have a nice homelab server that can spin up many virtual machines and do it quickly, putting them on the right storage pool, on a separate network.What is Incus?  Incus is a next-generation system container, application container, and virtual machine manager. It provides a user experience similar to that of a public cloud. With it, you can easily mix and match both containers and virtual machines, sharing the same underlying storage and network. - Incus DocsBasically, once you install Incus you can ask it for virtual machines or system containers (not Docker containers, but system conatiners) and it will go and build them for you.Physical HostI’m using Ubuntu 24.04 on my homelab server.$ cat /etc/lsb-release DISTRIB_ID=UbuntuDISTRIB_RELEASE=24.04DISTRIB_CODENAME=nobleDISTRIB_DESCRIPTION=\"Ubuntu 24.04.1 LTS\"It’s an older server, but it’s got a lot of memory.$ free -h               total        used        free      shared  buff/cache   availableMem:           188Gi       3.1Gi       185Gi       673Mi       2.5Gi       185GiSwap:          8.0Gi          0B       8.0GiAnd lots of room for disks and such, including a 2TB NVMe drive, which I’ll use for my main storage pool.Install IncusI’ll be following the Incus docs - https://linuxcontainers.org/incus/docs/main/installing/#installingFirst, install the incus and qemu packages; need qemu for the VM support.apt install incus qemu-systemIncus is a small set of packages, qemu is a fair bit larger.Add your user to the incus group.$ sudo adduser curtis incus-admininfo: Adding user `curtis' to group `incus-admin' ...Log out and log back in to get the new group, or use newgrp, whatever you want.$ incus ls+------+-------+------+------+------+-----------+| NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS |+------+-------+------+------+------+-----------+Storage PoolI have a NVMe drive mounted on /mnt/nvme0n1 and I want to use that to back my incus managed virtual machines.$ sudo mkdir -p /mnt/nvme0n1/incus$ incus storage create p1 dir source=/mnt/nvme0n1/incusStorage pool p1 created$ incus storage ls+------+--------+--------------------+-------------+---------+---------+| NAME | DRIVER |       SOURCE       | DESCRIPTION | USED BY |  STATE  |+------+--------+--------------------+-------------+---------+---------+| p1   | dir    | /mnt/nvme0n1/incus |             | 0       | CREATED |+------+--------+--------------------+-------------+---------+---------+Some files and directories are created in /mnt/nvme0n1/incus.$ ls /mnt/nvme0n1/incus/buckets     containers-snapshots  custom-snapshots  virtual-machinescontainers  custom                images            virtual-machines-snapshotsVery slick and easy to create the storage pool.Now to build a VM using that storage pool.$ incus launch images:ubuntu/24.04 test --vm --storage p1Launching test                                          The instance you are starting doesn't have any network attached to it.  To create a new network, use: incus network create  To attach a network to an instance, use: incus network attachNote that I don’t have a network configured, so this didn’t actually start the VM.But, files are created for the VM in the storage pool.$ sudo tree /mnt/nvme0n1/incus//mnt/nvme0n1/incus/├── buckets├── containers├── containers-snapshots├── custom├── custom-snapshots├── images├── virtual-machines│   └── test│       ├── agent-client.crt│       ├── agent-client.key│       ├── agent.crt│       ├── agent.key│       ├── backup.yaml│       ├── config│       │   ├── agent.conf│       │   ├── agent.crt│       │   ├── agent.key│       │   ├── agent-mounts.json│       │   ├── files│       │   │   ├── hostname.tpl.out│       │   │   ├── hosts.tpl.out│       │   │   └── metadata.yaml│       │   ├── incus-agent│       │   ├── install.sh│       │   ├── lxd-agent -&gt; incus-agent│       │   ├── nics│       │   ├── server.crt│       │   ├── systemd│       │   │   ├── incus-agent.service│       │   │   └── incus-agent-setup│       │   └── udev│       │       └── 99-incus-agent.rules│       ├── metadata.yaml│       ├── OVMF_VARS_4M.ms.fd│       ├── qemu.nvram -&gt; OVMF_VARS_4M.ms.fd│       ├── root.img│       └── templates│           ├── hostname.tpl│           └── hosts.tpl└── virtual-machines-snapshots16 directories, 25 filesNetworkingOK, I love networking, but it can also be a pain, especially when we’re dealing with bridges and virtual machines, etc, etc. I like to think of networking as moving packets as quickly as possible, not configuring bridges, but there’s just no avoiding it.The physical host has the below netplan configuration, where I’ve added a VLAN to eth3.$ sudo cat 50-cloud-init.yaml network:    ethernets:        eno1: {}        eth3: {}    version: 2    vlans:        eno1.101:            addresses:            - 10.100.1.20/24            id: 101            link: eno1            nameservers:                addresses:                - 10.100.1.3                search: []            routes:            -   to: default                via: 10.100.1.1        eth3.105:            id: 105            link: eth3        eth3.106:            id: 106            link: eth3I’m going to tell incus to create a bridge on a network interface that has a VLAN tag on it, eth3.106.$ incus network create br106 \\  --type=bridge \\  bridge.external_interfaces=eth3.106 \\  ipv4.dhcp=false \\  ipv4.nat=false \\  ipv6.nat=false \\  ipv4.address=none \\  ipv6.address=noneThat command creates this config:$ incus network show br106config:  bridge.external_interfaces: eth3.106  ipv4.address: none  ipv4.dhcp: \"false\"  ipv4.nat: \"false\"  ipv6.address: none  ipv6.nat: \"false\"description: \"\"name: br106type: bridgeused_by:- /1.0/instances/testmanaged: truestatus: Createdlocations:- noneDHCP is actually provided by my physical switch, not incus. So when I launch a VM, it starts with DHCP, but that DHCP address is coming from the upstream switch, not incus. This is what I want.I can launch a VM with this network on the previously configured storage pool.$ incus launch images:ubuntu/24.04 test --vm --storage p1 --network br106And list the VMs to see the new one, note that we can see the IP address of the VM even though Incus isn’t doing the IP address management.$ incus ls+------+---------+---------------------+------+-----------------+-----------+| NAME |  STATE  |        IPV4         | IPV6 |      TYPE       | SNAPSHOTS |+------+---------+---------------------+------+-----------------+-----------+| test | RUNNING | 10.100.6.250 (enp5s0) |      | VIRTUAL-MACHINE | 0         |+------+---------+---------------------+------+-----------------+-----------+Hop onto that VM and ping 1.1.1.1:$ incus shell testroot@test:~# ping -c 3 1.1.1.1PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data.64 bytes from 1.1.1.1: icmp_seq=1 ttl=53 time=5.80 ms64 bytes from 1.1.1.1: icmp_seq=2 ttl=53 time=3.43 ms64 bytes from 1.1.1.1: icmp_seq=3 ttl=53 time=3.47 ms--- 1.1.1.1 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2003msrtt min/avg/max/mdev = 3.425/4.232/5.802/1.110 msNetwork is online!For various reasons I use Mikrotik switches/routers in my homelab, so this interface might look different on your network. Obviously I don’t have a lot of DHCP going on. :)[admin@MikroTik] &gt; /ip dhcp-server lease print Flags: X - disabled, R - radius, D - dynamic, B - blocked  #   ADDRESS                                                                    MAC-ADDRESS       HOST-NAME                                 SERVER                                 RATE-LIMIT                                 STATUS  0 D 10.100.6.250                                                                 00:16:3E:4D:15:97 distrobuilder-705ecd65-121a-4b5b-8cdc-... dhcp1                                                                             bound  [admin@MikroTik] &gt; And we can see the DHCP lease is for the VM.Incus ProfilesFinally, I’ll create a profile for the VM, or rather I’ll edit the default profile to use the bridge network and the storage pool.$ incus profile show defaultconfig: {}description: Default Incus profiledevices:  eth0:    network: br106    type: nic  root:    path: /    pool: p1    type: diskname: defaultused_by:- /1.0/instances/testConclusionAnd there you have it. Incus is now managing my virtual machines, putting them on my storage pool, and giving me a bridge network with IPs from my DHCP server."
    } ,
  
    {
      "title"    : "Why Aren&#39;t You Using Incus to Create Containers and Virtual Machines?",
      "category" : "",
      "tags"     : "",
      "url"      : "/2024/10/12/incus.html",
      "date"     : "2024-10-12 00:00:00 -0400",
      "content"  : "IncusVirtual machines remain the main building block of pretty much all infrastructure. We tend to forget about the technology and just how entrenched it is in our daily technical lives. Ok, that’s a pretty heavy statement for a blog post, but I do think we forget about virtual machines and just how valuable and secure the technology is–most public cloud services are loss leaders for the VM part of their business.Anyways, what I want to talk about is Incus, a way to easily create containers AND virtual machines.  NOTE: I use Incus to exclusively create virtual machines, and don’t use the container functionality that much. I would imagine that most people use the container functionality more. So while I’ll touch on the system container functionality, I use Incus for local VMs.What is Incus?  When using Incus, you can manage your instances (containers and VMs) with a simple command line tool, directly through the REST API or by using third-party tools and integrations. Incus implements a single REST API for both local and remote access. The Incus project was created by Aleksa Sarai as a community driven alternative to Canonical’s LXD. Today, it’s led and maintained by many of the same people that once created LXD. - Incus DocsWith Incus, you can easily create virtual machines and containers.I don’t know all the history of the project, where it comes from in terms of LXD/LXC, etc, but I do know that I need a way to easily create virtual machines on my local computer, and that I really enjoy using Incus. So easy.Example of creating a virtual machine:incus launch ubuntu:22.04 my-server --vmIt’s that easy. Especially on Ubuntu 24.04, where you can just install the incus package from the default repositories.I also alias the incus command to this script because I always forget the incus command syntax, and I’m super lazy. So this would create a default sized VM with just vm launch my-vm.#!/bin/bashlaunch_vm() {  if [ -z \"$1\" ]; then    echo \"Usage: $0 launch &lt;vm-name&gt;\"    echo \"Example: $0 launch my-ubuntu-vm\"    exit 1  fi  local name=\"$1\"  local cpu=2  local memory=\"4GiB\"  local disk=\"40GiB\"  incus launch images:ubuntu/24.04 \"$name\" --vm \\    --device root,size=\"$disk\" \\    -c limits.cpu=\"$cpu\" \\    -c limits.memory=\"$memory\"}list_vms() {  incus ls}show_help() {  echo \"Usage: $0 &lt;command&gt; [options]\"  echo  echo \"Commands:\"  echo \"  launch &lt;vm-name&gt;  Launch a new VM\"  echo \"  ls                List all VMs\"  echo \"  help              Show this help message\"  echo  echo \"For other commands, this script will pass them directly to incus.\"}# Main command handlercase \"$1\" in  launch)    launch_vm \"$2\"    ;;  ls)    list_vms    ;;  help)    show_help    ;;  *)    if [ -z \"$1\" ]; then      show_help    else      # If the command isn't recognized, pass it to incus      incus \"$@\"    fi    ;;esacUsing IncusAs mentioned earlier, I almost exclusively use Incus to get a virtual machine.E.g. with my script I just run:vm launch a-vmOr with the bare Incus command it’s just as easy:incus launch images:ubuntu/24.04 a-vm --vmNow I can shell into the VM very quickly.$ incus shell a-vm # or with my script, vm shell a-vmroot@a-vm:~# And you are in a nice little virtual machine that you can install anything you want into.Getting a ContainerWriting this post was the first time I used the container functionality of Incus! Getting a container is the default mode of operation, and it’s super easy.$ incus launch images:ubuntu/22.04 ubuntu-container# Image is downloaded, and the container is createdLaunching ubuntu-container$ incus ls | grep ubuntu-container| ubuntu-container | RUNNING | 10.57.7.201 (eth0)           | fd42:af1f:b7c8:a36c:216:3eff:fee9:32e4 (eth0)   | CONTAINER       | 0         |$ vm shell ubuntu-containerThat is lightning fast. But again, important to note: this is a “system container” and not a “application container”, or in simpler terms, it’s not a docker container. If you have ever used LXC, then you will be right at home.  Application containers (as provided by, for example, Docker) package a single process or application. System containers, on the other hand, simulate a full operating system similar to what you would be running on a host or in a virtual machine. You can run Docker in an Incus system container, but you would not run Incus in a Docker application container. - Incus DocsYou may also want to understand the differences between a virtual machine and a system container:  Virtual machines create a virtual version of a physical machine, using hardware features of the host system. The boundaries between the host system and virtual machines is enforced by those hardware features. System containers, on the other hand, use the already running OS kernel of the host system instead of launching their own kernel. If you run several system containers, they all share the same kernel, which makes them faster and more lightweight than virtual machines. - Incus DocsWhy Use Incus?You can see a list of major features here but what I like about it might not be the same as what you like about it.  It’s very fast - There is an agent in the image that makes getting a shell into the VM super fast. The images are also small and download like lightning, at least in my experience.  It’s easy to manage - Incus has a simple syntax for launching VMs and containers  Image based - Incus uses images, instead of futzing around with qemu backing files and such  You use Linux as your workstation and need to easily get a VM, or a system containerYou can also try it online: https://linuxcontainers.org/incus/try-it/Why Wouldn’t You Use Incus?  It’s not Docker - it’s a different style of containerization, which many people are not used to.  It’s relatively new, and a lot of work is being done on it - But on Ubuntu 24.04 it’s easy to install and get started.  I do have some trouble with outbound access from the VMs and have futzed around with Iptables to get it working, but it’s not as easy as one would think–I expect I’m missing something obvious from the docs.That’s about all I can think of.Pairing Incus with My Kubernetes Install ScriptFYI - If you create an 8GB VM with 4 CPUS, my single node Kubernetes install script pairs nicely with Incus, and is often what I use it for.Incus 6.6 Was Just ReleasedSee here. There is also a video overview of the new features hereInstall it, try it out. Have fun easily creating VMs and containers!"
    } ,
  
    {
      "title"    : "Building an Insecure App...on Purpose (So That GenAI Can Fix It)",
      "category" : "",
      "tags"     : "",
      "url"      : "/2024/10/02/building-an-insecure-app.html",
      "date"     : "2024-10-02 00:00:00 -0400",
      "content"  : "tldr;tldr; I built an insecure web application (on purpose) for testing LLMs and here it is: https://github.com/ccollicutt/insecure-nextjs-guestbook.Dealing with Technical Debt using GenAIIs cybersecurity largely a technical issue? An engineering issue? It’s difficult to say. Certainly human psychology plays a big part of it, but, then again, we’re building (insecure) software things and putting them out into the world. We write billions of lines of code, and we can’t do that without making mistakes…so there are billions of mistakes too. That code has bugs, it gets worse over time, and is hard (read: expensive) to maintain. It ends up being a technical liability–a security liability. The reality of software development is an important part of the cybersecurity story. Not the whole story, but an important part.For this line of thinking, the question is, can Generative Artificial Intelligence (GenAI) help us deal with all this overwhelming technical debt? I believe that GenAI can code, and code well enough to help us get rid of technical debt. And what’s more, this ability can be automated and has the potential to be fast–very fast–so it can potentially take care of a lot of technical debt in a short period of time. Now, not everyone may agree with me, but that’s my opinion, and I’m sticking to it!So if you believe, or can suspend your disbelief, that GenAI can help you deal with technical debt, then you’ll want to test it, just like I do. But how do you test code that generates code?I’m going to build an insecure web application and then use GenAI to try to fix it.What Does “Insecure” Mean?However, building an insecure web application is a bit of a challenge. On the one hand, we have all kinds of technical debt that’s easy to accumulate in the real world, but on the other hand, when we write a new application, the frameworks, libraries, and tools we use are working behind the scenes to keep us as secure as possible, so in some ways it’s a challenge to build an insecure application, at least out of the gate.And yet I managed to do it. At least partially.So, what is an insecure web application? What are common examples of insecurity in a web application?OWASP Top 10One way to think about web app vulnerabilities is through the OWASP Top 10.Here’s the current OWASP Top 10, as of 2021:  A01:2021-Broken Access Control  A02:2021-Cryptographic Failures  A03:2021-Injection  A04:2021-Insecure Design  A05:2021-Security Misconfiguration  A06:2021-Vulnerable and Outdated Components  A07:2021-Identification and Authentication Failures  A08:2021-Software and Data Integrity Failures  A09:2021-Security Logging and Monitoring Failures  A10:2021-Server-Side Request Forgery (SSRF)Let’s look at A01:2021-Broken Access Control, as defined by OWASP:  Violation of the principle of least privilege or deny by default, where access should only be granted for particular capabilities, roles, or users, but is available to anyone.  Bypassing access control checks by modifying the URL (parameter tampering or force browsing), internal application state, or the HTML page, or by using an attack tool modifying API requests.  Permitting viewing or editing someone else’s account, by providing its unique identifier (insecure direct object references)  Accessing API with missing access controls for POST, PUT and DELETE.  Elevation of privilege. Acting as a user without being logged in or acting as an admin when logged in as a user.  Metadata manipulation, such as replaying or tampering with a JSON Web Token (JWT) access control token, or a cookie or hidden field manipulated to elevate privileges or abusing JWT invalidation.  CORS misconfiguration allows API access from unauthorized/untrusted origins.  Force browsing to authenticated pages as an unauthenticated user or to privileged pages as a standard user.Forced browsing, as an example, sounds fun and technical–but it’s really just about browsing pages you aren’t supposed to know exist, pages that just happen to have additional permissions or access that the average user doesn’t have.  Forced browsing is an attack where the aim is to enumerate and access resources that are not referenced by the application, but are still accessible. An attacker can use Brute Force techniques to search for unlinked contents in the domain directory, such as temporary directories and files, and old backup and configuration files. These resources may store sensitive information about web applications and operational systems, such as source code, credentials, internal network addressing, and so on, thus being considered a valuable resource for intruders. - https://owasp.org/www-community/attacks/Forced_browsingAnd, in a similar vein, (typically SQL) injection, as defined by OWASP:  User-supplied data is not validated, filtered, or sanitized by the application.  Dynamic queries or non-parameterized calls without context-aware escaping are used directly in the interpreter.  Hostile data is used within object-relational mapping (ORM) search parameters to extract additional, sensitive records.  Hostile data is directly used or concatenated. The SQL or command contains the structure and malicious data in dynamic queries, commands, or stored procedures.Some of these are more interesting than others, and for some–it’s hard to believe that they are still happening in 2024.Building an Insecure Web AppWhile there are a handful of “webgoat”-style applications that will help you learn about these vulnerabilities, I decided to build my own so that I would know exactly what problems I was introducing - and thus I would know exactly what problems I was trying to fix with GenAI.I was working on learning NodeJS and NextJS, so I decided to build my insecure web application using those technologies.A few points:      I wanted to make a guestbook app of all things because it would be easy to build, and the fact that anyone should be able to post to it would make it easier to introduce vulnerabilities.        I immediately put the clear text authentication into a SQLite database. However, in the real world, no one would put cleartext authentication in a SQLite database–or even use their own authentication system. There are many, many libraries and SaaS services that provide authentication as a service, which is much, much more secure, and that is what people will use. (That is, they’re not as easy to configure, and they’re error-prone, but they’re still much more secure than doing it yourself). I imagine most people building a new web application would either use a third party or NextAuth.        There is an admin user with a default password.  // Insert admin user if not existsdb.get(`SELECT * FROM users WHERE username = 'admin'`, (err, row) =&gt; {  if (!row) {    db.run(`INSERT INTO users (username, password, admin) VALUES ('admin', 'admin', 1)`);  }});  I wanted it to be susceptible to SQL injection–but interestingly, SQLite does a lot of work to prevent that, so I had to do some work to make it vulnerable in terms of using raw queries. For the most part, SQLite just does the right thing, and you have to do some work to make it vulnerable.// Vulnerable to SQL injectionconst query = `SELECT * FROM users WHERE username = '${username}' AND password = '${password}'`;      I also added an admin page that was supposed to be protected, but wasn’t.        Originally the app didn’t use a sessionID in the URL, but I added that to make the webapp EVEN MORE VULNERABLE. But you don’t see sessionIDs in the wild, so I’m not sure if that’s realistic.  What I have so far is a webapp that is vulnerable to a number of attacks, including  SQL injection  Forced browsing  Session hijacking  Probably Cross-Site Scripting (XSS)Testing the VulnerabilitiesIn addition to building the insecure application, we need to test for the presence of these vulnerabilities. So there is also a script to test for them. Please note that this is not an exhaustive list of vulnerabilities, but rather a set of examples meant to be illustrative, and in fact many of them do not work.$ ./tests.sh Usage: ./tests.sh [test_name]Available tests:  login                  Test common logins  sql_injection          Run SQL Injection Test  drop_table             Drop messages table with SQL Injection  xss                    Run Cross-Site Scripting (XSS) Test  insecure_auth          List all users and get admin password via SQL Injection  sensitive_data         Run Sensitive Data Exposure Test  security_misconfig     Run Security Misconfiguration Test  known_vulnerabilities  Run Known Vulnerabilities Test  insufficient_logging   Run Insufficient Logging &amp; Monitoring Test  list_tables_and_entries List all tables and entries in the database  help                   Display this help message  list_users             List all users in the database  list_nonexistent_users List all users in the database  list_tables_and_entries List all tables and entries in the database  list_nonexistent_users List all users in the databaseHere’s an example of SQL injection:$ ./tests.sh sql_injection#################################################### Running SQL Injection Test to create admin user ####################################################Step 1: Attempting SQL injection to create admin user...SQL Injection Response: {\"message\":\"Login successful\",\"sessionId\":\"d86976cace3f01e5ae248e037483d70d\",\"isAdmin\":true,\"redirectUrl\":\"/?sessionId=d86976cace3f01e5ae248e037483d70d&amp;username=admin' --&amp;isAdmin=true\"}Step 2: Inserting hacker user with admin privileges...Step 3: Attempting to login as the new admin user 'hacker'...Login response: {\"message\":\"Login successful\",\"sessionId\":\"d2cfc602ff3fd33d201d69f0fac9bdd2\",\"isAdmin\":true,\"redirectUrl\":\"/?sessionId=d2cfc602ff3fd33d201d69f0fac9bdd2&amp;username=hacker&amp;isAdmin=true\"}User 'hacker' logged in successfully with admin privileges. SQL Injection successful.Step 4: Checking database for 'hacker' user...19|hacker|hackpass|1Step 5: Listing all users in the database...1|admin|admin|12|test|stsdf|03|admin|admin123|019|hacker|hackpass|1Or drop some tables:$ ./tests.sh drop_table############################################################################################################################### Running SQL Injection to Drop Table. This will attempt to drop the 'messages' table from the database using SQL injection. ###############################################################################################################################Logging in as admin to perform SQL Injection to drop the messages table...Logging in with admin:adminSessionId: f86bec743b5e8cd60ec886b4a6e9e3b1IsAdmin: true./tests.sh: line 143: get_cookie: command not foundDropping the messages table with SQL Injection...Response: {\"message\":\"Entry added successfully\",\"result\":{}}Querying the database to check if the messages table still exists...usersSuper hacker stuff, I know.Building Insecure Applications is a Lot of WorkAfter all, getting a bunch of vulnerabilities into an application is a lot of work. It’s not realistic to deal with every example that OWASP provides. Furthermore, real-world scenarios are often much more complicated and much  more technical and subtle. Most of what we focus on in cybersecurity is the problem of aging code and the vulnerabilities that come with it. There is less focus on the vulnerabilities that come from improper use of libraries and frameworks and their configuration, vulnerabilities that are subtle and harder to detect. The web application I’m building is more like using a sledgehammer instead of a scalpel, if you’ll pardon the mixed metaphors.I also need to do more research on OWASP, other tools like the Atomic Red Team, and what other “webgoat”-style applications are out there and how they work, and what they do best.Find the code, such as it is, here.Next UpIn future posts, I’ll look more at this insecure webapp, how to test and execute the exploitable vulnerabilities, as well as how to fix it, if possible, using GenAI and tools like Cursor."
    } ,
  
    {
      "title"    : "Easily Create a Single Node Kubernetes Cluster",
      "category" : "",
      "tags"     : "",
      "url"      : "/2024/08/22/install-kubernetes-script.html",
      "date"     : "2024-08-22 00:00:00 -0400",
      "content"  : "I’ve been working with Kubernetes for a long time. Too long, actually. So long, in fact, that I don’t really use it much anymore. Kubernetes has won in terms of being the default way to deploy modern applications. At this point, it’s kind of boring, which is great! We want boring infrastructure. Boring works. If you’re writing a new application today, the target is going to be a container, and that container is probably going to run in good old boring Kubernetes.For quite a while, the last few years, I have had a bunch of Kubernetes clusters running in my basement. I have half a rack there that used to be filled with servers. Then that changed to just running one larger server with a couple hundred gigs of memory, and that one server was running a bunch of Kubernetes clusters. But recently I shut that down. Mainly because it’s summer here in Toronto and that one big server was heating up the basement, and I wasn’t using it that much. I may turn it on again in the winter. Not sure. Anyways…Yesterday I needed a small k8s cluster. So I used my good old install-kubernetes.sh script to install it onto a VM running on my local workstation.tl;dr  I have a 500 line bash script that installs Kubernetes on Ubuntu 22.04, usually a small VM, 8 gigs of ram, 2-4 CPUs, 40 gigs of disk.  The script can create a single node “cluster”  Or you can deploy a bunch of virtual machines and make one a control plane node and the other workers  It only takes 2 or 3 minutes to get a k8s cluster. Below is a picture of the test I ran in a github action. Of course, github’s infrastructure is blazing fast–the speed of the installation will largely depend on how fast you can download packages to the host.There are other single node k8s tools, but I like mine, of course :)Install KubernetesFor a year and a half or so–first commit was March of 2023–I’ve had a script that will deploy a Kubernetes cluster into a virtual machine.That script can be found here:  https://github.com/ccollicutt/install-kubernetesI haven’t used it for a while, and so it was actually broken for the last bit because the upstream Kubernetes project changed where the packages for Ubuntu are located. So I just updated the script, like bumped it to Kubernetes 1.31, fixed a few other things, and now it’s good to go again to create either a cluster of Kubernetes instances or, perhaps more useful, a full Kubernetes deployment running in a single virtual machine instance (where the single node is both a control plane node and a worker node).Building a Single Node Kubernetes…“Cluster”First, get yourself an Ubuntu 22.04 virtual machine with at least 8 gigs of ram and around 40 gigs of disk. I’d probably also give it 4 CPUs.root@install-k8s-demo:~# source /etc/lsb-release; echo $DISTRIB_RELEASE22.04root@install-k8s-demo:~# nproc4root@install-k8s-demo:~# free -h           \ttotal    \tused    \tfree  \tshared  buff/cache   availableMem:       \t7.7Gi   \t124Mi   \t7.5Gi    \t17Mi   \t111Mi   \t7.4GiSwap:         \t0B      \t0B      \t0Broot@install-k8s-demo:~# lsblkNAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTSsda  \t8:0\t0   40G  0 disk├─sda1   8:1\t0  100M  0 part /boot/efi└─sda2   8:2\t0 39.9G  0 part /Then, login to that VM and get a root shell.Next, grab the install-kubernetes script.  NOTE: You’ll need git installed.root@install-k8s-demo:~# git clone https://github.com/ccollicutt/install-kubernetesroot@install-k8s-demo:~# cd install-kubernetesThere will be a few files there:root@install-k8s-demo:~/install-kubernetes# lsinstall-kubernetes.sh  makefile  manifests  README.mdThen, we simply run the install-kubernetes.sh script BUT using the “-s” option to set it so that it deploys a single node control plane + worker node.  NOTE: If you forget the “-s”, it is probably best to recreate the virtual machine and reinstall it. This is not idempotent, or at least it hasn’t been tested that way../install-kubernetes.sh -sThe output of that will look like:root@install-k8s-demo:~/install-kubernetes# ./install-kubernetes.sh -sStarting install...==&gt; Logging all output to /tmp/install-kubernetes-NMxK9WTKim/install.logChecking Linux distributionDisabling swapRemoving packagesInstalling required packagesInstalling Kubernetes packagesConfiguring systemConfiguring crictlConfiguring kubeletConfiguring containerdInstalling containerdStarting servicesConfiguring control plane node...Initialising the Kubernetes cluster via KubeadmConfiguring kubeconfig for root and ubuntu usersInstalling Calico CNI==&gt; Installing Calico tigera-operator==&gt; Installing Calico custom-resourcesWaiting for nodes to be ready...==&gt; Nodes are readyChecking Kubernetes version...==&gt; Client version: v1.31.0==&gt; Server Version: v1.31.0==&gt; Requested KUBE_VERSION matches the server version.Installing metrics serverConfiguring as a single node clusterConfiguring as a single node clusterDeploying test nginx podWaiting for all pods to be running...Install complete!### Command to add a worker node ###kubeadm join localhost:6443 --token &lt;redact&gt; --discovery-token-ca-cert-hash sha256:&lt;redact&gt;Now You Have a Kubernetes ClusterAt this point, you can run kubectl and access the local cluster.There’s a kubeconfig in:root@install-k8s-demo:~# ls ~/.kube/cache  configAnd, if there is an ubuntu user on the host, the config will be there too.root@install-k8s-demo:~# ls /home/ubuntu/.kube/configAnd we can connect to the “cluster”.root@install-k8s-demo:~# kubectl get pods -ANAMESPACE      \tNAME                                   \tREADY   STATUS\tRESTARTS   AGEcalico-apiserver   calico-apiserver-78d48b5579-j97lc      \t1/1 \tRunning   0      \t4m15scalico-apiserver   calico-apiserver-78d48b5579-kmcvr      \t1/1 \tRunning   0      \t4m15scalico-system  \tcalico-kube-controllers-7d868b8f66-fldb5   1/1 \tRunning   0      \t4m45scalico-system  \tcalico-node-pqfdn                      \t1/1 \tRunning   0      \t4m45scalico-system  \tcalico-typha-899c7464d-9vqzg           \t1/1 \tRunning   0      \t4m45scalico-system  \tcsi-node-driver-vqvnx                  \t2/2 \tRunning   0      \t4m36skube-system    \tcoredns-6f6b679f8f-tvtjs               \t1/1 \tRunning   0      \t4m49skube-system    \tcoredns-6f6b679f8f-zvcdl               \t1/1 \tRunning   0      \t4m49skube-system    \tetcd-install-k8s-demo                  \t1/1 \tRunning   0      \t4m57skube-system    \tkube-apiserver-install-k8s-demo        \t1/1 \tRunning   0      \t4m56skube-system    \tkube-controller-manager-install-k8s-demo   1/1 \tRunning   0      \t4m57skube-system    \tkube-proxy-9snr9                       \t1/1 \tRunning   0      \t4m49skube-system    \tkube-scheduler-install-k8s-demo        \t1/1 \tRunning   0      \t4m56skube-system    \tmetrics-server-5f94f4d4fd-sg2gh        \t1/1 \tRunning   0      \t4m35stigera-operator\ttigera-operator-b974bcbbb-4sjjz        \t1/1 \tRunning   0      \t4m49sYou Can Deploy Many Worker Nodes if You WantYou could also use this script to deploy a single control plane only node, the standard model for deploying K8s where the control plane is separated, and then create and add as many worker nodes to that control plane as makes sense. However, this script does not orchestrate all of this. You would have to log in to each VM and run the script, set it up as a worker node or a control plane node, and then join the worker nodes to the control plane node using the kubeadm join command. So this is not meant to be some kind of high-level k8s cluster creation orchestration mechanism, no magic here. Of course you can create as large a cluster as you want, you just have to set up each node individually.Some Design DecisionsIf you look at the script, here are some design decisions. It’s using:  Ubuntu Kubernetes packages  containerd  Calico as the CNIConclusionIf you need a throwaway Kubernetes cluster that can be created in an Ubuntu 22.04 VM in a few minutes (like two!) I think this is a nice way to do that. Certainly it works for me."
    } ,
  
    {
      "title"    : "Leapfrogging: Switching From OpenAI to Claude, and Github Copilot to Cursor",
      "category" : "",
      "tags"     : "",
      "url"      : "/2024/08/12/switching-to-claude-and-cursor.html",
      "date"     : "2024-08-12 00:00:00 -0400",
      "content"  : "I’m a fan of the recent wave of AI–specifically generative AI, or GenAI for short. I think of GenAI, these large language models, as a kind of compression. They take huge amounts of text–programming code, for example–and they are able to regurgitate it. So we take terrabytes of code, compress/feed it into an LLM that ends up being only a few gigabytes in size, and we can then talk to that LLM in natural language, and it can return code and other text…effectively uncompressing it.LeapfroggingWhat I want to focus on here, for this post, is that vendors are getting better and better at building LLMs for decompressing code and, as well, better at building out the user experience for coaxing code out of LLMs. These LLMs, this GenAI, combined with a chatbot interface or an integrated development environment, can do so, so much. We can just ask them for the code, or about the code, or how to code, and they will provide the code or help you figure out how to write it. It’s not always great code, or perfect code, but it’s usually good enough.A few notes:  There are many companies that make LLMs. A few are building “frontier” LLMs, such as OpenAI and Claude.  These companies are working to make LLMs and their interfaces better at interacting with humans and creating code.  There are also companies working on how programmers can best use LLMs to write code. They are not building the LLMs directly, instead finding out how we can best use them.In each of the above situations, at some point, one company will leapfrog another. And that, as far as I’m concerned, is what has happened recently.For the last few months I’ve been using OpenAI’s ChatGPT and Github’s Copilot via VSCode. But now I have almost completely switched from ChatGPT and CoPilot to Claude and Cursor. I used to pay monthly for ChatGPT and CoPilot, and I stopped paying for them and started paying for Claude and Cursor.            What I Used Before      What I Use Now                  OpenAI ChatGPT      Claude 3.5 Sonnet              GitHub Copilot      Cursor      The difference is amazing…for now!ClaudeFor a long time, OpenAI was great at coding. Over time, in my opinion, it started to slide. Maybe it got nerfed, I don’t know. Then Claude 3.5 Sonnet came out…and it blew me away. It’s just very, very good at spitting out the codez.But regurgitating code is one thing, doing it in an easy and intuitive way while programming is another. I don’t mind the chatbot style of interacting with LLMs, but it does get tedious. Claude 3.5 Sonnet has helped solve this UX/UI problem with a concept it calls artifacts.  …introducing Artifacts on Claude.ai, a new feature that expands how users can interact with Claude. When a user asks Claude to generate content like code snippets, text documents, or website designs, these Artifacts appear in a dedicated window alongside their conversation. This creates a dynamic workspace where they can see, edit, and build upon Claude’s creations in real-time, seamlessly integrating AI-generated content into their projects and workflows.Claud is absolutely the best LLM for coding right now. I pay for it. It will save you massive amounts of time.Find Claude at https://claude.ai/.CursorBut no matter how much we tweak the chatbot-style interface, it’s never going to be good enough. We need AI built right into the IDE–the Integrated Developer Environment–which is really just a fancy text editor. AI has to be built in, and totally er…integrated…into the IDE.This is what cursor is–AI built right into the IDE. It’s not perfect, but it’s certainly a good place to start. The best place right now.  Built to make you extraordinarily productive, Cursor is the best way to code with AI.Find Cursor at https://www.cursor.com/.CompetitionI don’t feel bad for Github CoPilot or OpenAI: that’s how competition works. As a consumer, I am in a great position to make choices about what tools I think are best, and given the pace of change, those tools will likely change over time, and perhaps Claude and Cursor will be leapfrogged by other companies and projects. What a fun time!The Future Looks FantasticI can’t describe where I think things are going with GenAI/LLMs and code better than this video. I heavily suggest watching it all, and perhaps even taking the time to watch the three hour video from which it came.On using Cursor + Claude 3.5 Sonnet + Tailwind to ship 20x faster (ft. @Shpigford) pic.twitter.com/lQ0yTjm8MF&mdash; Sahil Lavingia (@shl) August 8, 2024"
    } ,
  
    {
      "title"    : "Fine Tuning LLMs: Part 1 - Just Getting Started",
      "category" : "",
      "tags"     : "",
      "url"      : "/2024/05/16/fine-tuning-llms-part-one-getting-started.html",
      "date"     : "2024-05-16 00:00:00 -0400",
      "content"  : "There are a few ways we can customise a Large Language Model (LLM), and one of those ways is to fine-tune it.But why fine-tune an LLM?  Large language models (LLMs) like GPT-3 and Llama have shown immense promise for natural language generation. With sufficient data and compute, these models can produce remarkably human-like text. However, off-the-shelf LLMs still have limitations. They may generate text that is bland, inconsistent, or not tailored to your specific needs.This is where finetuning comes in. Finetuning is the process of taking a pre-trained LLM and customizing it for a specific task or dataset. With finetuning, you can steer the LLM towards producing the kind of text you want. - https://medium.com/@dave-shap/a-pros-guide-to-finetuning-llms-c6eb570001d3As well I just want to make a little disclaimer here on decisions I’ve made. :)DISCLAIMER  Please note that what I’ve done here is really a personal experiment in fine-tuning an LLM. There’s no particular rhyme or reason to the infrastructure and other choices I’ve made. I’m using a particular GPU supplier. I’m using a certain Python notebook. I’ve made some choices that might actually make things more difficult, or that might not make sense to an experienced fine-tuner. Also, in this post, I’m not tuning with a specific set of data or goal in mind. I’m just trying out a set of basic tools.Happy hacking!Quick Fine Tuning ExampleSteps:  Get a GPU from somewhere (I’m using brev.dev)  Create an instance with proper CUDA and pytorch versioning  Build a data set to fine-tune with (or use an existing one), NOTE: This step I will build on in later posts  Use a Unsloth iPython notebook  Step through the provided notebook and create a fine-tuned LLMGPU InstanceFirst, we need a GPU.  NOTE: The easiest thing to do would just be to use Google Colab and the notebook that Unsloth links to; that would be super easy. Google Colab is a free cloud service to run Jupyter Notebooks and provides access to GPUs. But I’m not using Colab for…some reason. You might want to. Keep that in mind!I’m using brev.dev to get access to a GPU instance, but there are tons of “GPU Brokers” out there.  NOTE: I have no relationship with brev.dev, I just randomly started using the service. I can’t tell you if it’s good or not, but the combination of the provider plus the docker image for CUDA + pytorch is working for me. Plus if you leave the GUI console for long enough, a cute DVD-style screen saver comes on. lol!Here I’m creating a small NVIDIA 4090 instance. Other much larger GPUs are available from brev.dev and other providers.Note that I’m using the “advanced container settings” and selecting the docker.io/pythorch/pytorch:2.2.0-cuda12.1-cudnn8-runtime image. This is key because I’ve had lots of problems matching up these versions, especially on my home workstation where I have a NVIDIA 3090.I find versioning CUDA and Pytorch challenging so this is a really nice feature of brev.dev, though it’s really just about dialing in the right image/settings/etc.Once the instance is running there is an option to connect to a notebook.And now we can use the notebook.Or you can login with the the brev shell. Here my instance is brilliantly named “aaa”.$ brev shell aaa⢿ waiting for SSH connection to be available Agent pid 9158Warning: Permanently added '[provider.pdx.nb.akash.pub]:31314' (ED25519) to the list of known hosts.Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.0-101-generic x86_64) * Documentation:  https://help.ubuntu.com * Management:     https://landscape.canonical.com * Support:        https://ubuntu.com/advantageThis system has been minimized by removing packages and content that arenot required on a system that users do not log into.To restore this content, you can run the 'unminimize' command.➜  verb-workspace Connection to provider.pdx.nb.akash.pub closed.Unsloth NotebookNext I’ll upload the unsloth conversational notebook, which I obtained by opening the Colab notebook and downloading the file, then uploading it into the brev.dev instance’s notebook.  NOTE: There are a lot of notebooks for getting started training Llama3 out there. For example brev.dev has some too. As well, unsloth provides some via huggingface.Unsloth - https://github.com/unslothai/unsloth  Unsloth is a lightweight library for faster LLM fine-tuning which is fully compatible with the Hugging Face ecosystem (Hub, transformers, PEFT, TRL). The library is actively developed by the Unsloth team (Daniel and Michael) and the open source community. The library supports most NVIDIA GPUs–from GTX 1070 all the way up to H100s–, and can be used with the entire trainer suite from the TRL library (SFTTrainer, DPOTrainer, PPOTrainer). At the time of writing, Unsloth supports the Llama (CodeLlama, Yi, etc) and Mistral architectures. - https://huggingface.co/blog/unsloth-trlI’m using the conversational notebook they link to in their README. That will bring you to aTrain the ModelNow we can simply step through the notebook and train an example model.After stepping through the cells, we come to the training cell.And we can see the memory usage.If you continue through the notebook you can save the model in various ways, upload it to hugging face, etc.ConclusionThe technical part of fine-tuning a model is fairly straightforward from a user perspective if you know a bit of Python and understand the concept of a Jupyter notebook and have one to follow through with. Really this blog post is just connecting some dots, like GPU providers and Python notebooks. However, navigating through a notebook and understanding exactly what it does are two different things. Also, once you start understanding the fine tuning process, it becomes a matter of what data we put in and what results we get out. That is the real work. This is just a basic skeleton, but everyone needs a first step!PS.One of the things I love about LLMs right now is just how messy the technology landscape is. There is so much going on, so many niche technologies, libraries, chunks of code, websites, notebooks, on and on. It’s an amazing time."
    } ,
  
    {
      "title"    : "Ollama &amp; Llama 3:8b Running Locally",
      "category" : "",
      "tags"     : "",
      "url"      : "/2024/04/23/ollama-running-locally.html",
      "date"     : "2024-04-23 00:00:00 -0400",
      "content"  : "First, we start up ollama.$ ollama serveOK, it’s running.$ curl localhost:11434 &amp;&amp; echoOllama is runningNow, let’s run Meta’s Llama 3:8b.  NOTE: This is only the 8b model, the smallest one so far, AFAIK.$ ollama run llama3:8bpulling manifest pulling 00e1317cbf74... 100% ▕█████████████████████████████████████████▏ 4.7 GB                         pulling 4fa551d4f938... 100% ▕█████████████████████████████████████████▏  12 KB                         pulling 8ab4849b038c... 100% ▕█████████████████████████████████████████▏  254 B                         pulling c0aac7c7f00d... 100% ▕█████████████████████████████████████████▏  128 B                         pulling db46ef36ef0b... 100% ▕█████████████████████████████████████████▏  483 B                         verifying sha256 digest writing manifest removing any unused layers success &gt;&gt;&gt; Send a message (/? for help)Checkout memory usage. Not bad. Pretty small.$ nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits -i 05308Intersecting PolygonsLets ask Llama 3:8b to solve a problem: write the code to determine if two polygons are intersecting. This is a problem that some LLMs can’t answer. (However, it is possible that Llama 3 has “learned” how to solve the problem from it being discussed recently on the internet, and given the fact that Llama 3 has new training data.)Below, it looks like we’ve gotten a pretty good answer, as it uses the Python shapely library, which is the easiest thing to do, and what I’d expect to see.&gt;&gt;&gt; Write a function that calculates the intersection of two polygons. The function is given two polygons...  represented as a list of points. A third polygon representing the intersection of the two is returne... d, or an empty list of points if the intersection is empty.Here's a Python function to calculate the intersection of two polygons:import shapely.geometryfrom typing import Listdef polygon_intersection(polygon1: List[List[float]], polygon2: List[List[float]]) -&gt; List[List[float]]:    \"\"\"    Calculate the intersection of two polygons.    Args:    polygon1 (List[List[float]]): The first polygon represented as a list of points.    polygon2 (List[List[float]]): The second polygon represented as a list of points.    Returns:    List[List[float]]: A list of points representing the intersection of the two polygons, or an empty list if the intersection is empty.    \"\"\"    # Convert the input polygons to Shapely Polygon objects    poly1 = shapely.geometry.Polygon(polygon1)    poly2 = shapely.geometry.Polygon(polygon2)    # Calculate the intersection polygon    intersection = poly1.intersection(poly2)    # If the intersection is not empty, return it as a list of points    if not intersection.is_empty:        return [(point.x, point.y) for point in intersection.exterior.coords]    else:        return []This function uses the Shapely library to work with geometric shapes. It first converts the input polygons into Shapely Polygon objects. Then it calculates the intersection of these two polygons using the `intersection` method.If the intersection is not empty, the function returns a list of points representing this polygon as a list of coordinates. If the intersection is empty, an empty list is returned.Here's how you can use this function:polygon1 = [(0, 0), (0, 2), (2, 2), (2, 0)]polygon2 = [(1, 1), (1, 3), (3, 3), (3, 1)]intersection_polygon = polygon_intersection(polygon1, polygon2)print(intersection_polygon)This will output the points that make up the intersection of the two polygons.However, this code, as is, as provided, will not run.$ python polygons.py Polygon 1:(0, 0)(0, 2)(2, 2)(2, 0)Polygon 2:(1, 1)(1, 3)(3, 3)(3, 1)Traceback (most recent call last):  File \"/home/curtis/working/intersecting-polygons/polygons.py\", line 43, in &lt;module&gt;    intersection_polygon = polygon_intersection(polygon1, polygon2)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  File \"/home/curtis/working/intersecting-polygons/polygons.py\", line 26, in polygon_intersection    return [(point.x, point.y) for point in intersection.exterior.coords]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  File \"/home/curtis/working/intersecting-polygons/polygons.py\", line 26, in &lt;listcomp&gt;    return [(point.x, point.y) for point in intersection.exterior.coords]             ^^^^^^^AttributeError: 'tuple' object has no attribute 'x'But I provide llama 3 the error and it gives me this response.&gt;&gt;&gt; I get this error: AttributeError: 'tuple' object has no attribute 'x'That's a great error message!It looks like the `point` variable is actually a tuple (a simple data structure in Python), not an object with an `x` attribute. This is because the `intersection.exterior.coords` returns a list of tuples, where each tuple represents a point on the polygon.To fix this, you can simply change the line:return [(point.x, point.y) for point in intersection.exterior.coords]to:return [tuple(point) for point in intersection.exterior.coords]This will convert each tuple (representing a point) into a new tuple with the same elements.Alternatively, you can also use a list comprehension to extract the `x` and `y` values from each tuple, like this:return [[p[0], p[1][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p]p[1][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p]p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][  NOTE: Interesting additional output (the [p]s). I copied and pasted directly. Not sure what happened there…I replaced that line of code as it suggested, and the program ran. For an 8b model this capability seems perfectly fine, at least to me, and was better than what I found ChatGPT 3.5 was capable of a few months ago, in that Llama 3 used the Shapely module, whereas ChatGPT 3.5 would not. I would imagine Llama 3:8b would fail if I told it not to use Shapely.Results of running the provided code:$ python polygons.py Polygon 1:(0, 0)(0, 2)(2, 2)(2, 0)Polygon 2:(1, 1)(1, 3)(3, 3)(3, 1)Intersection Polygon:(2.0, 2.0)(2.0, 1.0)(1.0, 1.0)(1.0, 2.0)(2.0, 2.0)…But It Can’t Graph and Save an ImageUnfortunately, Llama 3:8b was unable to provide the code to plot the polygons and save them as an image file, which would have looked something like the below image. It may have been able to do this with a different prompt.Overall: ImpressiveThis was a very quick test. I only spent a handful of minutes on it.Llama 3:8B. It’s fast. It can run locally. It’s pretty “smart”, although it would take a bit of manual configuration to get the code it output to work; I didn’t give it much of a chance to get things right. Overall, I’m impressed with this little LLM–its compressed a lot of information."
    } ,
  
    {
      "title"    : "My Cyberpunk Weekend - Part 3: Using Docker and GPUs",
      "category" : "",
      "tags"     : "",
      "url"      : "/2023/12/18/cyberpunk-weekend-3.html",
      "date"     : "2023-12-18 00:00:00 -0500",
      "content"  : "I’m working on running LocalAI. But I feel like running that out of Docker.So how to use a GPU with Docker (on Linux).First, need the nvidia-docker2 driver. Otherwise you get an error like this:docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].So install that.sudo apt install nvidia-docker2I had a fun thing to fix in that I had added some things to the “daemon.json” so had to fix that.$ sudo dpkg --configure -aSetting up nvidia-docker2 (2.13.0-1) ...Configuration file '/etc/docker/daemon.json' ==&gt; File on system created by you or by a script. ==&gt; File also in package provided by package maintainer.   What would you like to do about it ?  Your options are:    Y or I  : install the package maintainer's version    N or O  : keep your currently-installed version      D     : show the differences between the versions      Z     : start a shell to examine the situation The default action is to keep your current version.*** daemon.json (Y/I/N/O/D/Z) [default=N] ? D--- /etc/docker/daemon.json     2023-04-10 15:23:11.735382489 -0400+++ /etc/docker/daemon.json.dpkg-new    2023-03-31 09:10:49.000000000 -0400@@ -1,4 +1,8 @@ {-  \"registry-mirrors\": [\"http://10.8.24.123\"],-  \"insecure-registries\": [\"https://some.registry\"]+    \"runtimes\": {+        \"nvidia\": {+            \"path\": \"nvidia-container-runtime\",+            \"runtimeArgs\": []+        }+    } }Next, I have two NVIDIA GPUS, one old one and one newer, better one, the 3090, which is what I want to be using for LLMs.So, locally I have two, as shown below.$ nvidia-smi Mon Dec 18 11:35:39 2023       +-----------------------------------------------------------------------------+| NVIDIA-SMI 470.223.02   Driver Version: 470.223.02   CUDA Version: 11.4     ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||                               |                      |               MIG M. ||===============================+======================+======================||   0  NVIDIA GeForce ...  Off  | 00000000:06:00.0 Off |                  N/A ||  0%   32C    P8    12W / 350W |     10MiB / 24268MiB |      0%      Default ||                               |                      |                  N/A |+-------------------------------+----------------------+----------------------+|   1  NVIDIA GeForce ...  Off  | 00000000:07:00.0 N/A |                  N/A || 44%   71C    P0    N/A /  N/A |   2574MiB /  3015MiB |     N/A      Default ||                               |                      |                  N/A |+-------------------------------+----------------------+----------------------+                                                                               +-----------------------------------------------------------------------------+| Processes:                                                                  ||  GPU   GI   CI        PID   Type   Process name                  GPU Memory ||        ID   ID                                                   Usage      ||=============================================================================||    0   N/A  N/A      1445      G   /usr/lib/xorg/Xorg                  4MiB ||    0   N/A  N/A      3231      G   /usr/lib/xorg/Xorg                  4MiB |+-----------------------------------------------------------------------------+But we can specify to use “device=0” only in the container, so we should only see one GPU.$ docker run -it --gpus \"device=0\" nvidia/cuda:11.4.3-base-ubuntu20.04 nvidia-smiMon Dec 18 16:33:29 2023       +-----------------------------------------------------------------------------+| NVIDIA-SMI 470.223.02   Driver Version: 470.223.02   CUDA Version: 11.4     ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||                               |                      |               MIG M. ||===============================+======================+======================||   0  NVIDIA GeForce ...  Off  | 00000000:06:00.0 Off |                  N/A ||  0%   32C    P8    10W / 350W |     10MiB / 24268MiB |      0%      Default ||                               |                      |                  N/A |+-------------------------------+----------------------+----------------------+                                                                               +-----------------------------------------------------------------------------+| Processes:                                                                  ||  GPU   GI   CI        PID   Type   Process name                  GPU Memory ||        ID   ID                                                   Usage      ||=============================================================================|+-----------------------------------------------------------------------------+BOOM!One of the hard parts is figuring out what tag to use on the NVIDIA image. They are all listed here:  https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/supported-tags.mdExamples:11.4.3-base-ubuntu20.04 (11.4.3/ubuntu20.04/base/Dockerfile)11.4.3-cudnn8-devel-ubuntu20.04 (11.4.3/ubuntu20.04/devel/cudnn8/Dockerfile)11.4.3-cudnn8-runtime-ubuntu20.04 (11.4.3/ubuntu20.04/runtime/cudnn8/Dockerfile)11.4.3-devel-ubuntu20.04 (11.4.3/ubuntu20.04/devel/Dockerfile)11.4.3-runtime-ubuntu20.04 (11.4.3/ubuntu20.04/runtime/Dockerfile)Note that these will change over time, of course. But if Docker reports it can’t find the tag, it’s likely because the tag is wrong, or has changed."
    } ,
  
    {
      "title"    : "My Cyberpunk Weekend - Part 2: The Llama",
      "category" : "",
      "tags"     : "",
      "url"      : "/2023/12/10/cyberpunk-weekend-2.html",
      "date"     : "2023-12-10 00:00:00 -0500",
      "content"  : "Well, last week I picked up the 3090 GPU. This week I need to try to use it. That is not an easy feat because “drivers.”My good old workstation is on Ubuntu 20.04. I should probably upgrade. I should probably not use this machine for AI work. But, I am.Currently I’m using the nvidia-driver-470 that I’ve had for a while, as though it’s some sort of cherished antique that I’ll hand down to my children. I do remember it being a pain to get working, back when I only had one GPU.$ dpkg --list | grep nvidia-driverii  nvidia-driver-460                          470.223.02-0ubuntu0.20.04.1                   amd64        Transitional package for nvidia-driver-470ii  nvidia-driver-470                          470.223.02-0ubuntu0.20.04.1                   amd64        NVIDIA driver metapackageBut to use a llamafile I need the right CUDA toolkit and driver match up. At first I installed 12.3, but then realized that’s not the driver I have. Need to match those up.$ ./llava-v1.5-7b-q4-server.llamafile --n-gpu-layers 35building ggml-cuda with nvcc -arch=native...nvcc fatal   : Unsupported gpu architecture 'compute_30'/usr/local/cuda-12.3/bin/nvcc: returned nonzero exit statusbuilding nvidia compute capability detector...cudaGetDeviceCount() failed: CUDA driver version is insufficient for CUDA runtime versionerror: compute capability detector returned nonzero exit statusDriver:$ nvidia-smi | grep CUDA| NVIDIA-SMI 470.223.02   Driver Version: 470.223.02   CUDA Version: 11.4     |I didn’t want to break my workstation and thus for now wanted to stay on the 470 driver. So I installed the 11.4 CUDA toolkit.First I purged the 12.3 CUDA toolkit:$ dpkg -l | grep -E \"cuda|cublas|cufft|cufile|curand|cusolver|cusparse|gds-tools|npp|nvjpeg|nsight|nvvm\"$ # review that list$ # now removesudo apt-get --purge remove \"*cuda*\" \"*cublas*\" \"*cufft*\" \"*cufile*\" \"*curand*\" \\ \"*cusolver*\" \"*cusparse*\" \"*gds-tools*\" \"*npp*\" \"*nvjpeg*\" \"nsight*\" \"*nvvm*\"’’’  NOTE: This requires setting up the NVIDIA repo! Not shown here.Then I installed the 11.4 CUDA toolkit:$ sudo apt-get install cuda-toolkit-11-4Added this to my path:$ which nvcc/usr/local/cuda-11.4/bin/nvccNext I tried to run the llamafile again:$ ./llava-v1.5-7b-q4-server.llamafile --n-gpu-layers 35building ggml-cuda with nvcc -arch=native...nvcc fatal   : Value 'native' is not defined for option 'gpu-architecture'/usr/local/cuda-11.4/bin/nvcc: returned nonzero exit statusbuilding nvidia compute capability detector...building ggml-cuda with nvcc -arch=compute_86...NVIDIA cuBLAS GPU support successfully loadedggml_init_cublas: GGML_CUDA_FORCE_MMQ:   noggml_init_cublas: CUDA_USE_TENSOR_CORES: yesggml_init_cublas: found 2 CUDA devices:  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6  Device 1: NVIDIA GeForce GTX 660 Ti, compute capability 3.0cuBLAS error 3 at /home/curtis/.llamafile/ggml-cuda.cu:6091current device: 1But it was using the wrong card. I believe the error was due to using the old 660Ti and trying to compile for it using CUDA 11.4.Setting CUDA_VISIBLE_DEVICES=0 fixed that:$ env | grep CUDACUDA_VISIBLE_DEVICES=0$ ./llava-v1.5-7b-q4-server.llamafile --n-gpu-layers 35NVIDIA cuBLAS GPU support successfully loadedggml_init_cublas: GGML_CUDA_FORCE_MMQ:   noggml_init_cublas: CUDA_USE_TENSOR_CORES: yesggml_init_cublas: found 1 CUDA devices:  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6{\"timestamp\":1702258585,\"level\":\"INFO\",\"function\":\"main\",\"line\":2650,\"message\":\"build info\",\"build\":1500,\"commit\":\"a30b324\"}{\"timestamp\":1702258585,\"level\":\"INFO\",\"function\":\"main\",\"line\":2653,\"message\":\"system info\",\"n_threads\":6,\"n_threads_batch\":-1,\"total_threads\":12,\"system_info\":\"AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \"}Multi Modal Mode Enabledclip_model_load: model name:   openai/clip-vit-large-patch14-336clip_model_load: description:  image encoder for LLaVAclip_model_load: GGUF version: 3clip_model_load: alignment:    32clip_model_load: n_tensors:    377clip_model_load: n_kv:         19clip_model_load: ftype:        q4_0SNIP!That’s about as far as I’m getting this weekend.Here’s a fun command to watch the GPU:nvidia-smi --query-gpu=timestamp,name,pci.bus_id,driver_version,pstate,pcie.link.gen.max,pcie.link.gen.current,temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used --format=csv -l 5"
    } ,
  
    {
      "title"    : "My Cyberpunk Weekend - Part 1: The Video Card (Or Neural Core, Take Your Pick)",
      "category" : "",
      "tags"     : "",
      "url"      : "/2023/12/04/cyberpunk-weekend.html",
      "date"     : "2023-12-04 00:00:00 -0500",
      "content"  : "There are a couple of ways to think about this post:Option 1 (boring):  I bought a video card and installed it in my computer.Option 2 (cheesy cliché cyberpunk; more fun):  In the gray low-rent business suburbs on the edge of the city, where the air hums with the buzz of a thousand illicit transactions, I found myself trudging through a seedy strip mall, its flickering signs casting long shadows over an assortment of massage parlors. Here, amid the cacophony of distant traffic and the murmur of hushed conversation, lay my destination: a dubious, fly-by-night eBay store. The place was a cybernetic bazaar, a maze of used technology and questionable merchandise covered in handwritten labels. Navigating the cramped aisles, I sought a particular treasure–a used AI processor, a critical component for powering my large language model efforts. The store’s operators, engaged in a rapid exchange in a language completely foreign to me, barely acknowledged my presence as their faces, etched with lines of weary experience, hesitated for a brief moment before extracting the neural core straight from the guts of a humming, overworked system host. The device, a relic of technological ambition, was burning hot and singed their fingertips, but not enough to deter them from accepting the cash I offered.LOL. I’m not sure which one is better. I’ll let you decide.Generative Artificial IntelligenceLike most people, I have been surprised by the big changes in Artificial Intelligence (AI) over the last few years…surprised, caught off guard, out of the “know” and out of the loop.Also, like many people, I’ve been a big user of generative AI, but I don’t have a good understanding of how it works. I hope to change that. I want to be able to run Large Language Models (LLMs) locally, so first, I needed to get a video card–a GPU–capable of running these models.It’s Just a Video Card?It’s kind of amazing that I can use the phrase “video card” in connection with AI; that there’s any connection between the two at all. What’s a video card for? Connecting to a monitor. Playing video games. But for AI? It’s a bit of a stretch, but it’s true.So my first step was to find the right video card, the right graphics processing unit (GPU), to work with AI. After a bit of research it seemed like my best bet, the best value card, was to find a used NVIDIA 3090, mostly because it has 24GB of memory and is a good price at this time.There were a lot of comments and thoughts on sites like Reddit with this kind of advice:  The 3090 is the best bang for your buck. It comes with 24gb of nvram in a single consumer card with a built-in cooling solution and plugs right into your home rig. It lets you run 33b GPTQ models without fuss.Kijiji - The Canadian CraigslistLiving in Toronto has some advantages in that you can find anything you need used–it’s out there, you just have to search and wait. It’s kind of like what I imagine living in a big city in China would be like - everything is available, you just have to go out and find it, maybe meet some interesting people along the way.In Canada we have a site called Kijiji (not even sure how to spell it) which is like Craigslist–but a Canadian Craigslist–so I started looking for a used NVIDIA 3090 GPU with 24GB of memory.Of course, there are all kinds of problems with buying a used video card on Kijiji, or anything else for that matter, but I was willing to take the risk in this case. Plus, it can be fun if you don’t mind possibly losing the money on a bad purchase. I’ve bought quite a few things on Kijiji and never had a problem, it’s really about finding the right person to buy from, like anything else in life. I’ve never been ripped off, but you will find some difficult people. I have a whole story about buying a canoe on Kijiji, but that’s for another time. Of course, you always want to keep your wits about you and meet in a public place.I set up a search on Kijiji and there are usually a few 3090s for sale, usually around $1000 to $1200. Then I saw a post from a local person, just a few blocks away in fact, who was selling one for $700. “Quick sale,” the ad said. I contacted them, but I wasn’t quick enough, and they sold it in a couple of hours before I could get over there.Eventually, I saw another ad for a used 3090 that had been pulled from a Dell Alienware workstation for $800 (Canadian) and contacted them about it. They said to give them a few hours notice before coming by to pick it up. Seemed like a good deal, so I said I’d give it a shot. Presumably, if it was from an Alienware computer, it was probably used for gaming, not crypto mining, which is a positive. On the other hand, the people selling it probably knew the value if they were going to part it out, i.e., sell the Alienware box as pieces instead of the entire thing, which means they are professional in some respect.A day or two later I went to pick it up. Their store was in a strip mall surrounded by massage parlors, which seemed a little seedy at first because there were more than one, but next to the computer store was a regular car dealership, so I figured it couldn’t be that bad. I pulled open the door to the shop, which was so jammed I was not sure I could get it open, and walked into a room completely filled with old computers and a couple of people working feverishly testing them and putting large strips of tape with non-English words on them. Stacks and stacks of computers, half of them falling over.I told them I was there for the video card and they asked me to wait a few minutes and showed me the card, which looked to be in perfect condition. I asked them if they would benchmark it for me, i.e. put it in a computer and run some tests. They hummed and hawed, but finally agreed to do it. He put it in a computer and ran Furmark and it seemed to work fine. To be honest, I don’t know that much about graphics cards or how they’re supposed to work, I mostly just watched the temperature and made sure the card was working. While the benchmark was running, they were talking to each other in a language I didn’t recognize, so I was never sure exactly what they were saying to each other. Sadly I only speak one language. But they were busy, which means they don’t have time to mess around with people. Frankly, they seemed like exactly the kind of place where you’d buy a used video card pulled from a high-end workstation.During the benchmark, the temperature of the card went up quite a bit, I think around 85 degrees, but I wasn’t surprised. I asked them where they sold all these computers, the ones stacked around the place, and the elderly gentleman gruffly gave me a one-word answer: “ebay.” Then he went to pull out the card, but didn’t let it cool down and almost burned his fingers, which was a bit worrying; you’d think he’d know it was hot. I sure did.In the end, I paid them $800 cash and took the card home. Surprisingly, they gave me a 30-day warranty card.It felt very much like a William Gibson-esque cyberpunk experience, and I was happy to have the card.PowerIn preparation for getting this card, I did some research on maybe building a whole new computer. It was around Black Friday time, so there were a lot of deals. I could have just bought a whole new workstation, but my current one is only a few years old and works just fine. Also, while there was a lot of stuff on sale, there were no good CPUs available; they were all out of stock. Theoretically, I could put the 3090 in my current computer, it would be louder, which is annoying since the computer is in my office, and I would need a new power supply and have to replace it myself, but it should work and it would save some money as well. So for now, I’m just using my existing Linux workstation to host the 3090.These 3090s can draw up to about 350 watts, which is quite a bit of power. So I had to get a properly sized power supply, as my current workstation only has a 550 watt power supply. I would need a lot more than that, at least 1000 watts. So I started looking for a bigger power supply. I ended up buying a refurbished Corsair RM1000x for $150 from Canada Computers. It’s one of the last remaining computer stores in Toronto. That and Memory Express, which doesn’t even have a Toronto location. Canada Computers is about the best place we have to buy computer parts.InstallationI’m a bit of an odd person in that I have a lot of computers, like a lot, so much that I won’t write it down here. It’s just part of what I do for a living, and if you do it for long enough they start to accumulate. However, and I suppose this is the odd part, I don’t particularly like computer hardware, especially desktop computers. I don’t mind network switches for some reason, and rackmount computers, well, they’re okay (albeit desktops in another form). But everything else…not much fun. A lot of people take a considerable pride in their workstation setup, LED lights and all that, but that is not for me. With that in mind, I wasn’t super happy about having to change the power supply and open up the computer and move things around, but I did it. It took me a couple of hours, but I did it.Honestly, the new power supply went in really easily. There was a Youtube video that showed my exact power supply and a similar 3090, so that made me feel better about the power swap. I just had to pull three wires and put the new power supply in.However, my motherboard is a little unusual in that if you use the second M2 slot, the second PCIe slot is disabled, which is where I would put the 3090. I assumed that my NVMe card was in the first slot, so I installed the card and rebooted. But I couldn’t see the 3090 from Linux. Looking at the motherboard again, I realized that the technician who built my computer had put the NVMe card in the second slot, probably to get it farther away from the GPU so it wouldn’t be affected by the card’s heat. As soon as I moved the NVMe card to the first M2 slot, the second PCIe slot was enabled and I could see the 3090!$ nvidia-smi -L | grep 3090GPU 0: NVIDIA GeForce RTX 3090As you can see, I have an old 660ti as the video card that is connected to my monitors, and the 3090 is the second card. Nice to see the 24GB of memory, which is the whole point of all this “cyberpunk” work!$ nvidia-smi\t +-----------------------------------------------------------------------------+| NVIDIA-SMI 470.223.02   Driver Version: 470.223.02   CUDA Version: 11.4 \t||-------------------------------+----------------------+----------------------+| GPU  Name    \tPersistence-M| Bus-Id    \tDisp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|     \tMemory-Usage | GPU-Util  Compute M. ||                           \t|                  \t|           \tMIG M. ||===============================+======================+======================||   0  NVIDIA GeForce ...  Off  | 00000000:06:00.0 Off |              \tN/A ||  0%   30C\tP8 \t8W / 350W | \t10MiB / 24268MiB |  \t0%  \tDefault ||                           \t|                  \t|              \tN/A |+-------------------------------+----------------------+----------------------+|   1  NVIDIA GeForce ...  Off  | 00000000:07:00.0 N/A |              \tN/A || 34%   53C\tP8\tN/A /  N/A |\t976MiB /  3015MiB | \tN/A  \tDefault ||                           \t|                  \t|              \tN/A |+-------------------------------+----------------------+----------------------+                                                                          \t +-----------------------------------------------------------------------------+| Processes:                                                              \t||  GPU   GI   CI    \tPID   Type   Process name              \tGPU Memory ||    \tID   ID                                               \tUsage  \t||=============================================================================||\t0   N/A  N/A  \t1417  \tG   /usr/lib/xorg/Xorg              \t4MiB ||\t0   N/A  N/A  \t2346  \tG   /usr/lib/xorg/Xorg              \t4MiB |+-----------------------------------------------------------------------------+CoolingI assume I’ll have to find ways to cool this chassis once I start putting the 3090 through its paces.DriversBecause I had the 660ti installed already, I didn’t have to add any additional drivers to get the 3090 to show up. Finally a nice piece of luck!$ dpkg --list | grep nvidia-kernelii  nvidia-kernel-common-470                   470.223.02-0ubuntu0.20.04.1                   amd64        Shared files used with the kernel moduleii  nvidia-kernel-source-470                   470.223.02-0ubuntu0.20.04.1                   amd64        NVIDIA kernel source packageConclusionSo far, I’ve spent about $1000 CDN on this, which isn’t too bad. It remains to be seen if my older computer is up to the task of running the 3090; that it doesn’t get too hot and too loud; that I don’t end up buying a new computer anyway after all this power supply swapping. I might end up doing that if, for example, I decide I want to run multiple GPUs (two 3090s would be optimal) and/or reduce the noise, because I could put the second computer in the basement with all the other computers where I can’t hear it, and leave my trusty old relatively quiet workstation in my office."
    } ,
  
    {
      "title"    : "Restarting Kubernetes Pods When There Are New Secrets With Reloader",
      "category" : "",
      "tags"     : "",
      "url"      : "/2023/11/23/reloader-kubernetes.html",
      "date"     : "2023-11-23 00:00:00 -0500",
      "content"  : "I will tell you a secret—no, a story. Say, at some point, I had a Kubernetes webhook admission controller that I wrote and deployed, and then the TLS certificate was automatically (nice!) renewed by cert-manager, but the pod wasn’t restarted, so it still had the old certificate, and now all Kubernetes deployments failed. That is indeed a story, perhaps a sad one. I had this shiny new cert, but no one was using it. Say I wanted to fix that. One way would be with Reloader.Reloader  Reloader can watch changes in ConfigMap and Secret and do rolling upgrades on Pods with their associated DeploymentConfigs, Deployments, Daemonsets Statefulsets and Rollouts. - ReloaderInstall ReloaderFirst add the repo.$ helm repo add stakater https://stakater.github.io/stakater-charts$ helm repo updateCreate a namespace.$ k create ns reloadernamespace/reloader created$ kn reloader ✔ Active namespace is \"reloader\"Install reloader.$ helm install reloader stakater/reloaderNAME: reloaderLAST DEPLOYED: Thu Nov 23 09:36:22 2023NAMESPACE: reloaderSTATUS: deployedREVISION: 1TEST SUITE: NoneNOTES:- For a `Deployment` called `foo` have a `ConfigMap` called `foo-configmap`. Then add this annotation to main metadata of your `Deployment`  configmap.reloader.stakater.com/reload: \"foo-configmap\"- For a `Deployment` called `foo` have a `Secret` called `foo-secret`. Then add this annotation to main metadata of your `Deployment`  secret.reloader.stakater.com/reload: \"foo-secret\"- After successful installation, your pods will get rolling updates when a change in data of configmap or secret will happen.Now we’ve got pods.$ k get podsNAME                                 READY   STATUS    RESTARTS   AGEreloader-reloader-64df699b8d-tm5rn   1/1     Running   0          3m4sNice and easy. Thanks Helm!Simple TestCreate a secret.kubectl create secret generic foo-secret --from-literal=key1=barCreate a cert-manager certificate. (Of course you need cert-manager installed.)kubectl apply -f - &lt;&lt;EOFapiVersion: cert-manager.io/v1kind: Certificatemetadata:  name: foo-certs  namespace: foospec:  secretName: foo-certs  issuerRef:    name: kubeadm-ca    kind: ClusterIssuer  duration: 24h  # Validity period of the certificate  renewBefore: 12h   commonName: foo.foo.svc.cluster.local  dnsNames:    - foo.foo.svc.cluster.local    - foo.foo.svcEOFUse that secret in a deployment. Note the annotation for Reloader. We’re mounting the secret in /etc/foo and certificates /etc/certs.kubectl apply -f - &lt;&lt;EOFapiVersion: apps/v1kind: Deploymentmetadata:  name: foo  labels:    app: foo  annotations:    secret.reloader.stakater.com/reload: \"foo-secret,foo-certs\"spec:  replicas: 1  selector:    matchLabels:      app: foo  template:    metadata:      labels:        app: foo    spec:      containers:      - name: my-container        image: nginx        volumeMounts:        - name: secret-volume          mountPath: \"/etc/foo\"          readOnly: true        - name: certs          mountPath: \"/etc/certs\"          readOnly: true      volumes:      - name: secret-volume        secret:          secretName: foo-secret      - name: certs        secret:          secretName: foo-certsEOFRecreate the secret and check the logs of reloader.$ kubectl create secret generic foo-secret --from-literal=key1=foo --dry-run=client -o yaml | kubectl apply -f -Warning: resource secrets/foo-secret is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.secret/foo-secret configuredReloader logs. It has noticed the secret update and restarted the pod.$ k logs -n reloader reloader-reloader-64df699b8d-tm5rn time=\"2023-11-23T14:36:25Z\" level=info msg=\"Environment: Kubernetes\"time=\"2023-11-23T14:36:25Z\" level=info msg=\"Starting Reloader\"time=\"2023-11-23T14:36:25Z\" level=warning msg=\"KUBERNETES_NAMESPACE is unset, will detect changes in all namespaces.\"time=\"2023-11-23T14:36:25Z\" level=info msg=\"created controller for: configMaps\"time=\"2023-11-23T14:36:25Z\" level=info msg=\"Starting Controller to watch resource type: configMaps\"time=\"2023-11-23T14:36:25Z\" level=info msg=\"created controller for: secrets\"time=\"2023-11-23T14:36:25Z\" level=info msg=\"Starting Controller to watch resource type: secrets\"time=\"2023-11-23T15:18:53Z\" level=info msg=\"Changes detected in 'foo-secret' of type 'SECRET' in namespace 'foo', Updated 'foo' of type 'Deployment' in namespace 'foo'\"New pod should be starting.$ k get podsNAME                   READY   STATUS        RESTARTS   AGEfoo-5c67d96557-s6cj2   1/1     Running       0          18sfoo-75cb458f7d-xcszx   1/1     Terminating   0          2m30sNow it’s got the new secret.$ k exec -it foo-5c67d96557-s6cj2 -- cat /etc/foo/key1fooBoom.CertificatesAbove we crated a certificate with only 24 hours of validity that should renew after 12 hours. So when it’s renewed, there will be a new version of the secret, and reloader will restart the pod. Let’s see.$ k logs -n reloader reloader-reloader-7f4859f649-6cvqt time=\"2023-11-23T16:03:57Z\" level=info msg=\"Environment: Kubernetes\"time=\"2023-11-23T16:03:57Z\" level=info msg=\"Starting Reloader\"time=\"2023-11-23T16:03:57Z\" level=warning msg=\"KUBERNETES_NAMESPACE is unset, will detect changes in all namespaces.\"time=\"2023-11-23T16:03:57Z\" level=info msg=\"created controller for: configMaps\"time=\"2023-11-23T16:03:57Z\" level=info msg=\"Starting Controller to watch resource type: configMaps\"time=\"2023-11-23T16:03:57Z\" level=info msg=\"created controller for: secrets\"time=\"2023-11-23T16:03:57Z\" level=info msg=\"Starting Controller to watch resource type: secrets\"time=\"2023-11-23T16:06:18Z\" level=info msg=\"Changes detected in 'foo-secret' of type 'SECRET' in namespace 'foo', Updated 'foo' of type 'Deployment' in namespace 'foo'\"time=\"2023-11-24T04:44:56Z\" level=info msg=\"Changes detected in 'foo-certs' of type 'SECRET' in namespace 'foo', Updated 'foo' of type 'Deployment' in namespace 'foo'\"Looking at cert-manager logs we see:I1124 04:44:56.006536       1 trigger_controller.go:194] \"cert-manager/certificates-trigger: Certificate must be re-issued\" key=\"foo/foo-certs\" reason=\"Renewing\" message=\"Renewing certificate as renewal was scheduled at 2023-11-24 04:44:56 +0000 UTC\"SNIP!I1124 04:44:56.636293       1 conditions.go:263] Setting lastTransitionTime for CertificateRequest \"foo-certs-jk5sq\" condition \"Ready\" to 2023-11-24 04:44:56.636261134 +0000 UTC m=+4380108.430326366Right, so the secret was updated. Let’s see if the pod was restarted.$ k get podsNAME                  READY   STATUS    RESTARTS   AGEfoo-746699dd7-kr99d   1/1     Running   0          6h43m$ k describe pod foo-746699dd7-kr99d | grep -i started      Started:      Thu, 23 Nov 2023 23:44:59 -0500That time converts to 04:44:59 UTC, which is when the secret was updated. So it was restarted. This is great, so when a new certificate is issued, the pod will be restarted and mount the new secret and have access to the new certificate and key.There’s a reloader annotation as well.$ k get pods -oyaml | grep reloader      reloader.stakater.com/last-reloaded-from: '{\"type\":\"SECRET\",\"name\":\"foo-certs\",\"namespace\":\"foo\",\"hash\":\"94af434fda756e922affdd1c43d723b26f196f3e\",\"containerRefs\":[\"my-container\"],\"observedAt\":1700801096}'ConclusionPersonally, I would think that this kind of thing would be automatic, but it’s not. So this is a good way to make sure that your pods are restarted when there are new secrets.Kubernetes is a framework, and you have to pull in a lot of “libraries,” such as Reloader."
    } ,
  
    {
      "title"    : "Understanding Kubernetes Pod Security: runAsNonRoot and runAsUser",
      "category" : "",
      "tags"     : "",
      "url"      : "/2023/09/02/runasnonroot-vs-runasuser.html",
      "date"     : "2023-09-02 00:00:00 -0400",
      "content"  : "Security is a prime concern when deploying applications in a Kubernetes cluster. One of the security aspects in Kubernetes is controlling who can run what and as whom within a Pod. Kubernetes provides two important fields in the Security Context to achieve this: runAsNonRoot and runAsUser. While they might seem similar at first glance, they serve different purposes. This blog post aims to demystify these settings and help you make the right choice for your applications.nginx imagesI have to wonder what percentage of containers are just nginx instances that are there to test something out. Nginx is an easy image to deploy because you can just do:kubectl run nginx --image=nginxAnd you have a running nginx instance.However, that default nginx image will run as root (if your cluster allows that).$ k exec -it nginx -- cat /proc/1/status | grep \"Name\\|Uid\"Name:\tnginxUid:\t0\t0\t0\t0There is an nginx unprivileged image that will run as a non-root user; it runs as user 101. I would definitely recommend using this image if you are just testing something out. It’s a few more letters to type, but it’s worth it.kubectl run nginx-unprivileged --image=nginxinc/nginx-unprivilegedInspecting the images…$ docker image inspect nginx | jq '.[0].Config.User'\"\"$ docker image inspect nginxinc/nginx-unprivileged | jq '.[0].Config.User'\"101\"Here’s the nginx user in the unprivileged image:$ k exec -it runasnonroot-and-runasuser -- grep nginx /etc/passwdnginx:x:101:101:nginx user:/nonexistent:/bin/falseDifferences Between runAsNonRoot and runAsUser                   runAsNonRoot      runAsUser                  Purpose      Ensure container doesn’t run as root      Specify the exact UID for container              Settings      true/false      Numeric UID              Flexibility      Less flexible      More flexible              Specificity      General: just not root      Very specific: exact UID              Root Allowed      No      Yes, if specified      runAsNonRootThe runAsNonRoot field specifies that the container must not run as the root user. Setting this to true enforces that the container should be executed as a non-root user. If the container image specifies a user as root or numerically as 0, the container won’t start. It’s a way to ensure that your application doesn’t unintentionally run with more permissions than it needs.cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata:  name: non-root-podspec:  securityContext:    runAsNonRoot: true  containers:  - name: my-container    image: nginxinc/nginx-unprivilegedEOFCheck the id of the user running the container:$ kubectl exec -it non-root-pod -- iduid=101(nginx) gid=101(nginx) groups=101(nginx)But if we run the plain nginx image, it will fail:cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata:  name: non-root-pod-plainspec:  securityContext:    runAsNonRoot: true  containers:  - name: my-container    image: nginxEOFNote that the container is not running:$ k get podsNAME                 READY   STATUS                       RESTARTS   AGEnon-root-pod         1/1     Running                      0          112snon-root-pod-plain   0/1     CreateContainerConfigError   0          5sAnd the reason is:$ k describe pod non-root-pod-plain  | grep Error      Reason:       CreateContainerConfigError  Warning  Failed     1s (x5 over 42s)  kubelet            Error: container has runAsNonRoot and image will run as root (pod: \"non-root-pod-plain_runasnonroot(c5764bbb-c1cf-47b1-9606-3a3a49ebf666)\", container: my-container)runAsUserOn the other hand, runAsUser specifies which UID (User ID) the container process should run as. Unlike runAsNonRoot, this allows you to specify the exact UID of the user, including root if you explicitly set it to 0.cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata:  name: specific-user-pod-nginx-priviligedspec:  securityContext:    runAsUser: 1001  containers:  - name: my-container    image: nginxinc/nginx-unprivilegedEOFCheck the id of the user running the container:$ kubectl exec -it specific-user-pod-nginx-priviliged -- iduid=1001 gid=0(root) groups=0(root)But if we run the plain nginx image, it will fail:cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata:  name: specific-user-pod-nginx-plainspec:  securityContext:    runAsUser: 1001  containers:  - name: my-container    image: nginxEOFNote that the container is not running:$ k get pod specific-user-pod-nginx-plain NAME                            READY   STATUS             RESTARTS      AGEspecific-user-pod-nginx-plain   0/1     CrashLoopBackOff   1 (10s ago)   16sBoth runAsNonRoot and runAsUserYou can also use both runAsNonRoot and runAsUser together. In this case, runAsUser specifies the UID to use, and runAsNonRoot ensures that UID is not root.cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata:  name: runasnonroot-and-runasuserspec:  securityContext:    runAsUser: 1001    runAsNonRoot: true  containers:    - name: my-container      image: nginxinc/nginx-unprivilegedEOFCheck the id of the user running the container:$ k exec -it runasnonroot-and-runasuser -- iduid=1001 gid=0(root) groups=0(root)$ k get pods runasnonroot-and-runasuser NAME                         READY   STATUS    RESTARTS   AGErunasnonroot-and-runasuser   1/1     Running   0          22sAnd nginx is indeed running as user 1001:$ k exec -it runasnonroot-and-runasuser -- cat /proc/1/status | grep \"Name\\|Uid\"Name:\tnginxUid:\t1001\t1001\t1001\t1001  NOTE: PID 1 is the first process that runs in any operating system or containerized environment. When a container starts, it launches a single process with a PID (Process ID) of 1 within the isolated namespace of that container.nginx plain as non-root with runAsUserWill it blend?cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata:  name: nginx-plain-runasnonroot-and-runasuserspec:  securityContext:    runAsUser: 1001    runAsNonRoot: true  containers:    - name: my-container      image: nginxEOFNo.$ k get pod nginx-plain-runasnonroot-and-runasuser NAME                                     READY   STATUS             RESTARTS     AGEnginx-plain-runasnonroot-and-runasuser   0/1     CrashLoopBackOff   1 (7s ago)   12sUse Cases      runAsNonRoot: Use this setting when you want a general assurance that none of the containers in the Pod are running as root, without caring which user they run as.        runAsUser: Use this when you need more control over the exact user that runs the container process, such as for compliance with internal security policies that require specific UIDs for different types of applications. As well, some images require a specific UID to run properly.  ConclusionWhile runAsNonRoot and runAsUser both provide ways to control the user running a container, they serve different needs. runAsNonRoot is a more generalized setting to prevent root access, while runAsUser gives you fine-grained control over the user ID. Knowing when to use each can improve the security posture of your Kubernetes applications.That said, both runAsUser and runAsNonRoot can co-exist. When they do, runAsUser specifies which UID to use, and runAsNonRoot ensures that UID is not root.In normal, production systems one would never run a container as root so the image would, based on your organizations policies and image build process, have a user setup and the Kubernetes manifest would have runAsNonRoot set to true."
    } ,
  
    {
      "title"    : "Why My Flask App Refused to Crash: Understanding PID 1 in Containers and Kubernetes",
      "category" : "",
      "tags"     : "",
      "url"      : "/2023/09/02/pid-one-gunicorn-kubernetes.html",
      "date"     : "2023-09-02 00:00:00 -0400",
      "content"  : "You’ve just deployed your Python Flask app on Kubernetes. You’re using Gunicorn as your WSGI server, and you’re trying to test how the container would behave if the app crashed. But wait! You find out that the container never crashes. Why not? Oh, Gunicorn is being helpful–it keeps restarting the application.OK, maybe this isn’t “you” it’s “me”. I was trying to build a demo app that showed crash loop backoff in Kubernetes, and I couldn’t get the container to crash.Because pid 1 is Gunicorn, not the app itself.$ k exec -it crash-only-backend-0 -- cat /proc/1/status | grep \"Name\\|Uid\"Name:\tgunicorn-run.shUid:\t10001\t10001\t10001\t10001Let’s dive into why this happens and the importance of understanding PID 1 in containers.What the Heck is Gunicorn?  Gunicorn ‘Green Unicorn’ is a Python WSGI HTTP Server for UNIX. It’s a pre-fork worker model. The Gunicorn server is broadly compatible with various web frameworks, simply implemented, light on server resources, and fairly speedy. - GunicornWhat is a WSGI Server?A Web Server Gateway Interface (WSGI) server is a web server that implements the WSGI specification. The WSGI specification is a Python standard that describes how a web server communicates with web applications.Why Does Flask Need a WSGI Server?Flask is a micro web framework written in Python. It’s a WSGI application, which means it needs a WSGI server to run.  “Production” means “not development”, which applies whether you’re serving your application publicly to millions of users or privately / locally to a single user. Do not use the development server when deploying to production. It is intended for use only during local development. It is not designed to be particularly secure, stable, or efficient. - FlaskWhat is PID 1?In Unix-based systems, the process ID (PID) is a unique identifier for each running process. The very first process that runs when a system starts is the init system with PID 1. The init process has special responsibilities, like adopting orphaned child processes and handling signals.Gunicorn and PID 1When you run a container, the process you start becomes PID 1 within that container. In the case of my Flask app, Gunicorn becomes PID 1.  NOTE: Why not just run Flask directly? Because Gunicorn is a production-ready WSGI server that can handle multiple requests concurrently.Why Doesn’t the Container Crash?If your Flask app (running as a Gunicorn worker) crashes, Gunicorn will restart it. Since Gunicorn is PID 1, the container will remain alive as long as Gunicorn does. This is why even if your Flask app encounters an error, the container doesn’t crash.The Kubernetes PerspectiveIn a Kubernetes cluster, the kubelet will restart a crashed container based on its restartPolicy. However, if Gunicorn (PID 1) doesn’t crash, Kubernetes won’t know that something is wrong with your Flask app. This could lead to misleading metrics and logs, affecting your debugging and monitoring efforts.Killing Pid 1In the app I still wanted to use gunicorn which means to demonstrate an app crashing and Kubernetes restarting the container, I needed to kill gunicorn. I needed to kill PID 1.Here’s what I ended up with inside the app:def random_crash():    if random.randint(1, 100) &gt; 94:        logger.error(\"&lt;&lt;&lt;&lt; Crashing... &gt;&gt;&gt;&gt;\")        # gunicorn will restart the process, which is pretty cool, but for this        # app we want to purposely crash the whole container, so we kill the        # parent process which is gunicorn        os.kill(os.getppid(), 9)  NOTE: I’m purposely trying to crash the container to demonstrate crashLoopBackoff. As well, I’m not using sys.exit() because that would just exit the flask process, not the container. (Which, by the way, is what I originally did and why I couldn’t get the container to crash.)ConclusionUnderstanding PID 1 in containers is crucial for debugging, process management, and robustness, especially when deploying applications on Kubernetes.To many, this is elementary…even downright obvious. But I’ve been doing this for a while and I still learned something when building this little demo app. As well, while researching this I found a fair bit of confusion around using gunicorn in containers.Let me know if you have any questions or comments!"
    } ,
  
    {
      "title"    : "Three Steps to a Default Secure Kubernetes",
      "category" : "",
      "tags"     : "",
      "url"      : "/2023/08/12/three-steps-to-a-default-secure-kubernetes.html",
      "date"     : "2023-08-12 00:00:00 -0400",
      "content"  : "Kubernetes is a framework. We don’t usually describe it as a framework, but it is. IMHO, it’s a library we can use to deploy applications and imprint our organization’s policies and requirements on top of. That’s what makes it valuable, not the fact that it can create a container.Because it’s a basic framework, a set of lego blocks, it’s not designed to be secure “out of the box.” We’ve got to make it as secure as we need it to be.NOTE: Not a PanaceaThis post is an exploration of some things we could do to make Kubernetes more secure by default. Like what are a couple minimal steps we could take that have a large return on investment. It’s not meant to meet every organization’s requirements or be the end-all-be-all of security. It’s meant as an exploration of a secure starting point that could potentially work for everyone and every Kuberenetes.In fact I should say here that I’ve already had people give me diffrent opinions on these settings. For example for network policies here I’m thinking more of lateral movement, but many organizations would prefer to stop outbound access. It really depends on your organization’s requirements.1 - Pod Security StandardsFirst, if you’re not familiar with Pod Security Standards, it’s not a bad idea to go read up on them, but suffice it to say let’s make sure every namespace has the following label.kubectl label namespace &lt;NAMESPACE&gt; pod-security.kubernetes.io/enforce=restrictedWhich means:  Limit types of volumes  Don’t run as root user  No privilege escalation  Seccomp profile set to “RuntimeDefault” or “Localhost”  Drop all capabilities except perhaps add NET_BIND_SERVICEHere’s an example of running the nginx unprivileged container.cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata:  name: nginx-meets-pod-security-standards  namespace: enforcingspec:  containers:  - image: nginxinc/nginx-unprivileged\tname: nginx\tsecurityContext:  \tallowPrivilegeEscalation: false  \tcapabilities:    \tdrop:    \t- ALL  \trunAsNonRoot: true  \tseccompProfile:    \ttype: RuntimeDefaultEOFThat’s a pretty good start!2 - Network PoliciesHistorically, in Kubernetes, the connectivity was based on a giant, flat layer 3 network, which means every pod had an IP address and could talk to every other pod in the cluster. Obviously this doesn’t really work in enterprise environments, so Kubernetes added the ability to create Network Policies.Here let’s create a default policy to ensure that pods in a namespace can talk to one another, but cannot talk to pods OUTSIDE of their namespace. This is super basic, but I like it as a starting point. Note that services would still be accessible, just not pods directly.Here’s an example:apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: namespace-onlyspec:  podSelector: {} # Selects all pods in the namespace  policyTypes:  - Ingress  - Egress  ingress:  - from:    - podSelector: {} # Allow pods in the same namespace to talk to one another  egress:  - to:    - ipBlock:   \t cidr: 0.0.0.0/0 # Allow egress to any destinationObviously this can be taken a lot further, especially with a CNI that provides extra capabilities.3: Runtime Threat DetectionThe last step is to make sure that we’re monitoring our Kubernetes clusters for runtime threats. While we have a good baseline of security, something will always get through, no matter what we do. Any adversary worth their salt will find a way in, or mistakes will be made, etc. So we need to be able to detect them when they do. The only way to do that is to monitor the runtime environment.We can do that in a couple of ways, one is to use Sysdig Secure, which is a commercial CNAPP (Cloud Native Application Protection Platform) that has a decade of history in runtime protection. The other is to use Falco, which is an open source project that is part of the CNCF. Sysdig as a company supports the Falco project.3a - Sysdig SecureSysdig Secure is a security platform which has a decade of history in runtime protection.It’s easy to sign up for a free trial at Sysdig.Then, install the agent into Kubernetes clusters with Helm:helm repo add sysdig https://charts.sysdig.comhelm repo updatehelm install sysdig-agent --namespace sysdig-agent --create-namespace \\--set global.sysdig.accessKey=&lt;ACCESS_KEY&gt; \\--set global.sysdig.region=&lt;SAAS_REGION&gt; \\--set nodeAnalyzer.secure.vulnerabilityManagement.newEngineOnly=true \\--set global.kspm.deploy=true \\--set nodeAnalyzer.nodeAnalyzer.benchmarkRunner.deploy=false \\--set nodeAnalyzer.nodeAnalyzer.hostScanner.deploy=true--set global.clusterConfig.name=&lt;CLUSTER_NAME&gt; \\sysdig/sysdig-deployDone!3b - FalcoFalco is an open source project that is part of the CNCF. It’s a runtime threat detection engine that can be used to detect threats in Kubernetes clusters. Sysdig as a company supports the Falco project.  Falco is a cloud native runtime security tool for Linux operating systems. It is designed to detect and alert on abnormal behavior and potential security threats in real-time.At its core, Falco is a kernel monitoring and detection agent that observes events, such as syscalls, based on custom rules. Falco can enhance these events by integrating metadata from the container runtime and Kubernetes. The collected events can be analyzed off-host in SIEM or data lake systems. - FalcoInstalling Falco into Kubernetes is easy, just use Helm:helm repo add falcosecurity https://falcosecurity.github.io/chartshelm repo updatehelm install falco falcosecurity/falcoDone!AutomationI would prefer that this all be done automatically. Because Kubernetes is a framework there are ways to make these kinds of security settings default, including the concepts of building operators and admission controllers. That would be my next step, to set up some tooling that would automatically apply these settings to every cluster, to every namespace, and to every pod.So, look forward to a future blog post on that!ConclusionI want to be clear that the point here is to create something that is simple and at the same time really improves the default security of Kubernetes–like what’s the best bang for the buck we can get in terms of security.In this blog post we’ve seen how to create a higher level of default security for Kubernetes, and we looked at how to use Sysdig Secure and Falco to monitor the runtime environment for threats.Ultimately, this post is an exploration of how to configure a Kubernetes cluster so that it is much more secure “by default.” There’s no need to have Kubernetes be so wide open.PS. I’ve included an optional section discussing Buildpacks and how they can be used to create more secure container images.OPTIONAL: Buildpacks and PaketoBackgroundOften people are surprised to find out that there is more than one way to build a container image. I mean, what’s a container image: it’s just a fancy tar file. There are many ways one can make a fancy tar file.One way is buildpacks. I’ve written about them before. Paketo is a set of buildpacks that are designed to be used with Kubernetes.For the purposes of this blog post, the point of Vuildpacks is that they are a way to build a container image that is more secure by default. For example, buildpacks don’t run as root. If we just get rid of that one thing, we’ve made our container images more secure.The value of Buildpacks:  Security - Buildpacks run as non-root by default.  Advanced Caching - Robust caching is used to improve performance.  Auto-detection -Images can be built directly from application source without additional instructions.  Bill-of-Materials - Insights into the contents of the app image through standard build-time SBOMs in CycloneDX, SPDX and Syft JSON formats.  Modular / Pluggable- Multiple buildpacks can be used to create an app image.  Multi-language - Supports more than one programming language family.  Multi-process - Image can have multiple entrypoints for each operational mode.  Minimal app image - Image contains only what is necessary.  Rebasing - Instant updates of base images without re-building.  Reproducibility - Reproduces the same app image digest by re-running the build.  Reusability - Leverage production-ready buildpacks maintained by the community.PaketoPaketo is a set of buildpacks that are designed to be used with Kubernetes.First off this is a Python app and I’m using gunicorn, so we have a Procfile. This is really the only difference I have between a Dockerfile based image and a buildpack based image. Instead of a Dockerfile I have a Procfile, and the Procfile only describes the command to run the app, nothing else.  Procfiles define processes from your application’s code and contains one process per line.Here’s the example Procfile:$ cat Procfileweb: gunicorn app:appNow we can build the image with a straight forward command.  NOTE: This assumes, of course, the pack CLI is installed.pack build somepythonflaskapp \\  --buildpack paketo-buildpacks/python \\  --builder paketobuildpacks/builder:baseAnd deploy it into Kubernetes, port-forward to the service, and finally curl the app.  NOTE: This app is setup to report the user it’s running as.$ curl localhost:8000Application is running as user: cnbAbove we can see the app, which is configured to return the user it’s running as, is reporting that it is running as user “cnb” aka not root, aka Cloud Native Buildpacks. Done by default. Nice."
    } ,
  
    {
      "title"    : "Making Pod Security Standards the Default in Kubernetes",
      "category" : "",
      "tags"     : "",
      "url"      : "/2023/08/02/making-pod-security-standards-default.html",
      "date"     : "2023-08-02 00:00:00 -0400",
      "content"  : "In my opinion, the default level of security in Kubernetes is not enough. There’s some work that needs to be done to bring it up to some, perhaps arbitrary, level of security. This post is part of an exploration of that area.We used to have something called Pod Security Policies that we could use to increase the level of security, like reduce some exposure, but that model was deprecated. Now we have something called Pod Security Standards. These are a set of recommendations for securing pods and are managed by the Pod Security Admission Controller which is part of Kubernetes. However, by default, no Pod Security Standards are configured, enforcing, etc…but what if we want them enforcing by default on a newly created namespace?One Model: Mutating Requests Using KyvernoOne way to accomplish this is to use Kyverno, which, among its features, is the ability to mutate Kubernetes requests in, what I think, is a pretty straight forward fashion. For example, it can add a label to a namespace, such as specifying a Pod Security Standard. Thus, following this model, we can force every new namespace to require a certain security posture by default.Another way to do this would be to set up a mutating admission controller. Probably other methods as well. (Let me know!!!)I like “mutating” things because it really means having Kubernetes do the work for us. I like making Kubernetes do work for me instead of the other way around. :)Install Kyverno  Install Kyverno$ helm install kyverno kyverno/kyverno -n kyverno --create-namespaceNAME: kyvernoLAST DEPLOYED: Wed Aug  2 07:47:18 2023NAMESPACE: kyvernoSTATUS: deployedREVISION: 1NOTES:Chart version: 3.0.4Kyverno version: v1.10.2Thank you for installing kyverno! Your release is named kyverno.The following components have been installed in your cluster:- CRDs- Admission controller- Reports controller- Cleanup controller- Background controller⚠️  WARNING: Setting the admission controller replica count below 3 means Kyverno is not running in high availability mode.💡 Note: There is a trade-off when deciding which approach to take regarding Namespace exclusions. Please see the dDone.Make the Restricted Pod Security Standard the DefaultKyverno can mutate a request to create a namespace and add a label to it. In this case we’re telling it to add the label pod-security.kubernetes.io/enforce: restricted so that the pod security admission controller will enforce the Pod Security Standards on the namespace, in this case it will “enforce” the “restricted” profile.Configuring Kyverno to Add the Label to Namespaces  Add the ClusterPolicy to Kyverno  NOTE: In the “real world” we’d probably want to exclude some namespaces, but, again, only new namespaces will be affected by this policy.cat &lt;&lt;EOF | kubectl apply -f -apiVersion: kyverno.io/v1kind: ClusterPolicymetadata:  name: add-ns-labelspec:  background: false  rules:  - name: add-ns-label    match:      resources:        kinds:        - Namespace    mutate:      patchStrategicMerge:        metadata:          labels:            pod-security.kubernetes.io/enforce: restrictedEOFVerify:$ k get clusterpolicy add-ns-labelNAME       \tBACKGROUND   VALIDATE ACTION   READY   AGE\tMESSAGEadd-ns-label   true     \tAudit         \tTrue\t7h1m   ReadyGood.Create a Namespace and Test  Create a new namespace (the name doesn’t matter)kubectl create ns enforcing  Review that the label was added by Kyverno mutating the request for a namespace$ k get ns enforcing -oyamlapiVersion: v1kind: Namespacemetadata:  annotations:\tpolicies.kyverno.io/last-applied-patches: |  \tadd-ns-label.add-ns-label.kyverno.io: added /metadata/labels/pod-security.kubernetes.io~1enforce  creationTimestamp: \"2023-08-02T11:50:14Z\"  labels:\tkubernetes.io/metadata.name: enforcing\tpod-security.kubernetes.io/enforce: restricted  name: enforcing  resourceVersion: \"11055\"  uid: ce552efc-172e-4aa6-bc6d-13179f73372cspec:  finalizers:  - kubernetesstatus:  phase: ActiveThis label was added by Kyverno. This means the pod security admission controller will enforce the restricted Pod Security Standard in this namespace.pod-security.kubernetes.io/enforce: restricted  Create an “insecure” pod in the namespace, just any old pod will do…kubectl run nginx --image=nginx -n enforcingE.g. output:$ k run nginx --image=nginx -n enforcingError from server (Forbidden): pods \"nginx\" is forbidden: violates PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"nginx\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"nginx\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"nginx\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"nginx\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")The pod was not allowed to be created because it did not meet the Pod Security Standard.  Create a pod that meets the Pod Security Standard for the “restricted” profilecat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata:  name: nginx-meets-pod-security-standards  namespace: enforcingspec:  containers:  - image: nginxinc/nginx-unprivileged    name: nginx    securityContext:      allowPrivilegeEscalation: false      capabilities:        drop:        - ALL      runAsNonRoot: true      seccompProfile:        type: RuntimeDefaultEOFOk, that worked.$ k get podsNAME                             \tREADY   STATUS\tRESTARTS   AGEnginx-meets-pod-security-standards   1/1 \tRunning   0      \t6h26m  Connect to nginx running in the pod by port forwardingkubectl port-forward pod/nginx-meets-pod-security-standards 8080:8080 -n enforcing  Curl it$ curl localhost:8080&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;Works!What is the Pod Security Standard “Restricted” Profile?The Pod Security Standard “restricted” profile is defined in the Pod Security Standards and is, er, the most restrictive profile.  Limit types of volumes  Don’t run as root user  No privilege escalation  Seccomp profile set to “RuntimeDefault” or “Localhost”  Drop all capabilities except perhaps add NET_BIND_SERVICEConclusionNo one should run vanilla, default Kubernetes in production. No one should run root user pods. I mean we’re mostly running web services here, they can listen on port 8080 and don’t really need much in the way of permissions. Definitely our namespaces should have security limitations that are only reduced later on if they need to be.Using Kyverno to do this is one way, there are others.Ultimately, the way to secure general purpose CPUs is to limit what the workloads can do with them.References  https://www.jrcomplex.fi/securing-containers-in-kubernetes-with-seccomp/  https://kubernetes.io/docs/concepts/security/pod-security-standards/  https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/"
    } ,
  
    {
      "title"    : "Chain-Link - A Chain of Services in Kubernetes",
      "category" : "",
      "tags"     : "",
      "url"      : "/2023/07/30/chain-link-kubernetes-python-application.html",
      "date"     : "2023-07-30 00:00:00 -0400",
      "content"  : "tl;drI built an app called chain-link that will create a “chain” of apps in Kubernetes of an arbitrary length. It’s written in Python.While the point of this all is the app, what I learned most about what writing the CLI portion: the chain-link-cli. There’s actually way more code there (for better or worse) to deploy and manage the app than there is in the app itself.What is it?I wanted to do some work with a simple Python application that could allow creating a set of services that would form a chain that could be visualized in some kind of program (in this case it ended up being Zipkin, but it could be anything that can show traces).I wanted it to do a few things:  Support traces  Create a “chain” of services of an arbitrary length  Randomly insert some sleep time into the chain  Write a CLI that could be used to create and manage the chain  Deploy a loadgenerator to activate the chain  Deploy a tool that can visualize the chain through the traces it generatesHow does it work?The app is written in Python and uses Flask. The app itself is pretty basic. It just looks at the generated list of services and forwards the request to the next service in the chain. It also adds some headers to the request to help with tracing.This is what the pods look like in the cluster once they’re deployed:$ k get podsNAME                                       READY   STATUS    RESTARTS        AGEchain-link-deployment-0-855875c8d5-rz8wd   1/1     Running   2 (47d ago)     96dchain-link-deployment-1-6cc5965f45-ch8cj   1/1     Running   6 (5d5h ago)    96dchain-link-deployment-2-65dd5b4878-tnq2r   1/1     Running   6 (5d5h ago)    96dchain-link-deployment-3-7bf888dddb-lwwdr   1/1     Running   6 (5d5h ago)    96dchain-link-deployment-4-6c47c7dcb5-4sf4v   1/1     Running   2 (47d ago)     96dchain-link-deployment-5-85655c8d4f-2z2r6   1/1     Running   6 (5d5h ago)    96dloadgenerator                              1/1     Running   0               23mzipkin-deployment-69c4598df6-js95l         1/1     Running   209 (16m ago)   99dEach deployment is a separate “app” that the overall service chain is composed of. (Now that I write “service chain” this reminds me of my telecom days.)What does it look like in Zipkin?The CLIHere’s the CLI help:$ ./chain-link-cli WARNING Using existing config file: /home/curtis/.config/chain-link/chain-link-cli.confusage: chain-link-cli [-h] [--instances NUM_INSTANCES]                      [--namespace NAMESPACE]                      [--chain-link-image IMAGE_NAME]                      [--sleep-time SLEEP_TIME] [-d] [-v]                      [--config-file CONFIG_FILE]                      {deploy,validate,generate,dry-run} ...Deploy the chain-link application to a Kubernetes clusterpositional arguments:  {deploy,validate,generate,dry-run}    deploy              Deploy chain-link to Kubernetes    validate            Validate chain-link configuration    generate            Generate chain-link kubernetes yaml    dry-run             Generate chain-link kubernetes yamloptions:  -h, --help            show this help message and exit  --instances NUM_INSTANCES                        Number of instances to deploy  --namespace NAMESPACE                        Namespace to deploy to  --chain-link-image IMAGE_NAME                        ChainLink image to deploy  --sleep-time SLEEP_TIME                        Time to sleep between loadgenerator requests  -d, --info            Print lots of infoging statements  -v, --verbose         Be verbose  --config-file CONFIG_FILE                        Specify the path to the config fileAs you can see there are a few subcommands, like deploy, validate, etc.Here’s validate:$ ./chain-link-cli validateWARNING Using existing config file: /home/curtis/.config/chain-link/chain-link-cli.confINFO    Using the following configuration...INFO    Number of instances: 6INFO    Namespace: chain-link-2INFO    ChainLink image: ghcr.io/ccollicutt/chain-link:latestINFO    Loadgenerator sleep time: 60INFO    Validating chain-link configuration...INFO    Validating deployments...INFO    Deployments readyINFO    Validating pods...INFO    Pods readyINFO    All objects readyAll the config files are generated and placed in ~/.config/chain-link/manifests:$ ls ~/.config/chain-link/manifests/chain-link-2-namespace.yamlchain-link-deployment-0-deployment.yamlchain-link-deployment-1-deployment.yamlchain-link-deployment-2-deployment.yamlchain-link-deployment-3-deployment.yamlchain-link-deployment-4-deployment.yamlchain-link-deployment-5-deployment.yamlchain-link-service-0-service.yamlchain-link-service-1-service.yamlchain-link-service-2-service.yamlchain-link-service-3-service.yamlchain-link-service-4-service.yamlchain-link-service-5-service.yamlchain-link-services-configmap.yamlloadgenerator-pod.yamlzipkin-deployment-deployment.yamlzipkin-service-service.yamlThere’s a config file there too:$ cat ~/.config/chain-link/chain-link-cli.conf [DEFAULT]instances = 6namespace = chain-link-2chain_link_image = ghcr.io/ccollicutt/chain-link:latestsleep_time = 60There’s about 800 lines of Python for the CLI:$ wc -l *.py   95 arg_parser.py  569 chainlink.py   80 cli_manager.py   82 config.py    0 __init__.py   25 log_utils.py   36 utils.py  887 totalVersus 170 or so for the app itself.$ wc -l app.py 169 app.pyI need to do some research and see if there are some higher level abstractions in Python that can help reduce the number of lines in the CLI.Why a Python CLI and not Helm?YAML isn’t a programming language. Obviously you can do a lot “more” with Kubernetes using a real programming language. But of course, no one writes a Python CLI for every app they deploy to Kubernetes–that would make no sense.I mean, it would make more sense to do this in Helm if I thought other people would actually use it. But I don’t expect anyone ever would. This app was really just a learning experience for me, and part of the learning experience I wanted was to create a CLI that could manage the app for me in Kubernetes.Ultimately, I might prefer to use Python to manage Kubernetes environments. Then again, Helm’s ability to manage upgrades would be required in production. This is something I need to look into–how tools like Helm are looking at the state of a k8s app. Plus whatever other tooling exists–for example Pulumi (which I have never used).ConclusionI learned a lot about Python and Kubernetes writing this application and the associated CLI. I’m sure there are tons of bugs, and there is, embarrassingly, little testing. That’s something I learned I need to improve on: testing.For this app I just started writing, and this is what I ended up with. Is the end result perfect? No. Was the process of learning great? It sure was."
    } ,
  
    {
      "title"    : "blockfriday - Blocking Kubernetes Deployments with an Admission Controller",
      "category" : "",
      "tags"     : "",
      "url"      : "/2023/07/29/blockfriday-kubernetes-admission-controller.html",
      "date"     : "2023-07-29 00:00:00 -0400",
      "content"  : "Creating new deployments on a Friday is NOT allowed.Would you create a Kubernetes Admission Controller to block deployments on a Friday and use it in production? No. But you could, create one, say, as an example admission controller.So that is precisely what I have done, created a very, very (very) simple admission controller, written in Go, that blocks NEW Kubernetes deployments on a Friday. I call it blockfriday.What’s an Admission Controller?An admission controller is a piece of software that can intercept requests to the Kubernetes API server and either allow or deny them. Or well, more specifically the Kubernetes API will forward requests to the admission controller for validation. You can use them to enforce policies, like “no deployments on a Friday” or “all deployments must have a label of app: foo”. Using admission controllers you can apply “policy as code” to your Kubernetes cluster. (Note that there are a lot of admission controllers out there. This is just one example.)How does it work?This admission controller is a simple Go program, about 200 lines of code, that runs in a pod in the cluster. Once a ValidatingWebhookConfiguration is created which points to this service, the kube-api will send requests to the admission controller for validation. The admission controller will then either allow or deny the request.This is the main piece of code that does the work:if isTodayFriday() {\tklog.Infof(\"Denying the request to create a new Deployment on Friday. Deployment: %s, Namespace: %s\", deploymentName, namespace)\treturn makeAdmissionResponse(admissionReview.Request.UID, false, \"Creating new Deployments on Fridays is not allowed.\")}Ultimately, it’s a lot of setup to simply do the above. (Of course there are likely better ways to do this, presumably using Open Policy Agent, aka OPA, but the point here is to write an admission controller.)But First: CertificatesHonestly, the certificate portion of this admission controller was harder than writing the actual code.In the case of blockfriday:  I’m using cert-manager to, uh, manage certificates  The cluster as deployed with kubeadm, so there is a CA in /etc/kubernetes/pki/ca.crt  I use that CA as part of a Cluster Issuer  When deploying the admission controller, I use the Cluster Issuer to create a certificate for the admission controller  The admission controller mounts that certificate (which has a cert and a key) in /cert and uses it  The ValidatingWebhookConfiguration has a CA bundle that cert-manager injects (magically) for me based on the certificate that cert-manager created (nice), and then the kube-api can talk to the admission controller (though it would already be able to because I’m using the CA that kubeadm deployed, but you get my drift)Once it’s DeployedOnce the admission controller has been setup (certs, deployment, validatingwebhookconfiguration) it will block new deployments on a Friday.$ date +%AFriday$ k create -f test/deployment.yaml Error from server: error when creating \"test/deployment.yaml\": admission webhook \"blockfriday.serverascode.com\" denied the request: Creating new Deployments on Fridays is not allowed.Now we’re blockin’ Fridays!Checkout the code here."
    } ,
  
    {
      "title"    : "Cert Manager&#39;s CA Injector and Validating Webhooks",
      "category" : "",
      "tags"     : "",
      "url"      : "/2023/07/28/cert-manager-ca-injector.html",
      "date"     : "2023-07-28 00:00:00 -0400",
      "content"  : "The racoons are injecting the CA bundle, of course.I’ve been working on a simple validating webhook for Kubernetes. More on that later. However, one of the things that you need to provide when you create the Kubernetes manifest for a validating webhook is the CA bundle that the kube-api can use to validate the webhook. But…where does that come from? How do we get it into the manifest?Here’s the Kubernetes docs example of a validating webhook:apiVersion: admissionregistration.k8s.io/v1kind: ValidatingWebhookConfigurationmetadata:  name: \"pod-policy.example.com\"webhooks:- name: \"pod-policy.example.com\"  rules:  - apiGroups:   [\"\"]\tapiVersions: [\"v1\"]\toperations:  [\"CREATE\"]\tresources:   [\"pods\"]\tscope:   \t\"Namespaced\"  clientConfig:\tservice:  \tnamespace: \"example-namespace\"  \tname: \"example-service\"\tcaBundle: &lt;CA_BUNDLE&gt;  admissionReviewVersions: [\"v1\"]  sideEffects: None  timeoutSeconds: 5Note the CA_BUNDLE value.OK, so I have to provide that. But…I guess I just create that manually? At first I was doing the below.  NOTE: In my homelab I use the kubeadm generated certs, which live in /etc/kubernetes/pki/. You probably aren’t doing that–the point is that you need to get the CA bundle from somewhere.cat /etc/kubernetes/pki/ca.crt | base64 | tr -d '\\n'This is a huge pain. You’d have to do this manually every time you create a validating webhook. Why? I was thinking that there must be a better way…and then magically, I stumbled on it. Perhaps everyone else knows about this, but I didn’t, I had to dig a bit.Cert Manager CA InjectorI stumbled on the Cert Manager docs for CA Injection.The example they give is this:apiVersion: v1kind: Namespacemetadata:  name: example1---apiVersion: admissionregistration.k8s.io/v1kind: ValidatingWebhookConfigurationmetadata:  name: webhook1  annotations:\tcert-manager.io/inject-ca-from: example1/webhook1-certificatewebhooks:- name: webhook1.example.com  admissionReviewVersions:  - v1  clientConfig:\tservice:  \tname: webhook1  \tnamespace: example1  \tpath: /validate  \tport: 443  sideEffects: None---apiVersion: cert-manager.io/v1kind: Certificatemetadata:  name: webhook1-certificate  namespace: example1spec:  secretName: webhook1-certificate  dnsNames:  - webhook1.example1  issuerRef:\tname: selfsigned---apiVersion: cert-manager.io/v1kind: Issuermetadata:  name: selfsigned  namespace: example1spec:  selfSigned: {}Note the annotation:  annotations:\tcert-manager.io/inject-ca-from: example1/webhook1-certificateOnce that annotation is there, and of course cert-manager is deployed and issuers configured, etc, the CA bundle can automatically be injected.ConclusionHere it is in a real deployment (I’ve removed most of the actual bundle for brevity):$ k get validatingwebhookconfigurations.admissionregistration.k8s.io blockfriday -oyaml | grep -i cabundle\tcaBundle: LS0tLS1CRU &lt;SNIP!&gt;This really made my life easier.PS. If anyone has any other insights into better ways to do this, please let me know."
    } ,
  
    {
      "title"    : "Command Collections/Groups in Bash Scripts",
      "category" : "",
      "tags"     : "",
      "url"      : "/2023/03/31/using-bash-script-command-groups.html",
      "date"     : "2023-03-31 00:00:00 -0400",
      "content"  : "I work a lot with Kubernetes. So I need to have Kubernetes clusters. The way that I have usually been building them is with the Killer.sh training courses scripts, which can be found here:  https://github.com/killer-sh/cks-course-environment/tree/master/cluster-setup/latestI decided to take that script and update it and change it around a bit for my liking.The changes I’ve made can be found here:  https://github.com/ccollicutt/install-kubernetesOne of the things I found that I liked when writing this script is Bash command grouping.Bash Command Collections/GroupingFrom the bash docs:  Bash provides two ways to group a list of commands to be executed as a unit. When commands are grouped, redirections may be applied to the entire command list. For example, the output of all the commands in the list may be redirected to a single stream.{ list; }Here’s a snippet of the install Kubernetes script where I use a grouping....### install containerd from binary over apt installed versionfunction install_containerd(){  echo \"Installing containerd\"  {    wget -q https://github.com/containerd/containerd/releases/download/v${CONTAINERD_VERSION}/containerd-${CONTAINERD_VERSION}-linux-amd64.tar.gz    tar xvf containerd-${CONTAINERD_VERSION}-linux-amd64.tar.gz    systemctl stop containerd    mv bin/* /usr/bin    rm -rf bin containerd-${CONTAINERD_VERSION}-linux-amd64.tar.gz    systemctl unmask containerd    systemctl start containerd  } 3&gt;&amp;2 &gt;&gt; $LOG_FILE 2&gt;&amp;1}...It’s a function that downloads the latest binary of containerd and installs it. Hacky, sure. But it’s what I want to have done.But what you can see here is that all the commands are wrapped into a command group, ie. with the {}. This is useful because I can control the output of those commands from one place, where you see the: } 3&gt;&amp;2 &gt;&gt; $LOG_FILE 2&gt;&amp;1(More on the above later.)Basically I can take all the output of all the commands, there’s seven commands, and manage it with one command, as opposed to tagging a redirection onto each line. I think this is really useful. To create functions and put related commands into command groups. It made it a lot easier for me to understand this script.Outputting to a log fileWhat I wanted to do is have the script have a verbose flag. If that’s not set, then don’t output anything other than some basic information, like the below.sudo ./install-kubernetes.sh -c -vStarting install...==&gt; Logging all output to /tmp/install-kubernetes-XceXczAOta/install.logChecking Linux distributionDisabling swapRemoving packages...Configuring control plane node...Initializing the Kubernetes control planeConfiguring kubeconfig for root and ubuntu usersInstalling Calico CNI==&gt; Installing Calico tigera-operator==&gt; Installing Calico custom-resourcesWaiting for nodes to be ready...==&gt; Nodes are readyInstall complete!But if verbose is set, then show all the output of all the commands....### Log file ###E: Unable to locate package kubeletE: Unable to locate package kubeadmE: Can't select installed nor candidate version from package 'kubectl' as it has neither of themE: Unable to locate package kubernetes-cniE: No packages foundReading package lists...Building dependency tree...Reading state information...The following packages will be REMOVED:  moby-buildx moby-cli moby-compose moby-containerd moby-engine moby-runc0 upgraded, 0 newly installed, 6 to remove and 13 not upgraded.After this operation, 401 MB disk space will be freed.(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%...Do that that, I sent all the output to a log file. And if verbose is set, then cat the contents of that file.But I ran into one problem where because I was doing the command grouping, I couldn’t cat the file.The error I received:cat: $LOG_FILE: input file is output fileSo I went to stack overflow and ended up here:  https://unix.stackexchange.com/questions/448323/trap-and-collect-script-output-input-file-is-output-file-errorWhich gives a fix:#!/bin/bash...exit_handler () {    # 1. Make standard output be the original standard error    #    (by using fd 3, which is a copy of original fd 2)    # 2. Do the same with standard error    # 3. Close fd 3.    exec &gt;&amp;3 2&gt;&amp;3 3&gt;&amp;-    cat \"$logfile\"    curl \"some URL\" -F \"file=@$logfile\"}...This kind of hackery makes the script a bit harder to understand, but I still want it to work this way. Have functions, in the functions group commands, and then output the log file if the verbose flag is set. This definitely accomplishes that goal.ConclusionI’m a big fan of command grouping and functions in Bash. Of course Bash has been used like this for years, decades, longer…I’m not sure why I haven’t used them as much before. I still have a lot to learn about Bash. The learning never stops. For whatever reason, I really like this particular model of scripting.  Use functions  Put commands into command groups  Control the output into a log file  If verbose flag, cat the log fileLet me know if you have any thoughts on this model. Thanks!"
    } ,
  
    {
      "title"    : "Using AWS Nuke",
      "category" : "",
      "tags"     : "",
      "url"      : "/2023/01/16/using-aws-nuke.html",
      "date"     : "2023-01-16 00:00:00 -0500",
      "content"  : "I recently set up a second AWS account just to use for testing. I have a primary account, but I want one that I can easily wipe out absolutely everything in, specifically using AWS Nuke.What is AWS Nuke?AWS Nuke is a CLI applicaiton that can wipe out everything in an AWS account, if you want it to.What does it do? It removes everything from your AWS account. And I quote:  Remove all resources from an AWS account.Big red warning light:  NOTE: Of course, using AWS Nuke can be extremely…dangerous. You could wipe out everything in your account. AWS Nuke tries to be as safe as possible, but the point is to use it to delete everything.InstallationI downloaded the latest release, untarred it and insatlled the binary in my local bin.$ which aws-nuke/home/curtis/bin/aws-nukeHelp:$ aws-nuke -hA tool which removes every resource from an AWS account.  Use it with caution, since it cannot distinguish between production and non-production.Usage:  aws-nuke [flags]  aws-nuke [command]Available Commands:  completion     Generate the autocompletion script for the specified shell  help           Help about any command  resource-types lists all available resource types  version        shows version of this applicationSNIP!!UsageThe most important thing is the config file and below is a configuration file I’ve used.Notes on the configuration file example:  I’m just looking at us-east-1 for now (and global, i.e. IAM)  I don’t want AWS Nuke to remove the “curtis” user, or their key  Also filter out the MFA configuration for that user (though I don’t believe Nuke can remove it)regions:- us-east-1- globalaccount-blocklist:- \"REDACTEDID_ACCOUNT_NOT_TO_NUKE\" # personal i.e. prod accountaccounts:  # awstesting account to run nuke in  \"REDACTEDID_ACCOUNT_TO_NUKE\":    filters:      IAMUser:      - \"curtis\"      IAMUserPolicyAttachment:      - \"curtis -&gt; AdministratorAccess\"      IAMUserAccessKey:      - \"curtis -&gt; REDACTEDKEY1\"      IAMVirtualMFADevice:      - \"arn:aws:iam::REDACTEDID2:mfa/googleauth\"Otherwise, we’re going to remove everything from that account that we can, except the “curtis” user.Alias AccountsAWS Nuke wants you to alias accounts. It’s going to try to save you from deleting production by looking for the letters “prod” in the account alias.  “To avoid just displaying a account ID, which might gladly be ignored by humans, it is required to actually set an Account Alias for your account. Otherwise aws-nuke will abort.” - AWS Nuke documentation  For my testing account, I gave it this alias.aws iam create-account-alias --profile awstesting --account-alias awstesting  To validate…$ aws iam list-account-aliases --profile awstesting{    \"AccountAliases\": [        \"awstesting\"    ]}Now I can use AWS Nuke.Example UseLet’s create a user in the AWS Account I want to run Nuke in, i.e. I want this new account to be removed by AWS Nuke.First, validate I’m using my testing account.$ export AWS_PROFILE=awstesting$ aws sts get-caller-identity{    \"UserId\": \"REDACTED\",    \"Account\": \"REDACTED\",    \"Arn\": \"arn:aws:iam::REDACTED:user/curtis\"}  Next, create a user that will be removed by AWS nukeaws iam create-user --user-name nukeme  Now list users in the account, there should be only two…aws iam list-usersE.g. output:$ aws iam list-users{    \"Users\": [        {            \"Path\": \"/\",            \"UserName\": \"curtis\",            \"UserId\": \"REDACTED\",            \"Arn\": \"arn:aws:iam::REDACTED:user/curtis\",            \"CreateDate\": \"2023-01-13T15:14:00Z\"        },        {            \"Path\": \"/\",            \"UserName\": \"nukeme\",            \"UserId\": \"REDACTED\",            \"Arn\": \"arn:aws:iam::REDACTED:user/nukeme\",            \"CreateDate\": \"2023-01-16T16:28:15Z\"        }    ]}  Run AWS Nuke in dry-run mode  NOTE: Without a specific option, which I won’t show here, AWS Nuke will always run in dry-run mode.aws-nuke -c aws-nuke.yaml --profile awstestingIt will ask you to type in the alias of the account.If you want to actually for real delete everything, you will need to add the no dry run option, and you’ll be asked to type in the account profile name twice.$ aws-nuke -c aws-nuke.yaml --profile awstesting -qaws-nuke version v2.21.2 - Fri Dec  9 20:36:12 UTC 2022 - e76d21c263477ebd6648fae19f9e539049ad2b51Do you really want to nuke the account with the ID REDACTED and the alias 'awstesting'?Do you want to continue? Enter account alias to continue.&gt; awstestingSNIP!!2023/01/16 13:19:18 This operation, ListFleets, has been deprecatedglobal - IAMUser - nukeme - [Name: \"nukeme\"] - would removeScan complete: 67 total, 1 nukeable, 66 filtered.The above resources would be deleted with the supplied configuration. Provide --no-dry-run to actually destroy resources.  Run it again, but with the option to really delete everything$ aws-nuke -c aws-nuke.yaml --profile awstesting -q --no-dry-runaws-nuke version v2.21.2 - Fri Dec  9 20:36:12 UTC 2022 - e76d21c263477ebd6648fae19f9e539049ad2b51Do you really want to nuke the account with the ID REDACTED_ACCOUNT_TO_NUKE and the alias 'awstesting'?Do you want to continue? Enter account alias to continue.&gt; awstestingSNIP!!Do you really want to nuke these resources on the account with the ID REDACTED_ACCOUNT_TO_NUKE and the alias 'awstesting'?Do you want to continue? Enter account alias to continue.&gt; awstestingglobal - IAMUser - nukeme - [Name: \"nukeme\"] - triggered removeRemoval requested: 1 waiting, 0 failed, 66 skipped, 0 finishedglobal - IAMUser - nukeme - [Name: \"nukeme\"] - waitingRemoval requested: 1 waiting, 0 failed, 66 skipped, 0 finishedglobal - IAMUser - nukeme - [Name: \"nukeme\"] - removedRemoval requested: 0 waiting, 0 failed, 66 skipped, 1 finishedNuke complete: 0 failed, 66 skipped, 1 finished.Goodbye new account!ConclusionSo far I like AWS Nuke. I had tested out some Fargate workloads and removed them, but they had left a NAT Gateway running, which AWS Nuke found. As we all know, those NAT gateways cost a lot of money. I’m really thankful tools like this exist."
    } ,
  
    {
      "title"    : "Jenkins and Kubernetes: Getting the plugin working",
      "category" : "",
      "tags"     : "",
      "url"      : "/2022/12/22/jenkins-kubernetes-plugin-agent.html",
      "date"     : "2022-12-22 00:00:00 -0500",
      "content"  : "I wanted to try out using Kubernetes from Jenkins, and that is what this post is about.I have a Jenkins instance running on a host, specifically it is NOT running in Kubernetes. But I want that instance of Jenkins to talk to a Kubernetes cluster and use it as a “cloud”, where I’m using the term “cloud” in Jenkins parlance.tl;dr  Install the Jenkins Kubernetes plugin  Set up the Kubernetes namespace, service account, and roles/bindings  Create a long lived token for the service account  Add a “cloud” to Jenkins pointing to the Kubernetes cluster, using the token as authenticationCaveatThis is all just running in my homelab, where security isn’t as big an issue as it would be in a real world situation. Keep that in mind! There’s likely a lot that could be improved here from a security perspective.Install the Kubernetes PluginGiven this Jenkins instance is just in my homelab, I just click buttons. If I want a plugin, I just install it from the GUI. It’s fun to just click around for once. :)Set up Kubernetes for use by Jenkins  NOTE: Here I assume you have a Kubernetes cluster available.First, create a namespace for Jenkins to use.$ kubectl create ns jenkins-agentThen create a service account in that namespace with the proper role and rolebinding.apiVersion: v1kind: ServiceAccountmetadata:  name: jenkins-admin  namespace: jenkins-agent---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata:  name: jenkins  namespace: jenkins-agent  labels:  \"app.kubernetes.io/name\": 'jenkins'rules:- apiGroups: [\"\"]  resources: [\"pods\"]  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]- apiGroups: [\"\"]  resources: [\"pods/exec\"]  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]- apiGroups: [\"\"]  resources: [\"pods/log\"]  verbs: [\"get\",\"list\",\"watch\"]- apiGroups: [\"\"]  resources: [\"secrets\"]  verbs: [\"get\"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  name: jenkins-role-binding  namespace: jenkins-agentroleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: jenkinssubjects:- kind: ServiceAccount  name: jenkins-admin  namespace: jenkins-agentNow create a token.  NOTE, from the Kubernetes docs: If you want to obtain an API token for a ServiceAccount, you create a new Secret with a special annotation, kubernetes.io/service-account.name.Apply this YAML.apiVersion: v1kind: Secrettype: kubernetes.io/service-account-tokenmetadata:  name: jenkins-admin-token  annotations:    kubernetes.io/service-account.name: \"jenkins-admin\"Get the token from the secret that was created and decode it from base64. It will be used to configure the Kubernetes cloud in Jenkins as a “secret text” credential type.Add a Kubernetes “cloud”Go to “Dashboard -&gt; Manage Jenkins -&gt; Configure Clouds” and add a new Kubernetes cloud.Now configure that cloud instance.Set the URL of the Kubernetes API endpoint.Create the credential from here as well. Use the token we set up in Kubernetes and create a “secret text” credential.Create a PipelineCreate a new pipeline of “freestyle” type.Restrict where it can be run to the name you gave the Kubernetes cloud instance in Jenkins. In this case I called my “c2-kubernetes.”Here’s the cloud configuration where I’ve configured the name “c2-kubernetes.”Here’s the job configuration. Note that it says “1 cloud” which refers to the Kubernetes cloud we just added.I created a simple job that just echos some output.If you run that job now you’ll see a container get built in the Kubernetes cluster. It won’t take long to run.$ k get podsNAME                  READY   STATUS        RESTARTS   AGEc2-kubernetes-58rlp   1/1     Terminating   0          11sHere’s the console output of that Jenkins job.Started by user adminRunning as SYSTEMAgent c2-kubernetes-58rlp is provisioned from template c2-kubernetes---apiVersion: \"v1\"kind: \"Pod\"metadata:  labels:    jenkins: \"slave\"    jenkins/label-digest: \"396f736cb86bcc043738aedb977de7d31c574611\"    jenkins/label: \"c2-kubernetes\"  name: \"c2-kubernetes-58rlp\"  namespace: \"jenkins-agent\"spec:  containers:  - env:    - name: \"JENKINS_SECRET\"      value: \"********\"    - name: \"JENKINS_AGENT_NAME\"      value: \"c2-kubernetes-58rlp\"    - name: \"JENKINS_NAME\"      value: \"c2-kubernetes-58rlp\"    - name: \"JENKINS_AGENT_WORKDIR\"      value: \"/home/jenkins/agent\"    - name: \"JENKINS_URL\"      value: \"http://jenkins.example.com:8080/\"    image: \"jenkins/inbound-agent:4.11-1-jdk11\"    name: \"jnlp\"    resources:      limits: {}      requests:        memory: \"256Mi\"        cpu: \"100m\"    volumeMounts:    - mountPath: \"/home/jenkins/agent\"      name: \"workspace-volume\"      readOnly: false  hostNetwork: false  nodeSelector:    kubernetes.io/os: \"linux\"  restartPolicy: \"Never\"  volumes:  - emptyDir:      medium: \"\"    name: \"workspace-volume\"Building remotely on c2-kubernetes-58rlp (c2-kubernetes) in workspace /home/jenkins/agent/workspace/test-kubernetes-cloud[test-kubernetes-cloud] $ /bin/sh -xe /tmp/jenkins17424164001143670183.sh+ echo hi c2-kuberneteshi c2-kubernetesFinished: SUCCESSConclusionThis took a bit of testing to get right, but not that much work. I kinda like Jenkins in my homelab because I can just poke around at it and not worry too much about how replicable it all is. Jenkins is pretty good from that perspective, just install plugins, configure things manually, update plugins. Sometimes it’s nice just to do ClickOps.I’ve got a fair bit more to understand about this plugin though. There’s a lot more work to be done around Pod Templates…but that’s for another day. At least at this point Jenkins can create jobs in the Kubernetes cluster.Resources  How to Setup Jenkins Build Agents on Kubernetes PodsISSUE - tcpSlaveAgentListenerI had one issue with the container not completing properly because it couldn’t connect to Jenkins. Note the “tcpSlaveAgentListener” issue.$ k logs c2-kubernetes-xnb6nDec 21, 2022 11:28:49 PM hudson.remoting.jnlp.Main createEngineINFO: Setting up agent: c2-kubernetes-xnb6nDec 21, 2022 11:28:49 PM hudson.remoting.jnlp.Main$CuiListener &lt;init&gt;INFO: Jenkins agent is running in headless mode.Dec 21, 2022 11:28:50 PM hudson.remoting.Engine startEngineINFO: Using Remoting version: 4.11Dec 21, 2022 11:28:50 PM org.jenkinsci.remoting.engine.WorkDirManager initializeWorkDirINFO: Using /home/jenkins/agent/remoting as a remoting work directoryDec 21, 2022 11:28:50 PM org.jenkinsci.remoting.engine.WorkDirManager setupLoggingINFO: Both error and output logs will be printed to /home/jenkins/agent/remotingDec 21, 2022 11:28:50 PM hudson.remoting.jnlp.Main$CuiListener statusINFO: Locating server among [http://jenkins.example.com:8080/]Dec 21, 2022 11:28:50 PM hudson.remoting.jnlp.Main$CuiListener errorSEVERE: http://jenkins.example.com:8080/tcpSlaveAgentListener/ is invalid: 404 Not Foundjava.io.IOException: http://jenkins.example.com:8080/tcpSlaveAgentListener/ is invalid: 404 Not Found    at org.jenkinsci.remoting.engine.JnlpAgentEndpointResolver.resolve(JnlpAgentEndpointResolver.java:219)    at hudson.remoting.Engine.innerRun(Engine.java:724)    at hudson.remoting.Engine.run(Engine.java:540)I had to go into the Jenkin’s configuration and give the agent a port. As soon as I set that the containers were able to connect."
    } ,
  
    {
      "title"    : "Things I learned: Computer Security Acronyms",
      "category" : "",
      "tags"     : "",
      "url"      : "/2022/11/08/security-acronyms.html",
      "date"     : "2022-11-08 00:00:00 -0500",
      "content"  : "tl;dr  I recently starting working in security again, at a great company called Sysdig  I think the security industry has really been improving as of late…more work to be done, but I see progress  I need to learn what a lot of security related acronyms mean  Here’s a good podcast on security acronyms  Categorization and definitions are ongoingBackgroundI started my career in security. That was a long time ago, back when Sun Microsystems was still around and quite popular. You know, before the Dotcom boom. What I remember from that time was 1) I managed a Checkpoint Firewall running on a Sun Microsystems box with 16 interfaces, and when you went to compile the rules often the whole box would crash (not good) and 2) security was just a person that said “NO”…regardless of what the question was, the answer was typically, if not always, no.(ASIDE: I recently bought a Sun Microsystems shirt off of Redbubble. Well before I wrote this post. Not completely sure why…but I digress.)To me, the failing firewall wasn’t as difficult as saying no. At the time, I didn’t have a lot of experience and made many mistakes, had the wrong (perhaps bad) attitude, but even then, saying no so often was hard on me. I didn’t see a good path forward in this part of the industry. It felt like the security world was failing, and eventually I stopped working strictly security focussed jobs and moved into open source infrastructure.Security is ImprovingOver the last few years I think things have improved in security. It might not seem like it, from a high level, but I see the ecosystem doing a lot of great things. We’re getting to the point where we’re doing a lot of work to shift security left, moving security more towards development practices, and, for example, starting to try to understand what software makes up our applications (SBOMs and the like). This is good progress. Lots more still to be done, and maybe it can never be “done done”, but good progress nonetheless.I recently came back to the security world, and started working at a great organization called Sysdig–a company that is doing some great work to shift security left while still watching right (i.e. runtime) and was built from the ground up for modern workloads and modern infrastructure.After a long time off from full time security work there are many newly invented acronyms that I need to learn, which is the real point of this post.AcronymsHere’s a few that I’ve come across so far.  NOTE: I should say that it’s quite possible I’ve got some things wrong. Let me know if I do. I’ll try to keep this up to date.GRC - Governance, Risk and ComplianceCIEM - Cloud Infrastructure Entitlements ManagementKSPM - Kubernetes Security Posture ManagementCSPM - Cloud Security Posture ManagementSOAR - Security Orchestration Automation and ResponseSIEM - Security Information and Event ManagementCNAPP - Cloud Native Application Protection PlatformXDR - eXtended Detection and ResponseCWPP - Cloud Workload Protection PlatformCASB - Cloud Access Security BrokerRASP - Runtime Application Self-ProtectionSAST - Static Application Security TestingDAST - Dynamic Application Security TestingIAST - Interactive Application Security TestingIOC - Indicator of CompromiseTDR - Threat Detection and ResponseTI - Threat IntelligenceCVSS - Common Vulnerability Scoring SystemDART - Detection and Response TeamCDR - Cloud Detection and ResponseVM - Vulnerability Management (not Virtual Machine)MDR - Managed Detection and ResponseCMDB - Configuration Management DatabaseDLP - Data Loss PreventionCloud Security Podcast with Anna BelakTo get better insight into security acronyms than I can provide, have a listen to this podcast:  EP94 Meet Cloud Security Acronyms with Anna BelakConclusionTechnology is complicated. We need ways to simplify and understand what all this complex technology does, what it means, and how it works. This is why organizations like Gartner exist. They create functional areas and categories such as “Cloud Security and Posture Management” to help reduce the cognitive load of the vast, vast security ecosystem. In a lot of ways they provide an important function.However, I think it’s paramount to understand that these acronyms and categories are not static, and in some cases not even accurate as to what end users need or are already doing. These acronyms change over time. They come into existence, and they disappear. Sometimes they are popular, other times not so much. They are adjusted over time. They merge and they split apart. I expect that we will see considerable change in these major categories, especially the ones that exist in fast moving areas like modern applications and public clouds as we, as an industry, better understand what problems we have and how best to solve them. On the one hand this might be obvious, but on the other sometimes we put too much faith in these categories."
    } ,
  
    {
      "title"    : "Homelab - Hardware and Layout",
      "category" : "",
      "tags"     : "",
      "url"      : "/2022/06/06/homelab.html",
      "date"     : "2022-06-06 00:00:00 -0400",
      "content"  : "I’ve got (what I think) is a fairly substantial, though definitely aging, homelab. I thought I’d write a bit of a post on what it consists of.HardwareCurrent setup:  3x ESXI hosts - each are a 1U Supermicro server with 192GB memory on a X9DRI-LN4F+ motherboard  1x Network storage server - A 2U Supermicro, 64GB of memory, X9DRI-LN4F+ motherboard  Intel Xeon CPU E5-2650 2.00GHz CPUs (old!)  1x Mikrotik 24 port CRS326-24S+2Q+RM  1x Mikrotik 24 port CRS326-24G-2S+RM  Battery backup - Cyberpower 1500VA  Firewall - Protectli Vault 6 Port running OpenBSDExtra hardware:  Juniper EX3300  Many server components: disks, motherboards, memory, etcStorage ServerThis is an Ubuntu 18.04 server using ZFS on linux, with four mirrored spinning disks and a zlog disk. Brilliantly and originally I’ve named the zpool tank.$ zpool status  pool: tank state: ONLINE  scan: scrub repaired 0B in 23h24m with 0 errors on Sun May  8 23:48:18 2022config:    NAME                        STATE     READ WRITE CKSUM    tank                        ONLINE       0     0     0      mirror-0                  ONLINE       0     0     0        wwn-0x5000cca221c07016  ONLINE       0     0     0        wwn-0x5000cca221c8e492  ONLINE       0     0     0      mirror-1                  ONLINE       0     0     0        wwn-0x5000cca221c8e026  ONLINE       0     0     0        wwn-0x5000cca221db40d0  ONLINE       0     0     0    logs      wwn-0x55cd2e404c0f5d34    ONLINE       0     0     0Also I have a NVMe disk that has 1.8T of storage, which is setup with XFS. I’ve called this one Mammoth. I’m surprised this drive is still working as it’s just a Western Digital Blue disk that I thought would quickly wear out, but it’s still going strong. I put it into a generic PCI adapter and it’s been working well…so far (though again, I expect it to fail at some point).$ mount | grep xfs/dev/nvme0n1 on /mammoth/1 type xfs (rw,relatime,attr2,inode64,noquota)The ZFS and XFS storage is exported via NFS to the ESXI hosts.ESXI HostsNot much special here, just some older 1U nodes. vCenter is running as a VM on these hosts. The E5-2650 CPUs likely won’t work with vSphere 8.Each node also has a 1TB SSD drive in it, but I don’t use these much. If I’m doing a nested deployment of vSphere, I’ll put the nested, virtualized ESXI hosts on these disks, and manually distribute them across the three hosts, but otherwise I don’t currently use them.Initially I used inexpensive flash USB drives to run the ESXI OS, but that stopped working and I had to install ESXI onto local drives, which right now are spinning disks that I had been using when I was testing out VSAN.NetworkingMikrotikAt this time, for the lab, I’m using Mikrotik network switches, mostly because they are extremely low power and incredibly quiet. Two switches is like 1/3 the watts of another vendor’s single switch.I have several other switches that could be in place, for example a Juniper EX3300 with 24x 1GB ports and 4x 10GB ports, but while it has better performance, it’s louder and adds another amp of power usage.  NOTE: Please take a look at the performance testing for Mikrotik switches and note the switching performance. They might not work for you. :)[admin@MikroTik] &gt; /system resource print                   uptime: 4w2h33m31s                  version: 6.42.12 (long-term)               build-time: Feb/12/2019 08:23:13         factory-software: 6.41              free-memory: 480.5MiB             total-memory: 512.0MiB                      cpu: ARMv7                cpu-count: 1            cpu-frequency: 800MHz                 cpu-load: 0%           free-hdd-space: 3896.0KiB          total-hdd-space: 16.0MiB  write-sect-since-reboot: 53897         write-sect-total: 102370               bad-blocks: 0%        architecture-name: arm               board-name: CRS326-24G-2S+                 platform: MikroTikThe 1GB switch is doing all the routing, the VLAN gateways live here. As well, it does DHCP for the services that need it.Configuring the Mikrotiks is a bit unusual compared to other major switch vendors, e.g. Juniper. There’s no commit/rollback model.[admin@MikroTik] &gt; /interface bridge port printFlags: X - disabled, I - inactive, D - dynamic, H - hw-offload #     INTER... BRIDGE   HW  PVID PR  PATH-COST INTERNA...    HORIZON 0   H ;;; defconf       ether1   bridge   yes    1 0x         10         10       noneSNIP!The bridge model is a bit unusual as well, note the “HW: yes” column.Juniper EX3300My EX3300 would likely make more sense as a switch in this environment, as I would just need the single switch as it has the four 10 gig connections (perfect for me with my four nodes), and it’s wire speed, but ultimately I just liked that the Mikrotiks are quieter and lower power, and, so far, I haven’t run into any performance problems (that I’m aware of).I’ve had both the EX3300 and the Mikrotiks in place in different versions of the lab. If performance was a key, then I would definitely use the EX3300, and accept the additional volume and power use. Honestly, the EX3300 is the perfect lab switch, but I’m not using it right now.Next time I rebuild the lab, I might use the EX3300. :)FirewallI run a small fanless OpenBSD based firewalling device that has six interfaces that I used to segregate my various home networks. I’m an OpenBSD fan, so I’m always looking for a spot to put some OpenBSD.  NOTE: This device runs really hot. I’ve read that some people change the thermal paste on these kinds of systems, though I have not done that. I’m expecting this device to fail at some point just due to being so high temperature all the time. Perhaps it was not a wise investment. That remains to be seen.Six interfaces sounds like a lot, but if you’re physically separating networks out, it’s just the right amount.# uname -aOpenBSD firewall 6.9 GENERIC.MP#473 amd64# ifconfig | grep emem0: flags=8843&lt;UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500em1: flags=8843&lt;UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500em2: flags=8843&lt;UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500em3: flags=8843&lt;UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500em4: flags=8843&lt;UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500em5: flags=8802&lt;BROADCAST,SIMPLEX,MULTICAST&gt; mtu 1500Only certain networks are allowed to talk to other networks: basic segregation.Battery BackupPower goes off in Toronto fairly regularly, one every three or four months, but usually it’s only a blip…maybe a minute or two of power failure. For a while I just kept restarting everything, but finally I experienced a corruption issue and had to rebuild, so it was time for a battery backup. Better to spend a couple hundred dollars than the time rebuilding and restarting. With this in place my nodes have not gone down once, though it would only last for maybe 10, 15 minutes, so if it’s an extended power loss, then everything will go down.DNS, NTP - LaptopI use an old IBM laptop for DNS and NTP. Because, as a laptop, it has a battery in it, this laptop has been up for a long, long time…years in fact:$ uptime 21:26:05 up 952 days, 23:24,  2 users,  load average: 0.00, 0.00, 0.00That’s right, 952 days. Insecure, yes, but wow, this is some serious uptime. The screen has stopped working, but I can still ssh in. I don’t know how this thing is still working, but at this point I have to leave it just to see how long it will continue to stay up!EnvironmentAll these servers and switches are installed into a medium sized enclosed rack that I bought off of Kijiji. I don’t run any air conditioning at all, and these servers live in my cold room, which isn’t that cold, and can easily be 30C or higher in the summer, but the whole system just keeps chugging along. It’s also dusty in the basement, and again, things–surprisingly–just keep working. Maybe once a year I shut most things down and clean everything off.What is this lab running?  vSphere 7  vSphere with Tanzu, using NSX Advanced Loadbalancer (AVI)  Tanzu Kubernetes Grid - internet restricted and non-restricted deployments  Many Kubernetes clusters (thanks to TKG and vSphere with Tanzu)  Used to run NSX, but it’s currently not deployed in this version of the labConclusionThis all uses 6 or 7 AMPs of power and has been running for well over three years. The Supermicro’s just keep running, no matter how dusty or hot. One major issue is that if I continue to run a vSphere lab, it’s unlikely the CPUs in these nodes will be supported. So to continue with vSphere 8 would be a major investment."
    } ,
  
    {
      "title"    : "Creating CUSTOM Secure Software Supply Chains with Tanzu",
      "category" : "",
      "tags"     : "",
      "url"      : "/2022/05/11/creating-custom-secure-software-supply-chains.html",
      "date"     : "2022-05-11 00:00:00 -0400",
      "content"  : "In the last post I looked at creating secure software supply chains with the Tanzu Application Platform (TAP). In that post I used a default supply chain. But what if we wanted to create our own, custom supply chain instead of using the “out of the box” examples provided with the platform?Quickly…What is the Tanzu Application Platform?TAP is:  VMware Tanzu Application Platform is a modular, application-aware platform that provides a rich set of developer tooling and a prepaved path to production to build and deploy software quickly and securely on any compliant public cloud or on-premises Kubernetes cluster. - VMware TanzuHere’s a key point:  It can run in any compliant Kubernetes clusterBut enough about TAP, let’s build a custom supply chain.Creating a Custom Supply ChainOk, so we have two supply chains by default in the TAP install.$ k get clustersupplychains.carto.runNAME                 READY   REASON   AGEbasic-image-to-url   True    Ready    5d2hsource-to-url        True    Ready    5d2hWe can also use the tanzu CLI to get the same information.PS C:\\Program Files\\tanzu&gt; tanzu apps cluster-supply-chain listNAME                         READY   AGE   LABEL SELECTORscanning-image-scan-to-url   Ready   23m   apps.tanzu.vmware.com/workload-type=websource-test-scan-to-url      Ready   23m   apps.tanzu.vmware.com/has-tests=true,apps.tanzu.vmware.com/workload-type=webThe reason we have these two is that when TAP was installed the TAP values file was configured with the below option.supply_chain: basicThere are other options to provide “out of the box templates”. That said, we expect that most organizations will build their own supply chains using our platform and its various building blocks.For the purposes of this blog post I start with the two basic chains and I’d like to add another custom chain, an extension of source-to-url.Creating a Custom Supply ChainLet’s say my goal is:  Take the “source-to-url” chain, create a new chain, and add “image scanning” to it, so that the image that is created is also scanned to see if there are any CVEsI’m going to grab the “source-to-url” chain and edit it.$ k neat get  -- clustersupplychains.carto.run source-to-urlapiVersion: carto.run/v1alpha1kind: ClusterSupplyChainmetadata:  annotations:    kapp.k14s.io/identity: v1;/carto.run/ClusterSupplyChain/source-to-url;carto.run/v1alpha1  name: source-to-urlspec:  params:  - default: main    name: gitops_branch  - default: supplychain    name: gitops_user_name  - default: supplychain    name: gitops_user_email  - default: supplychain@cluster.local    name: gitops_commit_message  - default: \"\"    name: gitops_ssh_secret  resources:  - name: source-provider    params:    - name: serviceAccount      value: default    - name: gitImplementation      value: go-git    templateRef:      kind: ClusterSourceTemplate      name: source-template  - name: deliverable    params:    - name: registry      value:        repository: tap-inner-loop-1-1-full/supply-chain        server: somerepo.example.com    templateRef:      kind: ClusterTemplate      name: deliverable-template  - name: image-builder    params:    - name: serviceAccount      value: default    - name: clusterBuilder      value: default    - name: registry      value:        repository: tap-inner-loop-1-1-full/supply-chain        server: somerepo.example.com    sources:    - name: source      resource: source-provider    templateRef:      kind: ClusterImageTemplate      name: kpack-template  - images:    - name: image      resource: image-builder    name: config-provider    params:    - name: serviceAccount      value: default    templateRef:      kind: ClusterConfigTemplate      name: convention-template  - configs:    - name: config      resource: config-provider    name: app-config    templateRef:      kind: ClusterConfigTemplate      name: config-template  - configs:    - name: config      resource: app-config    name: config-writer    params:    - name: serviceAccount      value: default    - name: registry      value:        repository: tap-inner-loop-1-1-full/supply-chain        server: somerepo.example.com    templateRef:      kind: ClusterTemplate      name: config-writer-template  selector:    apps.tanzu.vmware.com/workload-type: webNext, let’s change it so that it looks like the below.apiVersion: carto.run/v1alpha1kind: ClusterSupplyChainmetadata:  name: source-to-url-with-image-scanspec:  params:  - default: main    name: gitops_branch  - default: supplychain    name: gitops_user_name  - default: supplychain    name: gitops_user_email  - default: supplychain@cluster.local    name: gitops_commit_message  - default: \"\"    name: gitops_ssh_secret  resources:  - name: source-provider    params:    - name: serviceAccount      value: default    - name: gitImplementation      value: go-git    templateRef:      kind: ClusterSourceTemplate      name: source-template  - name: deliverable    params:    - name: registry      value:        repository: tap-inner-loop-1-1-full/supply-chain        server: somerepo.example.com    templateRef:      kind: ClusterTemplate      name: deliverable-template  - name: image-builder    params:    - name: serviceAccount      value: default    - name: clusterBuilder      value: default    - name: registry      value:        repository: tap-inner-loop-1-1-full/supply-chain        server: somerepo.example.com    sources:    - name: source      resource: source-provider    templateRef:      kind: ClusterImageTemplate      name: kpack-template  #scan-image  - name: scan-image    images:    - name: image      resource: image-builder    templateRef:      kind: ClusterImageTemplate      name: image-scanner-template        - images:    - name: image      resource: scan-image    name: config-provider    params:    - name: serviceAccount      value: default    templateRef:      kind: ClusterConfigTemplate      name: convention-template  - configs:    - name: config      resource: config-provider    name: app-config    templateRef:      kind: ClusterConfigTemplate      name: config-template  - configs:    - name: config      resource: app-config    name: config-writer    params:    - name: serviceAccount      value: default    - name: registry      value:        repository: tap-inner-loop-1-1-full/supply-chain        server: somerepo.example.com    templateRef:      kind: ClusterTemplate      name: config-writer-template  selector:    apps.tanzu.vmware.com/workload-type: web-image-scan  NOTE: The selector is now “web-image-scan”.At this point we have a diff something like this. All that’s happened is the insertion of the “scan-image” block into the chain, and changed the name to make it unique.$ git diff source-to-url-original.yml curtis-source-to-url-with-image-scan.ymldiff --git a/source-to-url-original.yml b/curtis-source-to-url-with-image-scan.ymlindex 06e8dff..3c22bac 100644--- a/source-to-url-original.yml+++ b/curtis-source-to-url-with-image-scan.yml@@ -1,12 +1,7 @@ apiVersion: carto.run/v1alpha1 kind: ClusterSupplyChain metadata:-  annotations:-    kapp.k14s.io/identity: v1;/carto.run/ClusterSupplyChain/source-to-url;carto.run/v1alpha1-  labels:-    kapp.k14s.io/app: \"1651760734110088811\"-    kapp.k14s.io/association: v1.4e1a1027543b1d663294132ebfdd4f33-  name: source-to-url+  name: source-to-url-with-image-scan spec:   params:   - default: main@@ -54,9 +49,17 @@ spec:     templateRef:       kind: ClusterImageTemplate       name: kpack-template-  - images:+  #scan-image+  - name: scan-image+    images:     - name: image       resource: image-builder+    templateRef:+      kind: ClusterImageTemplate+      name: image-scanner-template      +  - images:+    - name: image+      resource: scan-image     name: config-provider     params:     - name: serviceAccount@@ -86,4 +89,4 @@ spec:       kind: ClusterTemplate       name: config-writer-template   selector:-    apps.tanzu.vmware.com/workload-type: web\\ No newline at end of file+    apps.tanzu.vmware.com/workload-type: web-image-scan\\ No newline at end of fileThe new chain looks like this in Cartographer’s live editor. As you can see, there is now “scan-image” in the chain.Which, of course, is different from the non-image scan version. Note how there is no “scan image” box.Checkout the image scanner template.$ k neat get -- clusterimagetemplates.carto.run image-scanner-template -oyamlapiVersion: carto.run/v1alpha1kind: ClusterImageTemplatemetadata:  annotations:    kapp.k14s.io/identity: v1;/carto.run/ClusterImageTemplate/image-scanner-template;carto.run/v1alpha1    kapp.k14s.io/original-diff-md5: c6e94dc94aed3401b5d0f26ed6c0bff3  labels:    kapp.k14s.io/app: \"1651760721125747499\"    kapp.k14s.io/association: v1.7d6419553fe4d29522bcc6dc11d61feb  name: image-scanner-templatespec:  imagePath: .status.compliantArtifact.registry.image  ytt: |    #@ load(\"@ytt:data\", \"data\")    #@ def merge_labels(fixed_values):    #@   labels = {}    #@   if hasattr(data.values.workload.metadata, \"labels\"):    #@     labels.update(data.values.workload.metadata.labels)    #@   end    #@   labels.update(fixed_values)    #@   return labels    #@ end    apiVersion: scanning.apps.tanzu.vmware.com/v1beta1    kind: ImageScan    metadata:      name: #@ data.values.workload.metadata.name      labels: #@ merge_labels({ \"app.kubernetes.io/component\": \"image-scan\" })    spec:      registry:        image: #@ data.values.image      scanTemplate: private-image-scan-template      scanPolicy: scan-policyIf we look at the above definition, we see that it’s using “private-image-scan-template” of “kind: ImageScan”.Let’s look at those.$ k get scantemplates.scanning.apps.tanzu.vmware.comNAME                          AGEblob-source-scan-template     5d2hprivate-image-scan-template   5d2hpublic-image-scan-template    5d2hpublic-source-scan-template   5d2hNow the private scan template…$ k neat get -- scantemplates.scanning.apps.tanzu.vmware.com private-image-scan-template -oyamlapiVersion: scanning.apps.tanzu.vmware.com/v1beta1kind: ScanTemplatemetadata:  name: private-image-scan-template  namespace: defaultspec:  template:    containers:    - args:      - -c      - ./image/copy-docker-config.sh /secret-data &amp;&amp; ./image/scan-image.sh /workspace        scan.xml true      command:      - /bin/bash      image: registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:d3a8f3cae0db15e416e805dc598223f93059c3a295cbf33f1409bc6cb9a9709c      imagePullPolicy: IfNotPresent      name: scanner      resources:        limits:          cpu: 1000m        requests:          cpu: 250m          memory: 128Mi      volumeMounts:      - mountPath: /.docker        name: docker        readOnly: false      - mountPath: /workspace        name: workspace        readOnly: false      - mountPath: /secret-data        name: registry-cred        readOnly: true    imagePullSecrets:    - name: scanner-secret-ref    restartPolicy: Never    securityContext:      runAsNonRoot: true    volumes:    - name: docker    - name: workspace    - name: registry-cred      secret:        secretName: registry-credentialsOk, great. To do image scanning we’ll need a scan policy.Scan PolicyNext we need a scan policy.  NOTE: We are only looking for “Critical” vulnerabilities. Those will fail, everything else will pass the scan test.apiVersion: scanning.apps.tanzu.vmware.com/v1beta1kind: ScanPolicymetadata:  name: scan-policyspec:  regoFile: |    package policies    default isCompliant = false    # Accepted Values: \"Critical\", \"High\", \"Medium\", \"Low\", \"Negligible\", \"UnknownSeverity\"    violatingSeverities := [\"Critical\"]    ignoreCVEs := []    contains(array, elem) = true {      array[_] = elem    } else = false { true }    isSafe(match) {      fails := contains(violatingSeverities, match.Ratings.Rating[_].Severity)      not fails    }    isSafe(match) {      ignore := contains(ignoreCVEs, match.Id)      ignore    }    isCompliant = isSafe(input.currentVulnerability)That needs to be installed.$ k create -f image-scan-policy.ymlNow it’s available to use.Install and UseLet’s load that new, custom supply chain into TAP/k8s.$ k create -f curtis-source-to-url-with-image-scan.ymlclustersupplychain.carto.run/source-to-url-with-image-scan createdVoila:$ k get clustersupplychains.carto.runNAME                            READY   REASON   AGEbasic-image-to-url              True    Ready    5d2hsource-to-url                   True    Ready    5d2hsource-to-url-with-image-scan   True    Ready    19sNow to deploy the app.  NOTE: I’m flipping between an Ubuntu WSL terminal and a Powershell terminal. Here I’m using Powershell to run the tanzu CLI. Note the type is “web-image-scan”.$Env:TAP_DEV_NAMESPACE = \"default\"tanzu apps workload create tanzu-java-web-app-with-image-scan `  --git-repo https://github.com/sample-accelerators/tanzu-java-web-app `  --git-branch main `  --type web-image-scan `  --label app.kubernetes.io/part-of=tanzu-java-web-app-with-image-scan `  --label tanzu.app.live.view=true `  --label tanzu.app.live.view.application.name=tanzu-java-web-app-with-image-scan `  --annotation autoscaling.knative.dev/minScale=1 `  --namespace $env:TAP_DEV_NAMESPACE `  --yesI can check the results of the scan.$ k logs scan-tanzu-java-web-app-with-image-scan-qr9q4--1-9b4w9 | grep severity | sort | uniq -c     27               &lt;v:severity&gt;Low&lt;/v:severity&gt;      8               &lt;v:severity&gt;Medium&lt;/v:severity&gt;Many low, a few medium.Now we have imagescans:$ k get imagescans.scanning.apps.tanzu.vmware.comNAME                                 PHASE       SCANNEDIMAGE                                                                                                            AGE   CRITICAL   HIGH   MEDIUM   LOW   UNKNOWN   CVETOTALtanzu-java-web-app-with-image-scan   Completed   somerepo.example.com/tap-inner-loop-1-1-full/supply-chain/tanzu-java-web-app-with-image-scan-default@sha256:bb0da26d42537abaa7a7f02afac8eb77387c42524fbd413a265d716934ec2f4c   20m   0          0      3        12    0         15The app is up and running.PS C:\\Users\\curtis&gt; curl.exe http://tanzu-java-web-app-with-image-scan-default.apps.example.comGreetings from Spring Boot + Tanzu!ConclusionAt this point we’ve created a custom supply chain by adding image scanning to the default source-to-url chain.This is a simple example, but you can see how powerful, and modular, TAP is.Additional Links and Resources  Tanzu Application Platform’s OOTB Supply Chain with Testing and Scanning Events  Deep-dive on Tanzu Application Platform’s OOTB Supply Chain  SSSC learning path provided by VMware Tanzu."
    } ,
  
    {
      "title"    : "Secure Software Supply Chains and the Tanzu Application Platform",
      "category" : "",
      "tags"     : "",
      "url"      : "/2022/05/10/creating-secure-software-supply-chains-with-tanzu.html",
      "date"     : "2022-05-10 00:00:00 -0400",
      "content"  : "If you are a company that makes software, then you have a software supply chain, whether you want one or not. Building software is challenging, even without thinking about where all the underlying dependencies and other software comes from, and what’s in it, never mind cataloguing and checksumming it all, and being able to replace it within a few hours.This is all hard work, work that many companies spend thousands and thousands of engineering hours on trying to build themselves, often unsuccessfully. Other companies simply don’t have the people power–the time, the resources–to even try to implement secure pipelines.What’s a secure software supply chain?A “secure software supply chain” (SSSC) is…  …a fancy way of saying “we know all the components that went into building and deploying this software and trust those components.” It also includes the actual CI/CD pipeline that you trust and that’s resistant to third parties including malicious code, as we’ve seen happen in recent years. - Tanzu BlogHere are some outcomes organizations are looking for with regards to SSSC:  “We need to be able to deploy code to our staging and production environments reliably every time”  “When there is a CVE for one of our applications or dependencies, we need to be able to remediate the problem within 24 hours”  “We must ensure our software is validated during the build process and that it is built upon known secure images”If you want to find out more about secure software supply chains, take a look at this learning path provided by VMware Tanzu.The Tanzu Application PlatformI’ve talked about the Tanzu Application Platform (TAP) on this blog before. Suffice it to say that Kubernetes can do more than just containers, we can teach to do anything, and TAP is a way to show Kubernetes how to manage and secure applications; to turn it into more than just a “container orchestration engine”.With TAP we use Kubernetes as a base platform that we add on to, and then turn into a full blown application platform…a modular system that understands how to deploy, manage and secure applications on its own, without having to be told what to do (unless we want to).TAP Supply ChainsI’ve got the Tanzu Application Platform deployed into a single cluster (in this case Minikube running on my Windows workstation). It’s has a couple of software supply chains installed by default.As you can see, I’m asking the Kubernetes API what it knows about “clustersupplychains”, i.e. TAP and its components are NATIVE to Kubernetes–we’ve taught Kubernetes how to do supply chains (and more).$ k get clustersupplychains.carto.runNAME                 READY   REASON   AGEbasic-image-to-url   True    Ready    4d23hsource-to-url        True    Ready    4d23hWith TAP 1.1 you get a few default supply chains, e.g. basic-image-to-url and source-to-url.source-to-url is the easiest one to understand. This supply chain does the following:  Watch a git repository  When there are changes, build a new image using that code (no Dockerfile anywhere)  Deploy the new imageThis all happens from within Kubernetes, and can be across multiple clusters, each with specific duties. (In this case I just have a single cluster though.)ClusterSupplyChainsLet’s look at the YAML that defines the ClusterSupplyChain.  NOTE: I’m trying out the “neat” plugin for kubectl here. It removes some of the extra things you see when pulling the YAML from k8s. A few other things I removed by hand.$ k neat get -- clustersupplychain source-to-urlapiVersion: carto.run/v1alpha1kind: ClusterSupplyChainmetadata:  name: source-to-urlspec:  params:  - default: main    name: gitops_branch  - default: supplychain    name: gitops_user_name  - default: supplychain    name: gitops_user_email  - default: supplychain@cluster.local    name: gitops_commit_message  - default: \"\"    name: gitops_ssh_secret  resources:  - name: source-provider    params:    - name: serviceAccount      value: default    - name: gitImplementation      value: go-git    templateRef:      kind: ClusterSourceTemplate      name: source-template  - name: deliverable    params:    - name: registry      value:        repository: tap-inner-loop-1-1-full/supply-chain        server: somerepo.example.com    templateRef:      kind: ClusterTemplate      name: deliverable-template  - name: image-builder    params:    - name: serviceAccount      value: default    - name: clusterBuilder      value: default    - name: registry      value:        repository: tap-inner-loop-1-1-full/supply-chain        server: somerepo.example.com    sources:    - name: source      resource: source-provider    templateRef:      kind: ClusterImageTemplate      name: kpack-template  - images:    - name: image      resource: image-builder    name: config-provider    params:    - name: serviceAccount      value: default    templateRef:      kind: ClusterConfigTemplate      name: convention-template  - configs:    - name: config      resource: config-provider    name: app-config    templateRef:      kind: ClusterConfigTemplate      name: config-template  - configs:    - name: config      resource: app-config    name: config-writer    params:    - name: serviceAccount      value: default    - name: registry      value:        repository: tap-inner-loop-1-1-full/supply-chain        server: somerepo.example.com    templateRef:      kind: ClusterTemplate      name: config-writer-template  selector:    apps.tanzu.vmware.com/workload-type: webNow we can take that output and plug it into the Cartographer live editor, and it will show us a nice diagram which depicts the supply chain flow.  NOTE: Cartographer is the open source project that underlies some of TAP…it’s the k8s native component that ties all the disparate, separate functions together into a pipeline.Here’s a larger version. The arrows mean “depends on”, as opposed to the directional flow.If you look into the YAML, we can see the first section under resources is source-provider.SNIP!  resources:  - name: source-provider    params:    - name: serviceAccount      value: default    - name: gitImplementation      value: go-git    templateRef:      kind: ClusterSourceTemplate      name: source-templateSNIP!The kind “ClusterSourceTemplate” exists in the cluster.$ k get clustersourcetemplates.carto.runNAME                       AGEdelivery-source-template   4d23hsource-scanner-template    4d23hsource-template            4d23htesting-pipeline           4d23hAbove we can see there is one called “source-template”.$ k neat get -- clustersourcetemplates.carto.run source-template -oyamlapiVersion: carto.run/v1alpha1kind: ClusterSourceTemplatemetadata:  name: source-templatespec:  params:  - default: default    name: serviceAccount  - default: go-git    name: gitImplementation  revisionPath: .status.artifact.revision  urlPath: .status.artifact.url  ytt: |    #@ load(\"@ytt:data\", \"data\")    #@ def merge_labels(fixed_values):    #@   labels = {}    #@   if hasattr(data.values.workload.metadata, \"labels\"):    #@     labels.update(data.values.workload.metadata.labels)    #@   end    #@   labels.update(fixed_values)    #@   return labels    #@ end    #@ def param(key):    #@   if not key in data.values.params:    #@     return None    #@   end    #@   return data.values.params[key]    #@ end    ---    #@ if hasattr(data.values.workload.spec.source, \"git\"):    apiVersion: source.toolkit.fluxcd.io/v1beta1    kind: GitRepository    metadata:      name: #@ data.values.workload.metadata.name      labels: #@ merge_labels({ \"app.kubernetes.io/component\": \"source\" })    spec:      interval: 1m0s      url: #@ data.values.workload.spec.source.git.url      ref: #@ data.values.workload.spec.source.git.ref      gitImplementation: #@ data.values.params.gitImplementation      ignore: |        !.git      #@ if/end param(\"gitops_ssh_secret\"):      secretRef:        name: #@ param(\"gitops_ssh_secret\")    #@ end    #@ if hasattr(data.values.workload.spec.source, \"image\"):    apiVersion: source.apps.tanzu.vmware.com/v1alpha1    kind: ImageRepository    metadata:      name: #@ data.values.workload.metadata.name      labels: #@ merge_labels({ \"app.kubernetes.io/component\": \"source\" })    spec:      serviceAccount: #@ data.values.params.serviceAccount      interval: 1m0s      image: #@ data.values.workload.spec.source.image    #@ endA lot of the above YAML is a “template” built with Carvel’s YAML templating tool, ytt, which may look a little unusual to those who haven’t seen ytt before.From this section of the YAML, it’s somewhat obvious that this is a “if/then” template, and IF the source comes from GIT, then use “kind: GitRepository”. (And if it’s an IMAGE then use ImageRepository.)SNIP!    #@ if hasattr(data.values.workload.spec.source, \"git\"):    apiVersion: source.toolkit.fluxcd.io/v1beta1    kind: GitRepositorySNIP!The demo app, tanzu-java-web-app, is using a git repository, as defined in its YAML manifest / k8s object. Note the “source.git” section.$ k neat get -- workloads.carto.run tanzu-java-web-app  -oyamlapiVersion: carto.run/v1alpha1kind: WorkloadSNIP!  source:    git:      ref:        branch: main      url: https://github.com/sample-accelerators/tanzu-java-web-appSo there must be a manifest for that…yep.$ k get gitrepositories.source.toolkit.fluxcd.ioNAME                 URL                                                         READY   STATUS                                   AGEtanzu-java-web-app   https://github.com/sample-accelerators/tanzu-java-web-app   True    Fetched revision: main/f5cf96ef23f3fddba94616112dfad882882aabe4   4d23hNote the above is “…fluxcd.io”. TAP is using parts of the open source flux project, in this case specifically the flux capability to get code from a git repository.We can continue this k8s sleuthing to follow the entire software supply chain. So far we’ve just looked at how source code is retrieved.An important point is that this is all programmable, so much so that we can simply use the Cartographer live editor and paste in the YAML definition and it can easily produce an image.This is also what is visualized in the TAP web interface.ConclusionHere we’ve sleuthed through one stage of one of the supply chains, and looked at how we can visualize the chains with Cartographer’s live editor and the TAP GUI. In the next post we’ll create a custom supply chain."
    } ,
  
    {
      "title"    : "Deploy Tanzu Kubernetes Grid in an Offline/Airgapped Environment",
      "category" : "",
      "tags"     : "",
      "url"      : "/2022/05/03/offline-airgapped-tanzu-kubernetes-grid.html",
      "date"     : "2022-05-03 00:00:00 -0400",
      "content"  : "Tanzu Kubernetes Grid (TKG) is VMware’s distribution of Kubernetes that can be deployed into vSphere and public clouds like Azure and AWS. Sometimes customers prefer that the deployment and management of TKG is done “offline” without needing to obtain any artifacts of the deployment from Internet hosted resources, that the deployment is self-contained.This is a fairly common requirement, especially in organizations that consider segmentation of resources important, and it’s usually done at the network level, i.e. “network segmentation”, which has become even more popular as of late, around terms like microsegmentation and zero-trust networking.Documentation and Links  VMware Tanzu TKG - Internet Restricted Environments  Downloading TKG 1.5.3 CLIsOffline EnvironmentFirewall rules:  tkg-offline-* networks: no internet access, all packets dropped at the edge firewall  Office network can connect to anything, i.e. the host that copies the container images from VMware to the internal Harbor instance  Anything on lab switch is available to tkg-offline-*, e.g. vCenter, DNS, Harbor, no firewalling, i.e. the TKG management clusters can talk to vCenter, etcHardware, software:  TKG &gt;= 1.5.3  3 ESXI hosts  Running vSphere 7U2  Enough resources for TKG  A Linux + Docker instance to download the container images used in deployment, and to run the tanzu CLI from, as well as certain Carvel CLIsA Word on Container ImagesWhen we think of container images we think of running an application, like, oh I don’t know, nginx. But container images aren’t only used for software, they can also be used to store configuration information. That is something that TKG does a lot of. As the industry gets more and more mature around the use of container images, we will do more with them, and that will cause us to simply have more of them. Smaller images, but more of them. My point here is that the number of container images that TKG could use can seem considerable, around 500 or so, but most of them are quite small, on the order of a few megabytes or even kilobytes, and often contain configuration information like BOMs.In an online environment, one that is connected to the Internet, we probably wouldn’t even notice how many images are used by TKG when it’s being deployed. However, in an offline environment, one in which we need to copy these images, these artifacts, we have an opportunity to see first hand exactly how many images there are, and it can be surprising to some. I’d say there are about 500 total container images needed for ALL of TKG, and this is to support not only TKG itself, but also EVERY Kuberentes version that we support, which is many.A Word on TLS CertificatesIn offline environments organizations almost always also use self-signed certificates, or certificates that are not part of the typical bundle found in operating systems, mostly for browsers. When deploying TKG many container images are pulled from the internal container image registry, in this case Harbor, and that Harbor instance will have a custom TLS certificate.This means we need to ensure that TLS certificate, or certificate authority, is deployed into the TKG nodes, the virtual machines that underlie the Kubernetes clusters. Along with the various image artifacts, this is a big part of managing the offline deployment.Relocating Container ImagesOne of the things we have to do is relocate the necessary container images from VMware’s Internet available registry to the organizations offline registry. This requires some sort of intermediary system, a jumpbox/bastion host or similar.Currently our official docs provide a couple of scripts to do perform the actual relocation.First we generate a list of images to relocate.  To run this script we need to set some variables.export TKG_CUSTOM_IMAGE_REPOSITORY=\"&lt;harbor&gt;/&lt;project&gt;\"export TKG_IMAGE_REPO=\"projects.registry.vmware.com/tkg\"export TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE=&lt;base64 certificate&gt;I created a project in my Harbor called “tkg-1-5-3”.export TKG_CUSTOM_IMAGE_REPOSITORY=\"&lt;harbor&gt;/tkg-1-5-3\"I use mkcert to manage my certificates internally, so for the CA certificate I used that. You might take a different approach, but it’s the same idea.base64 -w 0 &lt; /home/curtis/.local/share/mkcert/rootCA.pemThe result of that command I put into TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE environment variable.Now I can run the script to generate the list of images to copy to the internal Harbor.$ ./gen-publish-images.sh &gt; image-copy-listThat will create this list of images. Most of the lines will be imgpkg commands. imgpkg is a CLI from the Carvel set of tools.imgpkg is used to:  Package, distribute, and relocate your Kubernetes configuration and dependent OCI images as one OCI artifact: a bundle. Consume bundles with confidence that their contents are unchanged after relocation.$ wc -l image-copy-list 4275 image-copy-listWhile that seems like a lot of lines, some of the image copy commands are duplicates. So if we sort and uniq them, there are many fewer lines.$ grep imgpkg image-copy-list | sort | uniq | wc -l568The download image script does filter out the duplicate lines, so don’t worry about doing that yourself. I’m just illustrating what it does.Then we use that list of images via another script to download each image and copy it to the Harbor instance.  NOTE: I’m using Harbor, but it could be any OCI compliant registry.$ ./download-images.sh image-copy-listDepending on the speed of your internet connection and your environment, this can take a few minutes or a couple hours.Here’s what it looks like in Harbor.Deploying TKGNow that all the images are copied to the internal container image registry, we can deploy TKG.First we need to set some configuration variables. These are the same as we set before for the image copy scripts, but now we’re going to set them up for TKG.tanzu config set env.TKG_CUSTOM_IMAGE_REPOSITORY &lt;harbor&gt;/&lt;project&gt;tanzu config set env.TKG_CUSTOM_IMAGE_REPOSITORY_SKIP_TLS_VERIFY falsetanzu config set env.TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE &lt;base64 certificate&gt;Now validate those are set.$ tanzu config get | grep TKGAt this point we can now go through the standard deployment.Usually I use the install GUI to setup the initial configuration file for the management cluster.$ tanzu mc create --ui --bind 0.0.0.0:8080 --browser noneValidating the pre-requisites...Serving kickstart UI at http://[::]:8080I connect to this host on port 8080 and fill out the install GUI, and that will generate a randomly named file in “~/.config/tanzu/tkg/clusterconfigs/” and the GUI will give you a command to run from the CLI (or you can launch it from the GUI, but I always stop the GUI process run it from the CLI)ConclusionThere are quite a few images to copy, but after that work has been done, and the CA certificate has been properly setup for use, the deployment is straightforward. So, for an offline deployment, it’s really two big steps:  Relocate all the images locally, and,  Determine what certificate is used in the Harbor instance and make sure that TKG knows about it.I’m skipping quite a few other steps here, but those steps will be the same in any TKG deployment, offline or not, such as uploading the OVA file, or deploying/obtaining a container image registry like Harbor."
    } ,
  
    {
      "title"    : "Tanzu Application Platform on a Windows Workstation",
      "category" : "",
      "tags"     : "",
      "url"      : "/2022/04/26/tanzu-application-platform-on-windows-workstation.html",
      "date"     : "2022-04-26 00:00:00 -0400",
      "content"  : "My current favorite VMware document is this:  Running Tanzu Application Platform Locally on Your LaptopIf you run through the above how-to document, which is affectionately called TAPonLAP, at the end you will have a functioning Tanzu Application Platform (TAP) profile-defined environment to use, and it’s all running locally on your personal workstation.What is TAP?TAP is:  …a modular, application-aware platform that provides a rich set of developer tooling and a prepaved path to production to build and deploy software quickly and securely on any compliant public cloud or on-premises Kubernetes cluster. - VMware TanzuUsing TAP we get all kinds of interesting and useful modular, Kubernetes native components which can work together, in concert with other tools and systems, to abstract way the complexity, and technical debt, of things like Dockerfiles and Kubernetes manifests, while also providing the ability to secure and understand workloads, even across multiple Kubernetes clusters.It’s important to understand that TAP isn’t a single binary–it’s not a monolithic thing–instead it’s a collection of tools that can work together, even across multiple Kubernetes clusters. Some TAP components will be deployed in production clusters that run the apps, other pieces will only be deployed into clusters that build images and compile code, and still more parts only need to be deployed locally for a developers inner loop (if desired). We use the concept of profiles to determine what tools get deployed where. Honestly–it’s a new paradigm for Kubernetes based application platforms.Another thing to keep in mind is that VMware Tanzu is extremely focused on Developer Experience (DX). Because of that focus we care very much about the developers “inner loop”…the things developers do with code before they commit it. With that in mind, the TAPonLAP document is focussed on building a local environment, specifically to meet the needs of that inner loop. I take it a little farther here and deploy most of the TAP components, but this won’t be commonplace, unless someone is learning about all of the components.My TAPonLAP EnvironmentMy main operating system is Linux (how do you know someone runs Linux on their desktop…just wait and they’ll tell you) but I use a Windows workstation for talking to customers with (Zoom, Teams, etc). Because I use Windows when I demonstrate software…well, that software has to run there too. So I’ve spent a fair amount time running through the TAPonLAP document, using the powershell commands, etc, overall making sure I’ve got the best local Windows development environment I can.What I have:  Windows 10  Hyper-V enabled  Minikube  32GB main memory, but using 12GB for the minikube instance  Enough disk (I use a Minikube instance with 80GB or more)  AMD Ryzen 5 3600 6-Core ProcessorTAPonLAPPlease note: I’m not going to go through the entire document and copy and paste the same commands here. However, I will detail some changes I made for my particular situation. So don’t expect to be able to follow this blog post and get TAP deployed–instead read the TAPonLAP doc, and this post, and then make your own decisions about how best to deploy TAP locally. Keep in mind it’s easy to delete or update TAP and redo the minikube install if needed. This combination of minikube and TAP is a great way to experiment.Minikube StartI use the below. Note that I am adding more disk, I think the default is ~20GB, which should be fine in most situations, but if you run the “full” TAP profile you’ll need more resources.minikube start `  --kubernetes-version='1.22.8' `  --cpus='8' --memory='12g' `  --driver='hyperv' `  --disk-size='80g'A Note About ProfilesTAP is modular. It has many components, and not all of them need to be deployed in every situation. The TAPonLAP document shows the “iterate” profile, but in this blog post I use the “full” profile.Most people using TAP locally, as part of their inner loop, would not run the full profile. Instead they are iterating on their own code before committing it, and want to make sure it mostly works before the commit, and when the CI system takes over. That is why the “iterate” profile exists.TAP Full ProfileThe TAPonLAP document shows using the iterative profile. Here’s an example full profile.  NOTE: This, of course, this will consume more resources that the “iterative” profile.  NOTE: I’m using “example.com” as the domain, which you may or may not want to do.  NOTE: You’ll have to fill in all the image registry information, users, passwords, etc.profile: fullceip_policy_disclosed: true # Installation fails if this is set to 'false'buildservice:  kp_default_repository: \"&lt;some repository&gt;/tap-inner-loop-1-1-full/build-service\"  kp_default_repository_username: \"\"  kp_default_repository_password: \"\"  tanzunet_username: \"\"  tanzunet_password: \"\"  enable_automatic_dependency_updates: truecnrs:  domain_name: apps.example.com  domain_template: \"{{.Name}}-{{.Namespace}}.{{.Domain}}\"  provider: localsupply_chain: basicootb_supply_chain_basic:  registry:    server: \"&lt;some repository&gt;\"    repository: \"tap-inner-loop-1-1-full/supply-chain\"  gitops:    ssh_secret: \"\"learningcenter:  ingressDomain: \"lc.example.com\"tap_gui:  service_type: ClusterIP  ingressEnabled: \"true\"  ingressDomain: \"example.com\"  app_config:    app:      baseUrl: http://tap-gui.example.com    catalog:      locations:        - type: url          target: https://github.com/sample-accelerators/tanzu-java-web-app/blob/main/catalog/catalog-info.yaml        - type: url          target: https://github.com/benwilcock/tap-gui-blank-catalog/blob/main/catalog-info.yaml    backend:      baseUrl: http://tap-gui.example.com      cors:        origin: http://tap-gui.example.commetadata_store:  app_service_type: LoadBalancergrype:  namespace: \"default\"  targetImagePullSecret: \"registry-credentials\"# e.g. App Accelerator specific values go under its nameaccelerator:  server:    service_type: ClusterIPcontour:  envoy:    service:      type: LoadBalancerOnce TAP deploysWe should see the below.PS C:\\Windows\\system32&gt; tanzu package installed get tap -n tap-installNAME:                    tapPACKAGE-NAME:            tap.tanzu.vmware.comPACKAGE-VERSION:         1.1.0STATUS:                  Reconcile succeededCONDITIONS:              [{ReconcileSucceeded True  }]USEFUL-ERROR-MESSAGE:HostnamesDepending on the profile in use, there may be many hostnames needed to be added to the Windows hosts file. Here’s a list of hostnames I use.  NOTE: I’m a big fan of our Learning Center tool, more about that in future posts I’m sure, so many of these hostnames are related to that project, which is part of TAP. If you don’t deploy it, then you won’t need these hostnames. It is part of the “full” profile though. That said, you can exclude packages from deployment.192.168.0.10 tap-gui.example.com tanzu-java-web-app.default.apps.example.com learning-center-guided.lc.example.com learning-center-guided-w01-s001.lc.example.com tanzu-java-web-app-default.apps.example.com learning-center-guided-w01-s001-editor.lc.example.com learning-center-guided-w01-s001-console.lc.example.com learning-center-guided-w01-s001-nginx.lc.example.com learning-center-guided-w01-s001-nginx-via-proxy.lc.example.com learning-center-guided-w01-s001-registry.lc.example.comDeploying an Application (AKA WOrkload)Using the Tanzu CLI, which itself is using Kubernetes under the hood (you could simply use the YAML that it shows as well) we can deploy a Java application.  NOTE: We are not building or managing the container image. A TAP component called the Tanzu Build Service is doing that for us. So no Dockerfiles to manage.  NOTE: Ensure you prepare the dev namespace as per the TAPonLAP document.E.g. command. Note that I’m just using the default namespace.$Env:TAP_DEV_NAMESPACE = \"default\"tanzu apps workload create tanzu-java-web-app `  --git-repo https://github.com/sample-accelerators/tanzu-java-web-app `  --git-branch main `  --type web `  --label app.kubernetes.io/part-of=tanzu-java-web-app `  --label tanzu.app.live.view=true `  --label tanzu.app.live.view.application.name=tanzu-java-web-app `  --annotation autoscaling.knative.dev/minScale=1 `  --namespace $env:TAP_DEV_NAMESPACE `  --dry-runE.g. output of that command:PS C:\\Windows\\system32&gt; tanzu apps workload create tanzu-java-web-app `&gt;&gt;   --git-repo https://github.com/sample-accelerators/tanzu-java-web-app `&gt;&gt;   --git-branch main `&gt;&gt;   --type web `&gt;&gt;   --label app.kubernetes.io/part-of=tanzu-java-web-app `&gt;&gt;   --label tanzu.app.live.view=true `&gt;&gt;   --label tanzu.app.live.view.application.name=tanzu-java-web-app `&gt;&gt;   --annotation autoscaling.knative.dev/minScale=1 `&gt;&gt;   --namespace $env:TAP_DEV_NAMESPACE `&gt;&gt;   --yesCreate workload:←[32m      1 + |---←[0m←[32m      2 + |apiVersion: carto.run/v1alpha1←[0m←[32m      3 + |kind: Workload←[0m←[32m      4 + |metadata:←[0m←[32m      5 + |  labels:←[0m←[32m      6 + |    app.kubernetes.io/part-of: tanzu-java-web-app←[0m←[32m      7 + |    apps.tanzu.vmware.com/workload-type: web←[0m←[32m      8 + |    tanzu.app.live.view: \"true\"←[0m←[32m      9 + |    tanzu.app.live.view.application.name: tanzu-java-web-app←[0m←[32m     10 + |  name: tanzu-java-web-app←[0m←[32m     11 + |  namespace: default←[0m←[32m     12 + |spec:←[0m←[32m     13 + |  params:←[0m←[32m     14 + |  - name: annotations←[0m←[32m     15 + |    value:←[0m←[32m     16 + |      autoscaling.knative.dev/minScale: \"1\"←[0m←[32m     17 + |  source:←[0m←[32m     18 + |    git:←[0m←[32m     19 + |      ref:←[0m←[32m     20 + |        branch: main←[0m←[32m     21 + |      url: https://github.com/sample-accelerators/tanzu-java-web-app←[0m←[32;1mCreated workload \"tanzu-java-web-app\"←[0mAbove you can see that the command displays the YAML that is actually deployed into Kubernetes. Note the kind.kind: WorkloadWith this command or a few lines of YAML, you can completely manage an application in Kubernetes, from source to running application.The Application is Deployed!The example Java application has now been deployed.  NOTE: By default the application is running in knative, which can scale to zero. But, in the above command we told knative that the minimum scale is 1, ie. not to scale to zero, so there will always be at least one pod running.PS C:\\Windows\\system32&gt; kubectl get podsNAME                                                   READY   STATUS      RESTARTS   AGEtanzu-java-web-app-00001-deployment-7fffdb9fcb-2s47s   2/2     Running     0          17mtanzu-java-web-app-build-1-build-pod                   0/1     Completed   0          19mtanzu-java-web-app-config-writer-69xhb-pod             0/1     Completed   0          18mIf we get the routes we can see the URL to connect to.PS C:\\Windows\\system32&gt; kubectl get routesNAME                 URL                                                  READY   REASONtanzu-java-web-app   http://tanzu-java-web-app-default.apps.example.com   TrueCurl it:PS C:\\Windows\\system32&gt; curl.exe http://tanzu-java-web-app-default.apps.example.comGreetings from Spring Boot + Tanzu!Done (for now)!Observe in the TAP GUITAP comes with a web GUI, which can be found at http://tap-gui.example.com if using the above “full” profile.We can start to “drill down” into the running application as well.We can also visualize our secure software flow, the secure supply chains that were created by default.Above we are using the default “source-to-url” supply chain.PS C:\\Windows\\system32&gt; kubectl get clustersupplychainsNAME                 READY   REASON   AGEbasic-image-to-url   True    Ready    29msource-to-url        True    Ready    29mConclusionTAP, and its modular components, are meant to run in many places in many ways, from large clusters, to minikube instances. If TAP is running locally, then the developer can use it to “iterate” on code: to write code, try deploying it, test it out, understand a bit about TAP, and then commit their code at which point TAP can again take over and build the image and run it in production, but of course, not locally, instead on a production Kubernetes cluster (or, more likely, clusters).This post just brings us through a bit of TAP, gives a few hints, and provides a starting point to understand more about the TAP paradigm.Extra NotesDon’t Forget to Run the Minikube TunnelIf you forget the below…minikube tunnelor didn’t leave it running while installing TAP, then the Ingress load balancer will stay pending and TAP won’t deploy completely.Kubeconfig into WSLThis might not be the right motion, but I prefer to use Linux to work with Kubernetes as opposed to a powershell…er shell. So here I send the kubeconfig for minikube into my WSL users kube config.  NOTE: This would, of course, destroy anything existing in your kubeconfig.  NOTE: This assumes your WSL instance is named “Ubuntu” and that your user’s name is “curtis” which is unlikely. :)PS C:\\Windows\\system32&gt; kubectl config view --flatten &gt; \\\\wsl$\\Ubuntu\\home\\curtis\\.kube\\configMinikube IPOne thing I’ve noticed is that the minikube IP will changed, say on a Windows OS reboot. So if that happens you’ll need to change the IP in the hosts file.Minikube PurgeI’ve had to purge the minikube instance once or twice. For some reason, I had one instance of minikube that was acting very slow. I didn’t take the time to try to figure out why it was acting slow. I don’t believe it was because of TAP, more likely because of some storage/disk issue.PS C:\\Windows\\system32&gt; minikube delete --purgeMemory UsageSo far it seems reasonable to run this on my desktop, though again, most of the time one would run a lighter TAP profile than the “full” profile."
    } ,
  
    {
      "title"    : "Tanzu Application Platform, knative, and a NodeJS App",
      "category" : "",
      "tags"     : "",
      "url"      : "/2022/01/12/tap-knative-nodejs.html",
      "date"     : "2022-01-12 00:00:00 -0500",
      "content"  : "Previously…In an earlier post I deployed a simple demo container image into Kubernetes via knative, and knative itself was installed as part of VMware Tanzu’s Cloud Native Runtimes, which is also Part of the Tanzu Application Platform. If that sounds like a lot, that’s OK, it is a lot.Tanzu Application Platform (provides -&gt;) Cloud Native Runtimes (provides -&gt;) knativeAll of this is using the Tanzu Application Platform and in this example is running on a GKE cluster.Deploy a NodeJS applicationIn this post I’ll deploy a NodeJS application into knative.Again, the same as the previous post, I have a GKE cluster with TAP installed.I’ve got no pods running.$ k get podsNo resources found in cnr-demo namespace.Though I still have the knative service I deployed in the last post running.$ /usr/local/bin/kn service listNAME         URL                                      LATEST             AGE   CONDITIONS   READY   REASONhello-yeti   http://hello-yeti.cnr-demo.example.com   hello-yeti-00001   17h   3 OK / 3     TrueNow I want to add a new knative service, but this time in an image that I build, and the app is running NodeJS.Here’s an example knative NodeJS app.First grab the code.$ git clone https://github.com/knative/docs.git knative-docs$ cd knative-docs/code-samples/serving/hello-world/helloworld-nodejsThere’s an index.js file. There’s really nothing to it.$ cat index.jsconst express = require('express');const app = express();app.get('/', (req, res) =&gt; {  console.log('Hello world received a request.');  const target = process.env.TARGET || 'World';  res.send(`Hello ${target}!\\n`);});const port = process.env.PORT || 8080;app.listen(port, () =&gt; {  console.log('Hello world listening on port', port);});Add npm packages.$ npm installI’m using this version of node.$ node --versionv16.13.0I’m going to update the Dockerfile to use 16-slim.$ git diff Dockerfilediff --git a/code-samples/serving/hello-world/helloworld-nodejs/Dockerfile b/code-samples/serving/hello-world/helloworld-nodejs/Dockerfileindex 14fc5a7f..5593df68 100644--- a/code-samples/serving/hello-world/helloworld-nodejs/Dockerfile+++ b/code-samples/serving/hello-world/helloworld-nodejs/Dockerfile@@ -1,6 +1,6 @@ # Use the official lightweight Node.js 12 image. # https://hub.docker.com/_/node-FROM node:12-slim+FROM node:16-slim  # Create and change to the app directory. WORKDIR /usr/src/appBuild the image.$ docker build -t knative-hello-world-nodejs .Sending build context to Docker daemon  37.38kBSNIP!found 0 vulnerabilitiesnpm noticenpm notice New minor version of npm available! 8.1.2 -&gt; 8.3.0npm notice Changelog: &lt;https://github.com/npm/cli/releases/tag/v8.3.0&gt;npm notice Run `npm install -g npm@8.3.0` to update!npm noticeRemoving intermediate container 05788fe68bda ---&gt; 0d87e2185381Step 5/6 : COPY . ./ ---&gt; e799b6c92ec9Step 6/6 : CMD [ \"npm\", \"start\" ] ---&gt; Running in 28f3c37d21e2Removing intermediate container 28f3c37d21e2 ---&gt; 4fc708b92f84Successfully built 4fc708b92f84Successfully tagged knative-hello-world-nodejs:latestTag and push the image to the registry.$ docker tag knative-hello-world-nodejs &lt;registry&gt;/random-builds/knative-hello-world-nodejs$ docker push &lt;registry&gt;/random-builds/knative-hello-world-nodejsUsing default tag: latestThe push refers to repository [&lt;registry&gt;/random-builds/knative-hello-world-nodejs]bd83fded2ed1: Pushed888c1936e335: Pushed602368557b6e: Pusheda58aa2b5afe6: Pushed2c1769b8f2cd: Pushedb5e79c5c6912: Pushed18be021c4ec0: Pushed4a67e24013ff: Pushedad6b69b54919: Pushedlatest: digest: sha256:ffe4ba5bed5e9e692d8ca8f441a9209f2d20ab7adef927f0128c027364d1a3e9 size: 2201Aply some knative Kubernetes YAML. (Could use the knative CLI as well, but hey, this time let’s write some YAML.)cat &lt;&lt; EOF | kubectl create -f -apiVersion: serving.knative.dev/v1kind: Servicemetadata:  name: knative-helloworld-nodejs  namespace: cnr-demospec:  template:    spec:      containers:        - image: &lt;registry&gt;/random-builds/knative-hello-world-nodejs          env:            - name: TARGET              value: \"Node.js Sample v1\"EOFNow I’ve got both my knative services running, one of which is the NodeJS hello world app.$ /usr/local/bin/kn service listNAME                        URL                                                                 LATEST                            AGE     CONDITIONS   READY   REASONhello-yeti                  http://hello-yeti-cnr-demo.cnrs.gke.&lt;redacted&gt;                  hello-yeti-00001                  3d12h   3 OK / 3     True    knative-helloworld-nodejs   http://knative-helloworld-nodejs-cnr-demo.cnrs.gke.&lt;redacted&gt;   knative-helloworld-nodejs-00001   2d19h   3 OK / 3     True   Once that’s up and running we can curl the app.  NOTE: The LB variable is my load balancer fronting the Kubernetes ingress service provided by the Tanzu Application Platform.$ http http://knative-helloworld-nodejs-cnr-demo.cnrs.gke.&lt;redacted&gt;HTTP/1.1 200 OKcontent-length: 25content-type: text/html; charset=utf-8date: Sat, 15 Jan 2022 10:54:45 GMTetag: W/\"19-9t2w57sw0IX9vcOiByda5bvW2a4\"server: envoyx-envoy-upstream-service-time: 2195x-powered-by: ExpressHello Node.js Sample v1!Hello NodeJS World indeed.Building container Images…knative used to do thisAs you can see from this post and the previous one, the container image used to run the knative service has to come from somewhere.When the knative project originated, building images was part of its mandate.  First version of Knative came with three parts: Serving, Eventing, and Build. These may sound like they are three orthogonal concerns, because they really were. Knative Build was the first part to get separated (and became the Tekton project). - Did we market Knative wrong?But, as can be read in the above paragraph, people felt that having build be part of knative was confusing, so a proposal to move build out into Tekton was made:  This removes Serving optional dependency on Knative Build, making Knative Build fully decoupled from the rest of the Knative components and only responsible to build images that will be using in services later on. This responsibility is shared with any projects capable of building images in Kubernetes. - Proposal: Knative Build deprecation in favor of Tekton PipelinesAnd “knative build” moved out into Tekton.  Tekton Pipelines is the technological successor to Knative Build. Tekton entities are based on Knative Build’s entities but provide additional flexibility and reusability. This page explains how to convert your Knative Build entities to Tekton entities of equivalent functionality. - Migrating from Knative BuildObviously having a container image is key to using knative, so we’ve got to build one somehow. From a knative project perspective, they moved the build from out of knative and into Tekton. But, is Tekton the best way to build images?What I can say for sure is that from the perspective of the Tanzu Application Platform, the way we (optionally, but by default) build container images is via the Tanzu Build Service (TBS), which is based on the open source projects kpack and Paketo.That said, Tekton Pipelines are also installed and used in the Tanzu Application Service (more on that in some other future post) but they are not used to directly build images, that’s done by TBS, at least by default. To run a container you have to have a container image, and thus to help in simplifying and securing Kubernetes TAP provides that capability. It’s a must have feature.ConclusionSo there’s not much more to this post than the previous one, the one difference being that in this post I build the container image (using docker build) and pushed it to a registry myself, as opposed to using an image that someone, or something, else has built. To build my image I needed to write/maintain/borrow/steal a Dockerfile, which is not a small amount of additional work. Maybe instead of using a Dockerfile I could somehow use Tekton, or, I can absolutely use the Tanzu Build Service through TAP and have zero Dockerfiles (like none).While I’m using knative to help simplify using Kubernetes, which is great, I still have a lot of work to do as a developer to participate in a container deployment workflow…again, for example, having to build and maintain (forever) a container image.Another developer concern: how do I test all this? What if I change the code? Now I need to manually build the container image? Well, of course, no one would want to do that every time so there are several different ways to solve that problem, but it’s still work.Plus we have all the other fun stuff like how do we observe our app while it’s running in production, how do we debug, etc, etc. More to think about!A Note on TAPI want to be clear here, the full Tanzu Application Platform takes on all these challenges and more. In these two posts I’ve simply been exploring the knative component of TAP, which is only a subset of TAP’s capabilities, and in fact, when all of TAP is utilized the power is more than the sum of its parts.What I’m doing with these posts is building up piece by piece the modular components of TAP to explore why they are important and what value they add."
    } ,
  
    {
      "title"    : "Tanzu Application Platform and knative",
      "category" : "",
      "tags"     : "",
      "url"      : "/2022/01/11/tap-knative.html",
      "date"     : "2022-01-11 00:00:00 -0500",
      "content"  : "tl;drThe point of this post is to show that knative is part of The Tanzu Application Platform, AKA TAP, and one can use knative outside of TAP, directly with the kn CLI, if desired. (Though, in the real world you would probably use the rest of TAP as well, but it’s modular so you don’t have to.)In this post TAP has been deployed into a GKE cluster (yep, a GKE cluster). TAP includes knative, so in this post I’ll deploy a simple demo app into the GKE cluster and that deployment will be done via the knative CLI. Serverless here we come!GKE ClusterFirst, as I mentioned, I’ve got a Google Kubernetes Cluster.$ k get nodesNAME                                       STATUS   ROLES    AGE    VERSIONgke-gke-tap-1-default-pool-4e593ad2-9dk4   Ready    &lt;none&gt;   4d2h   v1.22.3-gke.700gke-gke-tap-1-default-pool-4e593ad2-m8qs   Ready    &lt;none&gt;   4d2h   v1.22.3-gke.700gke-gke-tap-1-default-pool-8dc7b056-8g1l   Ready    &lt;none&gt;   4d2h   v1.22.3-gke.700gke-gke-tap-1-default-pool-8dc7b056-drtd   Ready    &lt;none&gt;   4d2h   v1.22.3-gke.700gke-gke-tap-1-default-pool-bb825ba8-3pgz   Ready    &lt;none&gt;   4d2h   v1.22.3-gke.700gke-gke-tap-1-default-pool-bb825ba8-6v55   Ready    &lt;none&gt;   4d2h   v1.22.3-gke.700It has a bunch of nodes and is running the “full” TAP profile.What is the Tanzu Application Platform?  VMware Tanzu Application Platform is a modular, application-aware platform that provides a rich set of developer tooling and a prepaved path to production to build and deploy software quickly and securely on any compliant public cloud or on-premises Kubernetes cluster.TAP is a set of modular components which extend Kubernetes, making it easier and more secure to use. It’s important to understand that underlying TAP is still Kubernetes–all the pieces of TAP are “native” Kubernetes which means they become part of the Kubernetes API. The way we interact with TAP is through the Kubernetes API, though that will often be hidden away by pipelines and developer inner loop tooling. (No one should have to use kubectl, but you can of course.)For an example of extending Kubernetes, once you’ve deployed TAP you have an image resource, which, in plain old vanilla Kubernetes, doesn’t exist. (NOTE: I believe the power of Kubernetes is not so much in orchestrating containers, instead the fact that it is a platform to build other platforms on top of, which is precisely what TAP is…it’s even in the name!).Below I have a couple of container images represented in Kubernetes for a demo application for Spring called Pet Clinic, which can be deployed via knative through TAP. knative is part of what VMware Tanzu calls Cloud Native RuntimesHere I ask what Kubernetes knows about container images.$ k get imagesNAME                              IMAGEpet-clinic-00001-cache-workload   &lt;registry&gt;/tap-beta-4/supply-chain/pet-clinic-default@sha256:3b4ef38a43d464750d63ca0226c67ae59fdf990efe01c37ae88e8e10d2f574e8pet-clinic-00002-cache-workload   &lt;registry&gt;/tap-beta-4/supply-chain/pet-clinic-default@sha256:edfeabd87ee782f06510a0f1bc984a6134ae923121a6437cd5f401c59ff815de  NOTE: The images resource is provided by the Tanzu Build Service (also part of TAP), which itself is built on the open source projects kpack and Paketo and years of history and experience with Cloud Foundry and Buildpacks.But that’s just an example of how we build a platform on Kubernetes, in this post I’ll try to stay focussed on knative.Cloud Native Runtimes and knativeCloud Native Runtimes is VMware’s product to provide various ways of running applications in Kubernetes. Currently there is only the single runtime, knative, but more will be added over time.So what’s knative? It’s an open source project that makes developers more productive by abstracting away some of the complexity of Kubernetes.  Knative components build on top of Kubernetes…by codifying the best practices shared by successful real-world implementations, Knative solves the “boring but difficult” parts of deploying and managing cloud native services so you don’t have to. - knative websiteknative provides:  Serverless - “Run serverless containers on Kubernetes with ease, Knative takes care of the details of networking, autoscaling (even to zero), and revision tracking. You just have to focus on your core logic.”  Eventing - “Universal subscription, delivery, and management of events. Build modern apps by attaching compute to a data stream with declarative event connectivity and developer-friendly object model.”I’d like to look specifically at just running a serverless app in knative, which in this case is easily provided/installed into Kubernetes by TAP. (That said, eventing is clearly important to modern applications, so it’s important to keep in mind that knative satisfies that need as well.)Deploying an App into knativeIn our GKE cluster we have many packages from TAP installed, including knative, aka Cloud Native Runtimes. That means we can run serverless workloads!$ tanzu package installed list --namespace tap-install\\ Retrieving installed packages...  NAME                      PACKAGE-NAME                                        PACKAGE-VERSION  STATUS                 accelerator               accelerator.apps.tanzu.vmware.com                   1.0.0            Reconcile succeeded    api-portal                api-portal.tanzu.vmware.com                         1.0.8            Reconcile succeeded    appliveview               run.appliveview.tanzu.vmware.com                    1.0.1            Reconcile succeeded    appliveview-conventions   build.appliveview.tanzu.vmware.com                  1.0.1            Reconcile succeeded    buildservice              buildservice.tanzu.vmware.com                       1.4.2            Reconcile succeeded    cartographer              cartographer.tanzu.vmware.com                       0.1.0            Reconcile succeeded    cert-manager              cert-manager.tanzu.vmware.com                       1.5.3+tap.1      Reconcile succeeded    cnrs                      cnrs.tanzu.vmware.com                               1.1.0            Reconcile succeeded    contour                   contour.tanzu.vmware.com                            1.18.2+tap.1     Reconcile succeeded    conventions-controller    controller.conventions.apps.tanzu.vmware.com        0.5.0            Reconcile succeeded    developer-conventions     developer-conventions.tanzu.vmware.com              0.5.0-build.1    Reconcile succeeded    fluxcd-source-controller  fluxcd.source.controller.tanzu.vmware.com           0.16.0           Reconcile succeeded    grype                     grype.scanning.apps.tanzu.vmware.com                1.0.0            Reconcile succeeded    image-policy-webhook      image-policy-webhook.signing.apps.tanzu.vmware.com  1.0.0            Reconcile succeeded    learningcenter            learningcenter.tanzu.vmware.com                     0.1.0            Reconcile succeeded    learningcenter-workshops  workshops.learningcenter.tanzu.vmware.com           0.1.0            Reconcile succeeded    metadata-store            metadata-store.apps.tanzu.vmware.com                1.0.1            Reconcile succeeded    ootb-delivery-basic       ootb-delivery-basic.tanzu.vmware.com                0.5.1            Reconcile succeeded    ootb-supply-chain-basic   ootb-supply-chain-basic.tanzu.vmware.com            0.5.1            Reconcile succeeded    ootb-templates            ootb-templates.tanzu.vmware.com                     0.5.1            Reconcile succeeded    scanning                  scanning.apps.tanzu.vmware.com                      1.0.0            Reconcile succeeded    service-bindings          service-bindings.labs.vmware.com                    0.6.0            Reconcile succeeded    services-toolkit          services-toolkit.tanzu.vmware.com                   0.5.0            Reconcile succeeded    source-controller         controller.source.apps.tanzu.vmware.com             0.2.0            Reconcile succeeded    spring-boot-conventions   spring-boot-conventions.tanzu.vmware.com            0.3.0            Reconcile succeeded    tap                       tap.tanzu.vmware.com                                1.0.0            Reconcile succeeded    tap-gui                   tap-gui.tanzu.vmware.com                            1.0.1            Reconcile succeeded    tap-telemetry             tap-telemetry.tanzu.vmware.com                      0.1.2            Reconcile succeeded    tekton-pipelines          tekton.tanzu.vmware.com                             0.30.0           Reconcile succeededThat’s a lot of stuff. Those pieces are currently part of the Tanzu Application Platform. But lets just use a single piece: knative.First I’ve got the kn CLI.$ which kn/usr/local/bin/knNext I’ll create a namespace (and switch to it).$ kubectl create namespace cnr-demoThen I’ll setup a reg secret because my registry requires authentication.$ kubectl create secret docker-registry registry-credentials \\  --docker-server=&lt;redacted&gt; \\  --docker-email=&lt;redacted&gt; \\  --docker-username=&lt;redacted&gt; \\  --docker-password=&lt;redacted&gt;Allow the namespace’s default SA to use it…$ kubectl patch serviceaccount default -p \"{\\\"imagePullSecrets\\\": [{\\\"name\\\": \\\"registry-credentials\\\"}]}\"Now deploy a workload, pointing to an image that resides in the registry I previously setup secret credentials for.$ kn service create hello-yeti -n cnr-demo \\  --image &lt;registry&gt;/hello-yeti --env TARGET='hello-yeti'Output:$ kn service create hello-yeti -n cnr-demo --image &lt;registry&gt;/hello-yeti --env TARGET='hello-yeti'Creating service 'hello-yeti' in namespace 'cnr-demo':  0.043s The Route is still working to reflect the latest desired specification.  0.091s ...  0.148s Configuration \"hello-yeti\" is waiting for a Revision to become ready.  5.639s ...  5.712s Ingress has not yet been reconciled.  5.756s Waiting for Envoys to receive Endpoints data.  6.108s Waiting for load balancer to be ready  6.329s Ready to serve.Service 'hello-yeti' created to latest revision 'hello-yeti-00001' is available at URL:http://hello-yeti.cnr-demo.example.comOnce that deploys there will be a pod running.  NOTE: The deployment will scale to zero if it’s not being used, something to remember.$ k get podsNAME                                          READY   STATUS    RESTARTS   AGEhello-yeti-00001-deployment-6f49f84f5-z6lgq   2/2     Running   0          13sList the knative deployments/services.$ kn service listNAME         URL                                      LATEST             AGE     CONDITIONS   READY   REASONhello-yeti   http://hello-yeti.cnr-demo.example.com   hello-yeti-00001   3m48s   3 OK / 3     True   Curl the app.  NOTE: TAP also deploys an ingress controller for you. To connect to the knative service we’ll need the IP of the Kubernetes loadbalancer for the ingress service.$ export LB=`kubectl get services -n tanzu-system-ingress envoy -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\"`$ curl -H \"Host: hello-yeti.cnr-demo.example.com\" $LBOutput:$ curl -H \"Host: hello-yeti.cnr-demo.example.com\" $LB              ______________            /               \\           |   hello from    |           |  cloud native   |           |    runtimes     |     .xMWxw.            \\______________\\ |   wY     Ym.                            \\|  C  ,  ,   O                                 \\  ww   /.                               ..x       x..                              .x   wwwww    x.                             .x               x.                             x   \\         /   x                             Y   Y         Y   Y                              wwv    x      vww                                \\    /\\    /                                :www:  :www:If the application isn’t accessed for a while it’ll scale to zero.$ k get podsNo resources found in cnr-demo namespace.If we try to hit the URL again, the pods will be restarted automatically by knative.So, that was pretty easy. Of course it’s a simple demo application which lives in an existing container image, but the point of this post was to illustrate that by deploying TAP we have access to knative, and we can even use knative outside of the TAP workflow if we want, or deploy it outside of TAP as well. knative is a major part of VMware Tanzu.ConclusionThe Tanzu Application Platform gives you cutting edge tools like serverless via knative. Now, the point of TAP isn’t necessarily to just directly use knative like this, but I wanted to show that knative is indeed part, an integral part, of TAP, that you can use if you want to (by default most TAP demos will use knative) but you don’t have to use it.Another thing to keep in mind in the context of this post, is exactly where did that hello-yeti container image come from? What if we want to deploy our own code? Not surprisingly, I’ll look into that in the next post in this series.Thanks for reading!"
    } ,
  
    {
      "title"    : "Azure Functions, Managed Identity, NodeJS, and Key Vault",
      "category" : "",
      "tags"     : "",
      "url"      : "/2021/12/21/azure-functions-keyvault.html",
      "date"     : "2021-12-21 00:00:00 -0500",
      "content"  : "Azure has functions. Azure as a way to manage secrets called Key Vault. How do these work together? If you create a function and you want to access a Key Vault secret, clearly it has to authenticate to the Key Vault service…but how?Managed identity is the answer. But what is “managed identity”?  A managed identity from Azure Active Directory (Azure AD) allows your app to easily access other Azure AD-protected resources such as Azure Key Vault. The identity is managed by the Azure platform and does not require you to provision or rotate any secrets. - Azure DocsBasically your function can authenticate without having to login in the way that we typically think of “logging in”, ie. with a username and password. If we did have to use a username and password then the function would have to get that information from somewhere, and if that information became public in some way we’d have to rotate (ie. change) those secrets, which is a huge pain. But with managed identity we don’t have to do that, instead we configure Azure to allow our function to access Key Vault. Thus, the platform takes care of everything in the background, which is what platforms are supposed to do. :)Also, and this is interesting, in my NodeJS code I’m using the below to setup the credential so that I can access Key Vault secrets.const credential = new DefaultAzureCredential();The above assumes there are AZURE_TENANT_ID, AZURE_CLIENT_ID and AZURE_CLIENT_SECRET variables configured, ie. when developing locally, having logged in with az login or setup those variables. However, once the function has been pushed to Azure, if those variables are not available, the code will try to use a managed identity. So I don’t have to use one method locally and another in production.In my case, initially managed identity access wasn’t configured for the functionapp, so I received this error when running in Azure:2021-12-21T11:59:26.309 [Error] Executed 'Functions.etc-hosts' (Failed, Id=56fdb72b-eb86-41b9-a215-d7e7b3f22425, Duration=98ms)Result: FailureException: Error: Azure CLI could not be found.  Please visit https://aka.ms/azure-cli for installation instructions and then, once installed, authenticate to your Azure account using 'az login'.Stack: Error: Azure CLI could not be found.  Please visit https://aka.ms/azure-cli for installation instructions and then, once installed, authenticate to your Azure account using 'az login'.at AzureCliCredential.getToken I needed to setup managed identity and allow it to access a Key Vault.Configure Managed Identity Access for FunctionFirst, I setup some vars representing my function deployment. Of course these are filled out when I run it in my environment. They’re empty here.export RG=export REGION=export APPNAME=export STORAGE=export KV=Next, assign and identity.az functionapp identity assign --resource-group ${RG} --name ${APPNAME}Eg. output:$ az functionapp identity assign --resource-group ${RG} --name ${APPNAME}{  \"principalId\": \"&lt;redacted\",  \"tenantId\": \"&lt;redacted&gt;\",  \"type\": \"SystemAssigned\",  \"userAssignedIdentities\": null}Now we can just grab the principalId (or copy it from the above output).export PRINCIPAL_ID=$(az functionapp identity show -n ${APPNAME} --query principalId --resource-group ${RG} -o tsv)Finally setup a policy for key vault to allow this  principal to access the secrets.az keyvault set-policy -n ${KV} \\  --object-id ${PRINCIPAL_ID} \\  --resource-group ${RG} \\  --secret-permissions get list At this point, even when using DefaultAzureCredential(), when pushed into Azure the system is smart enough to use the managed identity."
    } ,
  
    {
      "title"    : "Dev Experience: My First Look at Azure Functions",
      "category" : "",
      "tags"     : "",
      "url"      : "/2021/12/13/dev-experience-azure-functions.html",
      "date"     : "2021-12-13 00:00:00 -0500",
      "content"  : "What Do I Want to Do?I’ve not used Azure Functions before, so I’m going to run through a quick start to deploy an example nodejs function.What I have/want to do:  Already have az installed and an Azure account to work with  Running from a Linux workstation  Don’t want to use VSCode integration currently, prefer to use CLI for now  Deploy a NodeJS 16 “hello world” function manuallyInstalling  DocsGet the core tools, which presumably includes the func CLI.$ sudo apt-get install azure-functions-core-tools-4Reading package lists... DoneSNIP!Preparing to unpack .../azure-functions-core-tools-4_4.0.3971-1_amd64.deb ...Unpacking azure-functions-core-tools-4 (4.0.3971-1) ...Setting up azure-functions-core-tools-4 (4.0.3971-1) ...Telemetry---------The Azure Functions Core tools collect usage data in order to help us improve your experience.The data is anonymous and doesn't include any user specific or personal information. The data is collected by Microsoft.You can opt-out of telemetry by setting the FUNCTIONS_CORE_TOOLS_TELEMETRY_OPTOUT environment variable to '1' or 'true' using your favorite shell.Now I have the func command.$ which func/usr/bin/funcInit a new project. I’m going to use nodejs.$ func init .Select a number for worker runtime:1. dotnet2. dotnet (isolated process)3. node4. python5. powershell6. customChoose option: 3nodeSelect a number for language:1. javascript2. typescriptChoose option: 1javascriptWriting package.jsonWriting .gitignoreWriting host.jsonWriting local.settings.jsonWriting /home/curtis/working/sparrow-dns-azure-function/.vscode/extensions.jsong /home/curtis/working/sparrow-dns-azure-function/MyFunctionProj/.vscode/extensions.jsonThat creates a few files.$ tree.├── host.json├── local.settings.json└── package.json0 directories, 3 filesNext, let’s create a function.Creating a Function  DocsCreate a function from a template “HTTP Trigger”.$ func new --name HttpExample --template \"HTTP trigger\" --authlevel \"anonymous\"Select a number for template:HTTP triggerFunction name: [HttpTrigger] Writing /home/curtis/working/sparrow-dns-azure-function/HttpExample/index.jsWriting /home/curtis/working/sparrow-dns-azure-function/HttpExample/function.jsonThe function \"HttpExample\" was created successfully from the \"HTTP trigger\" template.Interesting that there is a template option.Test LocallyNow in one terminal:$ func startExample output:$ func startAzure Functions Core ToolsCore Tools Version:       4.0.3971 Commit hash: d0775d487c93ebd49e9c1166d5c3c01f3c76eaaf  (64-bit)Function Runtime Version: 4.0.1.16815Functions:\tHttpExample: [GET,POST] http://localhost:7071/api/HttpExampleFor detailed output, run func with --verbose flag.info: Microsoft.AspNetCore.Hosting.Diagnostics[1]      Request starting HTTP/2 POST http://127.0.0.1:40553/AzureFunctionsRpcMessages.FunctionRpc/EventStream application/grpc -info: Microsoft.AspNetCore.Routing.EndpointMiddleware[0]      Executing endpoint 'gRPC - /AzureFunctionsRpcMessages.FunctionRpc/EventStream'[2021-12-13T11:29:47.400Z] Worker process started and initialized.[2021-12-13T11:29:52.128Z] Host lock lease acquired by instance ID '000000000000000000000000AC5DB4CC'.And in another terminal, curl..$ curl http://localhost:7071/api/HttpExampleThis HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized responseOK, great, but now how to publish to “the cloud”…Configure Azure to be able to Deploy the function to azure…First we need to configure a resource group, etc.Login.$ az loginCreate a resource group.export RG=sparrow-dns-functions-rgexport REGION=canadacentralexport APPNAME=&lt;project name&gt;export STORAGE=sparrowdnsfuncstorageaz group create --name $RG --location $REGIONStorage account.  NOTE: What’s the deal with the storage names, yeesh. Lower case letters or numbers only. Don’t like it.  Question: What does a function need a storage account for?az storage account create --name $STORAGE --location $REGION --resource-group $RG --sku Standard_LRSCreate the function app.  NOTE: Using node 16, not 14.  “…replace  with the name of the account you used in the previous step, and replace  with a globally unique name appropriate to you. The  is also the default DNS domain for the function app.\"az functionapp create --resource-group $RG \\--consumption-plan-location $REGION \\--runtime node --runtime-version 16 --functions-version 4 \\--name $APPNAME \\--storage-account $STORAGEoh no error.$ az functionapp create --resource-group $RG \\&gt; --consumption-plan-location $REGION \\&gt; --runtime node --runtime-version 16 --functions-version 4 \\&gt; --name $APPNAME \\&gt; --storage-account $STORAGEaz functionapp create: '4' is not a valid value for '--functions-version'. Allowed values: 2, 3.TRY THIS:az functionapp create --resource-group MyResourceGroup --plan MyPlan --name MyUniqueAppName --storage-account MyStorageAccountCreate a basic function app.https://docs.microsoft.com/en-US/cli/azure/functionapp#az_functionapp_createRead more about the command in reference docsMust need newer az CLI?$ sudo apt-get --only-upgrade install azure-cli$ az version{  \"azure-cli\": \"2.31.0\",  \"azure-cli-core\": \"2.31.0\",  \"azure-cli-telemetry\": \"1.0.6\",  \"extensions\": {}}Now run again…$ az functionapp create --resource-group $RG\\ --consumption-plan-location $REGION \\ --runtime node --runtime-version 16 --functions-version 4 \\ --name $APPNAME \\ --storage-account $STORAGEResource provider 'Microsoft.Web' used by this operation is not registered. We are registering for you.Registration succeeded.SNIP!That gives you a link to this page to see “Application Insights” which it seems will be deprecated.Onward!Deploy the FunctionNow we actually push the function to the function app.export APPNAME=&lt;project name&gt;func azure functionapp publish $APPNAMENow can access.$ func azure functionapp publish $APPNAMEGetting site publishing info...Creating archive for current directory...Uploading 1.3 KB [#####################################################################]Upload completed successfully.Deployment completed successfully.Syncing triggers...Functions in sparrow-dns:    HttpExample - [httpTrigger]        Invoke url: https://&lt;project name&gt;.azurewebsites.net/api/httpexampleConnect with httpie.$ http https://&lt;project name&gt;.azurewebsites.net/api/httpexampleHTTP/1.1 200 OKContent-Encoding: gzipContent-Type: text/plain; charset=utf-8Date: Mon, 13 Dec 2021 12:13:02 GMTRequest-Context: appId=cid-v1:dd33b7ce-86e0-4822-a59b-9dd8b4116385Transfer-Encoding: chunkedVary: Accept-EncodingThis HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response.Provide a name.$ http post https://&lt;project name&gt;.azurewebsites.net/api/httpexample name=curtisHTTP/1.1 200 OKContent-Encoding: gzipContent-Type: text/plain; charset=utf-8Date: Mon, 13 Dec 2021 12:14:01 GMTRequest-Context: appId=cid-v1:dd33b7ce-86e0-4822-a59b-9dd8b4116385Transfer-Encoding: chunkedVary: Accept-EncodingHello, curtis. This HTTP triggered function executed successfully.That’s it.ConclusionHereare some basic thoughts:  I find Azure naming unusual  I like that there is a “template” option for functions, should explore what that means (Can I create my own templates? Probably not. That’s something that the Tanzu Application Acclerator can do, template any application including Azure Functions)  Keep multiple functions in the same repo  Not sure why the az and func CLIs exist, can’t deploy a function with az?  I like the domain: your-project.azurewebsites.net/api/somefunction  Having links point me to services that are being deprecated is a bit concering, but Azure is a massive ecosystem so not unexpected, definitely good that there are application metrics/monitoring integrated of course  The localhost name of the function is “HttpExample” and the deployed version is “httpexample”  As is common with functions, I’m not sure what version of nodejs is being used everywhere….presumably 16 is being used in the cloud, as that is what I specified, but not sure locally what func does…Obviously this is my first time using Azure Functions, but so far, other than setting up the resource group and such, I quite like it. Didn’t take long to get a function deployed. Several languages are supported, including Java which I should experiment with.I’ll take Azure Functions a bit deeper in future posts. So far looks really great.Links  Azure Functions Developer Guide"
    } ,
  
    {
      "title"    : "Dev Experience: Managing Secrets with Doppler",
      "category" : "",
      "tags"     : "",
      "url"      : "/2021/12/01/dev-experience-doppler-secrets.html",
      "date"     : "2021-12-01 00:00:00 -0500",
      "content"  : "Secrets. I need to manage them. As part of my exploration of developer experience I plan on having many micro-services running on different platforms. However, I don’t want to have to manage the secrets across all of them individually…that would be a nightmare, never mind thinking about various environments (dev, test, prod…). For example, I’m using Basic Authentication as a simple API key and API secret key, and I need to manage those secrets across all services, my CLI, and of course, various environments (which should have different secrets).So I need something to help me do that.One option is Doppler. Doppler bills itself as a “universal secrets manager” and I think, after a bit of use, that’s a pretty accurate description.About DopplerWhat’s Doppler?  Doppler [has] launched the industry’s first Universal Secrets Manager, a modern secrets manager offering built to win the hearts and minds of developers. It works across every language, stack and infrastructure, increasing developer productivity while strengthening a company’s overall security. Early adopters, including Stripe, Point Banking, Snackpass, Kopa and Convictional, use Doppler to securely store secrets such as API keys, credentials, ENV variables and database URLs. - Press ReleaseSecrets and credentials are a major problem. Organizations have more environments in use than they are often willing to admit (or even track).  Secrets and credentials management is widely considered to be the most overlooked aspect of software development. Many teams struggle daily to organize and sync secrets between environments, with manually maintained .env files being one of the most common sources of frustration for developers and DevSecOps. - DopplerFocus on developer experience.  Security tools are often process heavy and come with horrible experiences which leads to low usage. At Doppler we strongly believe in building tools that developers will love. The more you love it, the more you will want to use it. - DopplerWhat a great attitude.tl;drI really like Doppler. I like pretty much everything about it. I like how it’s setup, I like how it guides you through using it. I like that it has a CLI that I can integrate with everything. I like that it had examples for Firebase. Looks like there are integrations for Netlify and Vercel (two services I plan on checking out). I also like that it will copy secrets to each environment, and let you know when environments have secrets that don’t exist in other environments.It’s just a really well thought out secrets as a service, and I’ve only–just barely–scratched the surface of using it.Installing doppler cliI’m running Linux as my main OS for writing software.To install doppler, get the package.# Install pre-reqssudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https ca-certificates curl gnupg# Add Doppler's GPG keycurl -sLf --retry 3 --tlsv1.2 --proto \"=https\" 'https://packages.doppler.com/public/cli/gpg.DE2A7741A397C129.key' | sudo apt-key add -# Add Doppler's apt repoecho \"deb https://packages.doppler.com/public/cli/deb/debian any-version main\" | sudo tee /etc/apt/sources.list.d/doppler-cli.list# Fetch and install latest doppler clisudo apt-get update &amp;&amp; sudo apt-get install dopplerNow we’ve got doppler.$ which doppler/usr/bin/dopplerLogin.Initial ConfigurationLogin.$ doppler login? Open the authorization page in your browser? NoComplete authorization at https://dashboard.doppler.com/workplace/auth/cliYour auth code is:&lt;SNIP!&gt;Waiting...Welcome, CurtisI’ve already setup a project called sparrow-dns.$ doppler setup? Select a project: sparrow-dns? Select a config: dev┌─────────┬─────────────┬──────────────────────────────────┐│ NAME    │ VALUE       │ SCOPE                            │├─────────┼─────────────┼──────────────────────────────────┤│ config  │ dev         │ /home/curtis/working/sparrow-dns ││ project │ sparrow-dns │ /home/curtis/working/sparrow-dns │└─────────┴─────────────┴──────────────────────────────────┘By default there are three environments configured, of course you could have more or fewer.$ doppler environments┌─────┬─────────────┬───────────────┬──────────────────────────┬─────────────┐│ ID  │ NAME        │ INITIAL FETCH │ CREATED AT               │ PROJECT     │├─────┼─────────────┼───────────────┼──────────────────────────┼─────────────┤│ dev │ Development │               │ 2021-12-04T12:45:27.568Z │ sparrow-dns ││ stg │ Staging     │               │ 2021-12-04T12:45:27.568Z │ sparrow-dns ││ prd │ Production  │               │ 2021-12-04T12:45:27.568Z │ sparrow-dns │└─────┴─────────────┴───────────────┴──────────────────────────┴─────────────┘Using DopplerTo use it with Firebase I’ve added the following to the scripts section of my package.json.    \"serve\": \"doppler run -- firebase emulators:start\",    \"shell\": \"doppler run -- firebase functions:shell\",    \"update_config\": \"firebase functions:config:unset env &amp;&amp; firebase functions:config:set env=\\\"$(doppler secrets download --config prd --no-file --silent)\\\"\",    \"deploy\": \"firebase functions:config:unset env &amp;&amp; firebase functions:config:set env=\\\"$(doppler secrets download --config prd --no-file --silent)\\\" &amp;&amp; firebase deploy --only functions\",I’ve also got a CLI for using the DNS arecords demo API (I’m currently calling it Sparrow DNS) and I can easily integrate with it, and set the endpoint URL to the local URL that is setup when I use the Firebase emulator. This helps a lot with local testing.For example, if I want to use the CLI with the dev secrets…doppler run -- ./scripts/sparrow-cli arecord listBut if I want to use the production environment…doppler run --config prd -- ./scripts/sparrow-cli arecord listI’ll need to think about how to integrated the doppler CLI into the Sparrow CLI.ConclusionAgain, so far I’m a big fan of Doppler, 1) because it works and 2) because it’s obvious that they are not only talking about creating a great developer experience, they are actually doing it!I would imagine I’ll write another post once I’ve worked more with Doppler and various “serverless” platforms, as well as tooling like Spring Cloud Config."
    } ,
  
    {
      "title"    : "Dev Experience: Writing a NodeJS REST API with Firebase",
      "category" : "",
      "tags"     : "",
      "url"      : "/2021/11/29/dev-experience-firebase.html",
      "date"     : "2021-11-29 00:00:00 -0500",
      "content"  : "Developer ExperienceWorking at VMware in the Tanzu group I’m always focussed on developer experience (DX). Often people think of VMware as an infrastructure only company, but we’re not. Tanzu is heavily focussed on developers. I’d say 10-20% of what we do is infrastructure related (Kubernetes, Cloud Foundry) and the other 80-90% is related to devops, security, developers, and software architects to name a few. What’s the point of platforms if there’s no apps running on them.Overall Tanzu is working extremely hard on improving DX. For example we recently released beta versions of the Tanzu Application Platform, a way to de-expose kubernetes to developers…by that I mean abstract it away into a 15 lines of YAML instead of 2000.But let me get to the point of this post–developers should not be futzing around with Kubernetes. They should be able to write code and put apps in production as easily as possible.With that in mind, I like to keep my eye on any products or tools or platforms that can improve DX. I decided to take a look at Google’s Firebase to see what it’s like, and how it helps DX. While Firebase has been around for a long time, I’ve never taken a look at it to understand what it does. Time to change that. :)Itch to Scratch - Simple hostname/arecords REST APIHistorically I’m not a developer (surprise!). I don’t write code every day, and I don’t normally have a reason to. But I have had an “itch to scratch” so to speak for a while, in that I want a way to easily manage my home DNS server. I have a homelab and it requires many host, ie. arecord, entries. My internal DNS server is dnsmasq and it can run off of the entries in /etc/hosts. So when I add internal DNS entries, I just add them to the /etc/hosts file on the dnsmasq server and that’s it.I wanted an API and CLI that I can use to easily do that, and then (eventually) a templated API response that will generate /etc/hosts (and other config files) for me based on those entries. The idea is that if I build the main API, then I can add microservices that can template out configuration files for any DNS server (not just dnsmasq). But that’s down the road….The thing I need to build first is a simple REST API for managing DNS arecords.What do I want?Base requirements:  Runs in “the cloud”; no infra required  Document database  Functions (no exposed k8s)  NodeJS support  Easy push to prod  Low cost entry (hopefully free for small projects)  Easy local developmentI know SQL is making a comeback in terms of the marketplace (not that it left) but for this use case I’m interested in a document database.FirebaseHistory  Firebase is a platform developed by Google for creating mobile and web applications. It was originally an independent company founded in 2011. In 2014, Google acquired the platform[1] and it is now their flagship offering for app development.Firebase has been around for 10 years. Much like App Engine, it doesn’t get much press, and I’m not even sure how I came across it, but after reading a bit, it seemed like an interesting platform to look at in terms of DX.This post, What is Firebase, The complete story, abridged, which I didn’t read until I started writing this post, gives some good perspective on what Firebase is, and isn’t.My Experience with FirebaseHere are my base base requirements.            Item      Supported      Comment                  nodejs      yes      first class, but hard to tell what version?              document database      yes      firestore              functions      yes      cloud run with deep integration, but need “blaze” plan level              low cost      yes      great for small projects like mine      But what else might one need for a good development platform? Here’s a few I thought about in this context.  NOTE: Please understand that I don’t write code every day. More experienced developers will have different opinions of what is important and what isn’t.            Item      Supported      Comment                  local development      yes      firestore emulator is amazing              getting to prod      yes      easy as firebase deploy              authentication      yes      hard for me to grasp difference between admin sdk and other users              data schema      yes, for users      filebase.rules is great, but doesn’t apply to admin sdk              data indexes      yes      need to manually apply them with cli or gui              logging      yes      didn’t explore              metrics      yes      didn’t explore              testing      yes      didn’t explore              ci/cd integration             didn’t explore      Certainly developers need a lot more than this, but I didn’t want to write out 100 needs.Some Things I Ran Into Using Firebase  Functions are not available in the “free tier” you have to go up to the Blaze level, which still has a free tier that is fairly substantial for a small app like mine, one which might see a few requests per day at most  Firebase rules (ie. data schemas) don’t apply to the admin sdk…lost some time on this  I had trouble figuring out what nodejs version is supportedSome ResultsHere’s using a simple httpie based script to talk to the API and perform CRUD operations. Right now I’m calling this project “Sparrow” for some reason.$ ./scripts/sparrow-cli arecord add www.example.com 192.168.88.10{    \"arecord\": {        \"_id\": \"JWgEXUlfVdK5FUQeLeBr\",        \"ip\": \"192.168.88.10\",        \"name\": \"www.example.com\"    }}info: added arecord$ ./scripts/sparrow-cli arecord list[    {        \"_id\": \"JWgEXUlfVdK5FUQeLeBr\",        \"ip\": \"192.168.88.10\",        \"name\": \"www.example.com\"    },    {        \"_id\": \"K3fwiWQOSS3wc6srpv9h\",        \"ip\": \"10.10.10.10\",        \"name\": \"new2.domain.com\"    },    {        \"_id\": \"O8OIERUTclPpg75cIAlq\",        \"ip\": \"10.10.10.10\",        \"name\": \"new.domain.com\"    },    {        \"_id\": \"ytgACQTVZZyXJi0pYPJ5\",        \"ip\": \"10.0.10.10\",        \"name\": \"some.domain.com\"    }]The script uses these environment variables to connect. The API keys have nothing to do with Firebase…they’re part of the app.$ env | grep SPARSPARROW_API_ENDPOINT=https://us-central1-&lt;my firebase project&gt;.cloudfunctions.net/apiSPARROW_API_KEY=&lt;key&gt;SPARROW_API_SECRET_KEY=&lt;secret&gt;Pretty straightforward, simplistic stuff…and yet, it’s a perfectly usable REST API.ConclusionOverall I used Firebase to provide a place to run nodejs functions that talk to a document store, where the functions and document store are provided by the platform. As well I made heavy use of the firebase CLI and emulator to test locally.With just over 100 lines of NodeJS I was able to write a functional REST API for my arecords app requirement.$ cloc --exclude-list-file=.clocignore .    8045 text files.    7117 unique files.                                              8051 files ignored.github.com/AlDanial/cloc v 1.82  T=0.93 s (6.5 files/s, 293.3 lines/s)--------------------------------------------------------------------------------Language                      files          blank        comment           code--------------------------------------------------------------------------------JavaScript                        4             30             34            156Bourne Again Shell                1              7              1             37JSON                              1              0              0              7--------------------------------------------------------------------------------SUM:                              6             37             35            200--------------------------------------------------------------------------------That’s pretty cool.The CLI firebase lets you setup a project, run the emulators, and push to prod.Maybe writing nodejs REST APIs isn’t the what most Firebase users do, but it certainly works for me.My use of Firebase gives me a great data point in my path to understanding great developer experience–what’s good, what’s bad, where innovation is required. That said, the main focus of Firebase is not building REST APIs, AFAIK, it’s more of a “backend as a service” where you don’t even have to write the API (but obviously I didn’t use that part of Firebase).Now to explore other platforms… :)"
    } ,
  
    {
      "title"    : "Deploy Tanzu Build Service into a vSphere with Tanzu Workload Cluster",
      "category" : "",
      "tags"     : "",
      "url"      : "/2021/09/10/deploy-tanzu-build-service-on-vsphere-with-tanzu.html",
      "date"     : "2021-09-10 00:00:00 -0400",
      "content"  : "In this post we’ll deploy the Tanzu Build Service (TBS) onto a vSphere with Tanzu Kubernetes workload cluster.Requirements  vSphere with Tanzu deployed and enough resources for the TBS workload cluster  A container image repository, such as Harbor, or Azure CR, etc, any compliant registry should do  A place to run commands (a linux host is best IMHO)About the Tanzu Build ServiceThere are several ways to build container images (not just Dockerfiles).The Tanzu Build Service makes building container images easier, ie. no Dockerfiles, and provides an image control plane, which I believe Kubernetes sorely misses.  You have plenty of options for building containers from source code. Yet many require significant effort and ongoing maintenance to use them properly. And it can be hard to enforce security and operational rigor at scale. Tanzu Build Service offers the convenience of these workflows with more automation and the governance capabilities enterprises need. - Tanzu Build ServiceAbout vSphere with TanzuvSphere with Tanzu, what you may remember as being called “Project Pacific”, is Kubernetes lifecycle management built directly into vSphere.VMware believes that you will have many Kubernetes clusters. Not just one or two or three. So, the main feature of vSphere with Tanzu is to manage the lifecycle of many Kubernetes clusters. The way we do this is by extending Kubernetes with something called Cluster API.This means, and this can be confusing, that when we want to create a Kubernetes cluster, we actually ask a specialized Kubernetes cluster–the Supervisor Cluster–to do this for us. So we use Kubernetes to deploy Kubernetes. Make sense?Once the Supervisor Cluster has created our “workload” cluster (and there will be many of these), we can then talk directly to that new workload cluster via its own, completely separate, Kubernetes API.So, to create a workload cluster we ask the supervisor cluster. Once the workload cluster is created, we talk to it to deploy applications into it. Simple enough once you get the hang of it.Deploy a vSphere with Tanzu Workload Cluster for TBSWe need a cluster to install TBS into. That cluster needs a couple of things:  Enough room on the nodes to build images - the default 16Gi disk size is not enough, we need a cluster with at least 50Gi on each node for the image builds that TBS does  The right RBAC configuration and permissionsFirst, let’s deploy the cluster with larger disks.Login to the Supervisor ClusterEnsure you are logged into your vSphere with Tanzu supervisor Kubernetes cluster.  NOTE: I alias kubectl to k.  NOTE: I’m logging into my vSphere with Tanzu supervisor cluster found at 10.0.14.128…yours will of course be different. I’m also using the admin account.$ k vsphere login --server 10.0.14.128 --insecure-skip-tls-verify -u administrator@vsphere.localPassword:Logged in successfully.You have access to the following contexts:   10.0.14.128   dev-team-purple   dev-team-tundra   test-nsIf the context you wish to use is not in this list, you may need to trylogging in again later, or contact your cluster administrator.To change context, use `kubectl config use-context &lt;workload name&gt;`Now that I’m logged in, I have access to the supervisor cluster as well as the supervisor namespaces. So there will be several kube contexts set up.  NOTE: I’m using kubectx aliased to kc instead of kubectl config use-context just because it’s what I always use and I find it easier, IMHO.$ kc10.0.14.128dev-team-purpledev-team-tundratest-nsSwitch the Desired Supervisor NamespaceI’m going to deploy the TBS workload cluster into the dev-team-tundra supervisor namespace. So I’ll switch to that config.$ kc dev-team-tundra✔ Switched to context \"dev-team-tundra\".Deploy a Workload ClusterSet up a few variables that will be dependent on how you have set up the supervisor cluster, storage, etc.  NOTE: Change these to fit your environment.export NS=dev-team-tundra # the supervisor namespaces to useexport SC=k8s-storage-policy # the storage policy configured when enabling workload managementexport CLUSTER_NAME=\"tanzu-build-service-cluster2\" # the name of the workload clusterexport K8S_VERSION=v1.20.7 # version of k8s to deployNow I’ll deploy a cluster to that supervisor namespace.  NOTE: This will deploy the cluster! Note the pipe at the top of the command to kubectl.  NOTE: Notice that in the YAML here we define a separate disk for /var/lib/containerd that is 50Gi in size. We need this for TBS.cat &lt;&lt; EOF | kubectl create -f-apiVersion: run.tanzu.vmware.com/v1alpha1kind: TanzuKubernetesClustermetadata:  name: $CLUSTER_NAME  namespace: $NSspec:  distribution:    version: $K8S_VERSION  topology:    controlPlane:      count: 1      class: best-effort-medium      storageClass: $SC      volumes:        - name: etcd          mountPath: /var/lib/etcd          capacity:            storage: 4Gi    workers:      count: 3      class: best-effort-medium      storageClass: $SC      volumes:        - name: containerd          mountPath: /var/lib/containerd          capacity:            storage: 50Gi  settings:    network:      services:        cidrBlocks: [\"10.96.0.0/16\"]      pods:        cidrBlocks: [\"172.20.0.0/16\"]    storage:      classes: [\"$SC\"]      defaultClass: $SCEOFAfter a few minutes the cluster will be deployed. (How long depends on the speed of your infrastructure, but say 15-20 minutes.)Login to the Workload ClusterOnce the new, completely separate k8s cluster is created, we use the kubectl vsphere plugin to login to the workload cluster, switch to that kube context, and from this point on we’ll talk to that cluster’s Kubernetes API, not the supervisor cluster.k vsphere login --server 10.0.14.128 --insecure-skip-tls-verify -u administrator@vsphere.local\\  --tanzu-kubernetes-cluster-name $CLUSTER_NAME \\  --tanzu-kubernetes-cluster-namespace $NSOnce that command completes you’ll have a new context for the workload cluster.$ kc10.0.14.128dev-team-purpledev-team-tundratanzu-build-service-cluster2test-nsUse that context to deploy TBS into that cluster.$ kc tanzu-build-service-cluster2✔ Switched to context \"tanzu-build-service-cluster2\".Once switched to taht config, we can talk to that cluster’s API, and for example, get the nodes that make up the cluster. There should be one control plane and three worker nodes, unless you adjusted the cluster YAML.$ k get nodesNAME                                                         STATUS   ROLES                  AGE   VERSIONtanzu-build-service-cluster2-control-plane-86zx9             Ready    control-plane,master   39m   v1.20.7+vmware.1tanzu-build-service-cluster2-workers-268cf-9686cf46d-4ccdh   Ready    &lt;none&gt;                 33m   v1.20.7+vmware.1tanzu-build-service-cluster2-workers-268cf-9686cf46d-6sznb   Ready    &lt;none&gt;                 33m   v1.20.7+vmware.1tanzu-build-service-cluster2-workers-268cf-9686cf46d-d7nld   Ready    &lt;none&gt;                 33m   v1.20.7+vmware.1Configure Pod Security PolicyThe supervisor cluster configures some default security which we will need to further configure to allow TBS to deploy into this cluster.  NOTE: Ensure your kubeconfig is set to the workload cluster, not the supervisor cluster!  NOTE: This RBAC is good for a PoC, it’s likely that we would want to customize this for production.cat &lt;&lt; EOF | kubectl create -f-apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: psp:privilegedrules:- apiGroups: ['policy']  resources: ['podsecuritypolicies']  verbs:     ['use']  resourceNames:  - vmware-system-privileged---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: all:psp:privilegedroleRef:  kind: ClusterRole  name: psp:privileged  apiGroup: rbac.authorization.k8s.iosubjects:- kind: Group  name: system:serviceaccounts  apiGroup: rbac.authorization.k8s.ioEOFNow we can deploy TBS!Install TBS  NOTE: Best to read through the official docs before proceeding.I’m going to skip through the requirements section and assume that you have read through it and downloaded all the correct pieces which are laid out in the above docs. There are quite a few pieces so please do read carefully. There are some activities you have to do, like downloading CLIs and accepting EULAs and the like.At this point we can start the deployment.Copy TBS Images to Your Container Image RepositoryWe use imgpkg to copy the TBS images to your repo.First set up a variable.export IMAGE_REPO=\"&lt;your.repo/some-repo&gt;\"In my example I’m using the Azure container registry. (Usually I would use Harbor, but I thought I’d try something different today.)  NOTE: This will take a few minutes to complete as we are copying several images from one repo to another.imgpkg copy -b \"registry.pivotal.io/build-service/bundle:1.2.2\" --to-repo $IMAGE_REPONow pull this image locally and unpack in /tmp.imgpkg pull -b $IMAGE_REPO:1.2.2 -o /tmp/bundleThere should be files in /tmp/bundle.$ find /tmp/bundle/ | head/tmp/bundle//tmp/bundle/values.yaml/tmp/bundle/.imgpkg/tmp/bundle/.imgpkg/images.yml/tmp/bundle/config/tmp/bundle/config/values.star/tmp/bundle/config/ca-cert.yaml/tmp/bundle/config/pod-webhook/tmp/bundle/config/pod-webhook/rbac.yaml/tmp/bundle/config/pod-webhook/deployment.yamlDeploy TBSAnd we can now deploy.First configure some variables.  NOTE: Please configure all of these variables. They should not be empty.export IMAGE_REPO=\"\" # where we we copied the TBS images toexport REGISTRY_USER=\"\"export REGISTRY_PASS=\"\"export TANZUNET_USER=\"\"export TANZUNET_PASS=\"\"Next, perform the deployment.  NOTE: We’re using various Carvel tools to perform the deployment. ytt, kbld, and kapp to name a few.  NOTE: If you supply the tanzunet user/password TBS will be configured to automatically talk to tanzunet and download the latest buildpacks so that you will always be automatically up to date. If they aren’t supplied, that’s fine, you’ll just be in charge of updating the underlying buildpacks. When supplying this information the last step in the deployment can take a while because it’s downloading and uploading images into your registry.  NOTE: Of course this assumes that you’ve followed the TBS docs and downloaded all the Carvel CLIs.ytt -f /tmp/bundle/values.yaml \\    -f /tmp/bundle/config/ \\    -v docker_repository=\"$IMAGE_REPO\" \\    -v docker_username=\"$REGISTRY_USER\" \\    -v docker_password=\"$REGISTRY_PASS\" \\    -v tanzunet_username=\"$TANZUNET_USER\" \\    -v tanzunet_password=\"$TANZUNET_PASS\" \\    | kbld -f /tmp/bundle/.imgpkg/images.yml -f- \\    | kapp deploy -a tanzu-build-service -f- -yThis will take a few minutes to deploy.Once it completes we can run kapp list.$ kapp listTarget cluster 'https://10.0.14.140:6443' (nodes: tanzu-build-service-cluster2-control-plane-86zx9, 3+)Apps in namespace 'default'Name                 Namespaces                      Lcs   Lca  tanzu-build-service  (cluster),build-service,kpack,  true  2m                       stacks-operator-system                  Lcs: Last Change SuccessfulLca: Last Change Age1 appsSucceededAnd would want to see succeeded.Build an ImageWith TBS installed we have extended the Kubernetes API so that it knows how to build container images. So to build images we’ll talk to the Kubernetes API using YAML, just like any other Kubernetes object such as pods.Configure Container Image Repository SecretDecide what namespace you want to have the images in. I’ll use the default namespace.  NOTE: I use kubectx to manage my clusters and namespaces.$ kn default✔ Active namespace is \"default\"Create a repository secret. TBS needs to have write access to the container image repo to push the resulting image to.Use kp to do that.  NOTE: kp is the kpack CLI. It’s a way to use TBS and kpack. But it’s important to understand that kp just talks to Kubernetes, we can get the same information out of Kubernetes using kubectl as we can with kp.Set up some vars.export SECRET_NAME=\"\"export REGISTRY=\"\"export REGISTRY_USER=\"\"export REGISTRY_PASS=\"\"Now create the secret.kp secret create $SECRET_NAME --registry $REGISTRY --registry-user $REGISTRY_USER --namespace defaultYou will have to enter the registry password on the command line.Now that we have TBS installed and a repo secret configured we can build an image.Build Spring PetclinicWe need to ensure we’re going to upload the newly built image to the right container image repository. This repository is where you want the resulting image to end up!export REPOSITORY=\"your.container.image.repo/some-repo\"Now ask TBS to build the image by using the kp CLI.  NOTE: This will take a while on the first build as all the maven dependencies will get downloaded…Spring Petclinic is written in Java.[ -z \"$REGISTRY\" ] &amp;&amp; echo \"ERROR: Please set REGISTRY variable\" || \\  kp image create spring-petclinic-image \\  --tag $REGISTRY/spring-petclinic-image \\  --git https://github.com/ccollicutt-tanzu/spring-petclinic \\  --git-revision mainWe can watch logs of the build with:kp build logs spring-petclinic-imageOnce the build is completed, the image will be pushed to the $REPOSITORY.  NOTE: I’ve pushed to Azure’s container image registry, yours would be different. Harbor is a great choice as well. I just wanted to try out Azure’s registry.$ az acr repository list --name $MY_REPO --output table | grep spring-petclinic-imagebuild-service/spring-petclinic-imageFrom here on we can magically build almost any application just from the artifacts or source, without having to write and manage a dockerfile. Amazing!ConclusionUltimately this post was about setting up a vSphere with Tanzu cluster that can accept a TBS deployment to it. We didn’t spend much time on why you’d want to use TBS. For that I’d suggest watching a video.Hat Tip  This repo"
    } ,
  
    {
      "title"    : "Using kubectl run to create privileged container",
      "category" : "",
      "tags"     : "",
      "url"      : "/2021/08/24/run-privileged-pod-kubectl-run.html",
      "date"     : "2021-08-24 00:00:00 -0400",
      "content"  : "This is the whole post. (Is there an easier way to do this?) kubectl run --rm -i --tty busybox --image=busybox --restart=Never --overrides='{\"spec\": {\"template\": {\"spec\": {\"containers\": [{\"securityContext\": {\"privileged\": true} }]}}}}' -- whoamiBoom! Now make sure you can’t do that in your cluster."
    } ,
  
    {
      "title"    : "Software Supply Chain Security Part 1 - Container Images",
      "category" : "",
      "tags"     : "",
      "url"      : "/2021/07/30/supply-chain-security-part-1.html",
      "date"     : "2021-07-30 00:00:00 -0400",
      "content"  : "Problem : I need to package softwareSolution: DockerfilesNew Problem: I need to manage DockerfilesNew solution: ???OverviewThere are many pieces to a modern, secure software supply chain. I say modern because this series of posts will focus on what it takes to build a secure software supply chain when the target for runnign these applications is Kubernetes. Kubernetes means containers…and containers mean, you guessed it, container images. So let’s start there.tl;drThere are many tools to build container images, not just Dockerfiles. In fact, in large organizations, Dockerfiles are IMHO an anti-pattern. You need a tool that can 1) build images without Dockerfiles and 2) separates the OS from the app. Buildpacks and more specifically Paketo solve these, and other, problems.Container ImagesI talk to many organizations about container images. The reality is that almost everyone equates container images with Dockerfiles, meaning most people believe the only way to create a container image, which they might call a “Docker image” is by using a Dockerfile.If you get any one thing from this post, it’s important to understand what a container image really is. What it really is…is an open source specification that defines the “file bundle” that makes up what we call a “container image.” (Ultimately, at this time, a container image is a glorified tar file.)  This specification defines how to create an OCI Image, which will generally be done by a build system, and output an image manifest, a filesystem (layer) serialization, and an image configuration. - OCIThe reality is that anyone can build a tool that can create an image which meets this specification. Anyone. It does not have to be based on Dockerfiles. In fact I would suggest that while Dockerfiles are great they are not necessarily the best tool to use as part of building a secure software supply chain. There are other solutions, not many, but there are definitely choices that can be made (and I present one of them in this post).Examples of OCI Compliant Container Image Build ToolsFirst, what other tools are out there for building OCI compliant images?  NOTE: Not all of these tools are maintained, and not all would be usable in production. This is just a list to show that there are several tools one can use to build an OCI compliant image, not all of which use Dockerfiles. (However, that said, I don’t think there are quite enough tools to show the vibrant OCI image building ecosystem that one would expect given the popularity of containers.)Here’s an incomplete list:  Buildpacks  img  Buildah  Kaniko  Jib  s2i  Buildkit  koAs can be seen from the above there are several tools which could be used, as opposed to “docker build…”. Each of these tools makes different architectural and usability choices. In fact some target only specific runtimes, eg. ko targets golang apps.What Makes a Good Secure OCI Image Build Tool?No DockerfilesMy opinion is that a secure supply chain requires that there is, effectively, preferably, no Dockerfile. In my opinion, there’s too much power in Dockerfiles, too many ways to make mistakes and create security issues to allow people to have access to them, or for them to even be available. Developers should not be spending time crafting Dockerfiles.To me Dockerfiles are an anti-pattern, especially in large organizations with many applications. However, please don’t get me wrong: Dockerfiles have been and will continue to be an amazing tool for developers to build container images, bringing that capability to the masses. That said, using them as part of a secure supply chain is challenging…I believe too challenging for most organizations. It’s preferable for the image build tool to not use Dockerfiles or at least abstract (hide) them away from the developers and application ops teams. However, when hiding things in technology we know that issues can and will still leak out, so perhaps best to just not have Dockerfiles at all.Separation of ConcernsI want a OCI build tool that separates, at the very least, the operating system from the application. I want to be able to swap out the OS without breaking, or even affecting, the application. This is because if there is one thing that we can depend on in IT, it’s that the OS will have security issues, ie. CVEs, and we’ll need to fix those CVEs to remain secure. However, if the application, and its dependencies, and its runtime, and the OS are all hashed together into a container image, and we can’t swap any one of those without affecting the other layers, then that is a major security issue, as development and/or application operation teams will be (very) hesitant to update the image because they don’t know what will happen to the application.An easy way to test if this capability is available in an OCI image tool is if we can use it to build an image “out of band” from the build pipeline. Can we update the OS of an image, most likely to fix any CVEs or bugs, and roll that image out to all applications that use it, without having to go through the entire application build pipeline? In most situations that would not be possible because the build pipeline and the images are so tightly intertwined that it is not feasible. But it should be.Other NeedsThere are other things that a great OCI image build tool should have. I’ll list a few here, but I don’t want this post to go on for too long. (I may tackle these in a future blog post.)  Reproducable builds  Built-in intelligence  Just enough customization capability  Bill of materials (BoM)  Minimal attack surface  Support many runtimes  Caching  Build images in unprivileged containersRecommended Solution: Buildpacks and PaketoI think that Buildpacks–and more specifically Paketo–present a solution to many of the problems organizations will encounter when trying to build a secure software supply chain using container images.With regards to my two main points:  No Dockerfiles - Buildpacks do not use Dockerfiles at all. They are not hidden away through abstraction, they just don’t exist. Can’t edit what doesn’t exist. (Though if you want to create custom buildpacks, you can use Dockerfiles)  Separation of Concerns - The application, dependencies, and OS are separated out and in fact the resulting images can be “rebased” in which one layer is changed without affecting the othersUsing Pack and Paketo Buildpacks (but not Dockerfiles)PaketoFirst, what’s Paketo?I would say that Paketo is a project that uses Buildpacks to provide container images that can run anywhere, including Kubernetes. As well they support many language runtimes. I would almost consider Paketo a distribution of modern, well-considered, usable, community generated buildpacks that have taken what buildpacks have done and built upon it, by making them even more composable and modular.For the purposes of this post, I’ll use Paketo Buildpacks to build apps which target Kubernetes.PackPack is the tool that actually generates the Buildpack images so let’s use pack to build a container image.First, install pack which can be used to create buildpack based images. I’m doing so on Linux.sudo add-apt-repository ppa:cncf-buildpacks/pack-clisudo apt-get updatesudo apt-get install pack-cliNow I’ve got the pack cli.$ which pack/usr/bin/pack$ pack version0.19.0Next, build an app. First checkout the sample.git clone https://github.com/paketo-buildpacks/samplescd samples/java/maven/Now build the app.  NOTE: Here I’m calling the image “applications/maven”.  NOTE: I’m using the Paketo buildpack found at “paketobuildpacks/builder:base”.pack build applications/maven --builder paketobuildpacks/builder:baseEg. output:  NOTE: This is pulling Java dependencies, and, well, there are many of those to pull on the first build.$ pack build applications/maven --builder paketobuildpacks/builder:basebase: Pulling from paketobuildpacks/builderDigest: sha256:4fae5e2abab118ca9a37bf94ab42aa17fef7c306296b0364f5a0e176702ab5cbStatus: Downloaded newer image for paketobuildpacks/builder:basebase-cnb: Pulling from paketobuildpacks/runDigest: sha256:a285e73bc3697bc58c228b22938bc81e9b11700e087fd9d44da5f42f14861812Status: Downloaded newer image for paketobuildpacks/run:base-cnb===&gt; DETECTING7 of 18 buildpacks participatingpaketo-buildpacks/ca-certificates   2.3.2paketo-buildpacks/bellsoft-liberica 8.2.0paketo-buildpacks/maven             5.3.2paketo-buildpacks/executable-jar    5.1.2paketo-buildpacks/apache-tomcat     5.6.1paketo-buildpacks/dist-zip          4.1.2paketo-buildpacks/spring-boot       4.4.2===&gt; ANALYZINGPrevious image with name \"applications/maven\" not found===&gt; RESTORING===&gt; BUILDINGPaketo CA Certificates Buildpack 2.3.2  https://github.com/paketo-buildpacks/ca-certificates  Launch Helper: Contributing to layer    Creating /layers/paketo-buildpacks_ca-certificates/helper/exec.d/ca-certificates-helperPaketo BellSoft Liberica Buildpack 8.2.0SNIP!Paketo Spring Boot Buildpack 4.4.2  https://github.com/paketo-buildpacks/spring-boot  Creating slices from layers index    dependencies    spring-boot-loader    snapshot-dependencies    application  Launch Helper: Contributing to layer    Creating /layers/paketo-buildpacks_spring-boot/helper/exec.d/spring-cloud-bindings  Spring Cloud Bindings 1.7.1: Contributing to layer    Downloading from https://repo.spring.io/release/org/springframework/cloud/spring-cloud-bindings/1.7.1/spring-cloud-bindings-1.7.1.jar    Verifying checksum    Copying to /layers/paketo-buildpacks_spring-boot/spring-cloud-bindings  Web Application Type: Contributing to layer    Reactive web application detected    Writing env.launch/BPL_JVM_THREAD_COUNT.default  4 application slices  Image labels:    org.opencontainers.image.title    org.opencontainers.image.version    org.springframework.boot.version===&gt; EXPORTINGAdding layer 'paketo-buildpacks/ca-certificates:helper'Adding layer 'paketo-buildpacks/bellsoft-liberica:helper'Adding layer 'paketo-buildpacks/bellsoft-liberica:java-security-properties'Adding layer 'paketo-buildpacks/bellsoft-liberica:jre'Adding layer 'paketo-buildpacks/bellsoft-liberica:jvmkill'Adding layer 'paketo-buildpacks/executable-jar:classpath'Adding layer 'paketo-buildpacks/spring-boot:helper'Adding layer 'paketo-buildpacks/spring-boot:spring-cloud-bindings'Adding layer 'paketo-buildpacks/spring-boot:web-application-type'Adding 5/5 app layer(s)Adding layer 'launcher'Adding layer 'config'Adding layer 'process-types'Adding label 'io.buildpacks.lifecycle.metadata'Adding label 'io.buildpacks.build.metadata'Adding label 'io.buildpacks.project.metadata'Adding label 'org.opencontainers.image.title'Adding label 'org.opencontainers.image.version'Adding label 'org.springframework.boot.version'Setting default process type 'web'Saving applications/maven...*** Images (d7dcc3fd9295):      applications/mavenAdding cache layer 'paketo-buildpacks/bellsoft-liberica:jdk'Adding cache layer 'paketo-buildpacks/maven:application'Adding cache layer 'paketo-buildpacks/maven:cache'Successfully built image applications/mavenThat’s created this “applications/maven” image in my local Docker.$ docker images | grep applicationapplications/maven                                                    latest             d7dcc3fd9295   41 years ago    269MB  NOTE: It says 41 years ago because it is a reproducable build. More on that maybe in other blog posts.But where is the Dockerfile?$ tree.├── bindings│   └── maven│       ├── settings.xml│       └── type├── mvnw├── mvnw.cmd├── pom.xml├── README.md└── src    ├── main    │   ├── java    │   │   └── io    │   │       └── paketo    │   │           └── demo    │   │               └── DemoApplication.java    │   └── resources    │       └── application.properties    └── test        └── java            └── io                └── paketo                    └── demo                        └── DemoApplicationTests.java14 directories, 9 filesThere is none! pack uses buildpacks and does NOT use a Dockerfile. Nice!Buildpack Rebasing - Out of Band Image UpdatesMost customers I talk to have to push an image through the entire build pipeline to build it…where “it” is the OS, dependencies, and application artifacts. This means that any time there is a CVE, the entire build must be run. What this also suggests is that the ability for the application to properly run is also tied to the entirety of the image. This makes updating images when there isn’t an application change challenging, as no one is really sure if it’s going to work or not…But with buildpacks, the application, OS, and runtimes, and dependencies (and more) are separated out into individual pieces that can be swapped out without harming the application. With buildpacks this is called rebasing.  Rebase allows app developers or operators to rapidly update an app image when its stack’s run image has changed. By using image layer rebasing, this command avoids the need to fully rebuild the app.The term rebase mostly comes from the world of git:  Rebasing is the process of moving or combining a sequence of commits to a new base commit. – git rebaseSo let’s rebase the image.Inspect the current version.$ pack inspect applications/mavenInspecting image: applications/mavenREMOTE:(not present)LOCAL:Stack: io.buildpacks.stacks.bionicBase Image:  Reference: 5eaa2a599cd59e0e1d67132de78d590ef0f34512ede6acefd09416548f52a994  Top Layer: sha256:10dd4d5e8186feb5b6ab2a877c80e1616e426ed383b7f19358b7703686fa4f9aRun Images:  index.docker.io/paketobuildpacks/run:base-cnb  gcr.io/paketo-buildpacks/run:base-cnbBuildpacks:  ID                                         VERSION        HOMEPAGE  paketo-buildpacks/ca-certificates          2.3.2          https://github.com/paketo-buildpacks/ca-certificates  paketo-buildpacks/bellsoft-liberica        8.2.0          https://github.com/paketo-buildpacks/bellsoft-liberica  paketo-buildpacks/maven                    5.3.2          https://github.com/paketo-buildpacks/maven  paketo-buildpacks/executable-jar           5.1.2          https://github.com/paketo-buildpacks/executable-jar  paketo-buildpacks/apache-tomcat            5.6.1          https://github.com/paketo-buildpacks/apache-tomcat  paketo-buildpacks/dist-zip                 4.1.2          https://github.com/paketo-buildpacks/dist-zip  paketo-buildpacks/spring-boot              4.4.2          https://github.com/paketo-buildpacks/spring-bootProcesses:  TYPE                  SHELL        COMMAND        ARGS  web (default)                      java           org.springframework.boot.loader.JarLauncher  executable-jar                     java           org.springframework.boot.loader.JarLauncher  task                               java           org.springframework.boot.loader.JarLauncherRebasing using a much older image. Of course, this would be done in reverse in the real world, where we would rebase with a newer image (that presumably has the security issues fixed). But for simplicity, given I’ve already created an image using the most recent run image, I’ll go backwards here just for fun. Same idea no matter which way we go.$ pack rebase applications/maven --run-image paketobuildpacks/builder:0.1.135-base0.1.135-base: Pulling from paketobuildpacks/builder71c12072e01c: Already exists 8ac523e239f0: Pulling fs layer SNIP!72ad9888618d: Pull complete 4f4fb700ef54: Pull complete Digest: sha256:06fc9acb3b8098f7b717420d35f9cd8485ea1f92ce540769a2924ad7a161dad7Status: Downloaded newer image for paketobuildpacks/builder:0.1.135-baseRebasing applications/maven on run image paketobuildpacks/builder:0.1.135-baseSaving applications/maven...*** Images (b72546026b22):      applications/mavenRebased Image: b72546026b22fff4797625e36b8f4a6c0e4a5386fcd5460c12d173cb1000718eSuccessfully rebased image applications/mavenInspect that version:$ pack inspect applications/mavenInspecting image: applications/mavenREMOTE:(not present)LOCAL:Stack: io.buildpacks.stacks.bionicBase Image:  Reference: a8b66bfbe49565ffa1c74374ed0a38fb91adb43fa4a7a7c740b3f099b93a9c78  Top Layer: sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6efRun Images:  index.docker.io/paketobuildpacks/run:base-cnb  gcr.io/paketo-buildpacks/run:base-cnbBuildpacks:  ID                                         VERSION        HOMEPAGE  paketo-buildpacks/ca-certificates          2.3.2          https://github.com/paketo-buildpacks/ca-certificates  paketo-buildpacks/bellsoft-liberica        8.2.0          https://github.com/paketo-buildpacks/bellsoft-liberica  paketo-buildpacks/maven                    5.3.2          https://github.com/paketo-buildpacks/maven  paketo-buildpacks/executable-jar           5.1.2          https://github.com/paketo-buildpacks/executable-jar  paketo-buildpacks/apache-tomcat            5.6.1          https://github.com/paketo-buildpacks/apache-tomcat  paketo-buildpacks/dist-zip                 4.1.2          https://github.com/paketo-buildpacks/dist-zip  paketo-buildpacks/spring-boot              4.4.2          https://github.com/paketo-buildpacks/spring-bootProcesses:  TYPE                  SHELL        COMMAND        ARGS  web (default)                      java           org.springframework.boot.loader.JarLauncher  executable-jar                     java           org.springframework.boot.loader.JarLauncher  task                               java           org.springframework.boot.loader.JarLauncherIf I run that app, which was rebased onto a much older run image…$ docker run --rm -p 8080:8080 applications/mavenSetting Active Processor Count to 12Calculating JVM memory based on 38994352K available memoryCalculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx38598153K -XX:MaxMetaspaceSize=88998K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 38994352K, Thread Count: 50, Loaded Class Count: 13299, Headroom: 0%)Adding 129 container CA certificates to JVM truststoreSpring Cloud Bindings EnabledPicked up JAVA_TOOL_OPTIONS: -Djava.security.properties=/layers/paketo-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -agentpath:/layers/paketo-buildpacks_bellsoft-liberica/jvmkill/jvmkill-1.16.0-RELEASE.so=printHeapHistogram=1 -XX:ActiveProcessorCount=12 -XX:MaxDirectMemorySize=10M -Xmx38598153K -XX:MaxMetaspaceSize=88998K -XX:ReservedCodeCacheSize=240M -Xss1M -Dorg.springframework.cloud.bindings.boot.enable=true  .   ____          _            __ _ _ /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )  '  |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot ::                (v2.5.3)2021-07-30 13:44:52.093  INFO 1 --- [           main] io.paketo.demo.DemoApplication           : Starting DemoApplication v0.0.1-SNAPSHOT using Java 11.0.12 on 331541f6d651 with PID 1 (/workspace/BOOT-INF/classes started by cnb in /workspace)2021-07-30 13:44:52.096  INFO 1 --- [           main] io.paketo.demo.DemoApplication           : No active profile set, falling back to default profiles: default2021-07-30 13:44:52.906  INFO 1 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 1 endpoint(s) beneath base path '/actuator'2021-07-30 13:44:53.184  INFO 1 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 80802021-07-30 13:44:53.196  INFO 1 --- [           main] io.paketo.demo.DemoApplication           : Started DemoApplication in 1.376 seconds (JVM running for 1.646)So as you can see it’s simple and fast to “rebase” an image, ie. swap out the version of the OS but NOT the application, without having to go through an entire build.ConclusionDockerfiles are great, but, IMHO, not for building a secure software supply chain (not without considerable extra work at least). There are other ways to build container images that lend themselves more easily to building a secure software supply chain.I should mention that buildpacks and pack are just part of a full solution for managing images. Please check out kpack and the Tanzu Build service for more thoughts on what else is needed in the Kubernetes ecosystem. More on that in future posts.ThanksPlease note that the container flipping image in the title image is borrowed from the ko project. I’m using it because I think it’s hilarious, not because I necessarily am suggesting ko is a great build tool–I haven’t used it."
    } ,
  
    {
      "title"    : "Thoughts on the Certified Kubernetes Security Specialist Certification Exam",
      "category" : "",
      "tags"     : "",
      "url"      : "/2021/07/27/thoughts-on-the-cks-exam.html",
      "date"     : "2021-07-27 00:00:00 -0400",
      "content"  : "First, let me say that Kubernetes is an extremely challenging piece of software to use, and, of course, to secure. I work at VMware in the Tanzu group and Kubernetes is a massive part of our portfolio–in fact it’s the base of almost everything we do. But it’s just the base. You have to add so much on top of Kubernetes to make it useful, and even more to secure it. But enough about that, let’s talk about the Certified Kubernetes Security Specialist Certification (CKS).tl;dr  It took me two tries to pass the test, but I didn’t do much extra in terms of studying for the second try. I think the first try just made me realize how fast I have to be to complete the test…get fast!  I used the Kodecloud and the Kim Wüstkamp Udemy course  At this time, when you register for the test you get access to two practice test sessions on killer.sh. When I initially registered, I did not have access to these from the CNCF, instead they came with the Udemy course. But right now, when you register for the test, you get two sessions on killer.sh “for free”. These are extremely helpful, as are any practical lab or questions available.  I find practical labs and tests to be far the most valuable…videos and other training is not as useful  If I went through this again I would use the Kodecloud class which has great automated labs, and the two killer.sh practice test sessionsSome Areas to Focus OnUltimately, in my opinion, the CKS test is–not surprisingly–a test taking exercise. Two hours is not a lot of time to answer all the questions, and it’s really about speed and confidence.  Ensure you are following all the exam requirements, eg. make sure your desk is clear so you aren’t spending time worrying about that just before writing the exam. Note that these requirements can and will change over time, so double check prior to your own exam!  Use the copy buttons on the test to copy text instead of typing it out (in case you make a typo)  Be great with the command line, know how to edit with vi–the better you are at vi the faster you will finish questions  Know how to setup kubectl completion and use it  Copy initial configs to something like .orig in case you break the deployment  Go through all the questions and use the notepad to note the number, points, and area of the question  Do the highest value easiest questions first, moving onto lower value, etc etc  Get all of the questions you easily can answer first, then move onto the harder questions, but don’t leave any high points questions unanswered (if you do, you aren’t fast enough yet)  Double check that you have answered all of the question components before moving on (and if you have time at the end, come back and check)  Don’t worry if you don’t pass the first time, you have a second try, and will do much better in the second examFrom a Technical PerspectiveKubernetes ServicesKnow how to:  configure the Kubernetes manifests of the major k8s services, such as kube-apiserver  find the docs for the settings that are available for each service  configure the services manifests and how they restart  find the logs for the containers  use admission controllers, especially ImagePolicyWebhookKuberentes ConfigKnow how to:  create network policies  copy an running pod/deploy config and edit it3rd Party ToolsKnow how to:  deploy an apparmor profile  configure a pod to use an app armor profile  add rules quickly to Falco and start falco  implement security best practices on Dockerfiles  use the docs for these services, but ONLY the ones that have been listed as OK to use during the CKS examConclusionNote that I have only listed a few major things on this blog post. To pass the test you’d need to know a lot more, and all of that is covered in the documentation for the test, at least in terms of what is on the test.I personally believe that practical..er…practice is most important in terms of studying for the test. Watch fewer videos and instead practice actually implementing practical technical things as quickly as possible via the command line and the vi editor. Make up your own questions if you have to. This is why Killer.sh and the Kodecloud labs and practice tests are so useful. I would spend at least 75% of my time, if not more, on practical hands on (timed if possible) labs and questions as opposed to standard video training. But…this is just my opinion.Best of luck on your exam!PS.I put up my CKS Chrome bookmarks in github, but again, the allowed sites may change over time so please double check."
    } ,
  
    {
      "title"    : "How to Fix a Blinking Cursor on Ubuntu Boot",
      "category" : "",
      "tags"     : "",
      "url"      : "/2021/07/03/blinking-cursor-linux.html",
      "date"     : "2021-07-03 00:00:00 -0400",
      "content"  : "Today I figured I’d update my Linux workstation. Honestly I don’t like to do it because stuff breaks all the time. But I don’t want an out of date system either, so I just ran it (eep), and of course the update crashed midway through, and when I went to hard reboot I had a blinking cursor, which, from an existential perspective is awful and yet perfect at the same time. Thanks Linux! Haha.Fortunately the fix is pretty easy, certainly easier than fixing urban ennui and lockdown depression.First, don’t panic.Second, use “CTRL + ALT + F3” to switch to a console, then login from there. (See, Linux is up and running, it’s just the GUI login that’s borked.)Third, fix gdm3 and the failed update.sudo dpkg --configure -asudo dpkg-reconfigure gdm3sudo service gdm3 restartAnd that should fix it. Or at least it did in my case. Best of luck!"
    } ,
  
    {
      "title"    : "OpenBSD 6.9 on Protecli 6 Port Firewall",
      "category" : "",
      "tags"     : "",
      "url"      : "/2021/05/31/openbsd-protecli.html",
      "date"     : "2021-05-31 00:00:00 -0400",
      "content"  : "I’ve had a homelab for a while. And an old printer. And a wifi network. And a office network. Up until now it’s been a bit of a free for all, everything was connected to everything else with no limitations or isolation. That’s probably not great security-wise. Also…all this ransomware talk…it’s concerning. Looks like it’s time to implement some network isolation. So I bought a six port fanless firewall and put OpenBSD on it.Protecli 6 PortThe device was about $500, which is pricey. But I wanted 6 ports.What I bought:Protectli Vault 6 Port, Firewall Micro Appliance/Mini PC - Intel Dual Core, AES-NI, 4GB RAM, 32GB mSATA SSD What it has:THE VAULT: Secure your network with a compact, fanless &amp; silent firewall. Comes with US-based Support &amp; 30-day money back guarantee!CPU: Intel Dual Core Celeron 3865U, 64 bit, 1.8GHz, 2MB Smart Cache, Intel AES-NI hardware supportPORTS: 6x Intel Gigabit Ethernet NIC ports, 4x USB 3.0, 1x RJ-45 COM, 1x HDMICOMPONENTS: 4GB DDR4 RAM, 32GB mSATA SSDCOMPATIBILITY: Firewalls tested with pfSense, untangle, OPNsense and other popular open-source software solutions.The CPU isn’t great, there are definitely better options, but it started to get to expensive. I think this CPU will be fine for my purposes.Install OpenBSDI installed via the com port. I used a USB to serial adapter connected to my Linux workstation.To connect:sudo screen /dev/ttyUSB0 115200I downloaded the OpenBSD 6.9 img file and dded it to a USB device.Then plugged that device into the Protecli.Next I set the BIOS in the Protecli to be “legacy only”, othewise OpenBSD will give this error:probing: pc0 com0 mem[352K 280K 2153M 83M 1M 1776M]disk: hd0 hd1*&gt;&gt; OpenBSD/amd64 BOOTX64 3.57boot&gt;cannot open hd0a:/etc/random.seed: No such file or directorybooting hd0a:/6.9/amd64/bsd.rd: 3818189+1590272+3878376+0+704512 [109+288+28]=0x989530entry point at 0x1001000So get into the BIOS by pressing the DEL key when the box is booting up. Then change to “legacy only” and OpenBSD should boot.Once OpenBSD boots up, enter the following to setup the com port.stty com0 115200set tty com0From there you should be good to install. I just use all the defaults for now.Here you can see all six ports:# ifconfig | grep emem0: flags=8802&lt;BROADCAST,SIMPLEX,MULTICAST&gt; mtu 1500em1: flags=8802&lt;BROADCAST,SIMPLEX,MULTICAST&gt; mtu 1500em2: flags=8802&lt;BROADCAST,SIMPLEX,MULTICAST&gt; mtu 1500em3: flags=8802&lt;BROADCAST,SIMPLEX,MULTICAST&gt; mtu 1500em4: flags=8802&lt;BROADCAST,SIMPLEX,MULTICAST&gt; mtu 1500em5: flags=8802&lt;BROADCAST,SIMPLEX,MULTICAST&gt; mtu 1500ConclusionI haven’t put this into use yet, but I should soon. I’m assuming it’s going to work fine, but I’ll update this post after I’ve used it for a while."
    } ,
  
    {
      "title"    : "Using xfreerdp from Linux to Windows (with i3)",
      "category" : "",
      "tags"     : "",
      "url"      : "/2021/02/10/xfreerdp-from-linux-to-windows.html",
      "date"     : "2021-02-10 00:00:00 -0500",
      "content"  : "I use Ubuntu 20.04. I have a Windows VM, mostly for doing things like Power Point. I like to connect to that VM with xfreerdp so that I can easily flit around and still use i3 on my desktop. I put the xfreerdp session on one of my virtual desktops.xfreerdp /u:USER /p:SOMEPASS /v:SOMEIP /f +fonts /floatbar /smart-sizing -grab-keyboard /sound /microphone /multimonI run the above command, whcih I usually alias. One of the key commands is the -grab-keyboard otherwise i3’s MOD key might not work.Then the xfreerdp window pops up. Then I:  Hit “MOD + SHIFT + Spacebar” to make the window floating  Hit “MOD” and drag the window to position it properly over the two monitorsThat’s it. That’s the post. I really find this a useful way of working.Sound works, but I can’t get the micrphone to work.Let me know if you have any ideas to make this better!"
    } ,
  
    {
      "title"    : "Linux Budget Workstation",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/12/15/linux-budget-workstation.html",
      "date"     : "2020-12-15 00:00:00 -0500",
      "content"  : "There’s not a lot of pieces to a desktop computer. Here’s what I have, bought just after the pandemic started and it became clear I was going to be working from home for a substantial amount of timeI had a few goals and requirements for this build:  Low cost  Lots of CPU  Not annoyingly loud  Minimum 64GB of memory  No need for gamingI knew that I would run several VMs on the workstation, mostly to split out different things I need to do, for example access work related systems via a VPN in a Windows based virtual machine, have a VM for doing demos, ect ect, so I would need at least 64gb of memory.The BuildThe cost of this build was, pre-tax, $1300 CDN! With Ontario tax and the $50 build fee it came out to just over $1500. (I should note that this build was done about 7 months prior to this post, so it’s been a while, a quick glance suggests the pricing is still about the same.)I bought the parts at Memory Express and had them build the system for an extra $50. They were great to work with.            Component      Item      Cost                  CPU      AMD Ryzen 5 3600 Processor      $259.99              Case      Corsair Carbide Series 100R Mid-Tower Case, Silent Edition, Black      $89.99              Memory      Corsair Vengeance LPX 64GB DDR4 2666MHz CL16 Dual Channel Kit (4x 16GB), Black      $389.99              Storage      Western Digital Blue SN550 M.2 PCI-E NVMe SSD, 1TB      $169.99              Power Source      Corsair RMx Series RM550x 80+ Gold Fully Modular ATX Power Supply, 550W      $144.99              Motherboard      Asus TUF B450-PRO GAMING w/ DDR4-2666      $169.99              Video Card      Gigabyte GeForce GT 710 2GB GDDR5 Low-Profile PCI-E w/ HDMI, DVI      $79.99        NOTE: This video card is pretty limited, but works great to power a HDMI monitor.ConclusionIn the end, I couldn’t be more happy with this build. It’s stable, quiet, and, as far as I’m concerned, high performance. But, that said, I don’t play video games, or do any GPU related activities. I just need to output to a monitor. It’s the perfect workstation for me, both from a cost and an experience perspective."
    } ,
  
    {
      "title"    : "Install Tanzu Kubernetes Grid in Azure",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/11/25/deploy-tkg-to-azure.html",
      "date"     : "2020-11-25 00:00:00 -0500",
      "content"  : "Tanzu Kubernetes Grid 1.2 (TKG) was recently released, and with it comes the ability to deploy TKG to Azure. Prior to 1.2 you could deploy to vSphere and AWS, but now, with 1.2, Azure is also supported. So you can now run the same Kubernetes with the same life cycle manager across vSphere, AWS, and Azure. That’s pretty powerful from a multicloud perspective.For this post, let’s focus on Azure.  NOTE: Read the official docs here.RequirementsTKG CLIUse this link to access my.vmware.com and download the TKG CLI. You’ll have to login to actually download.$ tkg versionClient:\tVersion: v1.2.0\tGit commit: 05b233e75d6e40659247a67750b3e998c2d990a5The above is the version of TKG that we’ll be using for this post.Azure CLIHere are the docs for the prerequisites for Azure’s Cluster API implementation. But don’t worry about reading that doc unless you want to. Not required.az account show This post assumes that the az CLI has been setup and configured enough so that you can run something like the above.DockerTKG uses a local Docker install to setup a small, ephemeral, temporary kind based Kubernetes cluster to build the TKG management cluster in Azure. (More about that later.) Thus we need Docker locally to run the kind cluster.I’m just using an Ubuntu 18.04 instance with the default docker.io package.$ cat /etc/lsb-release DISTRIB_ID=UbuntuDISTRIB_RELEASE=18.04DISTRIB_CODENAME=bionicDISTRIB_DESCRIPTION=\"Ubuntu 18.04.5 LTS\"$ dpkg --list docker.ioDesired=Unknown/Install/Remove/Purge/Hold| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)||/ Name                   Version          Architecture     Description+++-======================-================-================-==================================================ii  docker.io              19.03.6-0ubuntu1 amd64            Linux container runtimeThe Linux user doing the TKG deploy must be able to access Docker, ie. run something like the below.docker psEasy peasy.SSH KeyWe’ll also need an ssh key in ssh format. If there isn’t an ssh key setup, then the below command will create one.ssh-keygenIn this post I assume the key is in ~/.ssh/id_rsa.pub.Setup Azure and TKG VariablesFirst, decide on an Azure application name, such as “tkg-azure” and export that as a variable. This name is up to you.export AZURE_APP_NAME=\"tkg-azure\"Next get your Azure subscription ID.export AZURE_SUBSCRIPTION_ID=$(az account show --query id --output tsv)Now we can setup an Azure application with a service principle.export RETURNED_SP_APP_JSON=$(az ad sp create-for-rbac --name $AZURE_APP_NAME)And use the information returned by that command to configure the other variables we need.export AZURE_CLIENT_ID=$(echo \"$RETURNED_SP_APP_JSON\" | jq -r '.appId')export AZURE_CLIENT_SECRET=$(echo \"$RETURNED_SP_APP_JSON\" | jq -r '.password')export AZURE_TENANT_ID=$(echo \"$RETURNED_SP_APP_JSON\" | jq -r '.tenant')Need a couple of standard Azure variables.  NOTE: AZURE_ENVIRONMENT will always be “AzurePublicCLoud” but the AZURE_LOCATION can be any region you’d lke.export AZURE_LOCATION=\"canadacentral\"export AZURE_ENVIRONMENT=\"AzurePublicCloud\"Also, decide on a management cluster name, such as “tkg-mgmt”.export MANAGEMENT_CLUSTER_NAME=\"tkg-mgmt\"Setup a base64 encoded string of your ssh public key. Assuming your key is in ~/.ssh/id_rsa.pub.export AZURE_SSH_PUBLIC_KEY_B64=$(base64 &lt; ~/.ssh/id_rsa.pub | tr -d '\\r\\n')At this point we should have this many variables setup:$ env | grep \"AZURE\\|MANAGEMENT_CLUSTER\" | wc -l9If so, you are good to go to the next step.Accept Image LicenseAccept the license agreement for the images published to Azure. (This only has to be done once.)az vm image terms accept --publisher vmware-inc --offer tkg-capi --plan k8s-1dot19dot1-ubuntu-1804Deploy Management ClusterAt this point, with only a few Azure az commands, we’re setup to build the TKG management cluster.  NOTE: TKG uses Cluster API to manage the life cycle of Kubernetes clusters. TKG will first deploy a management cluster to Azure, which will contain Cluster API. To do that it uses a local Docker-based Kind cluster to bootstrap the management cluster. Once the management cluster has been bootstrapped into Azure the local Kind cluster is deleted, and going forward TKG will use the Azure based management cluster to build workload clusters. There are a few artifacts that should be kept, eg. ~/.kube  and ~/.tkg on the bootstrap node prior to its removal, but once those files are stored and secured the Linux virtual machine could be deleted.Now we can deploy the management cluster.Run tkg get mc to setup the ~/.tkg directory.tkg get mcNow build the management cluster.  NOTE: We are using the dev plan.tkg init --infrastructure=azure --name=\"$MANAGEMENT_CLUSTER_NAME\" --plan=dev -v 6With the management cluster deployed, we can now build many workload clusters.  NOTE: Like many others, I believe that organizations will require many Kubernetes clusters as opposed to a couple large ones. So TKG controls the life cycle of many, many clusters.Once that command has completed you’ll see something like:SNIP!Deleting Cluster=\"tkg-mgmt\" Namespace=\"tkg-system\"Deleting ClusterResourceSet=\"tkg-mgmt-cni-antrea\" Namespace=\"tkg-system\"Resuming the target clusterSet Cluster.Spec.Paused Paused=false Cluster=\"tkg-mgmt\" Namespace=\"tkg-system\"Context set for management cluster tkg-mgmt as 'tkg-mgmt-admin@tkg-mgmt'.Deleting kind cluster: tkg-kind-buv4teb68jjgrg38f0kgManagement cluster created!You can now create your first workload cluster by running the following:  tkg create cluster [name] --kubernetes-version=[version] --plan=[plan]At this point we can create workload clusters.Create Workload ClustersLet’s create a workload cluster.tkg create cluster workload-01 --plan=deveg. output looks like:$ tkg create cluster workload-01 --plan=devLogs of the command execution can also be found at: /tmp/tkg-20201125T132139302956804.logValidating configuration...Creating workload cluster 'workload-01'...Waiting for cluster to be initialized...Waiting for cluster nodes to be available...Waiting for addons installation...Workload cluster 'workload-01' createdBoom! Workload cluster done. That workload cluster can then be used for any Kubernetes applications.  NOTE: Use tkg get credentials workload-01 to obtain Kubernetes credentials for the workload cluster.ConclusionAt this point we have two clusters, one management, and one workload, and this was all done with a few commands. While the IaaS object configuration will look slightly different in each IaaS, the use of TKG will be the same though all.  Notice how we have Kubernets 1.19!$ tkg get clusters --include-management-cluster NAME         NAMESPACE   STATUS   CONTROLPLANE  WORKERS  KUBERNETES        ROLES       workload-01  default     running  1/1           1/1      v1.19.1+vmware.2  &lt;none&gt;      tkg-mgmt     tkg-system  running  1/1           1/1      v1.19.1+vmware.2  management At first glance, the bootstrapping process can seem a bit complex, but it’s only because we are using Kubernetes to bootstrap Kubernetes via Cluster API. When you view it through the lens of using Kubernetes constructs where ever possible with the product, it makes sense. Instead of building a separate bootstrapping installer, we use Cluster API, an open source Kubernetes project, the same that is used to build workload clusters. Why re-invent the wheel.Using TKG gets you the ability to manage the same kubernetes in the same way across many infrastructure as as service products. If multicloud capability is a goal for your organization, then TKG can definitely get you there in terms of Kubernetes. So TKG lets you bootstrap Cluster API onto several common IaaS solutions, thus abstracting away the underlying IaaS. And, of course, it provides life cycle management of Kubernetes clusters."
    } ,
  
    {
      "title"    : "Merge Kubernetes Config Files",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/11/06/merge-kubeconfigs.html",
      "date"     : "2020-11-06 00:00:00 -0500",
      "content"  : "Weirdly there aren’t a lot of examples of merging Kubeconfigs, I always end up on stackoverflow.Basically we use a env var to have multiple kubeconfigs set, the new standalone kube config and ~/.kube/config and merge them with –flatten.Here are the few CLI steps.First, make a copy of your kube config.$ cp ~/.kube/config ./kubeconfig-backup Next, setup a variable, KUBECONFIG, to point to both the config files and run kubectl config view --flatten, piping the output to a new file. Here I’m use the file name new-standalone.kubeconfig but your file name will be different, so change that.$ KUBECONFIG=./kubeconfig-backup:./new-standalone.kubeconfig kubectl config view --flatten &gt; new-kube-configCopy the new config file created to ~/.kube/config. Note that this over writes your existing config file, but for now you still have the backup copy that we created above.$ cp new-kube-config ~/.kube/config Validate that your config is working.$ kubectl config get-contextsFinally, if you are sure that your new config is good, remove the copy.$ rm ./kubeconfig-backupThat’s it!"
    } ,
  
    {
      "title"    : "Upgrade Tanzu Kubernetes Grid Multicloud 1.1.3 to 1.2",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/11/04/upgrade-tkg-multicloud.html",
      "date"     : "2020-11-04 00:00:00 -0500",
      "content"  : "This is just a quick example of upgrading Tanzu Kubernetes Grid (multicloud) 1.1.3 to 1.2. In this example, TKGm is running on vSphere.For those not familiar, TKG:  provides a consistent, upstream-compatible implementation of Kubernetes, that is tested, signed, and supported by VMware. Tanzu Kubernetes Grid is central to many of the offerings in the VMware Tanzu portfolio.TKGm, as I call it, can be deployed into various public clouds, more all the time, and provides the same Kubernetes no matter where it is deployed. 1.2 supports vSphere, Azure, and AWS as host infrastructure, and more will be added over time.What’s new in 1.2?  Moving from a separate loadbalancer to kube-vip  New default CNI: Antrea  Addition of Harbor as a shared service  Backup and restore management clusters with VeleroAnd more!Upgrade from 1.1.3 to 1.2 (on vSphere)The documentation for this process is great, and I’m mostly just repeating what it shows. Best to follow those docs, but sometimes having an example is nice.Initially I have the tkg 1.1.3 CLI.$ tkg versionClient:\tVersion: v1.1.3\tGit commit: 0e8e58f3363a1d4b4063b9641f44a3172f6ff406I’m just running one management and one workload cluster.$ tkg get cluster --include-management-cluster NAME        NAMESPACE   STATUS   CONTROLPLANE  WORKERS  KUBERNETES        ROLES   my-cluster  default     running  1/1           2/2      v1.18.6+vmware.1  &lt;none&gt;  tkg-mgmt    tkg-system  running  1/1           1/1      v1.18.6+vmware.1  &lt;none&gt; So the first step is to download the new 1.2 CLI as well as three OVAs. These artifacts are all downloaded from VMware.      Install the new CLI first.        Next, upload the three OVAs into vSphere and mark them as templates.  Kubernetes v1.19.1: Photon v3 Kubernetes v1.19.1 OVAKubernetes v1.18.8: Photon v3 Kubernetes v1.18.8 OVAKubernetes v1.17.11: Photon v3 Kubernetes v1.17.11 OVAFirst we upgrade the management cluster.  I’m conservative so I copied the old config files first.$ cp -rp ~/.tkg/ ~/.tkg-pre-1.2-upgrade  List the management cluster.$ tkg get management-clusterIt seems that the TKG settings on this system are out-of-date. Proceeding on this command will cause them to be backed up and overwritten by the latest settings.Do you want to continue? [y/N]: ythe old providers folder /home/ubuntu/.tkg/providers is backed up to /home/ubuntu/.tkg/providers-20201102220133-xryjaxetThe old bom folder /home/ubuntu/.tkg/bom is backed up to /home/ubuntu/.tkg/bom-20201102220133-sk8je1f4 MANAGEMENT-CLUSTER-NAME  CONTEXT-NAME             STATUS   tkg-mgmt *               tkg-mgmt-admin@tkg-mgmt  Success   Make sure to be using the mgmt cluster context.$ kubectl config use-context tkg-mgmt-admin@tkg-mgmt Switched to context \"tkg-mgmt-admin@tkg-mgmt\".  Add labels (new in 1.2):$ kubectl label -n tkg-system cluster.cluster.x-k8s.io/tkg-mgmt cluster-role.tkg.tanzu.vmware.com/management=\"\" --overwrite=truecluster.cluster.x-k8s.io/tkg-mgmt labeled  Run the upgrade of the mgmt cluster.$ tkg upgrade management-cluster tkg-mgmtSNIP!Patching MachineDeployment with the kubernetes version v1.19.1+vmware.2...Waiting for kubernetes version to be updated for worker nodes...Management cluster 'tkg-mgmt' successfully upgraded to TKG version 'v1.2.0' with kubernetes version 'v1.19.1+vmware.2'  Now the mgmt cluster has been upgraded.$ tkg get cluster --include-management-cluster NAME        NAMESPACE   STATUS   CONTROLPLANE  WORKERS  KUBERNETES        ROLES       my-cluster  default     running  1/1           2/2      v1.18.6+vmware.1  &lt;none&gt;      tkg-mgmt    tkg-system  running  1/1           1/1      v1.19.1+vmware.2  management   Can list the k8s versions. Note 1.19! Nice.$ tkg get kubernetesversions VERSIONS           v1.17.11+vmware.1  v1.17.3+vmware.2   v1.17.6+vmware.1   v1.17.9+vmware.1   v1.18.2+vmware.1   v1.18.3+vmware.1   v1.18.6+vmware.1   v1.18.8+vmware.1   v1.19.1+vmware.2    Finally, upgrade the workload cluster.$ tkg upgrade cluster my-clusterLogs of the command execution can also be found at: /tmp/tkg-20201102T223108680260342.logUpgrading workload cluster 'my-cluster' to kubernetes version 'v1.19.1+vmware.2'. Are you sure? [y/N]: yValidating configuration...Verifying kubernetes version...Retrieving configuration for upgrade cluster...Create InfrastructureTemplate for upgrade...Upgrading control plane nodes...Patching KubeadmControlPlane with the kubernetes version v1.19.1+vmware.2...Waiting for kubernetes version to be updated for control plane nodesUpgrading worker nodes...Patching MachineDeployment with the kubernetes version v1.19.1+vmware.2...Waiting for kubernetes version to be updated for worker nodes...Cluster 'my-cluster' successfully upgraded to kubernetes version 'v1.19.1+vmware.2'Well, that was pretty easy.OPTIONAL: Create a New ClusterHere I create a new cluster. Note the use of the --vsphere-controlplane-endpoint-ip which is available in 1.2 so you can set the k8s API IP address and presumably pre-set the DNS entry for your end users.tkg create cluster --plan dev 1-2-cluster --vsphere-controlplane-endpoint-ip 10.0.6.10Logs of the command execution can also be found at: /tmp/tkg-20201103T173324591159647.logValidating configuration...Creating workload cluster '1-2-cluster'...Waiting for cluster to be initialized...Waiting for cluster nodes to be available...Waiting for addons installation...Workload cluster '1-2-cluster' createdThat’s it!VSPHERE_TEMPLATE IssueDue to the way I had originally deployed TKGm, I had set the VSPHERE_TEMPLATE in the TKG config file and to deploy a new 1.2 cluster I needed to comment that out. Most users won’t have this set AFAIK, and it’s a simple config file change.$ tkg create cluster --plan dev 1-2-cluster --vsphere-controlplane-endpoint-ip 10.0.6.10Logs of the command execution can also be found at: /tmp/tkg-20201103T172950440339415.logValidating configuration...Error: : workload cluster configuration validation failed: vSphere config validation failed: vSphere template kubernetes version validation failed: unable to get or validate VSPHERE_TEMPLATE for given k8s version: incorrect VSPHERE_TEMPLATE (/Datacenter/vm/photon-3-kube-v1.18.6_vmware.1) specified for Kubernetes version (v1.19.1+vmware.2). TKG CLI will autodetect the correct VM template to use, so VSPHERE_TEMPLATE should be removed unless required to disambiguate among multiple matching templatesDetailed log about the failure can be found at: /tmp/tkg-20201103T172950440339415.logCommented out the option:$ grep -i template ~/.tkg/config.yaml VSPHERE_HAPROXY_TEMPLATE: /Datacenter/vm/photon-3-haproxy-v1.2.4-vmware.1# VSPHERE_TEMPLATE will be autodetected based on the kubernetes version. Please use VSPHERE_TEMPLATE only to override this behavior#VSPHERE_TEMPLATE: /Datacenter/vm/photon-3-kube-v1.18.6_vmware.1"
    } ,
  
    {
      "title"    : "Deploy Harbor with Helm",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/10/03/deploy-harbor-with-helm.html",
      "date"     : "2020-10-03 00:00:00 -0400",
      "content"  : "I’ve put together a demo/workshop/lab/what-have-you for deploying Harbor to Kubernetes with Helm. There’s also a video below that goes through all the steps, showing deploying a Tanzu Kubernetes Grid workload cluster via vSphere with Tanzu, and using the Tanzu Application Catalog to consume a Harbor Helm Chart.The idea with this demo is to provide a starting place to build up a production deployment of Harbor via Helm.That said, any Kubernetes cluster should be fine, as long as it has load balancer and persistent volume capability.  Git repo with documentation deploying Harbor with HelmHere’s a couple links to vSphere with Tanzu and the Tanzu Application Catalog which were used as the Kubernetes and Helm chart provider respectively.  vSphere with Tanzu  Tanzu Application Catalog"
    } ,
  
    {
      "title"    : "Use a Github Personal Access Token with the Concourse CI/CD System",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/08/17/personal-access-token-concourse.html",
      "date"     : "2020-08-17 00:00:00 -0400",
      "content"  : "It’s actually pretty easy, but I don’t see many examples of setting it up online.First, you need a Gitub Personal Access Token configured.Next, here’s the github resource. I’m using a forked Spring Petclinic as an example repository, which comes from a variable, as well as the access token. Note the use of password: x-oauth-basic.resources:  - name: spring-petclinic    type: git    source:      uri: ((spring-petclinic-repo-uri))      icon: github      branch: main      username: ((github-apptoken))      password: x-oauth-basicThis is the credentials file:spring-petclinic-repo-uri: https://github.com/ccollicutt/spring-petclinic.gitgithub-apptoken: YOUR_GITHUB_TOKENThat’s it!"
    } ,
  
    {
      "title"    : "Remove Recent Commits from a Git Repo",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/08/12/remove-the-last-few-git-commits.html",
      "date"     : "2020-08-12 00:00:00 -0400",
      "content"  : "I’ve been building a workshop that uses the Spring Petclinic example application.I’ve made two commits updating the welcome message, and I’ve pushed those commits to a repo in github, but now I want to remove them.Here are the commits:$ git logcommit 9fec5226ec3dbc900bf3939c10f7f5e2b3c5ee94 (HEAD -&gt; main, origin/main, origin/HEAD)Author: curtis &lt;curtis@serverascode.com&gt;Date:   Tue Aug 11 21:30:01 2020 -0400    update welcomecommit 405d7567681871bd04439c2f49e13f96a0816d27Author: curtis &lt;curtis@serverascode.com&gt;Date:   Tue Aug 11 21:20:01 2020 -0400    change welcome messagecommit c42f95980a943634106e7584575c053265906978Author: Martin Lippert &lt;mlippert@gmail.com&gt;Date:   Wed Jul 29 16:43:31 2020 +0200    remove push-to-pws button from guide, since pws free trials endcommit 02cc84223b296e7b401cdba74905b2e18a87e51eAuthor: Stephane Nicoll &lt;snicoll@pivotal.io&gt;Date:   Sat Jul 11 08:57:33 2020 +0200    PolishSNIP!As you can see above there are two commits, 9fec5226ec3dbc900bf3939c10f7f5e2b3c5ee94 and 405d7567681871bd04439c2f49e13f96a0816d27 and I want to remove them from the remote repo.It’s actually quite easy! I’m going to reset to the commit prior to my two commits, c42f95980a943634106e7584575c053265906978.$ git reset --hard c42f95980a943634106e7584575c053265906978HEAD is now at c42f959 remove push-to-pws button from guide, since pws free trials endNow git log shows…$ git logcommit c42f95980a943634106e7584575c053265906978 (HEAD -&gt; main)Author: Martin Lippert &lt;mlippert@gmail.com&gt;Date:   Wed Jul 29 16:43:31 2020 +0200    remove push-to-pws button from guide, since pws free trials endcommit 02cc84223b296e7b401cdba74905b2e18a87e51eAuthor: Stephane Nicoll &lt;snicoll@pivotal.io&gt;Date:   Sat Jul 11 08:57:33 2020 +0200    Polishcommit 5ad6bc3ccde1c61379d751f0751a1731114761afAuthor: Stephane Nicoll &lt;snicoll@pivotal.io&gt;Date:   Sat Jul 11 08:56:32 2020 +0200    Upgrade to spring javaformat 0.0.22SNIP!And force push:$ git push --forceTotal 0 (delta 0), reused 0 (delta 0)To github.com:ccollicutt/spring-petclinic.git + 9fec522...c42f959 main -&gt; main (forced update)Done!"
    } ,
  
    {
      "title"    : "Quick Look at the Tanzu Build Service",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/08/06/tanzu-build-service.html",
      "date"     : "2020-08-06 00:00:00 -0400",
      "content"  : "Building container images isn’t easy. Dockerfiles make it look easy, but it’s not. Not in the real world anyways. Container image security is hard. Managing dependencies is hard. Building pipelines (good ones)…is hard. Building pipelines on your own to do the same thing that everyone deploying apps on Kubernetes has to do–i.e. build images–likely doesn’t make (business) sense. Why build it on your own with bash and CI/CD systems when you can use something like kpack or the Tanzu Build Service…and make it declarative ala Kubernetes (you know, the thing you are actually deploying the app to).Aside: I use the term pipelines all the time, but often CI/CD pipelines are just poorly tested bash spaghetti code that ties together a few actions in a row. See my homelab automation haha. That sounds harsh, but it’s fair to say that the quality level of pipeline code is a wide, wide spectrum. And, of course, is usually not declarative.Tanzu Build ServiceTanzu Build Service (TBS) helps with automating image creation, as well as using buildpacks as the base image. To quote our Tanzu Build Service site:  Tanzu Build Service uses the magic of Cloud Native Buildpacks to rebase app images when specialized contractual base images are updated in a registry. This means you can resolve certain common vulnerabilities and exposures (CVE) without a rebuild.TBS uses Kubernetes CRDs.  Tanzu Build Service follows a Kubernetes-native declarative model and executes builds automatically against user-defined configuration. It includes kpack, a set of open-source resource controllers for Kubernetes maintained by VMware Tanzu.It’s based on the upstream, open source tool kpack.If you want to review the features, checkout this page.How to Get the Tanzu Build ServiceAs of today, TBS is not GA (generally available) but if you sign up for the Tanzu Network you can download the 0.2 non-GA release.InstallI’m not going to go over the install as it currently uses Duffle. There’s a great blog post that covers the Duffle-based install, configuring git repos, credentials, ect. This is exactly what I followed for this blog post.In my case, I’m using Docker Hub as the image repository, but I have a local Gitlab install that I’m pulling code from. That said TBS can use most any image registry or git server.Build an ImageI’ve gone ahead and installed TBS. Now that it’s installed, we have several CRDs available.$ k get crd | grep pivotalbuilders.build.pivotal.io                             2020-08-04T19:50:47Zbuilds.build.pivotal.io                               2020-08-04T19:50:47Zclusterbuilders.build.pivotal.io                      2020-08-04T19:50:47Zcustombuilders.experimental.kpack.pivotal.io          2020-08-04T19:50:47Zcustomclusterbuilders.experimental.kpack.pivotal.io   2020-08-04T19:50:47Zimages.build.pivotal.io                               2020-08-04T19:50:47Zsourceresolvers.build.pivotal.io                      2020-08-04T19:50:47Zstacks.experimental.kpack.pivotal.io                  2020-08-04T19:50:47Zstores.experimental.kpack.pivotal.io                  2020-08-04T19:50:47ZI’ve already built an image based on the Spring Petclinic code.$ k get imgNAME               LATESTIMAGE                                                                                                                READYspring-petclinic   index.docker.io/ccollicutttanzu/spring-petclinic@sha256:994c9fb87f591c734a86c2a8d18ca4f0beb339ee9cbe1e6ae19c989b541fd773   TrueTBS comes with a CLI called kp.To create an image:kp image create spring-petclinic ccollicutttanzu/spring-petclinic --git git@gitlab.example.com:root/test-project.gitI’ve built it a few times already.$ kp build list spring-petclinicBUILD    STATUS     IMAGE                                                                                                                       STARTED                FINISHED               REASONSNIP!6        SUCCESS    index.docker.io/ccollicutttanzu/spring-petclinic@sha256:eaed806fb9027a1b12d507464faab637afe92e76e3f995cb318a55275b3f9fa4    2020-08-06 10:00:51    2020-08-06 10:21:28    TRIGGER7        SUCCESS    index.docker.io/ccollicutttanzu/spring-petclinic@sha256:994c9fb87f591c734a86c2a8d18ca4f0beb339ee9cbe1e6ae19c989b541fd773    2020-08-06 10:59:25    2020-08-06 11:03:47    COMMITFor build 6 I triggered it manually with the kp CLI.kp image trigger spring-petclinicFor build 7 I simply committed a change to the Spring Petclinic git repo (my git repo) and TBS noticed the commit and built the new image. So on every commit you’ll get a new image. Of course that sounds like a lot, but most organizations would have some kind of production branch, maybe it’s main, and code only makes it to that branch once it’s tested.Now that you have an image, you could use some kind of gitops workflow to deploy based on that new image, and you could use something like the kapp-controller to automate that.Here’s a snippet from the build logs.$ kp build logs spring-petclinic -b 7 -n build-service | tail -20Reusing layer 'launcher'Reusing layer 'paketo-buildpacks/bellsoft-liberica:class-counter'Reusing layer 'paketo-buildpacks/bellsoft-liberica:java-security-properties'Reusing layer 'paketo-buildpacks/bellsoft-liberica:jre'Reusing layer 'paketo-buildpacks/bellsoft-liberica:jvmkill'Reusing layer 'paketo-buildpacks/bellsoft-liberica:link-local-dns'Reusing layer 'paketo-buildpacks/bellsoft-liberica:memory-calculator'Reusing layer 'paketo-buildpacks/bellsoft-liberica:openssl-security-provider'Reusing layer 'paketo-buildpacks/bellsoft-liberica:security-providers-configurer'Reusing layer 'paketo-buildpacks/executable-jar:class-path'Adding 1/1 app layer(s)Adding layer 'config'*** Images (sha256:994c9fb87f591c734a86c2a8d18ca4f0beb339ee9cbe1e6ae19c989b541fd773):      ccollicutttanzu/spring-petclinic      index.docker.io/ccollicutttanzu/spring-petclinic:b7.20200806.145925Reusing cache layer 'paketo-buildpacks/bellsoft-liberica:jdk'Adding cache layer 'paketo-buildpacks/maven:application'Reusing cache layer 'paketo-buildpacks/maven:cache'===&gt; COMPLETIONBuild successfulAbove you can see buildpacks in use by TBS to create the container image.ConclusionThis is just a quick example of TBS. While this looks very simple, and sometimes simple things look like they have no value, I can assure you there are a lot of things happening in the background, and without something like TBS organizations are typically left on their own to build it, and often fail, or only partially succeed."
    } ,
  
    {
      "title"    : "vSphere 7 with Kubernetes and NSXT 3.0",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/07/20/vsphere-7-with-kubernetes-nsxt-3.html",
      "date"     : "2020-07-20 00:00:00 -0400",
      "content"  : "Kuberenetes is a commodity. There I said it. Sure, I work in the Tanzu business unit of VMware, and we heavily value Kubernetes, as does VMware overall. VMware is a massive contributor to upstream Kubernetes–see Cluster API for example, and, well, there’s the whole Kubernetes-built-into-vsphere thing. But we are also very interested in what happens AFTER you have Kubernetes.At VMware we make it easy to get Kubernetes dial tone so that you can get to the good stuff, the important stuff: deploying applications. Dial tone is the easy part, writing new apps quickly and getting them running in production…that’s hard. With that said, the majority of Tanzu’s focus is on what you do after you have Kubernetes, and of course getting apps in production.But, let’s talk about the dial tone here, and especially NSXT 3.0.Getting Kubernetes up and running in vSphere 7There are many great blog posts on the matter:  Tanzu vSphere 7 with Kubernetes on NSX-T 3.0 VDS Install Part 1: Overview  Deploy a Tanzu Kubernetes cluster on vSphere 7  Automated vSphere 7 and vSphere with Kubernetes Lab Deployment Script  vSphere with KubernetesAny one of those will get you up and running. This post is my version of the above. That said, likely there is not quite enough here to get you completley up and running, but it will certainly provide you some direction.  This is also a great post on troubleshootingNetworkingTo enable “Workload Management” in vSphere 7 (aka Kubernetes) NSXT is required. Designing networks is not easy, and there are many paths that can be taken, but I’ll describe my overly simplified lab/PoC style path and you can diverge from it where ever you feel necessary.I still have two nics in these ESXi hosts, I’ve left the various management interfaces, vmotion, storage, ect, on one nic and everything else, i.e. NSXT, on the other, though it’s not strictly necessary, and the advantages of NSXT would probably be better displayed with a single interface…but I’m too lazy to change it! Suffice it to say you can use a single nic now, which is great for PoC and lab work.Virtual SwitchesOne of the biggest changes to networking with NSXT 3.0 and vSphere 7 is that VDS can be used by NSXT, instead of having to hand over a physical nic completely to NSX. This makes deployment a lot simpler!  With the release of vSphere 7 comes the vSphere Distributed Switch 7.0. This latest version comes with support for NSX-T Distributed Port Groups. Now, for the first time ever it is possible to use a single vSphere Distributed Switch for both NSX-T 3.0 and vSphere 7 networking! - Rutger BlomIn a previous post I diagrammed a simple NSXT VDS and NVDS layout, shown below. This design has separate VDS and NVDS switches as well as a full nic provided to NSXT. (Right click and view image if you want to see it larger.)For vSphere 7 and NSXT 3.0, it looks like the below. Note that in this setup we’ve got separate VLANs for the “Edge TEP” and the “Host TEP” but they have to be able to connect to one another at layer 3. So, now we need to have a MTU &gt;=1600 on the VLANs, but also the router needs to be able to route packets &gt;=1600 (but only between the Edge and Hosts TEP). Very important.Wondering why we have two TEP networks? Here’s why:  You probably thinking why is there a need for a 2nd network specifically for Edge VMs. The reason is because we are using only 1 Physical NIC and therefore in order to “force” the Geneve TEP traffic egressing from the Edge VM to pass through the Physical NIC as the traffic would require routing when communicating with the ESXi hosts Geneve TEP interfaces. - Acepod blogHere’s a list of NSX switches, er, well, “switch”.[root@nested-esxi7-2:~] nsxdp-cli vswitch instance listDvsPortset-1 (DNSX)              50 0e 6d 8d 5b b6 35 9c-b5 af c9 18 f5 76 b4 a9Total Ports:4096 Available:4080  Client                         PortID          DVPortID                             MAC                  Uplink           Management                     100663301                                            00:00:00:00:00:00    n/a              vmnic0                         2248146952      8                                    00:00:00:00:00:00                     Shadow of vmnic0               100663305                                            00:50:56:52:bd:fe    n/a              vmk10                          100663307       739a47e2-3c93-483f-ac4d-df039cfd559d 00:50:56:6a:06:11    vmnic0           vmk50                          100663308       40335d75-be5f-41f8-a5d5-5e4be4f9e996 00:50:56:66:71:7d    void             vdr-vdrPort                    100663309       vdrPort                              02:50:56:56:44:52    vmnic0           nsx-edge-1.eth2                100663310       19                                   00:50:56:8e:b5:2a    vmnic0           nsx-edge-1.eth1                100663311       36                                   00:50:56:8e:9a:7d    vmnic0           SupervisorControlPlaneVM       100663314       69576165-6ade-491f-a429-999655c87028 00:50:56:8e:b9:69    vmnic0           (2).eth0                                                                                                                  SupervisorControlPlaneVM       100663315       f6fba7ac-e04b-4d7e-9f71-0aa07886c83f 04:50:56:00:c8:01    vmnic0           (2).eth1                                                                                                                Note just the one VDS, which I’ve called “DNSX” in my deployment.Subnetting and Layer 3 RoutingI am not the worlds greatest authority on networking. But one thing I think that I do that helps me to understand networking is separate broadcast domains from subnetting. A VLAN or a “segment” or a “logical switch”…these things are all about broadcast domains. What IPs are put onto those broadcast domains are completely separate items. Often people get quite confused and conflate the two.  A broadcast domain is a logical division of a computer network, in which all nodes can reach each other by broadcast at the data link layer. - CBTNuggetsSo, as far as I’m concerned, in the NVDS/VDS layout diagram we’re really assigning network interfaces and applying broadcast domains to them. After we do that THEN we can decide what subnets will be assigned where, and, as well, think about layer 3 routing for t0, though that is relatively easy when using a simple static route to forward the networks handled by NSXT (in my lab, I give it a huge /16, haha).Here’s the nice topology visualization NSXT provides, and this topology occurs AFTER the Workload Control Plane (WCP) is deployed, as well as a Tanzu Kubernetes Grid cluster.Here’s the subnets configured for WCP, which are entered initially during the “WCP Enable” wizard. The routable ingress and egress networks will be created by the enablement process, and, if using static routing, should be forwarded from the central network to the t0 router.Not routable:  Pod CIDRs = 10.244.0.0/21  Services CIDR = 10.96.0.0/24Routable:  Ingress CIDRs (load balancing) = 10.4.3.0/24  Egress CIDRs (NAT) = 10.4.4.0/24NSXT ConfigurationUltimately, NSXT configuration is about setting up some profiles and applying them to edge VMs and ESXI hosts.Host Transport Node ProfileNote how the switch in use is VDS, not NVDS.Uplink ProfilesHost uplink:Edge uplink:ConclusionI’ve left out quite a bit about the actual clicking to install NSXT and vSphere 7 with Kubernetes. But at the end, what I’m most interested in is how the networking is laid out. The rest o the work to enable Kubernetes in vSphere 7 is quite straight forward, such as setting up the Kubernetes content library, creating a storage policy, etc. Once the network is up and running, it’s very, very easy to get WCP enabled and get access to the ability to deploy your own Tanzu Kubernetes Grid clusters."
    } ,
  
    {
      "title"    : "Simple NSX-T Design - Dual N-VDS and Edge VM on VDS",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/07/03/nsxt-design-1.html",
      "date"     : "2020-07-03 00:00:00 -0400",
      "content"  : "There are a few different ways to “design” an NSX-T deployment. Mostly I think about what VLANs, virtual switches, and physical NICs are available and what gets assigned where.Please note the inspiration for the diagram and design come from this great site. I loved the layout of the diagram, so it is a huge influence for the image above (though I completely recreated and adapted it as a Power Point slide of all things).Design: Dual N-vDS, Edge VM on VDS, Four Physical Nics, Default NSX-T Profiles, and TEP on Access PortsThat’s quite a mouthful, but it is what it is. This particular design consists of the following:  Dual N-vDS: one for the overlay and one for the uplink  Four physical network interfaces: two for VDS and two for NSX-T  The NSX-T Edge VM is on vSphere VDS provided interfaces (thus VLAN 0)  Using the default NSX-T profiles (both using VLAN 0)  The Transport End Point (TEP) VLAN is on access ports for the physical NICs that make up the NSX-T managed interfacesI believe in NSX-T 2.5 you can get this down to having a single N-vDS, but then would need to put the Edge VM TEP on a separate VLAN and have traffic routable between the two, i.e. the Edge VM’s TEP interface would not be on a VDS port group, as well as some additional advantages. But I’ll try to explore that in another post. For now, I like this design because it is quite simple, and is a pretty good design for a PoC of Tanzu Kubernetes Grid Integrated Edition (TKGI, formerly known as PKS).Ultimately it’s pretty easy to create a new NSX-T profile for the ESXi hosts, a few clicks, a few seconds, and that is what most people do as the ESXi host’s TEP VLAN is often a trunk port. But in this design I’m just using an access port, so the default profile with VLAN 0 works.ConclusionI haven’t created this design for production use. This is what I think is the simplest design for a proof of concept (where NSX-T isn’t the PoC focus). I would imagine a production design would look much different. Also this design works great in a nested lab.Don’t forget to set your TEP VLAN MTU &gt;=1600. Most people set it to 9k and are done with it. :)"
    } ,
  
    {
      "title"    : "Kubernetes Tips and Tricks",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/06/02/kubernetes-tips-and-tricks.html",
      "date"     : "2020-06-02 00:00:00 -0400",
      "content"  : "A few tips and tricks I’ve come across. Starting off small!Export Cluster Config from KubeconfigSay you have a whole bunch of clusters set in your kubeconfig file and you want to extract one. Just one.Set your config to that cluster (maybe use kubectx) and do:kubectl config view --minify --raw &gt; cluster.kubeconfigBoom!Merge Kube ConfigsCopy your backup, set an env var pointing to the backup config and the new standalone file, and use config view with the flatten option to produce a new, merged, config file, and finally copy that file back to ~/.kube/config.$ cp ~/.kube/config ./config-backup $ KUBECONFIG=./config-backup:./new-standalone.kubeconfig kubectl config view --flatten &gt; new-kube-config$ cp new-kube-config ~/.kube/config Find what pod has what PVC$ kubectl get pods --all-namespaces -o=json | jq -c \\&gt; '.items[] | {name: .metadata.name, namespace: .metadata.namespace, claimName:.spec.volumes[] | select( has (\"persistentVolumeClaim\") ).persistentVolumeClaim.claimName }'Writing an Operator in Shell!See the shell operator. Good times!Troubleshoot DNSSee this k8s dockubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yamlMake sure you are in the default namespace.Run a dig command from the pod.$ kubectl exec -i -t dnsutils -- dig +short google.com172.217.165.14"
    } ,
  
    {
      "title"    : "TUF, Notary, and Harbor Registry",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/05/28/notary-with-harbor.html",
      "date"     : "2020-05-28 00:00:00 -0400",
      "content"  : "If you watch this video by Justin Cormack, he starts it off by talking about an attack vector that is used successfully by “evil doers” time and time again…software updates. Securing software updates is complicated and can’t be solved with simple solutions, and what’s worse, when using simple solutions can make software updates ineffective. Without some kind of process/framework for managing updates, securing systems is impossible. Sure, securing software updates will require signing things digitally, most technical people get that, but that’s just one part of the solution and it’s not enough on its own.That’s where TUF, “The Update Framework,” comes in.  The Update Framework (TUF) helps developers maintain the security of software update systems, providing protection even against attackers that compromise the repository or signing keys. TUF provides a flexible framework and specification that developers can adopt into any software update system.The overview for TUF is quite good. I don’t want to try (and fail) to reproduce that work here. So just go read it. :)HarborThe easiest way to describe Harbor is that it’s an image registry. But, with the recent release of Harbor 2.0, it is much more than just a simple image registry…it’s an OCI compliant registry!  This release makes Harbor the first OCI ( Open Container Initiative)-compliant open source registry capable of storing a multitude of cloud-native artifacts like container images, Helm charts, OPAs, Singularity, and much more.When Harbor is deployed it can have Notary enabled.NotaryNotary is an implementation of TUF, and, like TUF, it’s a CNCF project.  Notary is one of the industry’s most mature implementations of the TUF specification and its Go implementation is used today to provide robust security for container image updates, even in the face of a registry compromise. Notary takes care of the operations necessary to create, manage, and distribute the metadata needed to ensure the integrity and freshness of user content. Notary/TUF provides both a client, and a pair of server applications to host signed metadata and perform limited online signing functions.Quick Look at Notary and HarborI’ve got a Harbor installation setup to use. I’ve also installed the notary CLI.First, I tell Docker to use content trust and point it to the Notary instance that comes with Harbor. In this case Harbor was deployed via the Helm chart, and Notary is exposed on its own hostname via ingress.export DOCKER_CONTENT_TRUST=1export DOCKER_CONTENT_TRUST_SERVER=https://notary.example.comNext I try to push an image. By doing this, I’ll be initializing the keys for use with Notary. I’m glossing over what is happening here, as generating these keys should have a lot of thought put around it, and they should be properly stored and themselves secured. More on this in a future post, I hope.$ docker pull alpine$ docker tag alpine harbor.example.com/secure/alpine-signed:1.0$ docker push  harbor.example.com/secure/alpine-signed:1.0The push refers to repository [harbor.example.com/secure/alpine-signed]3e207b409db3: Pushed1.0: digest: sha256:39eda93d15866957feaee28f8fc5adb545276a64147445c64992ef69804dbf01 size: 528Signing and pushing trust metadataYou are about to create a new root signing key passphrase. This passphrasewill be used to protect the most sensitive key in your signing system. Pleasechoose a long, complex passphrase and be careful to keep the password and thekey file itself secure and backed up. It is highly recommended that you use apassword manager to generate the passphrase and keep it safe. There will be noway to recover this key. You can find the key in your config directory.Enter passphrase for new root key with ID 6b438eb:Repeat passphrase for new root key with ID 6b438eb:Enter passphrase for new repository key with ID aafd072:Repeat passphrase for new repository key with ID aafd072:Finished initializing \"harbor.example.com/secure/alpine-signed\"Successfully signed harbor.example.com/secure/alpine-signed:1.0This first signed image push initializes TUF. Files are created in ~/.docker/trust.$ find ~/.docker/trust/ | head/home/ubuntu/.docker/trust//home/ubuntu/.docker/trust/tuf/home/ubuntu/.docker/trust/tuf/harbor.example.com/home/ubuntu/.docker/trust/tuf/harbor.example.com/secure/home/ubuntu/.docker/trust/tuf/harbor.example.com/secure/alpine-signed/home/ubuntu/.docker/trust/tuf/harbor.example.com/secure/alpine-signed/changelist/home/ubuntu/.docker/trust/tuf/harbor.example.com/secure/alpine-signed/metadata/home/ubuntu/.docker/trust/tuf/harbor.example.com/secure/alpine-signed/metadata/targets.json/home/ubuntu/.docker/trust/tuf/harbor.example.com/secure/alpine-signed/metadata/root.json/home/ubuntu/.docker/trust/tuf/docker.ioUsing the Notary CLI we can see that image is registered.$ alias notary=\"notary -s https://notary.example.com -d ~/.docker/trust\"$ notary list harbor.example.com/secure/alpine-signedEnter username: adminEnter password:NAME    DIGEST                                                              SIZE (BYTES)    ROLE----    ------                                                              ------------    ----1.0     39eda93d15866957feaee28f8fc5adb545276a64147445c64992ef69804dbf01    528             targetsIn the Harbor GUI the image is show as being signed.The Harbor project we are pushing the image to has content trust enabled.NOTE: What enabling this means is that Harbor won’t allow any images to be pulled from this repo unless they are signed.As an example, here I’m trying to pull an UNSIGNED image into a k8s cluster, which, when “content trust” is enabled in the project, Harbor will not allow.$ k describe pod alpine-unsigned-1.0  | tail                 node.kubernetes.io/unreachable:NoExecute for 300sEvents:  Type     Reason     Age                From                                          Message  ----     ------     ----               ----                                          -------  Normal   Scheduled  48s                default-scheduler                             Successfully assigned default/alpine-unsigned-1.0 to central-tools-md-0-5bfcdd98d9-2w5kx  Normal   BackOff    21s (x3 over 47s)  kubelet, central-tools-md-0-5bfcdd98d9-2w5kx  Back-off pulling image \"harbor.example.com/secure/alpine-unsigned:1.0\"  Warning  Failed     21s (x3 over 47s)  kubelet, central-tools-md-0-5bfcdd98d9-2w5kx  Error: ImagePullBackOff  Normal   Pulling    9s (x3 over 47s)   kubelet, central-tools-md-0-5bfcdd98d9-2w5kx  Pulling image \"harbor.example.com/secure/alpine-unsigned:1.0\"  Warning  Failed     9s (x3 over 47s)   kubelet, central-tools-md-0-5bfcdd98d9-2w5kx  Failed to pull image \"harbor.example.com/secure/alpine-unsigned:1.0\": rpc error: code = Unknown desc = failed to pull and unpack image \"harbor.example.com/secure/alpine-unsigned:1.0\": failed to copy: httpReaderSeeker: failed open: unexpected status code https://harbor.example.com/v2/secure/alpine-unsigned/manifests/sha256:39eda93d15866957feaee28f8fc5adb545276a64147445c64992ef69804dbf01: 412 Precondition Failed - Server message: unknown: The image is not signed in Notary.  Warning  Failed     9s (x3 over 47s)   kubelet, central-tools-md-0-5bfcdd98d9-2w5kx  Error: ErrImagePullNote the error message Harbor responds with:  412 Precondition Failed - Server message: unknown: The image is not signed in Notary.Pulling a signed image is fine, of course.ConclusionHere’s a very basic example of using Notary and content trust in Harbor. To put this process into production would take a fair amount of consideration and should not be taken lightly. But if you want to quickly try out signing an image in combination with using content trust with Harbor, then it’s quite simple to do. Deploying Harbor with Helm is also straight forward if you have a k8s cluster to use. :)"
    } ,
  
    {
      "title"    : "Deploy Harbor with Helm and Custom CA Certs",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/05/21/install-harbor-with-helm.html",
      "date"     : "2020-05-21 00:00:00 -0400",
      "content"  : "This is just a quick post on how to use a custom CA with Helm and Harbor. I won’t show installing helm or anything like that.Below are the versions deployed. Note that Harbor 2.0 has recently been released, but here we are using Harbor 1.10.2 as the helm chart hasn’t been updated.$ helm lsNAME          \tNAMESPACE\tREVISION\tUPDATED                                \tSTATUS  \tCHART       \tAPP VERSIONharbor-central\tharbor   \t1       \t2020-05-21 07:20:04.248551864 -0700 PDT\tdeployed\tharbor-1.3.2\t1.10.2 I’m using mkcert, and I’ve generated the cert files. Note that I’m creating a cert for both the harbor and notary service.mkcert harbor.example.com notary.example.comNow we need to create a tls secret. NOTE: “tls” secret not a generic secret.kubectl create secret tls harbor-certs \\  --cert=harbor.example.com+1.pem \\  --key=harbor.example.com+1-key.pem Export the values file.helm show values harbor/harbor &gt; harbor-values.yamlAnd edit that file so that it knows about “harbor-certs”. E.g.$ grep -A 2 -B 2 harbor-certs harbor-values.yaml     # link on portal to download the certificate of CA    # These files will be generated automatically if the \"secretName\" is not set    secretName: \"harbor-certs\"    # By default, the Notary service will use the same cert and key as    # described above. Fill the name of secret if you want to use aAnd deploy!helm install harbor-central harbor/harbor -f harbor-values.yamlThere are probably several other ways to do this, but this is certainly one! Of course this CA is going to have to be distributed as well."
    } ,
  
    {
      "title"    : "A Week with Ubuntu 20.04",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/05/18/a-week-with-ubuntu-20-04.html",
      "date"     : "2020-05-18 00:00:00 -0400",
      "content"  : "I know a lot of people like to make fun of running Linux on the desktop, and maybe it deserves the criticism that has built up year over year, like a thousand paper cuts (which never seem to heal, but I digress). I’ll be honest and say that I love running Linux as my main OS, and am much more proficient with it than any other operating system. Some people like Linux on the desktop and some don’t. That’s fine with me!Because I feel like I’m going to be doing a lot more work from home, I decided it was time to get a new linux workstation/desktop. So I did. Given that Ubuntu 20.04 recently came out, I thought that sounded like a good distribution of Linux to run, and I especially liked the idea of using ZFS as the boot file system.  HardwareThe hardware I’m using is really entry level, but boy, does it feel faster than my mac laptop and, of course, my old circa 2012 desktop. (Amazing to have 1TB NVMe drive for $170 CDN.)AMD Ryzen™ 5 3600 Processor, 3.6GHz w/ 35MB Cache Corsair Vengeance LPX 64GB DDR4 2666MHz CL16 Dual Channel Kit (4x 16GB), Black Western Digital Blue SN550 M.2 PCI-E NVMe SSD, 1TB Asus TUF B450-PRO GAMING w/ DDR4-2666, 7.1 Audio, Gigabit LAN, CrossFire At least with 64G of main memory I can keep some tabs open.I have not seen any errors, issues, etc, with the hardware so far.  ZFSWhen I installed 20.04 I decided vto install via the experimental use of ZFS as the boot file system. The interesting thing about ZFS here is that you can get snapshots of your OS.  When a new package is added via apt a zfs snapshot will be taken. It’s possible to revert by booting from the snapshot. For example, here I install wireguard. Note the “INFO” lines regarding creating a snapshot and updating grub.$ sudo apt install wireguard[sudo] password for curtis: Reading package lists... DoneBuilding dependency tree       Reading state information... DoneThe following additional packages will be installed:  wireguard-toolsThe following NEW packages will be installed:  wireguard wireguard-tools0 upgraded, 2 newly installed, 0 to remove and 0 not upgraded.Need to get 85.3 kB of archives.After this operation, 341 kB of additional disk space will be used.Do you want to continue? [Y/n] yGet:1 http://ca.archive.ubuntu.com/ubuntu focal/universe amd64 wireguard-tools amd64 1.0.20200319-1ubuntu1 [82.4 kB]Get:2 http://ca.archive.ubuntu.com/ubuntu focal/universe amd64 wireguard all 1.0.20200319-1ubuntu1 [2,912 B]Fetched 85.3 kB in 0s (544 kB/s)      INFO Requesting to save current system state      Successfully saved as \"autozsys_hku7gp\"Selecting previously unselected package wireguard-tools.(Reading database ... 177462 files and directories currently installed.)Preparing to unpack .../wireguard-tools_1.0.20200319-1ubuntu1_amd64.deb ...Unpacking wireguard-tools (1.0.20200319-1ubuntu1) ...Selecting previously unselected package wireguard.Preparing to unpack .../wireguard_1.0.20200319-1ubuntu1_all.deb ...Unpacking wireguard (1.0.20200319-1ubuntu1) ...Setting up wireguard-tools (1.0.20200319-1ubuntu1) ...Setting up wireguard (1.0.20200319-1ubuntu1) ...Processing triggers for man-db (2.9.1-1) ...INFO Updating GRUB menu     There is a boot pool and a root pool.$ zpool listNAME    SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOTbpool  1.88G   260M  1.62G        -         -     0%    13%  1.00x    ONLINE  -rpool   920G   110G   810G        -         -     5%    11%  1.00x    ONLINE  -  Kubernetes in DockerI do a lot of work with k8s, and would prefer to use Kind locally, but it doesn’t work with ZFS. Minikube is fine because it runs in its own VM, which is perhaps better security-wise anyways.  kvm, virt-managerI run several VMs to keep some separation. I have work Windows, work Linux demo vm, etc. With 64G of ram I have enough room for several vms and many tabs. 12 cpus helps too. So far no problems with KVM.  CrashesI have had a couple of crashes. One when I installed Docker. Not sure what happened there. That concerns me. That said, every desktop I’ve used in the last year crashes. Windows crashes. OSX crashes. Linux vms crash on OSX. Linux workstation crashes. Crash crash crash crash crash crash. No OS stays up 100% of the time. Brutal out there.  GUIA lot of the features mentioned for Fossa are related to the Gnome GUI. I use i3, which pretty much abstracts away the underlying GUI so I don’t see it. It’s to the point where my workflow is just 100% i3 based.  SnapsI haven’t used snaps much. I use vscode and you can install it from snaps but I just did the package based install. That said I don’t have a problem with Snaps either. There does seem to be some controversy around it, but packaging is hard, and I’m all for options being available.$ snap listName               Version             Rev   Tracking         Publisher   Notescore18             20200427            1754  latest/stable    canonical✓  basegnome-3-34-1804    0+git.3009fc7       33    latest/stable/…  canonical✓  -gtk-common-themes  0.1-36-gc75f853     1506  latest/stable/…  canonical✓  -snap-store         3.36.0-74-ga164ec9  433   latest/stable/…  canonical✓  -snapd              2.44.3              7264  latest/stable    canonical✓  snapd  WireguardI do use Wireguard, so having it back ported to the Fossa kernel is useful,no compiling for kernel module necessary.  EncryptionI usually encrypt the OS device. But I haven’t in this workstation, I guess because of ZFS I just didn’t think of it at the time. Something to consider in the future.  xfreerdp and i3I run a couple of vms for demos and work, and this xfreerdp command works great and still allows me to flit around virtual desktops with $MOD+NUM without having to escape the virtual session. That said I haven’t tested the mic yet, but the “speaker” works great for audio in the vm. What I mean is that I can use i3 on the desktop and windows in the vm, and jump back to linux with $MOD+NUM.xfreerdp /u:curtis /p:$PASS /v:$IP /f +fonts /floatbar /smart-sizing -grab-keyboard /sound /microphoneThat is working really well and I’m super happy with it.Updateszfs and docker issue…I could not remove a container, and had to use the below script to temporarily recreate the pool and then rm the container.#!/bin/bashstuck=$(docker ps -a | grep Removal | cut -f1 -d' ')echo $stuckfor container in $stuck; do\tzfs_path=$(docker inspect $container | jq -c '.[] | select(.State | .Status == \"dead\")|.GraphDriver.Data.Dataset')\tzfs_path=$(echo $zfs_path|tr -d '\"')\tsudo zfs destroy -R $zfs_path\tsudo zfs destroy -R $zfs_path-init    sudo zfs create $zfs_path    sudo zfs create $zfs_path-init\tdocker rm $containerdoneConclusionSo far, so good. Other than ZFS as the boot file system, I don’t actually see that much different with Focal Fossa. Everything works fine like usual, yes, even sound!"
    } ,
  
    {
      "title"    : "Virtual NSX-t and TKGI Lab",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/05/13/simplest-nsx-t-tkgi-virtual-lab.html",
      "date"     : "2020-05-13 00:00:00 -0400",
      "content"  : "I used to do a lot of work in the telecom field, especially around Network Function Virtualization (NFV) and I’ve deployed several Software Defined Networks (SDN) into production. I’ve even seen a few good ones come into existence, and then sometimes disappear.Now that I work at VMware in the Tanzu group, I work with VMware’s SDN system NSX-t and, of course, Kubernetes. One way that VMware can provide customers with k8s is via Tanzu Kubernetes Grid Integrated Edition (TKGI) which was previously called the Pivotal Container Service, or PKS. IMHO, TKGI works best with NSX-t. Networking is key to running k8s effectively.NSX-t, TKGI, k8s…these are all complicated technologies. There is just no way around it. The best way to learn how it all works is to deploy it. Currently I have what I consider to be a simpler virtual lab setup of NSX-t and TKGI deployed as virtual infrastructure, meaning that the ESXI hosts are virtual machines.tl;drThis isn’t really a prescriptive document, it’s more like a bunch of notes on some things I did to build this lab. On one hand it seems complex due to the use of a virtual vcenter and esxi nodes, but ultimately this is the simplest deployment of NSX-t and TKGI on vSphere that is possible. It’s really not that challenging and I actually ran into ZERO issues. I didn’t have one single problem getting this working (unless I’ve just missed something, lol). That said, I’ve been running NSX-t and TKGI in my lab for quite a while. For this deployment I was looking at NSX-t 2.5 and EPMC, neither of which I’ve deployed previously.PS. NSX-t 3.0 is out!Lab DesignThe point of this particular lab is to create similar environment as to what would be deployed in a TKGI + NSX-t + vSphere proof of concept. This means that:  Only 2 nics are used on the ESXI hosts  There is no high availability at all, especially with regards to network traffic  Trying to deploy the least amount of resources  The EPMC system is used to abstract away the deployment of ops manager and bosh, and the initial setup of TKGI -&gt; NSX-t connectivityVirtual ESXIThank goodness for the fact that you can run ESXI hosts as virtual machines. This makes it a lot easier to build up lab infrastructure for learning how it all works and deploying it in many different ways. William Lam’s virtual ESXI content library is a godsend and allows me to build all kinds of fun infrastructure. having only recently been introduced to vSphere it’s taken me a while to get to the point where I’m building virtual clusters.I’ve got three virtual, nested, esxi nodes in a vSphere cluster.$ govc find . -type h/Datacenter/host/Cluster/10.0.1.237/Datacenter/host/Cluster/10.0.1.238/Datacenter/host/Cluster/10.0.1.239These are the resources they have:$ govc host.info /Datacenter/host/Cluster/10.0.1.237Name:              10.0.1.237  Path:            /Datacenter/host/Cluster/10.0.1.237  Manufacturer:    VMware, Inc.  Logical CPUs:    16 CPUs @ 2000MHz  Processor type:  Intel(R) Xeon(R) CPU E5-2650 0 @ 2.00GHz  CPU usage:       7694 MHz (24.0%)  Memory:          65534MB  Memory usage:    39759 MB (60.7%)  Boot time:       2020-05-10 01:32:26.217102 +0000 UTC  State:           connectedAnd these are all the vms that have been created for a full TKGI deployment, with one cluster deployed.$ govc find . -type m/Datacenter/vm/pks_vms/vm-6dbd0bf0-9b12-40fd-bd06-05898d3f6a18/Datacenter/vm/pks_vms/vm-05632c06-2abd-4997-a7db-d5366f798281/Datacenter/vm/pks_templates/sc-a0414145-7c72-4991-9c87-1539fe36efe6/Datacenter/vm/pks_vms/vm-d341f649-fe5b-4487-9702-18ea60c265c9/Datacenter/vm/pks_vms/vm-422cbe25-f60d-4ae4-a2bb-2254f9dcf1eb/Datacenter/vm/pks_vms/vm-1456f8d2-24c3-4790-a622-9e9c6ef579e0/Datacenter/vm/opsman-WnmxrhR3zg/Datacenter/vm/pks-management-console-1.7.0-rev.1-978372/Datacenter/vm/pks_vms/vm-3b5fbbc7-bc27-43b7-a40b-4c2984419b3b/Datacenter/vm/pks_templates/sc-6f5a71cb-5949-4697-9245-418ece3d0182/Datacenter/vm/nsx-unified-appliance-2.5.1.0.0.15314292/Datacenter/vm/pks_vms/vm-7fd9e63c-3276-439e-a358-e074d0f4d453/Datacenter/vm/pks_vms/vm-fbc5e464-56cf-4eca-aad6-0e46f1bb3144/Datacenter/vm/pks_templates/sc-8caf221d-a04c-4f48-8747-f7cead82e788/Datacenter/vm/nsx-edge1As you can see, there is an ops manager instance, a nsx manager, and an nsx edge instance. As well, there is a bosh director as well, and finally a TGKI management console (here called “pks-management-console”).There is a lot of abstraction going on here, e.g. EPMC deploys Ops Manager, which deploys the Bosh Director, which deploys the TKGI API, which deploys the TGKI clusters.One of the most important features of TKGI is that it is a k8s cluster life cycle manager, it can deploy and manage N k8s clusters.Enabling Nested NetworkingThere are a lot of ways to approach this setup. I took this approachfor h in $(govc find . -type h | grep -v nested); do \\    govc host.portgroup.add -vswitch vSwitch0 -vlan 4095 -host=$h NestedVLANTrunk; \\doneto add a VLAN trunk that the nested esxi hosts would have access to to get trunk VLAN traffic, i.e. see more than one VLAN on a virtual nic.In the image below the nested esxi vm has two nics, one on the NestedVLANTrunk and one on “nested-pks-poc-nsx-tep” which is a physical VLAN in the underlying physical network, but one specifically setup for the nsx tep traffic. So this design only has 2 nics, which is nice, and what would typically be done in a physical proof of concept.Then in the nested vcenter there are more networks defined, which are VLANS available on the NestedVLANTrunk interface. The initial VSS based networks were migrated onto VDS. So there is the “VM Network” (also VLAN101), and a network for “vmotion”  as well. And, of course, the “nsx-uplink” and “nsx-tep” networks. Again, these are all physical VLANs in this lab, and are presented to the virtual vsphere system, in this case via the VLAN trunk and the distributed port group.I suppose it’s a bit confusing that I’m using VSS in the physical hosts and VDS in the virtual ones, but that’s what I did.Also, and this is important, promiscuous networking must be enabled in the er…physical virtual switch in the physical esxi nodes. As well note that the MTU is set to 1600.Also the DVS in the nested VCSA has a 1600 MTU.NSX-t ConfigurationThe configuration for this lab is bog standard, 2 NIC setup. The topology is the standard, most commonly used NAT model.Everything is NATed. The T0 router has several DNAT and SNAT configurations.With TKGI 1.7, each k8s cluster gets its own T1 router. Right now there are 2 k8s clusters.I created 3 VLANs in the physical network for this lab. There is also the “VM Network” which is 10.1.0.0/24 and has most management interfaces. Note that I just use /24s because it’s my lab.name                    vlan rangenested-pks-poc-vmotion  119 10.0.19.0/24nested-pks-nsx-tep      120 10.0.20.0/24nested-pks-nsx-uplink\t121 10.0.21.0/24My habit, though it is a ridiculously sized network, is to set a static route for my NSX T0 routers and give them a /16. Within that /16 I’ve designated a couple /24s for use by NSX, most importantly the 10.2.1.0/24 range which is floating IPs for load balancers, etc. Because I’m using a simple static route there’s no need for BGP setup.10.2.0.0/16\tstatic route pointed to T0 router at 10.0.21.100\t10.2.0.0/24\tdeployment range10.2.1.0/24\tfloating ips10.2.0.0/24 is the deployment range, and that is where things like ops manager and bosh director end up.EPMCEPMC is a great tool that helps to abstract away the complexity of the underlying TKGI deployment, especially around ops manager, bosh, and NSX-t networking configuration. And what’s more, it allows operators to download the configuration as a set of files. EPMC has a familiar, wizard like interface to take operators through configuration all the TKGI components. Though note that NSX-t has to be in place already, but not configured for TKGI.Below is a screenshot of the network configuration as set up in EPMC.As mentioned, you can download the configuration file which is powerful, as you can reuse it.NSX EdgesWhen NSX-t is deployed it needs at least one edge node.One of the more challenging things to understand about deploying NSX is that the ESXI nodes are (usually) physical, and the edge nodes virtual, but, in this design, they all need to be able to access TEP interface IPs…they need that reachability. Layer 2 is not required for this reachability, but in the case of this particular design, the edge nodes have an interface on the TEP VLAN and vnic1 on the ESXI nodes is also connected to that same VLAN as an access port, i.e. it looks like VLAN 0 to them. Please note that NSX-t is extremely flexible, and this design is the most basic.This blog post describes a couple of different ways to deploy nsx edges. Typically 4 nics are expected. This particular deployment is the one they have a not so subtle warning about:  In a Collapsed Compute and Edge Cluster topology, compute host is prepared for NSX-T which implies that this host has N-VDS installed and is also configured with a Tunnel End point. This deployment option is NOT recommended on a host with two pNICs. The simple reason being, this host has two virtual switches now, a VSS/VDS and a N-VDS, each consuming one pNIC. So, there is no redundancy for either of the virtual switches. A host with 4 pNICs or more is recommended in this deployment option. NSX-T Edge VM deployment or configuration doesn’t change from option 1 as the Edge VM still leverages the VSS/VDS portgroups.But that’s fine for this lab because it’s not meant for production.So the nsx edge, a virtual machine, has 3 nics in use:  Access to the TEP network  Access to the uplink network  A management interfacePingsCan’t network without pings. :)ConclusionSo there you have it, a somewhat haphazard tour of a virtual, 2 nic TKGI + NSX-t deployment. You could do this on a single ESXI host."
    } ,
  
    {
      "title"    : "Spring Cloud Gateway - Animal Rescue",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/05/06/spring-clould-gateway-animal-rescue.html",
      "date"     : "2020-05-06 00:00:00 -0400",
      "content"  : "Animal Rescue is a sample application used to demonstrate Spring Cloud Gateway, and Single Sign On, in the Tanzu Application Service (TAS), which is based on Cloud Foundry. This particular demo has an an automated script that will deploy the microservice based Animal Rescue into TAS, and in doing so will setup a Spring Cloud Gateway (SCG) instance via a service in TAS. By using marketplace services in TAS developers get easy-to-use self-service access to single sign on and a API gateway.What’s an API gateway?  An API gateway takes all API calls from clients, then routes them to the appropriate microservice with request routing, composition, and protocol translation. Typically it handles a request by invoking multiple microservices and aggregating the results, to determine the best path. It can translate between web protocols and web‑unfriendly protocols that are used internally. - NginxAn API gateway organizes client requests to various microservices. It can manipulate those requests, aggregate multiple microservices together, translate protocols, etc, etc. They can consolidate authorization and other cross cutting concerns. Gateways can also help reduce the number of calls clients need to make, i.e. they don’t need to understand every backend service, just enough to talk to the gateway. Another useful thing about gateways is that they can add in security. They can kind of do anything, and frankly that is what often makes them confusing, especially to people like myself who don’t write code 100% of the time.Spring Cloud GatewayIf you write apps in Java there is a very high likelihood that you also are using the Spring Framework and Spring has it’s own gateway, thoughtfully called  Spring Cloud Gateway.Spring Cloud (API?) Gateway:  This project provides a library for building an API Gateway on top of Spring MVC. Spring Cloud Gateway aims to provide a simple, yet effective way to route to APIs and provide cross cutting concerns to them such as: security, monitoring/metrics, and resiliency. - Spring Cloud docsA few key features of SCG:  Built on Spring Framework 5, Project Reactor and Spring Boot 2.0  Able to match routes on any request attribute  Predicates and filters are specific to routes  Hystrix Circuit Breaker integration  Spring Cloud DiscoveryClient integration  Easy to write Predicates and Filters  Request Rate Limiting  Path RewritingInstall Spring Gateway Tile into Tanzu Application ServiceIn order to deploy Animal Rescue with Spring Cloud Gateway, we need the Spring Cloud Gateway Tile deployed in Tanzu Application Service (TAS). This is not the only way to use Spring Cloud Gateway, but it’s how Animal Rescue is expecting to access it, at least in terms of the bash script that comes with the repo that can initialize and deploy the app. So in this example, SCG will be a service managed by the platform (TAS) that can be bound to an application.The Single Sign On Tile is also required and in my case I enabled it for all orgs.There are a few tiles installed in this TAS instance, but the important ones for Animal Rescue are the SSO and SCG tiles.Deploy Animal Rescue to Tanzu Application ServiceClone the repo:git clone https://github.com/spring-cloud-services-samples/animal-rescuecd animal-rescueLogin to TAS:cf login -a api.sys.yourdomain.comI needed make, g++, jq installed, there may be other requirements my Linux box already had installed.sudo apt install make g++ jq -yRun the init script.$ ./scripts/cf_deploy.sh initnpm WARN prepare removing existing node_modules/ before installation&gt; core-js@2.6.11 postinstall /home/curtis/working/animal-rescue/frontend/node_modules/babel-runtime/node_modules/core-js&gt; node -e \"try{require('./postinstall')}catch(e){}\"Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!The project needs your help! Please consider supporting of core-js on Open Collective or Patreon: &gt; https://opencollective.com/core-js &gt; https://www.patreon.com/zloirock SNIP!BUILD SUCCESSFUL in 3s4 actionable tasks: 4 executedThen deploy to TAS:$ ./scripts/cf_deploy.sh deployService instance gateway-demo not foundGateway service does not exist, creating...Creating service instance gateway-demo in org java / space spring-cloud-gateway as admin...OKCreate in progress. Use 'cf services' or 'cf service gateway-demo' to check operation status.Waiting for service instance to be ready...SNIP!&lt; X-Content-Type-Options: nosniff&lt; X-Frame-Options: DENY&lt; X-Vcap-Request-Id: 8c89f955-543f-4b3a-7e51-a9b42b1fbe31&lt; X-Xss-Protection: 1 ; mode=block&lt; Date: Thu, 07 May 2020 15:37:11 GMT&lt; * Connection #0 to host gateway-demo.apps.sf.vsphere.local left intact=====Bound app animal-rescue-backend route configuration update response status: 204Now I’ve got the frontend and backend apps running on internal urls.$ cf appsGetting apps in org java / space spring-cloud-gateway as admin...OKname                     requested state   instances   memory   disk   urlsanimal-rescue-backend    started           1/1         1G       1G     animal-rescue-backend.apps.internalanimal-rescue-frontend   started           1/1         1G       1G     animal-rescue-frontend.apps.internalI can access that via the browser, login, and adopt animals.If I use the gateway org I can see the instance of the gateway service:$ cf appsGetting apps in org p-spring-cloud-gateway-service / space fb081bbf-fa60-4a40-bbb0-4f4c47ab2c0a as admin...OKname      requested state   instances   memory   disk   urlsgateway   started           1/1         1G       1G     gateway-demo.apps.sf.vsphere.local, gateway-fb081bbf-fa60-4a40-bbb0-4f4c47ab2c0a.apps.internalAnd then that can be accessed in the browser. The login is via SSO.Look at all those cute animals up for microservice adoption.Spring Cloud Gateway Configuration and ServiceIn the apps GUI, we can see the SCG instance.And the corresponding routes.Note that the route is internal.It’s worthwhile to have a look at the routes that are configured in the gateway, frontend, and backend gateway-config.json files.ConclusionAPI gateways can be really helpful with microservices. There are several solutions out there, but one of the most flexible is Spring Cloud Gateway. It’s absolutely worth looking at and is a serious competitor and will be improving, and innovating, rapidly. In this particular example, SCG was deployed and managed as a service in TAS, which makes it extremely easy for developers to use, but it can also be managed in other manners.This post only barely scratches the surface of what Spring Cloud Gateway can do, and in combination with TAS. There’s tons of docs out there on getting started with SCG."
    } ,
  
    {
      "title"    : "GraalVM Native Images",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/05/05/graalvm-native-image.html",
      "date"     : "2020-05-05 00:00:00 -0400",
      "content"  : "People are often concerned about the startup speed of Java applications. I don’t think it matters for most use cases, but there are, of course, some situations where a very fast startup time and less memory usage would be useful…say small microservices. GraalVM Native Images can, in certain situations, help with this.  GraalVM Native Image allows you to ahead-of-time compile Java code to a standalone executable, called a native image. This executable includes the application classes, classes from its dependencies, runtime library classes from JDK and statically linked native code from JDK. It does not run on the Java VM, but includes necessary components like memory management and thread scheduling from a different virtual machine, called “Substrate VM”. Substrate VM is the name for the runtime components (like the deoptimizer, garbage collector, thread scheduling etc.). The resulting program has faster startup time and lower runtime memory overhead compared to a Java VM. - GraalVM DocsDave Syer, a key, long time member of the Spring open source project, recently wrote a post discussing the performance of GraalVM Native Images and the JIT when using Spring Cloud Function with AWS Lambda.  Using Spring Cloud Function is a very convenient way to develop functions that run on AWS and other platforms. If you also use the experimental Spring Graal Native Feature project to compile the result to a native binary executable they can run faster than the same application on a regular JVM.A couple reasons why you might use native images:  Once compiled to a platform specific native-image applications should have very fast startup and a more reliable memory profile (no JIT causing memory spikes at the beginning). - Spring BlogCompile using GraalVM Native ImageLet’s build a native image with GraalVM.I’ll use sdkman to install the Java and GraalVM requirements.sdk install java 20.0.0.r8-grlAlso we need gcc and zlib1g-dev, at least on Ubuntu 18.04 anyways.  For compilation native-image depends on the local toolchain, so please make sure: glibc-devel, zlib-devel (header files for the C library and zlib) and gcc are available on your system. - GraalVM docssudo apt install gcc zlib1g-dev -yClone the repo.git clone https://github.com/spring-projects-experimental/spring-graal-nativecd spring-graal-native/spring-graal-native-samples/function-nettyNow I’ve got Java 8.$ java -versionopenjdk version \"1.8.0_242\"OpenJDK Runtime Environment (build 1.8.0_242-b06)OpenJDK 64-Bit Server VM GraalVM CE 20.0.0 (build 25.242-b06-jvmci-20.0-b02, mixed mode)And then use gu to install native-image.$ gu install native-imageDownloading: Component catalog from www.graalvm.orgProcessing Component: Native ImageDownloading: Component native-image: Native Image  from github.comInstalling new component: Native Image (org.graalvm.native-image, version 20.0.0)At the root of the project, run build.sh.$ pwd/home/curtis/working/spring-graal-native$ ./build.sh Then build the function-netty sample.$ cd spring-graal-native-samples/function-netty$ ./build.shI was initially building this on a VM with 8GB of memory and no swap, and it crashed out. I bumped the memory to 12GB and it built. I see a note in the native image docs about memory for builds, but couldn’t quite grok it.SNIP!Error: Image build request failed with exit status 137com.oracle.svm.driver.NativeImage$NativeImageError: Image build request failed with exit status 137\tat com.oracle.svm.driver.NativeImage.showError(NativeImage.java:1527)\tat com.oracle.svm.driver.NativeImage.build(NativeImage.java:1289)\tat com.oracle.svm.driver.NativeImage.performBuild(NativeImage.java:1250)\tat com.oracle.svm.driver.NativeImage.main(NativeImage.java:1209)real\t3m44.437suser\t11m4.659ssys\t0m17.460sFAILURE: an error occurred when compiling the native-image.With enough memory now, the build takes a few minutes:$ ./build.sh === Building function-netty sample ===Packaging function-netty with MavenUnpacking function-netty-0.0.1-SNAPSHOT.jarCompiling function-netty with GraalVM Version 20.0.0 CESUCCESSTesting executable 'function-netty'SUCCESSBuild memory: 7.07GBImage build time: 432.0sRSS memory: 87.4MImage size: 79.2MStartup time: 0.178 (JVM running for 0.182)Start up the function.$ ./target/function-netty   .   ____          _            __ _ _ /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )  '  |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot ::                        2020-05-05 14:12:00.458  INFO 9705 --- [           main] com.example.demo.DemoApplication         : Starting DemoApplication on tanzu-ubuntu-2 with PID 9705 (/home/curtis/working/spring-graal-native/spring-graal-native-samples/function-netty/target/function-netty started by curtis in /home/curtis/working/spring-graal-native/spring-graal-native-samples/function-netty)2020-05-05 14:12:00.458  INFO 9705 --- [           main] com.example.demo.DemoApplication         : No active profile set, falling back to default profiles: default2020-05-05 14:12:00.515  INFO 9705 --- [           main] o.s.c.f.web.flux.FunctionHandlerMapping  : FunctionCatalog: org.springframework.cloud.function.context.catalog.BeanFactoryAwareFunctionRegistry@7f0b166628d82020-05-05 14:12:00.527  WARN 9705 --- [           main] io.netty.channel.DefaultChannelId        : Failed to find the current process ID from ''; using a random value: 3985827042020-05-05 14:12:00.528  INFO 9705 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port(s): 80802020-05-05 14:12:00.528  INFO 9705 --- [           main] com.example.demo.DemoApplication         : Started DemoApplication in 0.081 seconds (JVM running for 0.083)0.081 seconds, that’s pretty quick.Curl it:$ curl -s localhost:8080/ -d world -H \"Content-Type: text/plain\"; echohi world!What is target/function-netty?$ file target/function-nettytarget/function-netty: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/l, for GNU/Linux 3.2.0, BuildID[sha1]=680512d5d6ab33e499ecdb9a45d4b199841de0fb, with debug_info, not strippedSize:$ du -hsc target/function-netty80M\ttarget/function-netty80M\ttotalNow I’ll push it to Cloud Foundry (also now known as Tanzu Application Service downstream-wise): $ cf push -b binary_buildpack -c target/function-netty cc-niPushing app cc-ni to org Canada / space ccollicutt as ccollicutt@pivotal.io...Getting app info...Creating app with these attributes...+ name:         cc-ni  path:         /home/curtis/working/spring-graal-native/spring-graal-native-samples/function-netty  buildpacks:+   binary_buildpack+ command:      target/function-netty  routes:+   cc-ni.cfapps.ioCreating app cc-ni...Mapping routes...Comparing local files to remote cache...Packaging files to upload...Uploading files... 41.83 MiB / 41.83 MiB [===============================================================================================================================================================================] 100.00% 9sWaiting for API to complete processing files...Staging app and tracing logs...   Downloading binary_buildpack...   Downloaded binary_buildpack   Cell 850bb788-0cb8-4092-b839-32661cb636dc creating container for instance c3c3ad17-10a8-4885-a762-4b3ae5811cc4   Cell 850bb788-0cb8-4092-b839-32661cb636dc successfully created container for instance c3c3ad17-10a8-4885-a762-4b3ae5811cc4   Downloading app package...   Downloaded app package (56.9M)   -----&gt; Binary Buildpack version 1.0.36   Exit status 0   Uploading droplet, build artifacts cache...   Uploading droplet...   Uploading build artifacts cache...   Uploaded build artifacts cache (214B)   Uploaded droplet (56.4M)   Uploading complete   Cell 850bb788-0cb8-4092-b839-32661cb636dc stopping instance c3c3ad17-10a8-4885-a762-4b3ae5811cc4   Cell 850bb788-0cb8-4092-b839-32661cb636dc destroying container for instance c3c3ad17-10a8-4885-a762-4b3ae5811cc4   Cell 850bb788-0cb8-4092-b839-32661cb636dc successfully destroyed container for instance c3c3ad17-10a8-4885-a762-4b3ae5811cc4Waiting for app to start...name:              cc-nirequested state:   startedroutes:            cc-ni.cfapps.iolast uploaded:     Tue 05 May 15:15:33 EDT 2020stack:             cflinuxfs3buildpacks:        binarytype:            webinstances:       1/1memory usage:    1024Mstart command:   target/function-netty     state     since                  cpu    memory    disk      details#0   running   2020-05-05T19:15:47Z   0.0%   0 of 1G   0 of 1G   And curl that:$ curl -s cc-ni.cfapps.io -d \"online curtis\" -H \"Content-Type: text/plain\"; echohi online curtis!Nice!It’s only using 40mb of memory.$ cf app cc-niShowing health and status for app cc-ni in org Canada / space ccollicutt as ccollicutt@pivotal.io...name:              cc-nirequested state:   startedroutes:            cc-ni.cfapps.iolast uploaded:     Tue 05 May 15:15:33 EDT 2020stack:             cflinuxfs3buildpacks:        binarytype:           webinstances:      1/1memory usage:   1024M     state     since                  cpu    memory        disk           details#0   running   2020-05-05T19:15:48Z   0.5%   40.4M of 1G   114.7M of 1G   Push the Jar FileI can also just push the jar file instead of the native image binary.$ cf push -p target/function-netty-0.0.1-SNAPSHOT.jar cf-ni-jarSlightly more memory in use, about 110M more than the native image.$ cf app cf-ni-jarShowing health and status for app cf-ni-jar in org Canada / space ccollicutt as ccollicutt@pivotal.io...name:              cf-ni-jarrequested state:   startedroutes:            cf-ni-jar.cfapps.iolast uploaded:     Tue 05 May 15:30:15 EDT 2020stack:             cflinuxfs3buildpacks:        client-certificate-mapper=1.11.0_RELEASE container-security-provider=1.18.0_RELEASE                   java-buildpack=v4.30-offline-https://github.com/cloudfoundry/java-buildpack.git#6986fd5                   java-main java-opts java-security jvmkill-agent=1.16.0_RELEASE open-jdk...type:           webinstances:      1/1memory usage:   1024M     state     since                  cpu    memory         disk           details#0   running   2020-05-05T19:30:33Z   0.6%   158.3M of 1G   127.2M of 1G Also can curl it… $ curl -s cf-ni-jar.cfapps.io -d \"online curtis\" -H \"Content-Type: text/plain\"; echohi online curtis!ConclusionSo this is really interesting…the ability to create a single binary file for a Java and Spring app. I’d like to understand this better, especially what it is doing for 5 or 6 minutes while compiling, which also takes a considerable amount of RAM. I’m sure there is a lot of ongoing work, and that this will mostly be applicable in certain sitations, specifically cloud functions, but progress is definitely being made. Overall, faster startup time and lower memory usage will be extremely valuable.There’s a good article here that discusses some of the pros and cons of native image.It also appears that Java will take on building in some of this functionality itself in a project called Leyden.  Leyden will address these pain points by introducing a concept of staticimages to the Java Platform, and to the JDK.      A static image is a standalone program, derived from an application,  which runs that application – and no other.    A static image is a closed world: It cannot load classes from outside  the image, nor can it spin new bytecodes at run time.  "
    } ,
  
    {
      "title"    : "Tanzu Kubernetes Grid and Antrea",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/05/02/tanzu-kubernetes-grid-antrea.html",
      "date"     : "2020-05-02 00:00:00 -0400",
      "content"  : "In this post I’ll briefly discuss Tanzu Kubernetes Grid, and then get a bit into the Antrea Container Network Interface (CNI) for Kubernetes.Tanzu Kubernetes GridTanzu Kubernetes Grid, otherwise known as TKG, is the underlying Kuberenetes distribution for all of VMware’s Kuberenete based products. It’s part of vSphere with Kubernetes. It’s part of TKGI (what was once PKS). But it’s not just a distribution…it’s also a standalone lifecycle manager that heavily utilizes Cluster API to manage virtual machines on which Kubernetes runs.I’m not going to get into how TKG works, other than to say one of the first things you do with TKG is to deploy a management cluster. That cluster is then used to manage the life cycle of many other k8s “workload” clusters.Here’s all the CRDs that are part of the management cluster.$ k get crds | grep \"machine/|vsphere\\|cluster\"clusterinformations.crd.projectcalico.org                 2020-04-21T19:03:46Zclusterissuers.cert-manager.io                            2020-04-21T19:03:48Zclusters.cluster.x-k8s.io                                 2020-04-21T19:05:35Zhaproxyloadbalancers.infrastructure.cluster.x-k8s.io      2020-04-21T19:05:46Zkubeadmconfigs.bootstrap.cluster.x-k8s.io                 2020-04-21T19:05:39Zkubeadmconfigtemplates.bootstrap.cluster.x-k8s.io         2020-04-21T19:05:39Zkubeadmcontrolplanes.controlplane.cluster.x-k8s.io        2020-04-21T19:05:43Zmachinedeployments.cluster.x-k8s.io                       2020-04-21T19:05:35Zmachinehealthchecks.cluster.x-k8s.io                      2020-04-21T19:05:35Zmachinepools.exp.cluster.x-k8s.io                         2020-04-21T19:05:36Zmachines.cluster.x-k8s.io                                 2020-04-21T19:05:36Zmachinesets.cluster.x-k8s.io                              2020-04-21T19:05:36Zproviders.clusterctl.cluster.x-k8s.io                     2020-04-21T19:03:46Zvsphereclusters.infrastructure.cluster.x-k8s.io           2020-04-21T19:05:46Zvspheremachines.infrastructure.cluster.x-k8s.io           2020-04-21T19:05:46Zvspheremachinetemplates.infrastructure.cluster.x-k8s.io   2020-04-21T19:05:47Zvspherevms.infrastructure.cluster.x-k8s.io                2020-04-21T19:05:47ZsYou can see some fun ones like machines and machinesets, as well as CRDs related to using vSphere.Using TKG I have deployed three workload clusters.$ tkg get clusters+--------------+-------------+| NAME         | STATUS      |+--------------+-------------+| dc-cluster   | Provisioned || edge-cluster | Provisioned || tkg-antrea   | Provisioned |+--------------+-------------+One of them is using Antrea as the container networking interface (tkg-antrea). The other two clusters are using the current TKG default CNI, Calico.AntreaThere are quite a few networking options with k8s. One of the best is VMware’s NSX-t. But another interesting networking option that VMware supports is the open source project Antrea.  Antrea is a Kubernetes networking solution intended to be Kubernetes native. It operates at Layer3/4 to provide networking and security services for a Kubernetes cluster, leveraging Open vSwitch as the networking data plane.Basically it orchestrates Open vSwitch. Version 0.6 was released only a few days ago.William Lam has a blog post on how to deploy a TKG workload cluster with Antrea instead of Calico. It’s pretty straight forward. TKG uses the concept of “plans” to configure k8s clusters. These plans are basically YAML templates, and by default Calico is set up in the template, but Antrea can be swapped in.$ diff cluster-template-dev-antrea.yaml cluster-template-dev.yaml | head247,952c247,1036&lt;      ---&lt;      apiVersion: apiextensions.k8s.io/v1beta1&lt;      kind: CustomResourceDefinition&lt;      metadata:&lt;        labels:&lt;          app: antrea&lt;        name: antreaagentinfos.clusterinformation.antrea.tanzu.vmware.com&lt;      spec:&lt;        group: clusterinformation.antrea.tanzu.vmware.comFollow Mr. Lam’s blog post if you want to try this out. A few quick commands and you’ll be all set.AntreaI configured the tkg-antrea workload cluster to have three worker nodes. Then I applied the below busybox deployment.$ cat deployment.yml apiVersion: apps/v1kind: Deploymentmetadata:  name: busybox-deployment  labels:    app: busyboxspec:  replicas: 6   strategy:     type: RollingUpdate  selector:    matchLabels:      app: busybox  template:    metadata:      labels:        app: busybox    spec:      containers:      - name: busybox        image: busybox        imagePullPolicy: IfNotPresent                command: ['sh', '-c', 'echo Container 1 is Running ; sleep 3600']Here are the pods.NOTE: I always alias kubectl to k.$ k get podsNAME                                  READY   STATUS    RESTARTS   AGEbusybox-deployment-66458f7d4b-4s6vh   1/1     Running   11         11hbusybox-deployment-66458f7d4b-5kv5l   1/1     Running   11         11hbusybox-deployment-66458f7d4b-fgjzx   1/1     Running   11         11hbusybox-deployment-66458f7d4b-gqs8g   1/1     Running   11         11hbusybox-deployment-66458f7d4b-h25sf   1/1     Running   11         11hbusybox-deployment-66458f7d4b-r977v   1/1     Running   11         11hWe can also checkout their IP addresses. (Hat tip to this blog.)$ k get pods -o custom-columns='name:.metadata.name,pod ip:.status.podIPs[0].ip,node:.spec.nodeName' --sort-by='.spec.nodeName'name                                  pod ip       nodebusybox-deployment-66458f7d4b-4s6vh   100.96.1.3   tkg-antrea-md-0-548d498b47-2xjf8busybox-deployment-66458f7d4b-h25sf   100.96.1.4   tkg-antrea-md-0-548d498b47-2xjf8busybox-deployment-66458f7d4b-fgjzx   100.96.2.3   tkg-antrea-md-0-548d498b47-ckwvdbusybox-deployment-66458f7d4b-r977v   100.96.2.4   tkg-antrea-md-0-548d498b47-ckwvdbusybox-deployment-66458f7d4b-5kv5l   100.96.3.4   tkg-antrea-md-0-548d498b47-f4rmzbusybox-deployment-66458f7d4b-gqs8g   100.96.3.3   tkg-antrea-md-0-548d498b47-f4rmzWe have two pods per node. With Antrea, each k8s node gets a /24. In this case node tkg-antrea-md-0-548d498b47-2xjf8 has 100.96.1.0/24m the next node is 100.96.2.0/24, and so on.$ k get nodes -o custom-columns=Name:.metadata.name,PodCIDR:.spec.podCIDRName                               PodCIDRtkg-antrea-control-plane-fzb44     100.96.0.0/24tkg-antrea-md-0-548d498b47-2xjf8   100.96.1.0/24tkg-antrea-md-0-548d498b47-ckwvd   100.96.2.0/24tkg-antrea-md-0-548d498b47-f4rmz   100.96.3.0/24antctl is quite useful.$ k -n kube-system exec -it antrea-agent-2d8xp antrea-agent -- antctl get ovsflows -hDefaulting container name to antrea-agent.Use 'kubectl describe pod/antrea-agent-2d8xp -n kube-system' to see all of the containers in this pod.Dump all the OVS flows or the flows installed for the specified entity.Usage:  antctl get ovsflows [flags]Aliases:  ovsflows, ofExamples:  Dump all OVS flows  $ antctl get ovsflows  Dump OVS flows of a local Pod  $ antctl get ovsflows -p pod1 -n ns1  Dump OVS flows of a NetworkPolicy  $ antctl get ovsflows --networkpolicy np1 -n ns1  Dump OVS flows of a flow Table  $ antctl get ovsflows -t IngressRule  Antrea OVS Flow Tables:  0\tClassification  10\tSpoofGuard  20\tARPResponder  30\tConntrackZone  31\tContrackState  40\tDNAT  50\tEgressRule  60\tEgressDefaultRule  70\tL3Forwarding  80\tL2Forwarding  90\tIngressRule  100\tIngressDefaultRule  105\tConntrackCommit  110\tOutputSNIP!We can dump the flows using antctl based on the above flow table names. There are no network polices in place right now, so the returned flows are few.NOTE: Use an antrea agent that is on a worker node.$ k -n kube-system exec -it antrea-agent-hs5kp antrea-agent -- antctl get ovsflows -T IngressRuleDefaulting container name to antrea-agent.Use 'kubectl describe pod/antrea-agent-hs5kp -n kube-system' to see all of the containers in this pod.FLOW                                                                                                  table=90, n_packets=155728, n_bytes=14819958, priority=210,ct_state=-new+est,ip actions=resubmit(,105)table=90, n_packets=17156, n_bytes=1269544, priority=210,ip,nw_src=100.96.1.1 actions=resubmit(,105)  table=90, n_packets=15, n_bytes=1110, priority=0 actions=resubmit(,100)   Above we can see the default flows when there are no network polices.Network Policiesk8s started out with the idea that every pod could talk to every other pod over the network. Obviously it couldn’t stay like this…we need to be able to limit and control connectivity. So Antrea supports Kubernetes Network Policies.  By default, pods are non-isolated; they accept traffic from any source. Pods become isolated by having a NetworkPolicy that selects them. Once there is any NetworkPolicy in a namespace selecting a particular pod, that pod will reject any connections that are not allowed by any NetworkPolicy. (Other pods in the namespace that are not selected by any NetworkPolicy will continue to accept all traffic.) - k8s docsDeploy nginx:$ cat nginx-deployment-service.yaml ---apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2kind: Deploymentmetadata:  name: nginxspec:  strategy:    type: Recreate  selector:    matchLabels:      app: nginx  replicas: 3 # tells deployment to run 1 pods matching the template  template: # create pods using pod definition in this template    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx        ports:        - containerPort: 80---apiVersion: v1kind: Servicemetadata:  name: nginx  namespace: default  labels:    app: nginx  annotations:    service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"spec:  externalTrafficPolicy: Local  ports:  - name: http    port: 80    protocol: TCP    targetPort: 80  selector:    app: nginx  type: NodePortClusterIP:$ k get svcNAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGEkubernetes   ClusterIP   100.64.0.1       &lt;none&gt;        443/TCP        23hnginx        NodePort    100.70.238.176   &lt;none&gt;        80:31710/TCP   30mWith nginx deployed, I can exec into one of the busybox instances I already created and do this:/ # wget -q -O - 100.70.238.176 | grep title&lt;title&gt;Welcome to nginx!&lt;/title&gt;But now lets apply a network policy.$ cat network-policy.yml kind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata:  name: nginx-allowspec:  podSelector:    matchLabels:      app: nginx  ingress:  - from:      - podSelector:          matchLabels:            app: nginx$ k apply -f network-policy.yml networkpolicy.networking.k8s.io/nginx-allow created$ k get networkpolicyNAME          POD-SELECTOR         AGEnginx-allow   app=nginx            3sA busybox pod can no longer connect to the nginx deployment (only pods with 32app=nginx).$ k -n default exec -it busybox-deployment-66458f7d4b-4s6vh -- /bin/sh/ # wget -q -O - 100.70.238.176 | grep titlewget: can't connect to remote host (100.70.238.176): Connection timed outLet’s check the flows now.$ k -n kube-system exec -it antrea-agent-hs5kp antrea-agent -- antctl get ovsflows -T IngressRuleDefaulting container name to antrea-agent.Use 'kubectl describe pod/antrea-agent-hs5kp -n kube-system' to see all of the containers in this pod.FLOW                                                                                                  table=90, n_packets=155929, n_bytes=14839032, priority=210,ct_state=-new+est,ip actions=resubmit(,105)table=90, n_packets=17178, n_bytes=1271172, priority=210,ip,nw_src=100.96.1.1 actions=resubmit(,105)  table=90, n_packets=0, n_bytes=0, priority=200,ip,nw_src=100.96.3.5 actions=conjunction(1,1/2)        table=90, n_packets=0, n_bytes=0, priority=200,ip,nw_src=100.96.2.5 actions=conjunction(1,1/2)        table=90, n_packets=0, n_bytes=0, priority=200,ip,nw_src=100.96.1.5 actions=conjunction(1,1/2)        table=90, n_packets=0, n_bytes=0, priority=200,ip,reg1=0x6 actions=conjunction(1,2/2)                 table=90, n_packets=0, n_bytes=0, priority=190,conj_id=1,ip actions=resubmit(,105)                    table=90, n_packets=15, n_bytes=1110, priority=0 actions=resubmit(,100)  More flows!Those IPs are of the nginx pods.$ k get pods -o custom-columns='name:.metadata.name,pod ip:.status.podIPs[0].ip,node:.spec.nodeName' --sort-by='.spec.nodeName' --selector app=nginxname                     pod ip       nodenginx-85ff79dd56-2ndjq   100.96.1.5   tkg-antrea-md-0-548d498b47-2xjf8nginx-85ff79dd56-fdrsp   100.96.2.5   tkg-antrea-md-0-548d498b47-ckwvdnginx-85ff79dd56-cxjvw   100.96.3.5   tkg-antrea-md-0-548d498b47-f4rmz$ k -n kube-system exec -it antrea-agent-hs5kp antrea-agent -- antctl get networkpoliciesDefaulting container name to antrea-agent.Use 'kubectl describe pod/antrea-agent-hs5kp -n kube-system' to see all of the containers in this pod.NAMESPACE NAME        APPLIED-TO                           RULESdefault   nginx-allow 766a9e51-f132-5c2f-b862-9ac68e75d77d 1 Above we can ask Antrea about network polices as well.ConclusionOpen vSwitch is the swiss army knife of networking. It’s open. It’s widely used. It can run on Linux and Windows. It serves as a great basis for a software defined networking system for Kubernetes.Checkout Antrea’s roadmap to see where they are going. Many great features on the horizon!PS. Please note that TKG does not officially support Antrea at the time I wrote this, but they are definitely looking at it."
    } ,
  
    {
      "title"    : "Install the Harbor Container Image Registry Locally",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/04/28/local-harbor-install.html",
      "date"     : "2020-04-28 00:00:00 -0400",
      "content"  : "In this post I’ll deploy a Harbor image registry on a local OSX workstation/laptop.Requirements:  OSX  Brew  Docker for DesktopKindKind stands for “Kubernetes in Docker” and is an easy way to get a local Kubernetes cluster running.Note: Docker for Desktop also provides Kubernetes functionality, but in this example we are using Kind to provide k8s.Install kind with brew.brew install kindCreate a kind based cluster.Note the use of “extraPortMappings”.cat &lt;&lt;EOF | kind create cluster --config=-kind: ClusterapiVersion: kind.x-k8s.io/v1alpha4nodes:- role: control-plane  kubeadmConfigPatches:  - |    kind: InitConfiguration    nodeRegistration:      kubeletExtraArgs:        node-labels: \"ingress-ready=true\"        authorization-mode: \"AlwaysAllow\"  extraPortMappings:  - containerPort: 80    hostPort: 80    protocol: TCP  - containerPort: 443    hostPort: 443    protocol: TCPEOFOnce that kind cluster is created we can see k8s nodes.$ kubectl get nodesNAME                 STATUS   ROLES    AGE     VERSIONkind-control-plane   Ready    master   2m31s   v1.17.0Nice!Contour IngressContour is an advanced open source ingress system supported in part by VMware.  Contour is an open source Kubernetes ingress controller providing the control plane for the Envoy edge and service proxy.​ Contour supports dynamic configuration updates and multi-team ingress delegation out of the box while maintaining a lightweight profile.Deploy Contour.kubectl apply -f https://projectcontour.io/quickstart/contour.yamlPatch it:kubectl patch daemonsets -n projectcontour envoy -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":{\"ingress-ready\":\"true\"},\"tolerations\":[{\"key\":\"node-role.kubernetes.io/master\",\"operator\":\"Equal\",\"effect\":\"NoSchedule\"}]}}}}'Contour should now be running.Note: kubectl is provided by Docker for Desktop.$ kubectl get pods -n projectcontourNAME                       READY   STATUS      RESTARTS   AGEcontour-54df6b8854-dlsnr   1/1     Running     0          83scontour-54df6b8854-m2w8k   1/1     Running     0          83scontour-certgen-n78dz      0/1     Completed   0          83senvoy-kwr8x                2/2     Running     0          10sContour will provide an easy way to get ingress. Of course, Contour offers a lot more than just ingress, but it’s all we need for now.Install HelmGetting helm is quite easy with brew.brew install helmAlso we get helm 3!Install HarborNow that we have Kubernetes and Helm, installing Harbor is straight forward, though there are many options available in the Helm chart. We will not be making any changes and use the defaults provided.Add the Harbor Helm repository.helm repo add harbor https://helm.goharbor.ioAnd install Harbor:helm install local-harbor harbor/harborAfter a few minutes, there should be several Harbor k8s objects, such as pods.$ k get podsNAME                                                 READY   STATUS    RESTARTS   AGElocal-harbor-harbor-chartmuseum-bd9c45cbc-gwkbj      1/1     Running   0          58slocal-harbor-harbor-clair-865c9bc5db-cvbk8           1/2     Running   2          58slocal-harbor-harbor-core-64479f8d85-rkqm2            1/1     Running   0          58slocal-harbor-harbor-database-0                       1/1     Running   0          58slocal-harbor-harbor-jobservice-8448b58df7-pgknp      1/1     Running   0          58slocal-harbor-harbor-notary-server-5bd9f5d966-56kk6   1/1     Running   0          58slocal-harbor-harbor-notary-signer-5fbfb48945-l2x54   1/1     Running   0          58slocal-harbor-harbor-portal-756d5d7d9d-xlv2g          1/1     Running   0          58slocal-harbor-harbor-redis-0                          1/1     Running   0          58slocal-harbor-harbor-registry-57989b6446-w8vd8        2/2     Running   0          58sNOTE: There are a lot of services running with Harbor. Future work for this post will include trying to make it a bit easier on resources on a workstation.Access HarborAdd a hostname to your /etc/hosts file.127.0.0.1 core.harbor.domainNow open a browser session to http://core.harbor.domain/harborlogin: adminpassword: Harbor12345At this point Harbor is available for local use.Delete Harbor and KindTo remove everything added:helm uninstall local-harborkind delete clusterThen shutdown Docker for Desktop.ConclusionThere are still a few things to be ironed out here. My Mac laptop starts sounding like a 747 pretty quickly. In a future post or update I’ll see how small we can make harbor, for now whatever helm deploys by default is what we are getting. The Harbor helm chart has many options that can be applied, but all defaults are being used here."
    } ,
  
    {
      "title"    : "Use .local Domain in Ubuntu 18.04",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/03/28/use-dot-local-domain-ubuntu.html",
      "date"     : "2020-03-28 00:00:00 -0400",
      "content"  : "I have a homelab and part of that homelab is a vSphere deployment. For whatever reason I used vsphere.local as the domain for some functionality. But Ubuntu doesn’t like the .local domain, I believe because it’s usually used with multicast DNS. TBH I’m not going to look to deep into why or why not one should use .local, the fact is that I am and I’m not changing it right now. :)To use .local in Ubuntu I did this:$ grep Domain /etc/systemd/resolved.conf Domains=vsphere.localAnd moved the “dns” option in /etc/nsswitch.conf to be before mdns…#hosts:          files mdns4_minimal [NOTFOUND=return] dns myhostnamehosts:          files dns mdns4_minimal [NOTFOUND=return]After that I could resolve .local domains.Happy .local domaining!"
    } ,
  
    {
      "title"    : "Change or Rewrite cluster.local in Kubernetes",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/03/25/rewrite-change-cluster-local-kubernetes.html",
      "date"     : "2020-03-25 00:00:00 -0400",
      "content"  : "Some organizations may want to use a domain other than cluster.local as the service domain in Kubernetes.One way to do that is to add a rewrite substring rule in CoreDNS. (Of course this assumes the use of CoreDNS.)I have a VMware Enterprise PKS cluster to test with.The CoreDNS configmap starts out looking like this: $ kubectl get -n kube-system cm/coredns -o yamlapiVersion: v1data:  Corefile: |    .:53 {        errors        health        kubernetes cluster.local in-addr.arpa ip6.arpa {          pods insecure          upstream          fallthrough in-addr.arpa ip6.arpa        }        prometheus :9153        proxy . /etc/resolv.conf {          policy sequential # needed for workloads to be able to use BOSH-DNS        }        cache 30        loop        reload        loadbalance    }kind: ConfigMapmetadata:  annotations:    kubectl.kubernetes.io/last-applied-configuration: |      {\"apiVersion\":\"v1\",\"data\":{\"Corefile\":\".:53 {\\n    errors\\n    health\\n    kubernetes cluster.local in-addr.arpa ip6.arpa {\\n      pods insecure\\n      upstream\\n      fallthrough in-addr.arpa ip6.arpa\\n    }\\n    prometheus :9153\\n    proxy . /etc/resolv.conf {\\n      policy sequential # needed for workloads to be able to use BOSH-DNS\\n    }\\n    cache 30\\n    loop\\n    reload\\n    loadbalance\\n}\\n\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"name\":\"coredns\",\"namespace\":\"kube-system\"}}  creationTimestamp: \"2020-03-25T14:21:39Z\"  name: coredns  namespace: kube-system  resourceVersion: \"1341\"  selfLink: /api/v1/namespaces/kube-system/configmaps/coredns  uid: 0dd853af-a4d7-498d-9745-752cbf8fbffbAll I want to do is add this line:rewrite name substring svc.example.com svc.cluster.localSo the full file looks like:apiVersion: v1data:  Corefile: |    .:53 {        errors        health        rewrite name substring svc.example.com svc.cluster.local        kubernetes cluster.local in-addr.arpa ip6.arpa {          pods insecure          upstream          fallthrough in-addr.arpa ip6.arpa        }        prometheus :9153        proxy . /etc/resolv.conf {          policy sequential # needed for workloads to be able to use BOSH-DNS        }        cache 30        loop        reload        loadbalance    }kind: ConfigMapmetadata:  name: coredns  namespace: kube-systemLet’s replace the existing configmap with the new one.$ k replace -f coredns-configmap-rewrite.yml configmap/coredns replacedAfter a few seconds, CoreDNS will restart.I’ve deployed two nginx based servcies.$ k get svcNAME         TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGEkubernetes   ClusterIP      10.100.200.1     &lt;none&gt;        443/TCP        3h30mnginx-1      LoadBalancer   10.100.200.226   10.1.4.18     80:31668/TCP   138mnginx-2      LoadBalancer   10.100.200.160   10.1.4.19     80:31692/TCP   116mI’ll exec into one and run dig.root@nginx-1-56958fdfdd-d26tj:/# dig +short nginx-1.default.svc.cluster.local10.100.200.226root@nginx-1-56958fdfdd-d26tj:/# dig +short nginx-2.default.svc.cluster.local10.100.200.160root@nginx-1-56958fdfdd-d26tj:/# dig +short nginx-2.default.svc.example.com  10.100.200.160root@nginx-1-56958fdfdd-d26tj:/# dig +short nginx-1.default.svc.example.com10.100.200.226Note how both domains work. Nice!That’s it."
    } ,
  
    {
      "title"    : "PKS and Persistent Volumes",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/03/21/pks-max-persistent-disks-statefulsets.html",
      "date"     : "2020-03-21 00:00:00 -0400",
      "content"  : "I work at VMware in the Tanzu group as a Solutions Engineer. Occasionally customers ask me numbers questions, by that I mean they ask about so called “speeds and feeds” like “How many of this can you do?” and “How fast is that?”. The answers to these change all the time, and overall exact numbers aren’t typically important to the business outcomes. That said, sometimes it’s good to explore what the limitation are, and perhaps more importantly where the limitations exist.Actually, as I write this, I don’t think limitations is the right word, as it’s really about how systems and products work together to create constraints. Yes, I like that better. Constraints.StatefulSetIn this lab I have:  vSphere 6.7u3 - Single physical host  NSX-t 2.5  Harbor 1.10  PKS 1.6 and 1.7PKS has kindly created a small cluster, called small-pks. Actually this cluster is 1.7, but that doesn’t have any bearing on this quick test.$ pks cluster small-pksPKS Version:              1.7.0-build.19Name:                     small-pksK8s Version:              1.16.7Plan Name:                smallUUID:                     f5879fcf-46ec-4a7e-b2cb-7a74004aed87Last Action:              UPGRADELast Action State:        succeededLast Action Description:  Instance update completedKubernetes Master Host:   small.example.comKubernetes Master Port:   8443Worker Nodes:             3Kubernetes Master IP(s):  10.197.123.128Network Profile Name:The cluster has 3 worker nodes, so we should be able to get a max of stateful sets that is 3x…something.  But what is that something? I’m making a guess the limitation is not more than 64, so let’s try 200. (This example is from here.)$ cat simple-statefulset.yaml ---apiVersion: v1kind: Servicemetadata:  name: nginx  labels:    app: nginxspec:  ports:  - port: 80    name: web  clusterIP: None  selector:    app: nginx---apiVersion: apps/v1 #  for k8s versions before 1.9.0 use apps/v1beta2  and before 1.8.0 use extensions/v1beta1kind: StatefulSetmetadata:  name: web  labels:    app: nginxspec:  serviceName: \"nginx\"  selector:    matchLabels:      app: nginx  replicas: 200  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: k8s.gcr.io/nginx-slim:0.8        ports:        - containerPort: 80          name: web        volumeMounts:        - name: www          mountPath: /usr/share/nginx/html  volumeClaimTemplates:  - metadata:      name: www    spec:      accessModes: [ \"ReadWriteOnce\" ]      resources:        requests:          storage: 1Gi      storageClassName: ci-storageAfter that runs for a while, I end up stuck with 132 replicas.$ k get statefulsetsNAME   READY     AGEweb    132/200   5h12mWhy is that?Well, it turns out, and this post does a better job of discussing it, that vSphere allows for 4 SCSI adapters per virtual machine, and PKS will use 3 of them. Each of those 3 adapters can have 15 disks. Thus we end up with a constraint of 45 disks per vm for use with persistent volumes.In the image below you can see the vm has 48 disks. 3 disks are for the OS and managing the vm itself via bosh, and are attached to the first SCSI adapter, the other 45 are persistent volumes attached to the remaining 3 adapters. (Here are the maximums for vSphere 6.7.)In the end the constraint is around the number of SCSI adapters possible, how PKS uses them, and how many virtual drives can be attached. With those all in play, we get a constraint of 45 persistent volumes per virtual machine in PKS. Is 45 a lot, a little, or just right? Hard to say. Sounds about right.PS. Note that numbers like this change all the time. By the time you read this, maybe it will be higher than 45 PVs per node."
    } ,
  
    {
      "title"    : "Wireguard, Dante, and Firefox",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/03/01/wireguard-danted-firefox.html",
      "date"     : "2020-03-01 00:00:00 -0500",
      "content"  : "I usually proxy my Firefox through to a remote server running in a public cloud.  Typically I just do that with ssh.e.g. I run the below in a separate terminal and just leave it open.  (I could have used autossh, but never quite got around to it.)ssh -D 8888 remote-vpnThen Firefox is configured to use the local proxy. Note that I set it to proxy DNS as well.It’s a bit of a weird setup, but it’s simple and I’m used to it.Wireguard Instead of sshI’ve been using Wireguard in another situation, and decided it’s time to move from a manually setup ssh command to letting wireguard take care of it.On local laptop:# pwd/etc/wireguard# cat wg0.conf [Interface]Address = 192.168.100.3/32PrivateKey = &lt;redacted&gt;ListenPort = 21842[Peer]PublicKey = &lt;redacted&gt;Endpoint =&lt;redacted&gt;:&lt;redacted&gt;AllowedIPs = 192.168.100.1/32PersistentKeepalive = 25On the remote server. Note that I’m enabling/disabling nat for the wg0 interface IP based on whether the wg0 interface is up or down.# cat wg0.conf [Interface]Address = 192.168.100.1/24PrivateKey = &lt;redacted&gt;ListenPort = &lt;redacted&gt;PostUp = iptables -t nat -A POSTROUTING -s 192.168.100.1/32 -o eth0 -j MASQUERADEPostDown = iptables -t nat -D POSTROUTING -s 192.168.100.1/32 -o eth0 -j MASQUERADE[Peer]PublicKey = &lt;redacted&gt;AllowedIPs = 192.168.100.3/32iptables config. OF course packets must be forwarded too.# sysctl net.ipv4.ip_forwardnet.ipv4.ip_forward = 1# iptables -L -n -t natChain PREROUTING (policy ACCEPT)target     prot opt source               destination         Chain INPUT (policy ACCEPT)target     prot opt source               destination         Chain OUTPUT (policy ACCEPT)target     prot opt source               destination         Chain POSTROUTING (policy ACCEPT)target     prot opt source               destination         MASQUERADE  all  --  192.168.100.1        0.0.0.0/0  On the laptop, enable and start wg0.sudo systemctl enable wg-quick@wg0sudo systemctl start wg-quick@wg0And I’m now connected: $ sudo wg showinterface: wg0  public key: &lt;redacted&gt;  private key: (hidden)  listening port: &lt;redacted&gt;peer: &lt;redacted&gt;  endpoint: &lt;redacted&gt;  allowed ips: 192.168.100.1/32  latest handshake: 1 minute, 44 seconds ago  transfer: 149.80 MiB received, 10.38 MiB sent  persistent keepalive: every 25 secondsssh is convenient because it can do proxying without any extra work. But that is not so with wireguard. I need a second proxy system. In this case, the easiest thing to use seemed to be dante.I’ve configured danted in /etc/danted.conf. (This configuration could probably use some improvement.)NOTE: Only listening on wg0. Don’t put it on the external interface.# cat /etc/danted.conflogoutput: /var/log/socks.loginternal: wg0 port = 1080external: wg0clientmethod: nonesocksmethod: noneuser.privileged: rootuser.notprivileged: nobodyclient pass {        from: 0.0.0.0/0 to: 0.0.0.0/0        log: error connect disconnect}client block {        from: 0.0.0.0/0 to: 0.0.0.0/0        log: connect error}socks pass {        from: 0.0.0.0/0 to: 0.0.0.0/0        log: error connect disconnect}socks block {        from: 0.0.0.0/0 to: 0.0.0.0/0        log: connect error}Configure firefox.NOTE: Firefox I guess doesn’t support user/password for proxies? Very weird.Sessions through dante. Note the “nobody” user.# lsof -Pni :1080 | headCOMMAND   PID   USER   FD   TYPE DEVICE SIZE/OFF NODE NAMEdanted    683 nobody   26u  IPv4 609554      0t0  TCP 192.168.100.1:1080-&gt;192.168.100.3:56096 (ESTABLISHED)danted    685 nobody   20u  IPv4 609980      0t0  TCP 192.168.100.1:1080-&gt;192.168.100.3:56166 (ESTABLISHED)danted    685 nobody   22u  IPv4 609961      0t0  TCP 192.168.100.1:1080-&gt;192.168.100.3:56160 (ESTABLISHED)danted    685 nobody   58u  IPv4 622465      0t0  TCP 192.168.100.1:1080-&gt;192.168.100.3:57406 (ESTABLISHED)danted  30447 nobody    9u  IPv4 575223      0t0  TCP 192.168.100.1:1080 (LISTEN)danted  32313 nobody   12u  IPv4 619529      0t0  TCP 192.168.100.1:1080-&gt;192.168.100.3:57080 (ESTABLISHED)danted  32313 nobody   16u  IPv4 612166      0t0  TCP 192.168.100.1:1080-&gt;192.168.100.3:56324 (ESTABLISHED)danted  32313 nobody   22u  IPv4 610614      0t0  TCP 192.168.100.1:1080-&gt;192.168.100.3:56212 (ESTABLISHED)danted  32313 nobody   62u  IPv4 618755      0t0  TCP 192.168.100.1:1080-&gt;192.168.100.3:57010 (ESTABLISHED)ConclusionUsing ssh was definitely simpler, but I wanted to try something else, specifically wireguard. But this means 1) setting up wireguard (FYI: is an out of tree kernel module), 2) adding a proxy and 3) configuring nat. At least one valuable option in using wireguard is that I can send all traffic through wireguard if I want to. I’m not right now, but I could.That said, I need to do some more work related to the proxy configuration, and whether dante is really the best option here. I’ll experiment with this setup for a while and determine if there are better options. Do I recommend this setup? I think wireguard is an important technology, but I don’t have a great understanding of it yet. So, of course, your mileage may vary.PS. I also need to check on ipv6 support for this setup, but I don’t think my home internet provider supports it (lol)."
    } ,
  
    {
      "title"    : "Overriding Docker Entrypoint when Running from CLI",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/02/22/ovrride-docker-entrypoint.html",
      "date"     : "2020-02-22 00:00:00 -0500",
      "content"  : "I have a very simple Dockerfile that I’m using as an example of layers.$ cat Dockerfile FROM ubuntu RUN echo \"layer 1\" &gt;&gt; /layersRUN echo \"layer 2\" &gt;&gt; /layersRUN echo \"layer 3\" &gt;&gt; /layersRUN echo \"layer 4\" &gt;&gt; /layersENTRYPOINT cat /layersI’ve built it. Now if I run it…$ docker run --rm layer-examplelayer 1layer 2layer 3layer 4But what if I don’t want to use that entrypoint? I can override the entyrpoint and then provide another command.docker run --entrypoint \"/usr/bin/env\" --name layer-example layer-example /bin/bash -c 'while true; do echo sleeping; sleep 2; done'Note that I’m using env as the entrypoint, and then the actual command I want ot run is /bin/bash -c 'while true; do echo sleeping; sleep 2; done'.Interestingly env just forwards the command on and runs it.$ /usr/bin/env echo hihiIf I run it:$ docker run --entrypoint \"/usr/bin/env\" --name layer-example layer-example /bin/bash -c 'while true; do echo sleeping; sleep 2; done'sleepingsleepingsleepingsleepingsleeping...In another terminal I stop and remove the container via:docker rm -f layer-example"
    } ,
  
    {
      "title"    : "20 Tools to Manage Kubernetes Manifests",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/02/16/20-tools-to-manage-kubernetes-maniftest.html",
      "date"     : "2020-02-16 00:00:00 -0500",
      "content"  : "  Of all the problems we have confronted, the ones over which the most brainpower, ink, and code have been spilled are related to managing configurations. - Brendan Burns, Brian Grant, David Oppenheimer, Eric Brewer, and John Wilkes, Google Inc.I spent some time tracking down tools that seem like they manage K8S manifests. I found…many. This list of ~20 only touches the surface of a bubbling ecosystem.Some of these tools are large and some are small. Some are simple templating systems, others can manage the entire life cycle of applications across many kinds of infrastructure. Some are libraries for programming languages, others complete languages themselves. A few are mature projects, some brand new, and others side projects of a single developer. The breadth and variance is quite amazing!Please note that these are in no specific order, and neither does that order represent any kind of preference. Further, some of these tools, eg. Ksonnet, are no longer maintained. This is not an exhaustive list!But let’s get to it.20 Tools (and 2 Bonus Tools)      Jsonnet - A data templating language. Used by many tools in this list behind the scenes. Powerful, but sometimes with great power comes with great, uh, cognitive load?        jk - “jk generates all your JSON, YAML and arbitrary text configuration files. With a little luck, you will not have to touch a YAML file again. Ever.” Bold claim!        Cue - Apparently created by the person (or persons) behind the Borg Configuration Language (BCL) used at Google. “CUE is an open source language, with a rich set APIs and tooling, for defining, generating, and validating all kinds of data: configuration, APIs, database schemas, code, … you name it.”        Isopod  - “Isopod is an expressive DSL framework for Kubernetes configuration. Without intermediate YAML artifacts, Isopod renders Kubernetes objects as  Protocol Buffers, so they are strongly typed and consumed directly by the Kubernetes API.” Interesting that they skip the YAML. Based on  Skycfg. A good intro  here.        Configula - “Configula is inspired by the  JSX language in  React that combines Javascript and HTML tags. Configula defines a similar pattern for Python and YAML (for now).” Joe Beda went over it in  TKIG. It’s a Go utility that calls out to Python!        Ko - Point your K8S container image to a Go repo. It will build a container image using the binary generated from the repo. Does not use Docker to actually build the image. Wraps kubectl in some places.        Kustomize - “kustomize targets kubernetes; it understands and can patch kubernetes style API objects. It’s like make, in that what it does is declared in a file, and it’s like sed, in that it emits edited text.” I’ve seen Kustomize used a few times; seems popular. Also it is conveniently part of Kubectl.        Helm - Helm is likely the most commonly used tool on this list. Recently Helm 3 was released removing its main criticism regarding the use of the tiller server.        Kubecfg - “Kubecfg relies heavily on  jsonnet to describe Kubernetes resources, and is really just a thin Kubernetes-specific wrapper around jsonnet evaluation.” ‘nuff said!        Ksonnet - Ksonnet is no longer worked on, but I think it’s a good thing to have in this list as a reminder ensure that not every tool will stick around. Experimentation will continue in this space.        k14s - “We believe that working with  simple, single-purpose tools that easily interoperate with one another results in a better, workflow compared to the all-in-one approach chosen by Helm. We have found this approach to be easier to understand and debug.” k14s is developed by people at Pivotal (where I work, now part of VMware). There’s a  TGIK episode on it. The Unix philosophy has done us all well over time, perhaps this is the right approach to managing K8S manifests.        Pulumi  - Allows you to use “real” programming languages to manipulate cloud environments, including K8S. Interestingly supports multiple programming languages. “Define infrastructure in JavaScript, TypeScript, Python, Go, or any .NET language, including C#, F#, and VB.”        Ballerina  - _    “Ballerina is an open source programming language and platform for cloud-era application programmers to easily write software that just works.”_ Write code in the ballerina language that can build K8S objects for you.        Kapitan - “…a tool to manage complex deployments using jsonnet, kadet (alpha) and jinja2. Use Kapitan to manage your Kubernetes manifests, your documentation, your Terraform configuration or even simplify your scripts.” Another tool wrapping jsonnet.        Ansible  - Ansible can manage K8S objects and is also fairly good a templating, allowing you to easily write custom inventories and pull in metadata about your systems. Simple and powerful.        Terraform  - Terraform can manage K8S objects. Read about why one would use Terraform and K8S  here. The fact that Terraform is declarative would likely be a good match for K8S.        Habitat  - “Chef Habitat is open source software that creates platform-independent build artifacts and provides built-in deployment and management capabilities.” I can’t do Habitat any justice here, other than to say my impression is that it is a can manage applications on several different types of platforms, of which K8S is one.        Skaffold  - “Skaffold is a command line tool that facilitates continuous development for Kubernetes applications. You can iterate on your application source code locally then deploy to local or remote Kubernetes clusters.” Aimed at the inner dev loop, but can deploy to K8S as well.        Kuby - “Kuby wraps kubectl and provides commands like kuby prepare to replace variables in YAML or kuby deploy to prepare and apply deployments to Kubernetes.”        Dekorate - “The are tons of tools out there for scaffolding / generating kubernetes manifests. Sooner or later these manifests will require customization…Using external tools, is often too generic. Using build tool extensions and adding configuration via xml, groovy etc is a step forward, but still not optimal.” I think that introduction is apropos given this list. Keep manifest generation as part of your development/language environment, makes sense to me especially if your org is focussed on one language, i.e. Java.        BONUS:  Kpack - A system for using Cloud Native  buildpacks. Buildpacks are a more structured, opinionated way to build container images. Joe Beda ran a  TKIG on it. The pack command line can be used to build local images, or kpack can be deployed into K8S and will automatically rebuild images when code is changed in repositories. Kpack is focussed in image builds, not so much on generating manifests. (NOTE: Built by Pivotal, now VMware.)        BONUS: Any programming language - I wanted to make sure that I put this on the list, as ultimately any language that can print to standard out could be used to generate K8S manifests. Of course, we would likely not want to do something that simplistic, but I don’t think it’s far fetched or unreasonable to use templates and a generic programming language to build manifests.  ConclusionAs I mentioned at the start this list is random–the order it comes in was the order I discovered them on the Internet. (Certainly there are some cool tools I have missed, let me know in the comments.)Ultimately, these tools provide some kind of templating, wrapper, abstraction, or other convenience around K8S, though, of course, they all do it in different ways and chose different approaches. The differences and value of these individual tools is typically under the surface layer and thus require some investigation to truly understand when, or when not, to use them.With so many options it does seem like the market (made up of us technical people–the people who want and use K8S) hasn’t decided how exactly to use Kubernetes. A “silver bullet” approach probably won’t work. Will one of these tools win out? A handful? Will we get something new? Or does Platform as a Service still make the most sense? Time will tell. Until then, we all have to perform some due diligence and keep innovating.PS. Once I found ~20 I stopped. A few minutes later I found this spreadsheet with 120 similar tools!"
    } ,
  
    {
      "title"    : "Kubernetes and Mimesis - The YAML is not the (M)App",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/02/15/the-yaml-is-not-the-app-or-map.html",
      "date"     : "2020-02-15 00:00:00 -0500",
      "content"  : "100% of organizations I have worked with and talk to either want to automate or already doing so. Everyone “automates.” Everyone “DevOps.”  “Yes, yes, we have DevOps, we have automation. Been doing that forever. But it’s not enough.” - CustomerMost want to do more than what automation can provide on its own. In my opinion, doing more demands abstraction.Fear of AbstractionTechnology leaders often equate abstraction with “lock-in,” be it technical lock-in, contractual lock-in, or other. Abstraction is, to them, a concerning concept and something to be avoided.To engineers the mention of abstraction conjures a fear of the inscrutable. To engineers, abstraction is a black box. Key, even sacred, information is hidden within the confines of the dreaded cube. Of course, some information hiding is precisely the goal of abstraction. However, as an engineer might ask you, if information is hidden, then how can one truly understand every component of their vast and complex platform? Full stack or full stop.Thus we end up, as an industry, in a situation where abstraction is alarming to both technical leadership and engineers. So it is typically avoided, especially in terms of infrastructure.Instead of innovating and building powerful business outcomes on abstraction, we recreate–we imitate–the existing terrain by drawing an extremely detailed but novel map (just like the old one). With a new and detailed map in hand engineers feel safe and energized. With a new and detailed map, technology leaders have an artefact that can be displayed under glass with a label of progress. But I’m getting ahead of myself.Release the Shackles!Prior to, oh say around about 1910, give or take, artists painted real things and those paintings looked like real things (as much as a two dimensional image can, anyway). In fact, if you didn’t paint real things that looked like real things, say something “abstract,” it was cause for serious concern. Take, for example, the harsh criticism of James Whistler’s “Nocturne in Black and Gold – The Falling Rocket” via which he eventually entered bankruptcy.When Malevich created his most well known work, “Black Square,” it was arguably “the first time someone made a painting that wasn’t of something.” Malevich’s work was designed, in part, to free the art world from its current limitations related to the explicit desire to continually imitate something real.Most importantly, I believe, in Malevich’s work “the white backgrounds against which they were set mapped the boundless space of the ideal.” When we avoid abstraction in technology, we also miss out on the benefits of the open playing field it creates. Without abstraction we don’t have room to run.Kubernetes and MimesisFirst off, no, mimesis is not some new Kubernetes related project or product (at least…I don’t think it is). The typical definition of mimesis revolves around the idea of imitating or reproducing reality. To reiterate the quote I have from Richard Spalding above, “Malevich’s intent [was] to liberate painting from the shackles of mimesis.” I believe mimesis is an easy trap to fall into with k8s.Make no mistake, I am a big fan of Kubernetes. I even spent three hours wrangling YAML for the Certified Kubernetes Administrator exam. That said, I do feel like we, as an industry, have to be careful not to end up creating yet another detailed map of the territory. We should endeavour not to simply imitate existing constructs. We have to be willing to not only accept the wide open playing field the box/square/abstraction provides but to have an appetite for it. And, most importantly, use k8s to create it.This point has been well made by some of the originators of Kubernetes, people such as Joe Beda and Craig McLuckie of Heptio, and now VMWare, fame. Others too.Kubernetes role is not to mimic the existing platforms and systems, ie. a map where the scale is a mile to a mile. Rather its mission is to provide the advanced machinery necessary to develop new abstractions on which we can then create novel, and financially beneficial, work. Abstraction doesn’t create black boxes, it provides the freedom to invent.In conclusion, suffice it to say, the YAML is not the (m)app."
    } ,
  
    {
      "title"    : "Investigating Curl Bash Installs with Docker",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/02/02/curl-bash-investigation-docker-diff.html",
      "date"     : "2020-02-02 00:00:00 -0500",
      "content"  : "Many sytems install with some version of curl https://install.example.com | bash. Using this model for installation is typically DIScouraged. But what if you don’t have much of an option for installation? How can you investigate what this bash script does?One simple way is to use docker diff.sdkman Install InspectionRecently I have started working with Java. Pretty new to it all. I’m used to basic Python development and often use virtual environments. What provides a similar pattern in the Java ecosystem? One example may be sdkman. I’m not sure, haven’t used it. However, to install it requires the curl bash (anti?) pattern. I’d like to investigate what that curl bash does.First, an image with the base requirements for sdkman. It uses which to check for zip, ect.localhost$ cat Dockerfile FROM fedoraRUN dnf install -y unzip zip curl whichlocalhost$ docker build . -t sdkman-testRun it.docker run -it --name sdkman-test-run sdkman-test /bin/bashManually install sdkman into that container through the curl bash.container$ curl -s \"https://get.sdkman.io\" | bash Inspect the differences in the filesystem in the RUNNING container.localhost$ docker diff sdkman-test-runC /rootC /root/.bashrcA /root/.sdkmanA /root/.sdkman/extA /root/.sdkman/srcA /root/.sdkman/src/sdkman-offline.shA /root/.sdkman/src/sdkman-selfupdate.shA /root/.sdkman/src/sdkman-uninstall.shA /root/.sdkman/src/sdkman-use.shA /root/.sdkman/src/sdkman-availability.shA /root/.sdkman/src/sdkman-env-helpers.shA /root/.sdkman/src/sdkman-install.shA /root/.sdkman/src/sdkman-list.shA /root/.sdkman/src/sdkman-update.shA /root/.sdkman/src/sdkman-version.shA /root/.sdkman/src/sdkman-current.shA /root/.sdkman/src/sdkman-default.shA /root/.sdkman/src/sdkman-help.shA /root/.sdkman/src/sdkman-main.shA /root/.sdkman/src/sdkman-broadcast.shA /root/.sdkman/src/sdkman-flush.shA /root/.sdkman/src/sdkman-upgrade.shA /root/.sdkman/src/sdkman-cache.shA /root/.sdkman/src/sdkman-path-helpers.shA /root/.sdkman/src/sdkman-utils.shA /root/.sdkman/tmpA /root/.sdkman/tmp/sdkman-5.7.4+362.zipA /root/.sdkman/tmp/stageA /root/.sdkman/varA /root/.sdkman/var/candidatesA /root/.sdkman/var/versionA /root/.sdkman/archivesA /root/.sdkman/binA /root/.sdkman/bin/sdkman-init.shA /root/.sdkman/candidatesA /root/.sdkman/etcA /root/.sdkman/etc/configA /root/.zshrcSeems like /root/.bashrc has changed. Unfortunately docker diff can’t diff individual files. So I’ll need to build a new image and diff the base Fedora file.Let’s add the curl bash to the image so we can diff it.localhost$ cat Dockerfile FROM fedoraRUN dnf install -y unzip zip curl whichRUN curl -s \"https://get.sdkman.io\" | bash Build again.localhost$ docker build . -t sdkman-testNow grab the two bashrc file, one from the base fedora image and one from the image just built.docker run --rm -v `pwd`:/out sdkman-test /bin/cp /root/.bashrc /out/bashrc-sdkmandocker run --rm -v `pwd`:/out fedora /bin/cp /root/.bashrc /out/bashrc-baseDiff.localhost$ diff bashrc-base bashrc-sdkman 12a13,16&gt; &gt; #THIS MUST BE AT THE END OF THE FILE FOR SDKMAN TO WORK!!!&gt; export SDKMAN_DIR=\"/root/.sdkman\"&gt; [[ -s \"/root/.sdkman/bin/sdkman-init.sh\" ]] &amp;&amp; source \"/root/.sdkman/bin/sdkman-init.sh\"Not surprisingly the install has added sourcing an sdkman init script to ~/.bashrc.ConclusionObviously there is no limit to the amount of inspection one could do on foreign code, nor the amount of changes an imported bash file could do to an OS. In this case all I’ve done is find out what files sdkman adds or changes, and review the diffs on one file. But at least it gives me an idea of what is going on with the install.Using docker to evaluate tools makes a lot of sense to me, mostly because I don’t want to mess up my physical OS (which is actually running in a virtual machine, but I digress). Some people take this much farther and use containers to run things like sdkman.Even though I’ve worked a lot with Docker, I’ve never tried to inspect the diffs of image layers. Likely that would be a better option that what I’ve done above. Something to look into…"
    } ,
  
    {
      "title"    : "Cloud Native Buildpacks",
      "category" : "",
      "tags"     : "",
      "url"      : "/2019/12/16/buidpack-pack.html",
      "date"     : "2019-12-16 00:00:00 -0500",
      "content"  : "How does code turn into an application running on Kubernetes? Good question. Many things have to happen: Dockerfiles written and rewritten, base images picked, builds pushed to repositories, and, of course, much k8s YAML wrangled. These simple phrases, e.g. “Dockerfile written,” represent considerable complexity.How can we make getting code into production simpler for everyone?One way is to use buildpacks, which help to make the generation of OCI images easier. What if I told you that with buildpacks you don’t even NEED a Dockerfile?Building Container Images is Hard - Use BuildpacksFor quite a while Docker was the only easy way to build container images. It’s still responsible for building the vast majority container images. However, with the creation of the OCI image spec other tools have been developed.It can’t be denied that giving development teams the ability to create their own runtime images improved the developer experience. They could run the same image locally as what would, in theory, go into production. They knew the apps dependencies and could add them to the Dockerfile.However, in the socio-technical realm of enterprise organizations, operations typically does not allow arbitrary container images in production, for various reasons, some valid, some not. Further to that, many ops organizations will try to manage container images exactly like they manage virtual machine templates (if they manage them at all) which is an anti-pattern.Ultimately Dockerfiles make building operationally resilient images look easy, but it’s not.Here are a few considerations one has to make when building container images:  Keeping image sizes small  Dealing with dependencies  Picking the right base image  Ensuring appropriate application memory settings  Day 2 ops - e.g. How to update the JDK without breaking the image cache or rebasing on a new base image  Defining who is responsible for the images - Ops, Dev, or DevOps?  Dealing with CVEsIssues with typical container images:  Lack of app awareness  Not composable - How do we combine images? (Only have multistage builds in Dockerfiles)  Leaky abstraction - e.g. Container images mix operational concerns with application development concerns  Non-reproducible/untestable builds  Security - e.g. many Dockerfiles assume running as root  Treating containers images as VMsEnter Buildpacks.io  Buildpacks are pluggable, modular tools that translate source code into OCI images.Buildpacks are currently a CNCF Sandbox project supported by companies like Pivotal and Heroku.  Buildpacks have always been about the developer experience. We want buildpacks to make your job easier by eliminating operational and platform concerns that you might otherwise need to consider when using containers.Buildpacks are a higher level abstraction than Dockerfiles and are really meant for developers.What do you get?      Provide a balance of control that reduces the operational burden on developers and supports enterprise operators who manage apps at scale    Ensure that apps meet security and compliance requirements without developer intervention    Provide automated delivery of both OS-level and application-level dependency upgrades, efficiently handling day-2 app operations that are often difficult to manage with Dockerfiles    Rely on compatibility guarantees to safely apply patches without rebuilding artifacts and without unintentionally changing application behavior  Using BuildpacksI run on Linux so I just downloaded the binary into my home bin directory and made it executable.$ which pack~/bin/packLet’s clone a repo, spring-music, and build an image.$ git clone https://github.com/cloudfoundry-samples/spring-musicNow cd into spring-music. Note no Dockerfiles exist. Just plain code and build files.$ cd spring-music$ lsbuild.gradle  gradle  gradle.properties  gradlew  gradlew.bat  LICENSE  manifest.yml  README.md  srcNow, with one simple command, let’s build a hardened container image that has been run millions of times on the Pivotal Platform and Heroku.$ pack build spring-musicHere’s the resulting image…264MB:$ docker images spring-musicREPOSITORY          TAG                 IMAGE ID            CREATED             SIZEspring-music        latest              0ea601302547        15 minutes ago      264MBRun it.$ docker run --rm -d -p 8080:8080 spring-musicCurl localhost:8080 to test. $ curl -s localhost:8080 | grep title    &lt;meta name=\"title\" content=\"Spring Music\"&gt;    &lt;title&gt;Spring Music&lt;/title&gt;Well, that was pretty easy…and no Dockerfiles!A Bit MoreWhile there is a lot to…er…“unpack,” here, I’d like to point out a couple of things that I think are interesting and important about buildpacks.  Disconnection of Operating System from ApplicationWith standard Docker images when a security issue is discovered in a operating system package many, if not all, of the image layers have to be rebuilt. This makes redeploying applications slow. It also handcuffs the OS requirements to the app requirements. With buildpacks these concerns are separated.  Java BuildPack Memory CalculatorI find that it’s quite easy to ignore or forget JVM memory settings…especially in a container centric world. Buildpacks use the Java BuildPack Memory Calculator to dynamically set requirements for memory. I have yet to see a Dockerfile that implements this or anything like it. *ConclusionBuilding hardened, operationally reliable container images is difficult. Using Buildpacks not only makes Dockerfiles unnecessary, but provides access to images that have been in constant, heavy production use for years.Using pack and buildpacks you don’t need to:  Write your own Dockerfiles  Add individual files before app code to improve caching  Dance around for user permissions and root access  Force docker to rebuild all layers for a security patch  Understand the base operating system  Couple applications to the build pipeline  Burn in images over time to feel that they are trustworthyRunning applications in production isn’t easy. Using buildpacks can help to reduce the operational burden, for everyone.I recommend watching a couple of videos for more in-depth information on buildpacks.* Hat tip to my colleague Adib Saikali for information on the Java memory calculator. Watch his Toronto Java meetup talk on Spring and Kubernetes."
    } ,
  
    {
      "title"    : "Speedy Software Distribution",
      "category" : "",
      "tags"     : "",
      "url"      : "/2019/11/27/speedy-software-distribution.html",
      "date"     : "2019-11-27 00:00:00 -0500",
      "content"  : "I would wager most, if not all, developers have edited a file on a remote server to update an application. Those were the days. Now the spectrum of software distribution ranges from manually copying files, to container images, to bit-torrent and more.The Current Standard: Container ImagesContainer images have vastly affected how we distribute software. Given the desire for many organizations to use Kubernetes, which relies on containers, which in turn rely on container images, the model of distributing singular binaries of an OS file system has become the default modern day standard.While in the past we may have used simple things like copying and rsync-ing files, operating system packages, even bit-torrent, we now strive to distribute OCI images.*We’re All Distributed Programmers NowCornelia Davis, CTO Pivotal, has this to say about modern day software development:  “…we’re all distributed systems programmers now. When I started my degree 30 years ago, that was niche. Now, in the cloud, everything is distributed…”The prevalence of distributed systems means that we need to get software running on &gt;1 application instance. Maybe thousands. How can we quickly, efficiently, and accurately distribute software to thousands of servers running around the globe?The Need for SpeedI believe speed is key to the future of software distribution. How container images, or some other package, get from host is as important as how those packages are defined. I would like to have my software running globally in hundreds of milliseconds.One doesn’t have to look much further than Uber’s Kraaken project to see what might be involved in speeding up software distribution.Kraaken is a peer to peer Docker registry:  Docker containers are a foundational building block of Uber’s infrastructure…but as the number and size of our compute clusters grew, a simple Docker registry setup with sharding and caches couldn’t keep up with the throughput required to distribute Docker images efficiently.Another interesting and speedy software distribution component is Chrome’s Courgette (from 2009 no less). Courgette can be 10x more efficient than something like bsdiff (which itself is extremely bit thrifty). Courgette does not directly help container images, but it represents an efficient way to create a binary delta. Smaller deltas mean faster distribution. Faster distribution can mean more frequent updates.  If the update is a tenth of the size, we can push ten times as many per unit of bandwidth.In that post they show the improvement in data transfer using Courgette.Full update: 10,385,920 bytesbsdiff update: 704,512 bytesCourgette update: 78,848 bytes700k to 78k…that’s a an impressive improvement resulting in many bits that don’t have to traverse the network (and again, bsdiff is extremely efficient). For those of you that would like more information on the Courgette model, there is considerable research in the area, going back at least to exediff.What’s NextAt this time I believe there are at least three major things to think about when thinking about distributing code:  Peer to peer deployment (eg. Kraaken)  Advanced delta encoding (ala Courgette and exediff)  Improvements in code injection / application restartsThis post has only, just barely, grazed the surface of software distribution. As we continue down the distributed programming path and focus on getting obstacles out of the way of developers, we will find that distributing applications at insanely fast speeds will be a key enabler.* Of course these binary images are made up of things like OS packages, but lets not think about that too much right now. Also, what we are really distributing is the layers of these images."
    } ,
  
    {
      "title"    : "Mikrotik CRS326-24G-2S+RM",
      "category" : "",
      "tags"     : "",
      "url"      : "/2019/09/29/mikrotik-CRS326-24G-2S-RM.html",
      "date"     : "2019-09-29 00:00:00 -0400",
      "content"  : "I have a bunch of loud noisy switches…cough Juniper EX4200s cough. I recently moved and haven’t had the time yet to setup proper sound proofing for all my homelab gear and thus I need to be quieter where I can be. Lower power consumption would help too.So I bought a Mikrotik CRS326-24G-2S+RM to be my main 1GB switch. I purchased the switch from Solimedia and it was about $250 CDN.Power Consumption and VolumeRight now the switch has about 20 ports populated and is doing a fair amount of work. It’s pulling only 17 watts. My EX4200 pull about 135 watts; that’s a substantial difference. Based on my calculations, running an EX4200 24/7 would cost about $8.70/month in Ontario and the CRS326 about $1.50.The switch also has no fans, so it’s virtually silent.ConfigurationThe challenging part of this switch is that Mikrotik has many different products, and they use many different CPUs and ASICs.In their C3XX line you configure VLANs differently than their other switches, as they do hardware VLANS. They have some docs on configuration. These short docs are not detailed, but they are accurate in my experience.Example 3 is probably the one most people would use…“VLAN Example #3 (InterVLAN Routing by Bridge)”:/interface bridgeadd name=bridge1 vlan-filtering=yes/interface bridge portadd bridge=bridge1 interface=ether6 pvid=200add bridge=bridge1 interface=ether7 pvid=300add bridge=bridge1 interface=ether8 pvid=400/interface bridge vlanadd bridge=bridge1 tagged=bridge1 untagged=ether6 vlan-ids=200add bridge=bridge1 tagged=bridge1 untagged=ether7 vlan-ids=300add bridge=bridge1 tagged=bridge1 untagged=ether8 vlan-ids=400/interface vlanadd interface=bridge1 name=VLAN200 vlan-id=200add interface=bridge1 name=VLAN300 vlan-id=300add interface=bridge1 name=VLAN400 vlan-id=400/ip addressadd address=20.0.0.1/24 interface=VLAN200add address=30.0.0.1/24 interface=VLAN300add address=40.0.0.1/24 interface=VLAN400A few notes:  The bridge concept is an abstraction you don’t see in a lot of other network gear, so it will take a few minutes to get used to it  Make sure you are reading configuration examples for the C3XX line of products as other products are configured differently  For access ports set the PVID (as shown and documented), otherwise they will get dynamically added to PVID 1, for trunk ports leaving off the PVID seems the proper config  Notice how the bridge “bridge1” is being added as a tagged port; don’t forget that if you want to route inter-VLAN  Set each VLAN ID separately so you can edit them easier later  /interface bridge port and /interface bridge vlan will be your friend  RouterOS reports “current tagged” and “current untagged” which can be confusing, use “print detail” to see what is actually tagged and untagged  Note “vlan-filtering=yes” during bridge creation. Many of the docs show starting with “vlan-filtering=no” while building the configuration because they don’t want you to lock yourself out of the switch (if you are connected over the network) before you are done. So their examples are create the bridge, configure the switch, then finally turn vlan filtering on once you are done. But it would be easy to forget to turn it on. I changed the example above to set it to yes at the start.Also note that if you don’t have an “H” beside a port then it’s not being managed in hardware and will be slow.[admin@MikroTik] &gt; /interface bridge port print Flags: X - disabled, I - inactive, D - dynamic, H - hw-offload  #     INTERFACE                                           BRIDGE                                          HW  PVID PRIORITY  PATH-COST INTERNAL-PATH-COST    HORIZON 0   H ether1                                              bridge1                                         yes  102     0x80         10                 10       none 1   H ether2                                              bridge1                                         yes  103     0x80         10                 10       none 2   H ether3                                              bridge1                                         yes  104     0x80         10                 10       none 3   H ether4                                              bridge1                                         yes    1     0x80         10                 10       none 4   H ether5                                              bridge1                                         yes  106     0x80         10                 10       none SNIP!ConclusionFor some reason I like Mikrotik gear. I have quite a bit of it, including some wireless (don’t get me started on the whole capsman thing). What I don’t like about Mikrotik is that the documentation is somewhat lacking. Configuring a Juniper switch can be done easily with some surface level googling as pretty much every Juniper question has been answered succinctly and search engine indexed. Not so with Mikrotik. Their large and changing product line, part of their value proposition, makes this even more difficult.One thing I do like about Mikrotik is that the OS comes with many features, even MPLS. This system is not just a dumb switch, it is a full featured router.Finally, the switch is only 24 ports. Technically I’d have to have two of them to match the 48 ports of the EX4200. When you have many boxes ports will run out quickly.You can certainly find cheaper, older used switches, but they will likely be loud and power hungry."
    } ,
  
    {
      "title"    : "Mikrotik RB2011 Won&#39;t Reset to Default Configuration",
      "category" : "",
      "tags"     : "",
      "url"      : "/2019/07/01/mikrotik-rb2011-default-config.html",
      "date"     : "2019-07-01 00:00:00 -0400",
      "content"  : "tl;drIf you can’t get your RB32011 router to go back to default configuration, ie. it just comes up with a blank configuration, you’ll likely need to netinstall it, which wipes everything.MikrotikI have an old Mikrotik RB2011 and it’s been sitting unused in a box for several years. But I’m back on my Mikrotik thing as I’m wiring up my house with more than one WIFI device. I don’t know if this will actually improve my home WIFI, but I’m doing it anyways.I like Mikrotik because they support MPLS and I like to pretend I know what I’m doing. Not that I’ll use MPLS for a wireless network, but I like the possibility of using it…even if I don’t.Mikrotik was in the news last year for some security issues, which I have not paid enough attention to.Default ConfigurationAs I try to figure out how to configure CAPsMAN, Mikrotik’s multi AP management system, I needed to put my RB2011 back into factory default configuration more than once (as I make mistakes and lose access to the router). But, after an upgrade to the OS and a reset, it wouldn’t come back up with the default settings, rather it boots with a completely blank configuration, which makes it challenging to work with as you can’t access it easily over the network (of course, the serial console works). I only found one post about it and it didn’t help.At any rate, my assumption is the old default script stored in the device wasn’t compatible with the new OS version. Unfortunately I didn’t keep a copy of the old default-config script, which you can view with:/system default-configuration printBelieve me that the most recent default config file, the one that exists after a netinstall, is much different than the original one I had on my RB2011 with the older OS.To ge the new script installed I had to reinstall via NetInstall, which actually worked. I had to use my Windows computer, but I was able to reinstall in less than 10 minutes, which is nice.So, after netinstalling, I could again run the reset command./system reset-configurationOnce the node came back up, it has this default config.PS. Serial AccessAlso, serial access works just fine using a USB serial console device and cable to the back of the RB2011.sudo screen /dev/ttyUSB0 115200"
    } ,
  
    {
      "title"    : "The Hard Thing About Hard Things",
      "category" : "",
      "tags"     : "",
      "url"      : "/2019/07/01/hard-things-book-review.html",
      "date"     : "2019-07-01 00:00:00 -0400",
      "content"  : "For some reason I was expecting this book to be about how to do hard things. In my mind a “hard thing” was a single project that was technically difficult. I’m not sure where that idea came from, but that isn’t what this book is about. The Hard Thing about Hard Things is about being a CEO and how challenging that job is.I enjoyed the book and it’s something one could come back to again and again over the span of a career.Loudcloud/OpswareI remember hearing about Opsware waaaaay back in the day. At the time it wasn’t something I would keep track of; I wasn’t paying attention to the whole Silicon Valley thing at the time. I was just getting out of school and into a job that was almost as far away from Silicon Valley as you could get (Northern Alberta). I should do some more reading on Loudcloud and Opsware, as it sounds like they were far ahead of the game for a while, and this was prior to even virtualization. (In fact in the book Horowitz mentions that the invention of virtualization was a part of why he decided to sell Opsware to HP, in that they would have had to spend a lot of time and money supporting virtualization in their product.)A few notes from my reading  The right kind of ambition  At a macro level, a company will be most successful if the senior managers optimize for the company’s success (think of this as global optimization) as opposed to their own personal success (local optimization)  When I spoke to Mark Cranney…it was difficult to get him to discuss his personal accomplishments. He really only wanted to discuss how his old company won.      Much discussion of wartime vs peacetime CEOs…what wartime CEOs do is much different than peacetime, and most of the time one person is not great at both styles. So if you are “at war” you need a different CEO.        “The struggle” of the CEO    …if there is one skill that stands out, it’s the ability to focus and make the best move when there are no good moves. It’s the moments where you feel most like hiding or dying that you can make the biggest difference as CEO…I offer some lessons on how to make it through the struggle without quitting or throwing up too much.  TrainingThis book is very pro-training. I love it. Horowitz quotes Andy Grove:  Training is, quite simply, one of the highest-leverage activities a manager can perform.      Andy Grove, Bill Campbell - Horowitz mentions these people many times throughout the book. A new book about Bill Campbell is out. Definitely going to read it.        Being honest    If you run a company, you will experience overwhelming psychological pressure to be overly positive. Stand up to the pressure, face your fear, and tell it like it is.I’m assuming Horiwitz determined when to be honest very carefully.  $1.6 billion sale to HPEverything in the book is working towards this sale. That number is the success of the company. Nothing else matters. Any other choices Horowitz could have made over the lifetime of the company might have watered down this number. He fought hard to get a specific share price and was willing to have the deal fall through if he didn’t get it.  Mark CranneyI found any discussion around Mark Cranney fascinating. I have a pet theory, which I think is more than a theory, that most high ranking managers and sales people are…just tall. Note that I am exactly average height so maybe that has something to do with it. :)  I interviewed Mark Cranney. He wasn’t what I expected: he didn’t fit the stereotype of a hard-charging sales executive. For starters, Mark was average height, whereas most sales executives tend to be rather tall. Next, he was as wide as he was tall. not fat, just square. His square body seemed to fit rather uncomfortably into what must have been a custom-tailored suit–there is no way an off-the-rack business suite would fit a square guy like Mark.Later in the book Horowitz says that if Cranney had better met the physical stereotype of an executive he would have been CEO of IBM, ie. that Opsware got Cranney at a discount.  Hire for strengthHe discusses reasons why CEOs fail executives and then have to fire them. One of the reasons is not hiring for strengths.  You hired for lack of weakness rather than for strengths. This is especially common when you run a consensus-based hiring process. The group will often find the candidate’s weaknesses, but they won’t place a high enough value on the areas where you need the executive to be a world-class performer. As a result you hire an executive…who is mediocre where you need her to be great.There’s much more to get out of this book. Like most of Silicon Valley, I heavily suggest just picking up a copy."
    } ,
  
    {
      "title"    : "WIFI - Error Connection activation failed, Secrets were required, but not provided.",
      "category" : "",
      "tags"     : "",
      "url"      : "/2019/05/31/secrets-were-required.html",
      "date"     : "2019-05-31 00:00:00 -0400",
      "content"  : "Prior to a presentation, I was trying to connect to a Cisco Meraki based wireless network with a Lenovo Gen3 X1 Fedora 30 laptop, and kept getting this error:$ nmcli dev wifi connect &lt;SSID&gt; password &lt;password&gt;Error: Connection activation failed: (7) Secrets were required, but not provided.Obviously I provided the password. I was totally stumped. Then I saw this Stackexchange post and decided to give it a try.This is my wireless device:$ lspci -vv -s 04:00.004:00.0 Network controller: Intel Corporation Wireless 7265 (rev 59)\tSubsystem: Intel Corporation Dual Band Wireless-AC 7265\tControl: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+\tStatus: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx-\tLatency: 0, Cache Line Size: 64 bytes\tInterrupt: pin A routed to IRQ 49\tRegion 0: Memory at f1000000 (64-bit, non-prefetchable) [size=8K]\tCapabilities: &lt;access denied&gt;\tKernel driver in use: iwlwifi\tKernel modules: iwlwifiI should note that I’ve connected to several WIFI networks since installing Fedora 30 and have had no issues. Just this one Meraki based network so far.I added the below:$ tail -2 /etc/NetworkManager/NetworkManager.conf[device] wifi.scan-rand-mac-address=noAnd restarted NetworkManager, and lo behold was able to connect to the WIFI. I am not sure why this works. The MAC address randomization is a good thing to have security/privacy-wise. I’d prefer that it was enabled. But I really do need to connect to WIFI as easily as possible. This caused me major issues as this cropped up prior to a presentation (ugh). Previously I was on Fedora 26 for a (too) long time. Presumably this feature is new. Or it is some other related bug.However, that said, I don’t know why this would cause the problem.  Related bug report  Related post  Article on WIFI MAC randomizationThe bug report seems to be related to USB WIFI devices but there are a few onboard as well.The reality is turning off the randomization allowed me to connect. sigh"
    } ,
  
    {
      "title"    : "What is StarlingX?",
      "category" : "",
      "tags"     : "",
      "url"      : "/2019/05/20/what-is-starlingx.html",
      "date"     : "2019-05-20 00:00:00 -0400",
      "content"  : "I recently joined the Technical Steering Committee (TSC) of the open source StarlingX project. As one of the members of the TSC, I work to enable the governance model of the project, which is part of the OpenStack Foundation. Good open source governance is important, but it’s not the most interesting part. :)What is StarlingX?StarlingX, which I usually abbreviate to STX, is an open source project that provides a customized Linux distribution which delivers a fully managed Kubernetes and OpenStack deployment on one, two, or more physical servers. It’s designed to run mission critical workloads on the edge.If you are working in the world of Network Function Virtualization (NFV), “Industry 4.0”, Internet of Things (IoT), and other similar areas, STX would likely be interesting as it presents a way to provide de-facto standard infrastructure APIs in edge locations–locations where it may only be feasible to have one or two physical servers. STX can manage hundreds of compute nodes if needed, but for the edge single and dual modes are the most compelling.OpenStack Foundation - More than OpenStackStarlingX is open source project that is supported by the OpenStack Foundation. The seed code came from Wind River’s Titanium product. Going forward, Wind River Titanium will be the downstream commercial based on the upstream open source code. Of course, as open source software, anyone is free to take the code and artifacts generated and deploy it or build it into their own product.For those of you that don’t know, the OpenStack Foundation provides structure for more open source projects than OpenStack. At this time there are four other major projects within the OpenStack Foundation:  StarlingX - Edge OpenStack and Kubernetes  Zuul - CI/CD and project gating  Kata Containers - Virtual machines instead of containers  Airship - Declarative OpenStack and Kubernetes automationFeatures of STX - Designed for the EdgeSTX does a lot of things, but I think the most important features that help to differentiate it are:  It can run on a single physical server and still provide Kubernetes and OpenStack APIs. This is called “simplex” mode.  It can run on two physical servers, providing a highly available version of Kubernetes and OpenStack. This is called “duplex” mode.  It runs OpenStack on top of a fully managed Kubernetes deployment–OpenStack is just an app in k8s.  StarlingX has made design choices and created specialized software services and features designed specifically to run mission critical workloads. High availability. Integrity. Safety. These are words the project’s values.(Note that STX has many other features and integrated systems that I haven’t mentioned here in an attempt to keep this post short.)Open Source Software - Help Wanted!Much of the work going on in the project now is to upstream the seed code into various open source projects, from the Linux kernel to OpenStack Neutron. However, StarlingX is also advancing and adding new features and functionality. A good example is the Kubernetes integration work occurring right now.If OpenStack, Kubernetes, and new technology paradigms such as the edge, NFV, IoT, and others are of interest to you, I heavily suggest getting involved in the project. Have a look at the community page on the website and get in touch if you’d like to participate. Or, if your organization would like to run a proof of concept or has questions about the project, reach out and let us know."
    } ,
  
    {
      "title"    : "Install and Use Podman (Instead of Docker)",
      "category" : "",
      "tags"     : "",
      "url"      : "/2019/05/19/docker-podman-fedora-30.html",
      "date"     : "2019-05-19 00:00:00 -0400",
      "content"  : "tl;drI reinstalled my laptop with Fedora 30. I tried to install a stable Docker, but the Docker repo for Fedora 30 stable doesn’t exist. I realized podman is available and is a command for command replacement for Docker. So far I’m quite happy with it and am actually kind of glad I was forced into it.What Distro to Run?Recently I reinstall my laptop with Fedora 30. I won’t mention what version of Fedora I “upgraded” from, but suffice it to say I was using my old install for quite a while. I wouldn’t have minded using Fedora Silverblue, Fedora CoreOS, or CentOS 8. However, two of those don’t have anything to install yet, and Silverblue…it seems a bit too early for me. So I stuck with good old Fedora.I don’t actually need much from Linux. I use the i3 window manager, Firefox browser, smartcd, VSCode (from Microsoft) and a few other tools. As long as I can apt/yum/dnf install common tools the Linux distribution I use doesn’t really matter. I don’t actually even use Docker locally that much, but I do use it to manage this blog, and it’s nice to have a container runtime available.Docker on Fedora 30 (or lack thereof)After reinstalling my laptop, I went to install docker. In retrospect I should have read the docs better:  To install Docker, you need the 64-bit version of one of these Fedora versions:   28   2930 is not 28 or 29. :) Regardless, I kept going.I installed the official repo.$ sudo dnf config-manager \\&gt;     --add-repo \\&gt;     https://download.docker.com/linux/fedora/docker-ce.repoAdding repo from: https://download.docker.com/linux/fedora/docker-ce.repo$ head /etc/yum.repos.d/docker-ce.repo [docker-ce-stable]name=Docker CE Stable - $basearchbaseurl=https://download.docker.com/linux/fedora/$releasever/$basearch/stableenabled=1gpgcheck=1gpgkey=https://download.docker.com/linux/fedora/gpg[docker-ce-stable-debuginfo]name=Docker CE Stable - Debuginfo $basearchbaseurl=https://download.docker.com/linux/fedora/$releasever/debug-$basearch/stableThen I tried to install docker.$ sudo dnf install docker-ce docker-ce-cli containerd.ioDocker CE Stable - x86_64                                                                                                                   1.7 kB/s | 577  B     00:00    Failed to synchronize cache for repo 'docker-ce-stable'Fedora Modular 30 - x86_64                                                                                                                   46 kB/s |  17 kB     00:00    Fedora Modular 30 - x86_64 - Updates                                                                                                         69 kB/s |  16 kB     00:00    Fedora 30 - x86_64 - Updates                                                                                                                 55 kB/s |  17 kB     00:00    Fedora 30 - x86_64 - Updates                                                                                                                645 kB/s | 625 kB     00:00    Fedora 30 - x86_64                                                                                                                           52 kB/s |  17 kB     00:00    RPM Fusion for Fedora 30 - Free - Updates                                                                                                   592  B/s | 3.0 kB     00:05    RPM Fusion for Fedora 30 - Free                                                                                                             9.8 kB/s | 3.2 kB     00:00    RPM Fusion for Fedora 30 - Nonfree - Updates                                                                                                4.7 kB/s | 3.0 kB     00:00    RPM Fusion for Fedora 30 - Nonfree                                                                                                          4.4 kB/s | 3.2 kB     00:00    Ignoring repositories: docker-ce-stableNo match for argument: docker-ceNo match for argument: docker-ce-cliNo match for argument: containerd.ioError: Unable to find a matchDoh. Seems like no “docker-ce-stable” repo. I don’t want to run nightly or test. What to do? Get docker from somewhere else? Or is there an alternative…I remember something about podman…I removed the Docker repo. sudo rm docker-ce.repo Onto podman.Install podmanpodman, short for “pod manager” I believe, is:  …a daemonless container engine for developing, managing, and running OCI Containers on your Linux System. Containers can either be run as root or in rootless mode. Simply put: alias docker=podman.Install.sudo dnf install podmanThis is the version I have:$ rpm -q podmanpodman-1.2.0-2.git3bd528e.fc30.x86_64Now, the question is can I use it just like I use docker?The first test: JekyllAs I mentioned I used to use Docker to create a preview of this blog using Jekyll.Below is the command I previously used to build a preview site. I’ve been using this same command for at least a couple years.export JEKYLL_VERSION=3.5docker run --rm \\  --volume=\"$PWD:/srv/jekyll\" \\  -p 127.0.0.1:4000:4000 \\  -it jekyll/jekyll:pages \\  jekyll serve That would server up my Jekyll based blog on port 4000, and use the local directory as a volume. Would this command simply work if I replaced Docker with podman?$ podman run --rm \\&gt;   --volume=\"$PWD:/srv/jekyll\" \\&gt;   -p 127.0.0.1:4000:4000 \\&gt;   -it jekyll/jekyll:pages \\&gt;   jekyll serve ruby 2.6.3p62 (2019-04-16 revision 67580) [x86_64-linux-musl]Configuration file: /srv/jekyll/_config.yml            Source: /srv/jekyll       Destination: /srv/jekyll/_site Incremental build: disabled. Enable with --incremental      Generating...      Build Warning: Layout 'nil' requested in atom.xml does not exist.                    done in 4.247 seconds. Auto-regeneration: enabled for '/srv/jekyll'    Server address: http://0.0.0.0:4000  Server running... press ctrl-c to stop.I was surprised when this worked without a problem.Same command to exec in.$ podman exec -it 895a3ca0845c /bin/bashbash-4.4# ps ax  PID TTY      STAT   TIME COMMAND    1 pts/0    Ss+    0:00 /bin/sh /usr/jekyll/bin/jekyll serve   15 pts/0    Sl+    0:20 ruby -r github-pages /usr/gem/bin/jekyll serve -H 0.0.0.0   52 pts/1    Ss     0:00 /bin/bash   58 pts/1    R+     0:00 ps axbash-4.4# How about good old hello-world?$ podman run hello-worldTrying to pull docker.io/library/hello-world...Getting image source signaturesCopying blob 1b930d010525 doneCopying config fce289e99e doneWriting manifest to image destinationStoring signaturesHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.    (amd64) 3. The Docker daemon created a new container from that image which runs the    executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it    to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/It “just works.”A few notesEven though podman is command for command the same as docker, there are some major differences, especially in philosophy.  podman uses runc  The Podman approach is simply to directly interact with the image registry, with the container and image storage, and with the Linux kernel through the runC container runtime process (not a daemon).$ runc -h | head -6NAME:   runc - Open Container Initiative runtimerunc is a command line client for running applications packaged according tothe Open Container Initiative (OCI) format and is a compliant implementation of theOpen Container Initiative specification.  You do not need to be root to run podman  As mentioned, same commands  When building Podman, the goal was to make sure that Docker users could easily adapt. So all the commands you are familiar with also exist with Podman. In fact, the claim is made that if you have existing scripts that run Docker you can create a docker alias for podman and all your scripts should work (alias docker=podman).  “Podman’s local repository is in /var/lib/containers instead of /var/lib/docker”  “Podman uses a traditional fork/exec model (vs. a client/server model) for running containers.”  You won’t be able to use docker-compose with podman, that could be an issue for some. There seems to be some work in making transitioning to podman easier  Surprise! podman can manage pods:  The ability for Podman to handle pod deployment is a clear differentiator to other container runtimes.  As a libpod maintainer, I am still realizing the advantages of having pods even in a localized runtime. There will most certainly be more development in Podman around pods as we learn how users exploit the use of them.  There is a introductory course on Katacoda, so you can try it out without even installingConclusionI like that podman is a command for command replacement for Docker. I also like the focus on security, and the fact that there is no docker server running. Not having docker compose could be problem for developers who have to install things like databases to get a development environment. I like that the community and RedHat have written quite a few blog posts about podman.Overall, podman, while it could maybe use a better name, is interesting because it moves the container ecosystem forward and provides some diversity. I have no problems with Docker, it’s a simple fact that I just wasn’t able to install a stable version on Fedora 30 (yet) and thus ended up exploring podman.I barely touched the tip of the iceberg with the podman ecosystem, in future posts I’ll take a more in-depth look. I’m sure there are some edge cases. :)"
    } ,
  
    {
      "title"    : "DevOps - You Need a Factory",
      "category" : "",
      "tags"     : "",
      "url"      : "/2019/05/18/devops-needs-a-factory.html",
      "date"     : "2019-05-18 00:00:00 -0400",
      "content"  : "A few months ago I listened Kelsey Hightower speak at a function in Toronto. During that talk he said something that has stuck with me, and that is that you need a factory.DevOps Produces ArtifactsThe definition and application of DevOps is oft-debated. However, the fact is that in as technologists we produce things–perhaps they are applications, ie. code, or systems that manage these applications (with automation now also code). Maybe the product is an IoS application or maybe it’s complex infrastructure–OpenStack, Kubernetes, large networks, what have you. Regardless, we create a product. Increasingly that product is generated or operated via the DevOps paradigm.DevOps has, at this point, a relatively long history and is well documented in terms of its influences: W. Edward Deming, Agile, Toyota Kata, and Lean Manufacturing to name a few. Many of these theories and processes are designed for factories; for the creation of physical products. If we are doing “the DevOps”, and we are automating in the spirit of its influences, such as lean manufacturing, then…where is the factory?CI/CD - The (Too) Easy AnswerThe easy answer to the factory question is the use of continuous integration and delivery: CI/CD. Many organizations participating in DevOps know that CI/CD is important, but I’m not sure they know why. Often CI/CD is simply “cargo culted” into an organization. (You can usually tell because of the paralysis around selecting a CI/CD system.)In my opinion, to truly participate in a DevOps model you need a factory, and your CI/CD pipeline is the floor of that factory. The CI/CD system takes input, resources, materials, and work from real live people and produces artifacts which can be evaluated in terms of quality. Like a factory, it can create overstock and has bottlenecks. Most importantly, it is the main system that can be improved over time as part of an ongoing, continuous process.Build a (Virtual) FactoryI have talked to many organizations that say they are following the DevOps paradigm. They use small teams, have removed walls between groups, and cough use Slack. They might even do some CI/CD…but do they really have a factory? If not, that is something that should be striven for. Without a factory full of people there’s nowhere to continuously improve."
    } ,
  
    {
      "title"    : "Install and Boot an Older Kernel in Ubuntu",
      "category" : "",
      "tags"     : "",
      "url"      : "/2019/05/17/install-and-boot-older-kernel-ubuntu.html",
      "date"     : "2019-05-17 00:00:00 -0400",
      "content"  : "Stangely it was hard to find good instructions on installing an older kernel and setting it to boot on Ubuntu 16.04. Here are some quick instructions.The instance is Ubuntu 16.04 (in Digital Ocean) with the following kernel:root@old-kernely:~# uname -aLinux old-kernely 4.4.0-148-generic #174-Ubuntu SMP Tue May 7 12:20:14 UTC 2019 x86_64 x86_64 x86_64 GNU/Linuxroot@old-kernely:~# Install an older kernel, 4.4.0-22-generic.root@old-kernely:~# apt install linux-image-4.4.0-22-genericReading package lists... DoneBuilding dependency tree       Reading state information... DoneThe following package was automatically installed and is no longer required:  grub-pc-binUse 'apt autoremove' to remove it.Suggested packages:  fdutils linux-doc-4.4.0 | linux-source-4.4.0 linux-tools linux-headers-4.4.0-22-genericThe following NEW packages will be installed:  linux-image-4.4.0-22-generic0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.Need to get 18.7 MB of archives.After this operation, 55.4 MB of additional disk space will be used.Get:1 http://mirrors.digitalocean.com/ubuntu xenial-updates/main amd64 linux-image-4.4.0-22-generic amd64 4.4.0-22.40 [18.7 MB]Fetched 18.7 MB in 0s (69.0 MB/s)                 Selecting previously unselected package linux-image-4.4.0-22-generic.(Reading database ... 54537 files and directories currently installed.)Preparing to unpack .../linux-image-4.4.0-22-generic_4.4.0-22.40_amd64.deb ...Done.Unpacking linux-image-4.4.0-22-generic (4.4.0-22.40) ...Setting up linux-image-4.4.0-22-generic (4.4.0-22.40) ...Running depmod.update-initramfs: deferring update (hook will be called later)Examining /etc/kernel/postinst.d.run-parts: executing /etc/kernel/postinst.d/apt-auto-removal 4.4.0-22-generic /boot/vmlinuz-4.4.0-22-genericrun-parts: executing /etc/kernel/postinst.d/initramfs-tools 4.4.0-22-generic /boot/vmlinuz-4.4.0-22-genericupdate-initramfs: Generating /boot/initrd.img-4.4.0-22-genericW: mdadm: /etc/mdadm/mdadm.conf defines no arrays.run-parts: executing /etc/kernel/postinst.d/unattended-upgrades 4.4.0-22-generic /boot/vmlinuz-4.4.0-22-genericrun-parts: executing /etc/kernel/postinst.d/update-notifier 4.4.0-22-generic /boot/vmlinuz-4.4.0-22-genericrun-parts: executing /etc/kernel/postinst.d/x-grub-legacy-ec2 4.4.0-22-generic /boot/vmlinuz-4.4.0-22-genericSearching for GRUB installation directory ... found: /boot/grubSearching for default file ... found: /boot/grub/defaultTesting for an existing GRUB menu.lst file ... found: /boot/grub/menu.lstSearching for splash image ... none found, skipping ...Found kernel: /boot/vmlinuz-4.4.0-148-genericFound kernel: /boot/vmlinuz-4.4.0-148-genericFound kernel: /boot/vmlinuz-4.4.0-22-genericUpdating /boot/grub/menu.lst ... donerun-parts: executing /etc/kernel/postinst.d/zz-update-grub 4.4.0-22-generic /boot/vmlinuz-4.4.0-22-genericGenerating grub configuration file ...Found linux image: /boot/vmlinuz-4.4.0-148-genericFound initrd image: /boot/initrd.img-4.4.0-148-genericFound linux image: /boot/vmlinuz-4.4.0-22-genericFound initrd image: /boot/initrd.img-4.4.0-22-genericdoneroot@old-kernely:~# OK, now that the new kernel is installed, how can it be made the default? One way would be to reboot and chose that kernel at the grub menu but that is not that easy in a cloud environment, and wouldn’t be a permanent change.Figuring out what to put in /etc/default/grub isn’t that easy. Let’s look at the submenus and menuentries (hat tip).root@old-kernely:~# awk '/menuentry/ &amp;&amp; /class/ {count++; print count-1\"****\"$0 }' /boot/grub/grub.cfg                                            0****menuentry 'Ubuntu' --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-simple-3bfdf15c-91ab-470e-a04a-6d95c9a1fbac' {1****\tmenuentry 'Ubuntu, with Linux 4.4.0-148-generic' --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.4.0-148-generic-advanced-3bfdf15c-91ab-470e-a04a-6d95c9a1fbac' {2****\tmenuentry 'Ubuntu, with Linux 4.4.0-148-generic (recovery mode)' --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.4.0-148-generic-recovery-3bfdf15c-91ab-470e-a04a-6d95c9a1fbac' {3****\tmenuentry 'Ubuntu, with Linux 4.4.0-22-generic' --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.4.0-22-generic-advanced-3bfdf15c-91ab-470e-a04a-6d95c9a1fbac' {4****\tmenuentry 'Ubuntu, with Linux 4.4.0-22-generic (recovery mode)' --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.4.0-22-generic-recovery-3bfdf15c-91ab-470e-a04a-6d95c9a1fbac' {Under the submenu at entry 3 is the older kernel. However, numbering restarts with the submenu. So to boot the 4.4.0-22 kernel the required entry is submenu 1 and menuentry 2.We add this to /etc/default/grub. Currently it’s set to 0.root@old-kernely:~# grep GRUB_DEFAULT /etc/default/grubGRUB_DEFAULT=0Set it to “1&gt;2”.root@old-kernely:~# grep GRUB_DEFAULT /etc/default/grubGRUB_DEFAULT=\"1&gt;2\"Update grub.root@old-kernely:~# update-grubGenerating grub configuration file ...Found linux image: /boot/vmlinuz-4.4.0-148-genericFound initrd image: /boot/initrd.img-4.4.0-148-genericFound linux image: /boot/vmlinuz-4.4.0-22-genericFound initrd image: /boot/initrd.img-4.4.0-22-genericdoneReboot.root@old-kernely:~# rebootWhen the node reboots it’s using the older kernel specified.root@old-kernely:~# uname -aLinux old-kernely 4.4.0-22-generic #40-Ubuntu SMP Thu May 12 22:03:46 UTC 2016 x86_64 x86_64 x86_64 GNU/LinuxPresumably there is a better way to do this. Let me know in the comments!"
    } ,
  
    {
      "title"    : "Install a Linux Rootkit to Test Security Systems With",
      "category" : "",
      "tags"     : "",
      "url"      : "/2019/05/17/install-a-rootkit-for-testing.html",
      "date"     : "2019-05-17 00:00:00 -0400",
      "content"  : "Let’s say you wnat to install a rootkit to test with. There could be various reasons, maybe you are testing some kind of specialized security system. Regardless of the reason, you need a rootkit to test out.The first place I ended up at is Awesome Rootkits, a big list of rootkits.The one that I picked from that list, somewhat randomly, was LilyOfTheValley. I have no good reason as to why I picked this particular one, other than it seemed to suggest it would work on several kernel versions, and was a small amount of code.WARNINGObviously this is a rootkit. Don’t install it on your local machine. Get a temporary, disposable virtual machine. Also, please review the code in the rootkit files. You’ve been warned!InstallThe kernel version in this example is 4.4.0-22 on Ubuntu 16.04.# uname -aLinux old-kernely 4.4.0-22-generic #40-Ubuntu SMP Thu May 12 22:03:46 UTC 2016 x86_64 x86_64 x86_64 GNU/LinuxClone the rootkit repo.# git clone https://github.com/En14c/LilyOfTheValley# cd LilyOfTheValleyInstall build-essential.# apt install build-essential -yGet headers too.# apt install linux-headers-4.4.0-22-genericMake.# makemake -C /lib/modules/4.4.0-22-generic/build M=/root/LilyOfTheValley modulesmake[1]: Entering directory '/usr/src/linux-headers-4.4.0-22-generic'  CC [M]  /root/LilyOfTheValley/lilyofthevalley.o  Building modules, stage 2.  MODPOST 1 modules  CC      /root/LilyOfTheValley/lilyofthevalley.mod.o  LD [M]  /root/LilyOfTheValley/lilyofthevalley.komake[1]: Leaving directory '/usr/src/linux-headers-4.4.0-22-generic'Now there is a kernel module, lilyofthevalley.ko.# ls -1client.clilyofthevalley.clilyofthevalley.kolilyofthevalley.mod.clilyofthevalley.mod.olilyofthevalley.oMakefilemodules.orderModule.symversREADME.mdInstall that module.# insmod lilyofthevalley.ko Once it’s loaded /proc can be used to find commands that can be sent to the module.# cat /proc/lilyofthevalleyr00tkit ###########################LilyOfTheValley Commands###########################\t* [givemerootprivileges] --&gt;&gt; to gain root access\t* [hidepidPID] --&gt;&gt; to hide a given pid. replace (PID) with target pid\t* [unhidepidPID] --&gt;&gt; to unhide a given pid. replace (PID) with target pid\t* [hidingfiles] --&gt;&gt; just prepend lilyofthevalley to the file or dir name that u want to hideAlso, note how any files in the directory that started with the string “lilyofthevalley” now no longer appear in ls.# ls -1client.cMakefilemodules.orderModule.symversREADME.mdThis is because the rootkit makes those files “invisble.”See line 409 of lilyofthevalley.c.\t/*\thide any file in  the root filesystem, \tif first chars of it's name == r00tkit_name\t*/\tif (strncmp(name,R00TKIT_NAME,R00TKIT_NAMELEN) == 0)\t\treturn 0;Create a file that starts with that particular string. Note how it’s not visible in ls.# touch lilyofthevalley.txt# lsclient.c  Makefile  modules.order  Module.symvers  README.mdAfter rebooting the node the kernel module is no longer loaded and the missing files are again visible..# ls -1client.clilyofthevalley.clilyofthevalley.kolilyofthevalley.mod.clilyofthevalley.mod.olilyofthevalley.olilyofthevalley.txtMakefilemodules.orderModule.symversREADME.mdWith the module in place security systems can be tested to see if they can find it."
    } ,
  
    {
      "title"    : "First Look - Kubeaudit",
      "category" : "",
      "tags"     : "",
      "url"      : "/2019/04/10/kubeaudit.html",
      "date"     : "2019-04-10 00:00:00 -0400",
      "content"  : "I wanted to call this article “Gleaming the Kube” but someone already did that.Kubernetes is a relatively complex system, and I typically deal with OpenStack so that is saying something. With any new and complex (and valuable) system it’s easy to use that system to build, shall we say, services with less than desirable security settings. And thus we have people building tools that help to ensure systems and definitions are as secure as is reasonably possible. Enter kubeaudit.KubeauditKubeaudit is a helpful tool from the folks at Shopify. A Canadian company by the way!  kubeaudit is a command line tool to audit Kubernetes clusters for various different security concerns: run the container as a non-root user, use a read only root filesystem, drop scary capabilities, don’t add new ones, don’t run privileged, … You get the gist of it and more on that later. Just know: kubeaudit makes sure you deploy secure containers!Because go compiles into a nice clean binary, it’s easy to install kubeaudit.wget -P /tmp https://github.com/Shopify/kubeaudit/releases/download/v0.5.3/kubeaudit_0.5.3_linux_amd64.tar.gzmkdir ~/bin # if you don't have oneexport PATH=$PATH:~/bin# only untar kubeaudit, there is a licence and readme file in there as well which we don't needtar zxvf /tmp/kubeaudit_0.5.3_linux_amd64.tar.gz -C ~/bin kubeauditCheck version.$ kubeaudit versionINFO[0000] Kubeaudit version                             BuildDate=\"2019-03-29T15:36:37Z\" Commit=b19f6509d92abc22a8cf2789a98a740af20831e6 Version=0.5.3INFO[0000] Not running inside cluster, using local config INFO[0000] Kubernetes server version                     Major=1 Minor=13 Platform=linux/amd64Note that this will check the Kubernetes version as well through reading the .kube/config.AuditLet’s deploy an nginx container.NOTE: I alias kubectl to k$ k run nginx --image=nginxinc/nginx-unprivileged --port=8080It’s up and running.$ k get allNAME                         READY   STATUS    RESTARTS   AGEpod/nginx-6c6d45d55d-ckhzw   1/1     Running   0          51sNAME                    READY   UP-TO-DATE   AVAILABLE   AGEdeployment.apps/nginx   1/1     1            1           51sNAME                               DESIRED   CURRENT   READY   AGEreplicaset.apps/nginx-6c6d45d55d   1         1         1       51sNow audit it with kubeaudit, but we’ll only check for nonroot.$ kubeaudit nonroot -n nginx-unprivINFO[0000] Not running inside cluster, using local config ERRO[0000] RunAsNonRoot is not set in ContainerSecurityContext, which results in root user being allowed!  Container=nginx KubeType=deployment Name=nginx Namespace=nginx-unprivERRO[0000] RunAsNonRoot is not set in ContainerSecurityContext, which results in root user being allowed!  Container=nginx KubeType=pod Name=nginx-6c6d45d55d-ckhzw Namespace=nginx-unprivOk let’s fix that by patching the deployment.Here is the patch:$ cat nginx-unpriv-runasnonroot.ymlspec:  template:    spec:      containers:      - image: nginxinc/nginx-unprivileged        name: nginx        securityContext:          runAsNonRoot: true      securityContext:        runAsNonRoot: trueApply the patch.$ k patch deployment nginx --patch \"$(cat nginx-unpriv-runasnonroot.yml)\"deployment.extensions/nginx patchedA new pod should be created.~$ k get allNAME                         READY   STATUS    RESTARTS   AGEpod/nginx-686cc9b75c-jfc8t   1/1     Running   0          24sNAME                    READY   UP-TO-DATE   AVAILABLE   AGEdeployment.apps/nginx   1/1     1            1           3m50sNAME                               DESIRED   CURRENT   READY   AGEreplicaset.apps/nginx-686cc9b75c   1         1         1       24sreplicaset.apps/nginx-6c6d45d55d   0         0         0       3m50sAnd now to run audit again:$ kubeaudit nonroot -n nginx-unprivINFO[0000] Not running inside cluster, using local configNo errors.That’s it. A simple example of using kubeaudit."
    } ,
  
    {
      "title"    : "Check Host Keys in Ansible Tower/AWX",
      "category" : "",
      "tags"     : "",
      "url"      : "/2019/02/01/check-host-keys-tower-awx.html",
      "date"     : "2019-02-01 00:00:00 -0500",
      "content"  : "By default AWX doesn’t validate host keys.Users of plain old Ansible will know this is an option that you can enable or disable–by default in Ansible it’s enabled. However, by default in AWX it is disabled, which means that AWX does not validate host keys. However, you can enable it. The way that is done is a bit clunky, but it can be done.In the AWX web interface, go to “Settings -&gt; Jobs”. There you will see a section called “EXTRA ENVIRONMENT VAIRABLES.” Add \"ANSIBLE_HOST_KEY_CHECKING\": \"true\" as one of the variables. In my deployment the full configuration looked like this (yours might be different):{ \"HOME\": \"/var/lib/awx\", \"ANSIBLE_HOST_KEY_CHECKING\": \"true\"}Then ensure you save your new settings. At this point AWX should now start validating SSH host keys.NOTE: I’m not aware of a way to do this outside of using the GUI, but maybe there is? If so, please let me know!Example Failed Job RunI have an inventory that has already imported some hosts and a job has run against them, and thus the initial host key has already been set in AWX.In one of the hosts in the inventory I’ll force a new host key.ubuntu@c03-01:/etc/ssh$ sudo rm /etc/ssh/ssh_host_*ubuntu@c03-01:/etc/ssh$ sudo dpkg-reconfigure openssh-serverCreating SSH2 RSA key; this may take some time ...2048 SHA256:4dFI8aTPISOOZssvPKG+jUnASrft0xTQdWOJhqQLnPo root@c03-01 (RSA)Creating SSH2 DSA key; this may take some time ...1024 SHA256:t3bYKZW2FFIUnDxD5uvqpPJbh3h2jjT80GLxttzRU6U root@c03-01 (DSA)Creating SSH2 ECDSA key; this may take some time ...256 SHA256:GaONanT4kLLUMJuTH4rghjXfrrahAY0aa98jEL8pXEA root@c03-01 (ECDSA)Creating SSH2 ED25519 key; this may take some time ...256 SHA256:qvw390bj+BEOd15NjcHLYtdbk9qzaN5rCE4Xaoi6teQ root@c03-01 (ED25519)ubuntu@c03-01:/etc/ssh$ Now that that is done, let’s copy the key from /etc/ssh/ssh_host_ecdsa_key.pub.ubuntu@c03-01:/etc/ssh$ cat ssh_host_ecdsa_key.pub ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBK/6oDS05/WR6Cn8camUDFW3OuyRS3ThiTg+8AzS4s68fkP4EUS86IPVjvNb7w180JxGCBA2Dkmi5QdSCPZsLdg= root@c03-01Later we will add that to the AWX task container in /root/.ssh/known_hosts.I have a job that constantly runs. It just ensures all new hosts have python 2.7, for $reasons…Because I’ve changed the host key and set AWX to check host keys, that job now fails for that host.(tower-cli) ubuntu@awx-client:~$ tc job stdout 22314Identity added: /tmp/awx_22314_cU7Zqd/credential_2 (/tmp/awx_22314_cU7Zqd/credential_2)PLAY [all] *********************************************************************TASK [ensure python 2.7 is installed] ******************************************ok: [c02-03]ok: [c02-01]ok: [c02-04]ok: [c03-02]ok: [c02-02]fatal: [c03-01]: UNREACHABLE! =&gt; {\"changed\": false, \"msg\": \"Failed to connect to the host via ssh: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\\\r\\\\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\\\\r\\\\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\\\r\\\\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\\\\r\\\\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\\\\r\\\\nIt is also possible that a host key has just been changed.\\\\r\\\\nThe fingerprint for the ECDSA key sent by the remote host is\\\\nSHA256:GaONanT4kLLUMJuTH4rghjXfrrahAY0aa98jEL8pXEA.\\\\r\\\\nPlease contact your system administrator.\\\\r\\\\nAdd correct host key in /root/.ssh/known_hosts to get rid of this message.\\\\r\\\\nOffending ECDSA key in /root/.ssh/known_hosts:13\\\\r\\\\nECDSA host key for 172.20.50.16 has changed and you have requested strict checking.\\\\r\\\\nHost key verification failed.\\\\r\\\\n\", \"unreachable\": true}ok: [c01-01]ok: [c01-02]ok: [c01-03]TASK [ping] ********************************************************************ok: [c02-04]ok: [c02-03]ok: [c02-02]ok: [c02-01]ok: [c03-02]ok: [c01-03]ok: [c01-01]ok: [c01-02]PLAY RECAP *********************************************************************c03-01                 : ok=0    changed=0    unreachable=1    failed=0   c03-02                 : ok=2    changed=0    unreachable=0    failed=0   c02-01                 : ok=2    changed=0    unreachable=0    failed=0   c02-02                 : ok=2    changed=0    unreachable=0    failed=0   c02-03                 : ok=2    changed=0    unreachable=0    failed=0   c02-04                 : ok=2    changed=0    unreachable=0    failed=0   c01-01                 : ok=2    changed=0    unreachable=0    failed=0   c01-02                 : ok=2    changed=0    unreachable=0    failed=0   c01-03                 : ok=2    changed=0    unreachable=0    failed=0   OK. (changed: false)Note the big “IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!” warning.Add New Key to AWX and Rerun JobTo set the new host key, I’ll login to the AWX host. In my deployment AWX is “containerized” which means AWX is actually made up of several different Docker based containers.root@awx:~# docker psCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                                                 NAMES1169cf7d8918        ssl-haproxy                  \"/bin/bash\"              4 weeks ago         Up 4 weeks          0.0.0.0:443-&gt;443/tcp                                  haproxyc4bfceeba785        ubuntu:xenial                \"/bin/bash\"              4 weeks ago         Up 4 weeks                                                                tower-cli75ac023a773b        ansible/awx_task:2.1.2       \"/tini -- /bin/sh -c…\"   4 weeks ago         Up 4 weeks          8052/tcp                                              awx_task_10773f80c42cc        ansible/awx_web:2.1.2        \"/tini -- /bin/sh -c…\"   4 weeks ago         Up 4 weeks          0.0.0.0:80-&gt;8052/tcp                                  awx_web_154d91fe6c00f        memcached:alpine             \"docker-entrypoint.s…\"   4 weeks ago         Up 4 weeks          11211/tcp                                             awx_memcached_11540a632c139        ansible/awx_rabbitmq:3.7.4   \"docker-entrypoint.s…\"   4 weeks ago         Up 4 weeks          4369/tcp, 5671-5672/tcp, 15671-15672/tcp, 25672/tcp   awx_rabbitmq_16b8e0dfd7aab        postgres:9.6                 \"docker-entrypoint.s…\"   4 weeks ago         Up 4 weeks          5432/tcp                                              awx_postgres_1root@awx:~# I’ll login to the awx_task host and edit the line for this particular host, adding the ecdsa key.root@awx:~# docker exec -it 75ac023a773b /bin/bash[root@awx awx]# vi ~/.ssh/known_hosts # Add the key to the correct entryThe next time the job runs the host key will not cause an error.(tower-cli) ubuntu@awx-client:~$ tc job stdout 22329Identity added: /tmp/awx_22329_zl5Pd7/credential_2 (/tmp/awx_22329_zl5Pd7/credential_2)PLAY [all] *********************************************************************TASK [ensure python 2.7 is installed] ******************************************ok: [c01-01]ok: [c02-04]ok: [c01-02]ok: [c01-03]ok: [c03-02]ok: [c02-01]ok: [c02-02]ok: [c03-01]ok: [c02-03]TASK [ping] ********************************************************************ok: [c02-04]ok: [c01-01]ok: [c01-03]ok: [c01-02]ok: [c03-02]ok: [c02-01]ok: [c02-03]ok: [c02-02]ok: [c03-01]PLAY RECAP *********************************************************************c03-01                 : ok=2    changed=0    unreachable=0    failed=0   c03-02                 : ok=2    changed=0    unreachable=0    failed=0   c02-01                 : ok=2    changed=0    unreachable=0    failed=0   c02-02                 : ok=2    changed=0    unreachable=0    failed=0   c02-03                 : ok=2    changed=0    unreachable=0    failed=0   c02-04                 : ok=2    changed=0    unreachable=0    failed=0   c01-01                 : ok=2    changed=0    unreachable=0    failed=0   c01-02                 : ok=2    changed=0    unreachable=0    failed=0   c01-03                 : ok=2    changed=0    unreachable=0    failed=0   OK. (changed: false)All good again. :)ConclusionAll that I did in this post was enable  host key checking in AWX and then do a simple verification that it was indeed checking host keys, and failing with unknown host keys."
    } ,
  
    {
      "title"    : "Ten Million Packets per Second with Moongen",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/12/31/ten-million-packets-per-second.html",
      "date"     : "2018-12-31 00:00:00 -0500",
      "content"  : "Network Performance TestingOver the last few years I have been doing a lot of work in the Network Function Virtualization (NFV) world, specifically Virtual Network Function (VNF) on-boarding. When on-boarding any application, it’s important to test it, and one part of testing is performance testing. But how do we effectively performance test network applications?We need to push performance to the breaking point. Many of these applications are designed to be extremely fast, thus it’s difficult to even push them to their limits without some kind of specialized traffic generator. Further, traffic generators themselves are just applications, and have the same issues getting high performance as the systems that are under test. Commercial traffic generators available but are typically esoteric, expensive, and inflexible.How Much of Our Network Do We Really Use?I’m going to make a statement that is perhaps controversial: we don’t really use much of our networks…they’re rarely pushed to their limits. In fact, I think we build many of our systems assuming that the network will hardly be used. Frankly I think it would be fair to apply this to much of what we do: compute, network, and storage. This is in part why virtualization is so successful. But, as usual, I digress…We should push our networks, and more specifically our network interfaces and the applications that create and manage packets, to the limit. Throughput is relatively easy, packets per second, however, is a bit harder–a good 10GB interface should be able to send about 10 million 64 byte packets per second (PPS). That’s what hardware can do, but what about software?The Linux Kernel and DPDKThe Linux Kernel, and presumably all other OS kernels, just can’t deal with that many packets. Here’s what CloudFlare has to say:  Unfortunately the speed of vanilla Linux kernel networking is not sufficient for more specialized workloads. For example, here at CloudFlare, we are constantly dealing with large packet floods. Vanilla Linux can do only about 1M pps. This is not enough in our environment, especially since the network cards are capable of handling a much higher throughput. Modern 10Gbps NIC’s can usually process at least 10M pps.This performance issue is well known and usually doesn’t come into play in most networks and applications. However, when we need a high level of performance we can get around the kernel with something like DPDK, where we pull most the networking into userland. While the Data Plane Development Kit (DPDK) is available to allow vastly increased performance, it still requires some complex programming…unless you use Moongen!MoongenMoongen is truly an amazing–and more importantly accessible–piece of software for generating extreme amounts of traffic.  MoonGen is a fully scriptable high-speed packet generator built on DPDK and LuaJIT. It can saturate a 10 Gbit/s connection with 64 byte packets on a single CPU core while executing user-provided Lua scripts for each packet. Multi-core support allows for even higher rates. It also features precise and accurate timestamping and rate control.Using Moongen one can access DPDK though simply Lua scripts. I was able to easily create a custom DNS UDP packet generator in less than a hundred lines of Lua. Using that code I could push about 6.5GB of DNS queries towards a DNS server to test its performance capability, all using an older x86 server with a common, and inexpensive 10GB Intel 82599ES network card.A quick example of running that code:# ./build/MoonGen ./examples/dns-query.lua -r 10000[INFO]  Initializing DPDK. This will take a few seconds...EAL: Detected 16 lcore(s)EAL: No free hugepages reported in hugepages-1048576kBEAL: Probing VFIO support...EAL: PCI device 0000:03:00.0 on NUMA socket 0EAL:   probe driver: 8086:1521 net_e1000_igbEAL: PCI device 0000:03:00.1 on NUMA socket 0EAL:   probe driver: 8086:1521 net_e1000_igbEAL: PCI device 0000:81:00.0 on NUMA socket 1EAL:   probe driver: 8086:10fb net_ixgbeEAL: PCI device 0000:81:00.1 on NUMA socket 1EAL:   probe driver: 8086:10fb net_ixgbe[INFO]  Found 3 usable devices:   Device 0: 0C:C4:7A:92:63:7D (Intel Corporation I350 Gigabit Network Connection)   Device 1: 0C:C4:7A:BB:70:82 (Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection)   Device 2: 0C:C4:7A:BB:70:83 (Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection)[WARN]  device.config() without named arguments is deprecated and will be removed. See documentation for libmoons device.config().PMD: ixgbe_dev_link_status_print(): Port 1: Link Up - speed 0 Mbps - half-duplex[INFO]  Waiting for devices to come up...[INFO]  Device 1 (0C:C4:7A:BB:70:82) is up: 10000 MBit/s[INFO]  1 device is up. starting arp starting sendstarting counterin sendin counter[INFO]  starting in moongen loop[nil] TX: 5.48 Mpps, 4562 Mbit/s (5439 Mbit/s with framing)[nil] TX: 6.56 Mpps, 5455 Mbit/s (6504 Mbit/s with framing)[nil] TX: 6.73 Mpps, 5601 Mbit/s (6679 Mbit/s with framing)[nil] TX: 6.74 Mpps, 5606 Mbit/s (6684 Mbit/s with framing)[nil] TX: 6.77 Mpps, 5636 Mbit/s (6719 Mbit/s with framing)[nil] TX: 6.77 Mpps, 5631 Mbit/s (6714 Mbit/s with framing)[nil] TX: 6.79 Mpps, 5645 Mbit/s (6731 Mbit/s with framing)[nil] TX: 6.77 Mpps, 5633 Mbit/s (6717 Mbit/s with framing)[nil] TX: 6.76 Mpps, 5624 Mbit/s (6706 Mbit/s with framing)[nil] TX: 6.79 Mpps, 5647 Mbit/s (6733 Mbit/s with framing)[nil] TX: 6.81 Mpps, 5664 Mbit/s (6753 Mbit/s with framing)SNIP!Pretty incredible how easy it is to write a little bit of Lua and generate (or process) such a huge number of packets! One server. One NIC. One CPU. 6Mpps of traffic.Use All Your Network (With Much Less CPU)What I found was that very few network applications, including the Linux kernel, can deal with that much traffic. In my simplistic testing it was easy to get the Linux kernel and network applications to drop packets by the millions, or get applications to just plain fall over and either crash or stop processing packets (looking at you named). Ultimately, dealing with this many packets is difficult, and a well known problem, but using things like DPDK and Moongen makes it much more accessible to solve.In situations that call for high performance using considerably fewer CPU resources, DPDK, possibly with Moongen, becomes an easy choice, and that goes double for performance testing with a relatively limited budget.Thanks Moongen (and CloudFlare too)Moongen is truly an amazing piece of software. Thanks to the relatively few people who put this project together, especially Mr. Paul Emmerich.I also want to give a shout out to the CloudFlare folks. I don’t know if they know how important their network performance blog posts are. I also used some of their example code. Thanks greatly!"
    } ,
  
    {
      "title"    : "Book Review - Leading the Unleadable",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/11/24/book-review-leading-the-unleadable.html",
      "date"     : "2018-11-24 00:00:00 -0500",
      "content"  : "tldr;Leading the Unleadable is really more about generic leadership principles than it is about dealing with difficult employees or other challenging interpersonal issues at work. But that does not diminish the value of the book overall, and I would suggest that the relatively short time it takes to read this book is well worth it.Technology is Easy; People are HardWe know some technology is hard (distributed systems I’m looking at you) but generally speaking as an industry I think we actually are pretty good at getting technology in place and working…eventually. Sure, usually it takes awhile and is more expensive than we’d like, but ultimately it gets done.However, if the DevOps paradigm has showed us anything, it’s that of the triumvirate of people, process, and tools, people are the most difficult. Then comes process (because it’s so related to people) and finally, technology.Leading the UnleadableLeading the Unleadable (LtU), by Alan Willett, is a management and leadership book about dealing with difficult people. Interestingly the preface of this book, ie. the very first page, contains the following sentence:  Sometimes leaders terminate difficult people too quickly, which harms the group by giving it no chance to change the difficult people and reclaim them.There is a lot to unpack in that sentence. Perhaps the most interesting part is the idea that leadership is often about helping a team integrate and deal with difficult people, as opposed to having a “leader” deal with them. Later on in the book (pg. 35) we get a supporting quote:  Exceptional leaders know that when encountering some behaviour or action that appears unacceptable their first thought should be to wonder what they don’t understand about the person and the communication process.The book suggests that more often than not difficult people can be understood and reclaimed or integrated by understanding their personal story; that overall people are good and should be approached as such. It is certainly a positive view on difficult people.Troublesome IndividualsThe book lays out a few archetypes of challenging people:  The Cynic  The Slacker  The Diva  The Pebble in the Shoe  The MaverickA couple of examples are given in terms of dealing with the above archetypes, specifically Divas and Mavericks. However, while the examples are good, I don’t feel that they in depth enough, nor do the archetypes cover enough possibilities, especially for a book that is supposed to deal with a specific subject: dealing with difficult people. Actually, not just “difficult” but “unleadable.”However, LtU does give an interesting model for determining whether or not to “remove or improve” an employee or team member (chpt 7). Leaders could adopt that model to help determine what to do with challenging employees.Leadership AdviceThe book does provide great leadership advice. For people who want to be leaders, it puts the following points in order of importance:  Provide a great return on investment to your employer  Improve yourself  Reduce your labour while dramatically improving the value you provideLtU also constantly reinforces the need for leadership to set the bar high in terms of what the organization can accomplish. Willett also discusses how exceptional leaders accept reality but don’t let it define them. He is definitely a proponent of “the need for mountains” and setting audacious goals. It would seem that Willett believes that many issues in the workplace are caused when people, teams, and even the organization itself, aren’t challenged.ConclusionI do recommend the book. It is a quick read and there is valuable information that can be gleaned from it, especially about general leadership.There are a couple of useful chapters on processes for dealing with difficult people, especially chapters 5 and 6. If the difficult people are really misunderstood people, then these ideas, tools, and processes should help.If there is any specific message that I took from the book, it’s that setting the bar high is important. Often people, teams, and organizations don’t give themselves enough credit in terms of what they can accomplish."
    } ,
  
    {
      "title"    : "Configure Bind to Respond with a Single IP to Any Query",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/11/18/configure-bind-authoritative-return-same-ip.html",
      "date"     : "2018-11-18 00:00:00 -0500",
      "content"  : "In this post I’ll lay out how to setup bind to be authoritative for a single domain, and to respond with a single IP address for any request for that domain.Why?I’m doing a bunch of DNS performance testing. I want to see how fast bind can respond to authoritative domains, but the requests could be for any hostname. Eg. If I request the IP for somerandomhost.example.com I want it to report the same IP as for someotherrandomhost.example.com.Configure BindRunning on Ubuntu 16.04, I first install bind9.Note that I’m using example.com. It might be better to use a internal domain in your case. At any rate, try to make sure that your testing doesn’t egress outside the boundaries of your lab.sudo apt updatesudo apt install bind9Next, configure an example.com zone file in /etc/bind.$TTL 86400@               IN      SOA     ns.yourdomain.com. hostmaster.yourdomain.com. (                                2008032701      ; Serial                                8H      ; Refresh                                2H      ; Retry                                1W      ; Expire                                1D)     ; Minimum                        NS      ns*                       A       127.0.0.1Note the “*” line that means respond to any request with 127.0.0.1.Add the below to named.conf.local to get bind to pickup the example.com domain.zone \"example.com\" in {           type master;           file \"/etc/bind/example.com\";};Add these options to named.conf.options. This will disable recursive queries. I’m only going to be testing authoritative requests and don’t want external requests at all.recursion no;additional-from-auth no;additional-from-cache no;Start/restart bind9.systemctl restart bind9systemctl status bind9Run a quick test to ensure recursion is not allowed.$ dig @localhost news.google.com; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Ubuntu &lt;&lt;&gt;&gt; @localhost news.google.com; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: REFUSED, id: 47477;; flags: qr rd; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1;; WARNING: recursion requested but not available;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;news.google.com.\t\tIN\tA;; Query time: 0 msec;; SERVER: 127.0.0.1#53(127.0.0.1);; WHEN: Sun Nov 18 11:09:29 UTC 2018;; MSG SIZE  rcvd: 44Note in the above “WARNING: recursion requested but not available”. That is what we want to see: no recursion.Once that has all been setup and bind9 restarted, we can do something like this:$ dig @localhost `cat /proc/sys/kernel/random/uuid`.example.com; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Ubuntu &lt;&lt;&gt;&gt; @localhost 69a65fd2-2223-485b-a6aa-156152db4318.example.com; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 3370;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 1, ADDITIONAL: 2;; WARNING: recursion requested but not available;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;69a65fd2-2223-485b-a6aa-156152db4318.example.com. IN A;; ANSWER SECTION:69a65fd2-2223-485b-a6aa-156152db4318.example.com. 86400\tIN A 127.0.0.1;; AUTHORITY SECTION:example.com.\t\t86400\tIN\tNS\tns.example.com.;; ADDITIONAL SECTION:ns.example.com.\t\t86400\tIN\tA\t127.0.0.1;; Query time: 0 msec;; SERVER: 127.0.0.1#53(127.0.0.1);; WHEN: Sun Nov 18 11:27:45 UTC 2018;; MSG SIZE  rcvd: 126bind responds that the IP for the host is at 127.0.0.1.Let’s run it once more, with +short.$ dig +short @localhost `cat /proc/sys/kernel/random/uuid`.example.com127.0.0.1Note how I’m using /proc/sys/kernel/random/uuid to generate a new…er random uuid. Neat huh.$ cat /proc/sys/kernel/random/uuidf6759215-323d-438b-b4da-535a8aabc63fIf you ever need a uuid, that is an easy way to get one without having to install any other software.ConclusionIf, for some reason, you want to configure bind to be  authoritative only (not resolve)  respond to any request for a single domain to be a single IPthen at this point you should be happy!"
    } ,
  
    {
      "title"    : "OpenStack Zun and Kata Containers",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/11/14/openstack-zun-and-kata-containers.html",
      "date"     : "2018-11-14 00:00:00 -0500",
      "content"  : "(diagram of kata containers)In this post I’ll explore using DevStack to deploy OpenStack Zun. After that, I’ll setup Zun to use Kata Containers as the container runtime.OpenStack ZunSo what is OpenStack Zun? The wiki page for Zun actually does a pretty good job of explaining it:  Zun is for users who want to create and manage containers as OpenStack-managed resource. Containers managed by Zun are supposed to be integrated well with other OpenStack resources, such as Neutron network and Cinder volume. Users are provided a simplified APIs to manage containers without the need to explore the complexities of different container technologies. Magnum is for users who want a self-service solution to provision and manage a Kubernetes (or other COEs) cluster.To unpack that paragraph, Zun provides an API that lets you manage containers. It is not Kubernetes, or Docker, or LXD, or any other Container Orchestration Engine (COE). The Zun API is its own API. It uses many pieces from OpenStack and other container related systems, but it’s its own API.I should say here that if one does want a COE, then the OpenStack Magnum project can provide that. What Zun and Magnum do are quite different.Kata Containers  Kata Containers is an open source project and community working to build a standard implementation of lightweight Virtual Machines (VMs) that feel and perform like containers, but provide the workload isolation and security advantages of VMs. – Kata Containers:Basically, Kata containers are virtual machines which act as much like containers as possible.(I don’t want to discuss what containers are in this blog post, as it would take up too much room, but it’s worth doing some reading about cgroups, namespaces, etc, and what exactly makes up containers.)DevStack + Zun + Kata ContainersThe best way to quickly try out Zun and Kata Containers is to use DevStack to deploy a small OpenStack deployment with the Zun plugin.  I’m mostly following this document with a few small changes (overall the linked document does work).First, we need a DevStack with Zun.DevStack + Zun will be deployed onto Ubuntu 16.04. Note that the stable/rocky release of both DevStack and Zun is used, more recent versions should work as well.NOTE: I am disabling Horizon because I kept running into a permissions bug that stops DevStack from completing. By the time you read this perhaps that bug is fixed in DevStack.apt updateapt install git screen -yscreen -R install sudo useradd -s /bin/bash -d /opt/stack -m stackecho \"stack ALL=(ALL) NOPASSWD: ALL\" | sudo tee /etc/sudoers.d/stacksu - stacksudo mkdir -p /opt/stacksudo chown $USER /opt/stackgit clone https://github.com/openstack-dev/devstack /opt/stack/devstackcd /opt/stack/devstack git checkout stable/rockyHOST_IP=\"$(ip addr | grep 'state UP' -A2 | tail -n1 | awk '{print $2}' | cut -f1 -d'/')\"git clone https://github.com/openstack/zun /opt/stack/zuncd /opt/stack/zungit checkout stable/rockycd /opt/stack/devstackcat /opt/stack/zun/devstack/local.conf.sample \\    | sed \"s/HOST_IP=.*/HOST_IP=$HOST_IP/\" \\    &gt; /opt/stack/devstack/local.confsed -i \"s/KURYR_CAPABILITY_SCOPE=.*/KURYR_CAPABILITY_SCOPE=local/\" /opt/stack/devstack/local.confecho \"ENABLE_CLEAR_CONTAINER=true\" &gt;&gt; /opt/stack/devstack/local.conf# bug in horizon deployment...echo \"disable_service horizon\" &gt;&gt;  /opt/stack/devstack/local.confFinally, run the stack.sh script to install DevStack../stack.shNOTE: stack.sh can take an hour to complete, depending on network connectivity.Once stack.sh completes, we should be able to access OpenStack and Zun.source /opt/stack/devstack/openrc admin admin# Make a couple of aliases for less typingalias os=openstack alias ac=\"openstack appcontainer\"os token issueac listHopefully os token issue returns a token, and ac list runs and shows no containers built yet. If not, then DevStack did not deploy properly.Setting up Zun and Kata Containers Post DevStack DeployDevStack has deployed OpenStack and Zun, but we still need to setup Zun to use Kata Containers.To install Kata Containers:sudo sh -c \"echo 'deb http://download.opensuse.org/repositories/home:/katacontainers:/release/xUbuntu_$(lsb_release -rs)/ /' &gt;&gt; /etc/apt/sources.list.d/kata-containers.list\"curl -sL  https://download.opensuse.org/repositories/home:/katacontainers:/release/xUbuntu_$(lsb_release -rs)/Release.key | sudo apt-key add -sudo -E apt-get updatesudo -E apt-get -y install kata-runtime kata-proxy kata-shimNext, we configure Docker to use the kata-runtime.sudo sed -i 's/\"cor\"/\"kata-runtime\"/' /etc/docker/daemon.jsonsudo sed -i 's/\"\\/usr\\/bin\\/cc-oci-runtime\"/\"\\/usr\\/bin\\/kata-runtime\"/' /etc/docker/daemon.jsonsudo systemctl daemon-reloadsudo systemctl restart dockerAt this point Zun should be able to use the kata-runtime.NOTE: We could also setup Zun to use kata-runtime as the default. If you’d like to do that, you can add the below line into the default section of /etc/zun/zun.conf.container_runtime = kata-runtimeThen restart Zun.for i in `systemctl list-unit-files | grep devstack@zun | cut -f 1 -d \" \"`; do sudo systemctl restart $i; doneIf that line is not added, then the default container runtime is runc, but you can request the kata-runtime with an option, --runtime kata-runtime.openstack appcontainer run --name kata --runtime kata-runtime nginx:latestWith the above command the container will use the kata-runtime. If the default run time is kata-container, then that option is not needed.Using Zun and Kata ContainersNow that OpenStack with Zun has been deployed, and Kata Containers installed and Zun/Docker configured to use the kata-runtime, we can finally boot a Kata Container.First, lets create a Kata based container.openstack appcontainer run --name kata nginx:latestAfter that command completes, a container should boot up.# openstack appcontainer list+--------------------------------------+------+--------------+---------+------------+--------------------------+-------+| uuid                                 | name | image        | status  | task_state | addresses                | ports |+--------------------------------------+------+--------------+---------+------------+--------------------------+-------+| c9f8e734-d616-404d-b3e1-425a0882e45b | kata | nginx:latest | Running | None       | 172.24.4.10, 2001:db8::7 | [80]  |+--------------------------------------+------+--------------+---------+------------+--------------------------+-------+Let’s introspect the container and see what runtime it is using.# openstack appcontainer show kata | grep runtime| runtime           | kata-runtimeAs can be seen above, it’s using the kata-runtime.Now lets create a runc based container.# openstack appcontainer run --name runc --runtime runc nginx:latestWhat runtime is it using?# openstack appcontainer show runc | grep runtime| runtime           | runc   So it’s using runc.Now let’s introspect each containers kernel.Here’s the physical hosts kernel version.# uname -aLinux kata6 4.4.0-134-generic #160-Ubuntu SMP Wed Aug 15 14:58:00 UTC 2018 x86_64 x86_64 x86_64 GNU/LinuxNow here’s the kernel version of the runc container.# openstack appcontainer exec runc uname -aLinux 8697605d373b 4.4.0-134-generic #160-Ubuntu SMP Wed Aug 15 14:58:00 UTC 2018 x86_64 GNU/LinuxNote that the host kernel and the runc container kernel are the same.Let’s see what kernel is in the kata-runtime based container.# openstack appcontainer exec kata uname -aLinux a41bb53476f6 4.14.67-139.container #1 SMP Mon Oct 22 22:43:15 UTC 2018 x86_64 GNU/LinuxAha!, that is 4.14.67-139.container not 4.4.0-134-generic. So that container is actually a virtual machine with its own kernel.We can also use the kata-runtime command to list the containers it is supporting.# kata-runtime listID                                                                 PID         STATUS      BUNDLE                                                                                                                 CREATED                          OWNERa41bb53476f64576bcfd2db999f8245c794ea9fce0784594a2239af1c972aaf3   10543       running     /run/containerd/io.containerd.runtime.v1.linux/moby/a41bb53476f64576bcfd2db999f8245c794ea9fce0784594a2239af1c972aaf3   2018-11-14T14:06:12.414823873Z   #0Also, if we do a ps ax | grep qemu we can see the container is actually running via a custom qemu process.# ps ax | grep qemu10526 ?        Sl     0:02 /usr/bin/qemu-lite-system-x86_64 -name sandbox-a41bb53476f64576bcfd2db999f8245c794ea9fce0784594a2239af1c972aaf3 SNIP!Note that it is qemu-lite-system-x86_64.ConclusionIf you like virtual machines, for whatever reason, perhaps security (all though that is tough to quantify), but you want to use them like containers, then Kata Containers is perfect for you. Also, if you are not interested in using COEs like Docker or Kubernetes, then Zun presents another possibility for obtaining a container.Ultimately deploying OpenStack and Zun and configuring Zun to use Kata Containers will not be the most common way to use containers (Kubernetes has certainly won that battle). However, it is an option, and an option that some might choose to utilize. Diversity is good. :)"
    } ,
  
    {
      "title"    : "Inverting, Reversing, or Mirroring a Binary Tree",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/10/10/reversing-mirroring-inverting-binary-tree.html",
      "date"     : "2018-10-10 00:00:00 -0400",
      "content"  : "Interviews based on algorithms and data structures will continue to be the norm. Here I take a look at inverting a binary tree.tldrReversing/mirroring/inverting, whatever you call it, has a nice recursive answer.def recurseInvertTree(root):    if root is not None:        root.left, root.right = invertTree(root.right), \\                                invertTree(root.left)    return root  That’s it. :)HistoryThe way organizations hire developers is varied. However, for better or worse, I feel that whiteboarding interviews that involve solving complex algorithm based problems is the default model, and will continue to be the default model. Some people like it, some don’t, but regardless it’s the defacto standard, and it’s unavoidable.One of the more famous kurfuffles around algorithmic whiteboarding interviews came out of this tweet:Google: 90% of our engineers use the software you wrote (Homebrew), but you can’t invert a binary tree on a whiteboard so fuck off.&mdash; Max Howell (@mxcl) June 10, 2015The developer of a popular and useful open source project, Homebrew, did not pass a whiteboarding interview in which he was apparently asked to invert a binary tree on a whiteboard.Despite the drama around this particular tweet, being able to perform well in a whiteboarding interview could mean the difference between getting an Okay job versus getting a great job, so it’s a skill worth having. Over a career it could mean hundreds of thousands of dollars in salary.So how can we invert/reverse/mirror a binary tree?Naming ThingsThe first problem I ran into when researching this was what to call it. Inverting seems like the most common name, but reversing or mirroring would work too.For the purposes of this blog post, I’ll use inverting. I think an example is easiest to explain.Original tree. (Sorry about printing it like this, I actually prefer it as you don’t run out of horizontal space as easily.)            14        6            13    2            12        5            110            10        4            9    1            8        3            7Inverted tree.            7        3            8    1            9        4            100            11        5            12    2            13        6            14It’s the same tree but the leafs have been flipped/reversed what have you.Aside: it’s kind of amazing how many names there could be for this. Inverted. Reversed. Flipped. Mirrored. Transposed. Part of answering an interview question like this would be getting some terms defined.RecursionI believe the reason that this question was originally asked is because it has such a nice recursive answer. I can imagine interviewers asking the question, then watching a developer muddle through it for X minutes, getting an answer, and then being shown that there is a ~4 line solution using recursion.def recurseInvertTree(root):    if root is not None:        root.left, root.right = invertTree(root.right), \\                                invertTree(root.left)    return root  I mean, ultimately, that is a pretty easy algorithm to whiteboard.But there are other ways to solve this as well.Depth First Search Stack solutionThis doesn’t seem too bad either.def stackInvertTree(root):    if root is not None:        nodes = []        nodes.append(root)        while nodes:            node = nodes.pop()            node.left, node.right = node.right, node.left            if node.left is not None:                nodes.append(node.left)            if node.right is not None:                nodes.append(node.right)    return rootI’m guessing that is what a lot of people would come up with if the recursive version wasn’t obvious to them.Breadth First Search with QueueWe can use a queue as well.def queueInvertTree(root):    queue = collections.deque([(root)])    while queue:        node = queue.popleft()        if node:            node.left, node.right = node.right, node.left            queue.append(node.left)            queue.append(node.right)    return rootThat’s nice too.ConclusionThese are the most common solutions I can find. There is a lot of computer science in this question, and I’m only touching the layperson’s surface here. Much more to discover.If you see any issues with what I’ve put up here, likely around naming, do let me know."
    } ,
  
    {
      "title"    : "How to Study for the Certified Kubernetes Administrator (CKA) Exam",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/10/06/how-to-study-for-the-certified-kubernetes-administrator.html",
      "date"     : "2018-10-06 00:00:00 -0400",
      "content"  : "Last week I successfully passed the CKA exam, and I wanted to give some pointers on how to best study for it.Obviously I can’t discuss any of the exam content. You’ll need to know a lot more than what I’ve listed here in terms of deploying and managing Kubernetes. I’ve only covered some basics–though I think the information on kubectl generators will really help. Kubernetes the hard way is also useful.The CKA ExamHere are a few basic skills you will need to pass the exam. Well, even to take the exam.  Exam time management  Linux command line  Command line text editor  Generating valid YAML with kubectl  Using and searching the kubernetes.io/docs pages  Editing YAML liveAgain, there is quite a bit more you will need to know, especially about deploying and managing Kubernetes, but these are the basics. Checkout the curriculum for more ideas on what to study.Exam Time ManagementIt’s a three hour exam and I expect that most people will use the entire time. Questions have differing levels of difficulty and value. You are allowed to open a note pad in the exam browser application, so it’s a good idea to take notes on what questions have what value and which one’s you have completed. Definitely figure out a strategy for picking which questions to answer first, and last.Linux Command LineYou need to be comfortable with the Linux command line. The basics would be Ok. If you can change directories, open files with vim, and run kubectl you will probably be fine.Command Line Text EditorThis is a big one. If you are not capable of opening and closing vim (or nano), typing into it, and search and replace, the exam will likely not be possible to pass. I would suggest being very comfortable with vim. Search and replace is a good skill to have: :s/new/old/g. :x is the same as :wq. :)Generating YAML with kubectlIt’s easy to create a deployment from the command line and then export the YAML. Actually there are several generators. Using kubectl run with different options can create: pods, deployments, replication controllers, jobs, and cron jobs.First create a deployment with the run command. Without any options a deployment is created.master $ kubectl run --image nginx nginxdeployment.apps/nginx createdNow we can export the YAML that kubectl generated behind the scenes.Note the --export option.master $ kubectl get deploy nginx -o yaml --exportapiVersion: extensions/v1beta1kind: Deploymentmetadata:  annotations:    deployment.kubernetes.io/revision: \"1\"  creationTimestamp: null  generation: 1  labels:    run: nginx  name: nginx  selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/nginxspec:  progressDeadlineSeconds: 600  replicas: 1  revisionHistoryLimit: 2  selector:    matchLabels:      run: nginx  strategy:    rollingUpdate:      maxSurge: 25%      maxUnavailable: 25%    type: RollingUpdate  template:    metadata:      creationTimestamp: null      labels:        run: nginx    spec:      containers:      - image: nginx        imagePullPolicy: Always        name: nginx        resources: {}        terminationMessagePath: /dev/termination-log        terminationMessagePolicy: File      dnsPolicy: ClusterFirst      restartPolicy: Always      schedulerName: default-scheduler      securityContext: {}      terminationGracePeriodSeconds: 30status: {}The best thing to do is to export that to a file.kubectl get deploy nginx -o yaml --export &gt; nginx-deploy.ymlThen delete that deploy if you don’t need it.kubectl delete deploy nginxNow we have an example deployment that can be used to create other deployments…a base file effectively.To generate a YAML for a pod, use --restart=Nevermaster $ kubectl run nginx --image=nginx --port=80 --restart=Neverpod/nginx createdmaster $ kubectl get pod nginx -o yaml --exportapiVersion: v1kind: Podmetadata:  creationTimestamp: null  labels:    run: nginxSNIP!Remember that other options can create jobs, cron jobs, recplication controllers, etc.To practice generating YAML you don’t need much more than a Katacoda playground.kubernetes.io/docsThis is a very useful page. Get used to being able to search it and find the documents for key resources.Editing YAML LiveIn some cases it might be easiest to edit the YAML live.kubectl edit deploy nginxThis will open up the YAML for the nginx deploy in $EDITOR, ie. vim. When you close it, the new YAML will be applied (or if the YAML has errors then it will error and reopen in vim, like visudo if you’ve ever used it).ConclusionThe CKA exam is a tough one, but mostly due to time limitations and the fact that you need to generate a lot of YAML quickly, without cutting and pasting. I think the CKA is a good certification to get at this point in time, and I’m glad that I only had to write it once."
    } ,
  
    {
      "title"    : "Add a User to Kubernetes",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/09/27/add-user-kubernetes.html",
      "date"     : "2018-09-27 00:00:00 -0400",
      "content"  : "Ok, so the title is a bit misleading. k8s doesn’t manage users, and instead expects them to be managed externally.  Normal users are assumed to be managed by an outside, independent service. An admin distributing private keys, a user store like Keystone or Google Accounts, even a file with a list of usernames and passwords. In this regard, Kubernetes does not have objects which represent normal user accounts. Normal users cannot be added to a cluster through an API call.Basically if a user can authenticate, it’s a user. That’s how I look at it anyways. Note that service accounts are a different story.So, in this example, I’m assuming you’ve deployed using kubeadm and aren’t managing users externally. kubeadm has a nice command to generate a kubeconfig for a “new user.” It has to read files /etc/kubernetes/pki.sudo kubeadm alpha phase kubeconfig user --client-name=\"curtis\" &gt; ~/curtis.kubeconfigNow that I’ve created a kubeconfig for the user “curtis” I can use it to access the k8s API.Or, well I could if RBAC was setup for the user. But at least I’m authenticating, I’m just not going to be authorized (if RBAC is enabled).You can also create the certs using openssl and bitnami has some good docs on doing that, so I won’t copy them here."
    } ,
  
    {
      "title"    : "Local Persistent Volumes with Kubernetes",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/09/19/persistent-local-volumes-kubernetes.html",
      "date"     : "2018-09-19 00:00:00 -0400",
      "content"  : "Let’s quickly discuss using persistent local volumes with Kubernetes.First, get yourself a k8s. I have one here running on a packet.net instance. It’s only a single node. Deployed with kubeadm and is running calico as the network plugin.# k get nodesNAME      STATUS    ROLES     AGE       VERSIONcka       Ready     master    6h        v1.11.1Let’s look at everything that’s running.# k get allNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGEservice/kubernetes   ClusterIP   172.17.0.1   &lt;none&gt;        443/TCP   6h# k get all -n kube-systemNAME                                           READY     STATUS    RESTARTS   AGEpod/calico-etcd-2hcdc                          1/1       Running   0          6hpod/calico-kube-controllers-74b888b647-qr86d   1/1       Running   0          6hpod/calico-node-5jmrc                          2/2       Running   17         6hpod/coredns-78fcdf6894-4ngmq                   1/1       Running   0          6hpod/coredns-78fcdf6894-gzqcw                   1/1       Running   0          6hpod/etcd-cka                                   1/1       Running   0          6hpod/kube-apiserver-cka                         1/1       Running   0          6hpod/kube-controller-manager-cka                1/1       Running   0          6hpod/kube-proxy-62hp2                           1/1       Running   0          6hpod/kube-scheduler-cka                         1/1       Running   0          6hNAME                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGEservice/calico-etcd   ClusterIP   172.17.0.136   &lt;none&gt;        6666/TCP        5hservice/kube-dns      ClusterIP   172.17.0.10    &lt;none&gt;        53/UDP,53/TCP   6hNAME                         DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR                     AGEdaemonset.apps/calico-etcd   1         1         1         1            1           node-role.kubernetes.io/master=   6hdaemonset.apps/calico-node   1         1         1         1            1           &lt;none&gt;                            6hdaemonset.apps/kube-proxy    1         1         1         1            1           beta.kubernetes.io/arch=amd64     6hNAME                                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEdeployment.apps/calico-kube-controllers    1         1         1            1           6hdeployment.apps/calico-policy-controller   0         0         0            0           6hdeployment.apps/coredns                    2         2         2            2           6hNAME                                                  DESIRED   CURRENT   READY     AGEreplicaset.apps/calico-kube-controllers-74b888b647    1         1         1         6hreplicaset.apps/calico-policy-controller-55b469c8fd   0         0         0         6hThe c2.medium.x86 (AMD EPYC!) has two extra SSDs.mkfs.ext4 /dev/sdcmkfs.ext4 /dev/sddmkdir -p /mnt/disks/sdcmkdir -p /mnt/disks/sddmount /dev/sdc /mnt/disks/sdcmount /dev/sdd /mnt/disks/sddNow both are formated and mounted.# lsblkNAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTsda      8:0    0 111.8G  0 disksdb      8:16   0 111.8G  0 disk├─sdb1   8:17   0   512M  0 part /boot/efi├─sdb2   8:18   0   1.9G  0 part└─sdb3   8:19   0 109.4G  0 part /sdc      8:32   0 447.1G  0 disk /mnt/disks/sdcsdd      8:48   0 447.1G  0 disk /mnt/disks/sddI usually abbreviate kubectl to k.alias k=kubectlSetup a storage class.cat &gt; storage-class.yml &lt;&lt;EOFkind: StorageClassapiVersion: storage.k8s.io/v1metadata:  name: local-storageprovisioner: kubernetes.io/no-provisionervolumeBindingMode: WaitForFirstConsumerEOFkubectl create -f storage-class.ymlList the available storage classes.# k get scNAME            PROVISIONER                    AGElocal-storage   kubernetes.io/no-provisioner   4sCreate a persistent volume.cat &gt; pv-sdc.yml &lt;&lt;EOFapiVersion: v1kind: PersistentVolumemetadata:  name: local-pv-sdcspec:  capacity:    storage: 440Gi  accessModes:  - ReadWriteOnce  persistentVolumeReclaimPolicy: Retain  storageClassName: local-storage  local:    path: /mnt/disks/sdc  nodeAffinity:    required:      nodeSelectorTerms:      - matchExpressions:        - key: kubernetes.io/hostname          operator: In          values:          - ckaEOFkubectl create -f pv-sdc.ymlSame for sdd.cat &gt; pv-sdd.yml &lt;&lt;EOFapiVersion: v1kind: PersistentVolumemetadata:  name: local-pv-sddspec:  capacity:    storage: 440Gi  accessModes:  - ReadWriteOnce  persistentVolumeReclaimPolicy: Retain  storageClassName: local-storage  local:    path: /mnt/disks/sdd  nodeAffinity:    required:      nodeSelectorTerms:      - matchExpressions:        - key: kubernetes.io/hostname          operator: In          values:          - ckaEOFkubectl create -f pv-sdd.ymlNow I have two PVs.# k get pvNAME           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM     STORAGECLASS    REASON    AGElocal-pv-sdc   440Gi      RWO            Retain           Available             local-storage             53slocal-pv-sdd   440Gi      RWO            Retain           Available             local-storage             4sCreate a persistent volume claim.cat &gt; pvc1.yml &lt;&lt;EOFkind: PersistentVolumeClaimapiVersion: v1metadata:  name: pvc1spec:  accessModes:  - ReadWriteOnce  storageClassName: local-storage  resources:    requests:      storage: 100GiEOFkubectl create -f pvc1.ymlPVC will be pending until we create a node that uses it.# k get pvcNAME      STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS    AGEpvc1      Pending                                       local-storage   3sNow a deployment that uses the pvc.cat &gt; deploy-nginx.yml &lt;&lt;EOFapiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deployment  labels:    app: nginxspec:  replicas: 1  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx:1.7.9        ports:        - containerPort: 80        volumeMounts:        - mountPath: \"/usr/share/nginx/html\"          name: storage      volumes:        - name: storage          persistentVolumeClaim:            claimName: pvc1EOFkubectl create -f deploy-nginx.ymlpvc is now bound.# k get pvcNAME      STATUS    VOLUME         CAPACITY   ACCESS MODES   STORAGECLASS    AGEpvc1      Bound     local-pv-sdc   440Gi      RWO            local-storage   21mExpose nginx.# kubectl expose deployment nginx-deployment --type=NodePortservice/nginx-deployment exposed# kubectl get svc nginx-deploymentNAME               TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGEnginx-deployment   NodePort   172.17.0.157   &lt;none&gt;        80:31813/TCP   7mNow create an index.html page. I’ll use a uuid just to show it’s not already setup.# uuid=`uuidgen`# echo $uuidb5d3c8cd-a56f-4252-a026-7107790fcd44# echo $uuid &gt; /mnt/disks/sdc/index.html# curl 172.17.0.157b5d3c8cd-a56f-4252-a026-7107790fcd44Let’s delete the pod.# k delete pod nginx-deployment-7d869874bc-dlz4spod \"nginx-deployment-7d869874bc-dlz4s\" deletedA new pod will automatically be created by Kubernetes.# k get podsNAME                                READY     STATUS    RESTARTS   AGEnginx-deployment-7d869874bc-fkt4m   1/1       Running   0          28sRun the same curl.# curl 172.17.0.157b5d3c8cd-a56f-4252-a026-7107790fcd44Same UUID.Lets check from the pod.# k exec -it nginx-deployment-7d869874bc-fkt4m -- cat /usr/share/nginx/html/index.htmlb5d3c8cd-a56f-4252-a026-7107790fcd44As expected.ConclusionKubernetes.io has a blog post announcing local persistent volumes. It’s worth a read. Anything I say here of value would just be copying from that. :) There are several reasons not to use this model for storage. Seems like it would be best for data heavy applications…data gravity. There is also a simple volume manager available as well, but I didn’t explore that in this blog post."
    } ,
  
    {
      "title"    : "Vent - Kubernetes the Hard Way with Ansible and Packet.net",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/09/03/vent-kubernetes-the-hard-way.html",
      "date"     : "2018-09-03 00:00:00 -0400",
      "content"  : "In preparation to write the Certificate Kubernetes Administrator exam I’m going through the ever popular Kubernetes the Hard way (kthw), as developed by Mr. Kelsey Hightower. I actually did the same thing a couple of years ago, in which I translated kthw from a step by step command line depoyment into some Ansible that deployed kthw to Amazon Web Services.In this blog post I discuss a repo I created that does something similar–it still converts the shell commands into Ansible, but instead of AWS it deploys Kubernetes to bare metal nodes on Packet.net.Packet.netI’m a big fan of packet.net. I like four major things about packet.net:  Bare Metal resources  Layer 3 networking  Simplicity  Spot pricingBaremetalObviously the main point of packet.net is that instead of getting virtual machines, you get bare metal nodes. I really like getting bare metal nodes. You don’t have to worry about sharing resources with other tenants. You get the full speed of bare metal. They can be pricey over time, but if I was an organization seeking speed, for example in CI/CD, it would be well worth it to utilize packet to push things as fast as they can go.Specifically for Vent I focussed on the AMD EPYC nodes which are under plan c2.medium.x86. With 64GB of RAM, 20GB of networking, 24 cores, and about 1TB of SSD, it’s perfect for Kubernetes.Other cloud providers also have bare metal nodes, but I haven’t used them. I think AWS provides bare metal, and they have the Nitro hypervisor, which is effectively bare metal. But that is a discussion for another day. :)NetworkingI like how Packet.net has done their networking–it’s all layer 3 based. As a public cloud, this is the right way to go. People often think networking means a shared layer 2 with a /24 netmask, like what you get with most clouds virtual private networking.Packet.net’s layer 3 model means that each node gets three IPs: a public IPv4, an Ipv6, and an internal, private RFC 1918 address (all on the same bond0 interface). The private address is in the 10.0.0.0/8 range, but it’s usually a /31, which means it just has a couple usable addresses–the hosts IP and the gateway. A route is setup to send all 10.0.0.0/8 addresses through that gateway, and packet must be doing filtering to allow “private networking” in the sense that other nodes in the same project are put into a filtering group (ie. network ACL), so that they can only talk to each other on the private network. So if you have three nodes, they each have their own /31 private IP space, but are part of an ACL group (presumably).This is a very scalable model for packet.net, and with an underlying layer 3 private network as a tenant you can set up an overlay network if you like, vxlan, what have you, and in the example of Kubernetes you could do something simple like use flannel, which is an overlay.When deploying things like Kubernetes and OpenStack I pay a lot of attention to networking models–it takes some thought about how to fit k8s into an environment. Overlays almost always work because they just ride on top of the underlying physical network and just need L3 connectivity between the virtual switches or whatever is taking care of the overlay. However, overlays are not alway the. Good architecture decisions must be made.One issue I have with this model is that I don’t necessarily want nodes listening on public IPv{4,6} address if they don’t need to be, but would still like them to have egress access to the Internet. It would be worthwhile for packet.net to have a nat gateway setup like AWS can do. They would have to perform some network magic in the background, but I think it’d be doable. More likely they will create a VPC-like model with a hidden overlay, perhaps using EVPN. Who knows. :)(Aside: I did do a lot of work and setup dnsmasq, squid, and apt-cacher-ng in packet.net. I removed the public IPv{4,6} addresses from the interface and setup nodes to go through a cache. This worked for 99% of what I was doing–but in some cases applications cough docker images cough are really annoying and require access to the Internet without a proxy. Again, another story for another day.)I had to do some tricks with macvlan to get flannel working with packet.net’s model, but it’s flannel’s issue. See the vent repo for more information on that. At least I finally learned about macvlan to work around the issue.NOTE: Packet also support setting up your own standard VLANs in some capacity, but I haven’t explored that yet.Spot pricingSpot pricing is essentially a way to bid on computing time. You put in your bid, and if it’s high enough you get the compute resources you asked for. Later on, if someone puts in a higher bid your resources get diverted to them. What this really means is that your bare metal server will be deleted.I’m a big fan of spot pricing, as I think that economics is really important in computing, or it should be at least. With spot markets you can really test out some economic theories. Or, at least, get less expensive short term resources, such as CI/CD or for testing out deployments, which I do a lot of. While working on Vent I usually was able to get the AMD EPYC nodes for about $.30 in spot pricing versus $1 they normally are per hour. That’s quite a discount.SimplicityThere is a core set of features that a public cloud requires, IMHO. Public clouds like AWS have hundreds of custom services. It can be very confusing to understand what is available and how they work, let alone their billing mode. I’m not saying one way is better than another, but it’s often refreshing to have a simpler view of public cloud resources. Digital Ocean is similar in their desire to keep things clean. Time will tell if this is a good product design, but I appreciate it. A good networking model, access to compute resources, and persistent volumes, elastic IPs…the basic requirements, it’s all I need. I’m not even a big fan of “security groups” which packet does not have. Load balancers are probably a good idea though.kthwMr. Hightower has changed kthw considerably over the years. As I write this I’m sure he’s creating a new version that will come out soon. WHen I last worked with Kubernetes and kthw it was version 1.3.0. Now we are on 1.10.7 for Vent, 1.10.2 for kthw, and 1.11 for Kubernetes proper.kthw is not at this time using docker in a way that you think it would. Docker is not actually installed, instead it’s using containerd. It also uses gvisor, another Google project.  Gvisor [is] a new kind of sandbox that helps provide secure isolation for containers, while being more lightweight than a virtual machine (VM). gVisor integrates with Docker and Kubernetes, making it simple and easy to run sandboxed containers in production environments.I really appreciate the work that Mr. Hightower has put into kthw as I have used it quite a bit to learn about Kubernetes.VentFinally, lets chat about Vent a bit. First, I have an overview document in the vent repo that discusses the choices I made and issues I ran into if you’d like something more in depth.With Vent I took kthw and turned it into a set of Ansible playbooks. Specifically I didn’t use roles, each step in kthw is a separate playbook. For convenience, they are all brought together in a monolithic file, all.yml. I chose this model to make it look like kthw, with a step by step process. In the future I may change this to be role based.Provisioning packet.net instances is not done with Ansible, and instead I’m using the packet.net golang based CLI. it has support for spot instances. You can provision Packet.net instances with Ansible or Terraform, and probably other systems, but Ansible and Terraform don’t seem to support spot instance pricing (at least at this time). I should probably contribute that to the Ansible module, but haven’t had time.One thing I don’t like about how I’m deploying k8s is that I’m relying pretty heavily on the flannel k8s deployment yaml to manage the networking. I’d like to have Ansible set that up instead of using the magic deployment from CoreOS. Also, I’d like to not use flannel at all and move to an Open vSwitch based networking model, but that will take some time to research. For now it’s magic flannel. :)Like kthw, I’m not putting the k8s control plane into containers or self-hosted in k8s itself. Self-hosting is what kubeadm and some other deployment tools do. Something to look into in the future.Going forward I hope to write a “vent” CLI that does a lot of the setup, including provisioning, setting up a few basic variables, etc.ConclusionThanks to packet.net for a great service. Thanks to Mr. Hightower for kthw. And wish me luck on the CKA exam!"
    } ,
  
    {
      "title"    : "Infinite Regress - Dependencies All the Way Down",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/07/18/its-all-about-dependencies.html",
      "date"     : "2018-07-18 00:00:00 -0400",
      "content"  : "I have a pet theory that it’s all about dependencies.I used to work in an area, Network Function Virtualization, NFV for short, that is made up of many complex systems. The most obvious example is OpenStack, which is notoriously considered complicated. To be fair, there are many more high level systems like it that are also part of the ecosystem. Also see Kubernetes or almost any Software Defined Network.Complexity ReignsIt’s “turtle’s all the way down” as OpenStack is built on the Linux ecosystem, which is itself extremely complex, never mind the underlying hardware Linux typically runs on: CPUs, proprietary firmware and drivers, etc. Code is everywhere. And code has dependencies.  “Turtles all the way down” is an expression of the problem of infinite regress.I’ve spent some of my 20 years in IT as a Linux systems administrator which has given me a particularly strong view of operating system dependencies. Having worked so much with OS packages (RPMs, deb files) and similar mechanisms (Pip, CPAN) I’m keenly aware of how important dependencies are, and how challenging they can be to manage.Hidden DependenciesUnless you write code or manage systems that have packages you might not be as aware just how many dependencies there are. Often organizations pay companies to abstract away dependency issues.As far as I’m concerned, a large part of what you are buying when you purchase commercial software, especially distributions of complex systems like OpenStack, is the ability to mitigate the risk of dependencies.However, ignoring dependencies through abstraction is a double edged sword. The thesis of this article is that “it’s all about dependencies.” In my opinion, if an organization is going to be good at managing complex technology they also must be (very) good at managing dependencies. If dependencies exist throughout the technology stack (“turtles all the way down”), there comes a point when every organization must deal with them.Breaking ChangesI recently read this fascinating paper, “Why and How Java Developers Break APIs”, which studied a set of Java libraries for breaking changes.  …we monitored 400 real world Java libraries and frameworks hosted on GitHub during 116 days. During this period, we detected 282 possible breaking changesAs part of the study, the authors attempted to contact the developers of the libraries to ascertain if the detected changes were actual breaking changes or not. Of those 282 potential breaking changes, 59 (39%) were confirmed by the library developers. (The rest remain unconfirmed and are still potential breaking changes.)Thus, over a relatively short period of about 4 months, 59 confirmed breaking changes occurred in only 400 libraries. That is a considerable number.You Own Your UptimeIf I can conclude with anything it’s that in order to manage complex systems (and apparently they are all complex) I believe organizations must be great at understanding and managing dependencies. To deal with complexity requires expertise in dependencies. Certainly we can try to push the risk onto vendors, but at some point we must take responsibility. You own your uptime."
    } ,
  
    {
      "title"    : "Using OpenShift&#39;s Docker Remote Registry",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/07/02/openshift-remote-registry-docker.html",
      "date"     : "2018-07-02 00:00:00 -0400",
      "content"  : "In this post I will cover using an OpenShift Origin deployment that has a registry as a remote registry for Docker images. Note that I’m not covering installing OpenShift–it’s already deployed. I’m just going to use its registry.Official DocumentationThis doc is pretty good; won’t get you all the way, but it’s close.About the DeploymentThis OpenShift installation is running in AWS (not that it really matters where it’s running) and the registry is backed by S3. As far as I’m concerned object storage is the best backend for Docker images. Well, at least it’s effectively infinite.The version of OpenShift Origin being used is 3.9.S3 PermissionsOpenShift was deployed using openshift-ansible, and the hosts file was configured to use an AWS user that has been configured with specific permissions.A bucket, openshift-1-registry, was created by an administrative user. The OpenShift AWS user was provided the below permissions to be able to use that bucket.{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": \"s3:*\",            \"Resource\": [                \"arn:aws:s3:::openshift-1-registry\",                \"arn:aws:s3:::openshift-1-registry/*\"            ]        }    ]}I’m sure that could be firmed up, wildcards are frowned upon, but at least the OpenShift user only has access to his particular bucket. The user can’t list all buckets, but can access openshift-1-registry.Below I’m using the AWS CLI as an example of access permissions.$ aws s3 lsAn error occurred (AccessDenied) when calling the ListBuckets operation: Access Denied$ aws s3 ls s3://openshift-1-registry                           PRE registry/Registry InformationAs root on the controller instance I can find out the route for the docker registry. Of course this assumes that OpenShift has been setup with a wildcard apps domain.openshift-controller# oc get routesNAME               HOST/PORT                                   PATH      SERVICES           PORT      TERMINATION   WILDCARDdocker-registry    docker-registry-default.apps.example.com              docker-registry    &lt;all&gt;     passthrough   Noneregistry-console   registry-console-default.apps.example.com             registry-console   &lt;all&gt;     passthrough   None*.apps.example.com is available through an ELB on AWS.NOTE: I’ve replaced the real URL with example.com.Local DockerThis post assumes you have a local docker instance. In this example, docker 1.13.1 is running in a CentOS 7 host.We need to add an insecure registry to the local docker (that is, of course, unless you have properly setup all the SSL certificates, which would be a good thing to do for production).Edit Docker’s daemon.json file to add the insecure registry.local-docker$ sudo cat /etc/docker/daemon.json{  \"insecure-registries\" : [\"docker-registry-default.apps.example.com:443\"]}The restart it.LoginOn the local docker, which also has the oc command line and access to OpenShift…local-docker$ oc loginlocal-docker$ docker login -u $(oc whoami) -p $(oc whoami -t) docker-registry-default.apps.example.com:443Pull and Push and ImageCheck what OC project you are in. Note that I’ve created a “example-project” project.local-docker$ oc projectUsing project \"example-project\" on server \"https://openshift.example.com:443\".Pull, tag, and push an image.local-docker$ docker pull docker.io/busyboxlocal-docker$ docker tag docker.io/busybox docker-registry-default.apps.example.com:443/example-project/busyboxlocal-docker$ docker push docker-registry-default.apps.example.com:443/example-project/busyboxConclusionNow you’re using a remote registry that is managed by OpenShift. The part I found most confusing was the project name and how it relates to the image tag."
    } ,
  
    {
      "title"    : "Using Cloud Images With KVM",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/06/26/using-cloud-images.html",
      "date"     : "2018-06-26 00:00:00 -0400",
      "content"  : "Most Linux distributions offer some kind of binary image that is typically used in “cloudy” situations–such as public cloud or with OpenStack. These images have been pre-built, use cloud-init, and are expecting to be configured based on metadata provided through a combination of the user and the infrastructure as a service (IaaS) system, either via network metadata or a “config drive” (a specially created ISO image that is attached to the vm).However, one doesn’t always have an IaaS system available, and sometimes we just want to use KVM and not have to attach a custom cloud init ISO image, so here are some quick instructions on using a cloud image with plain KVM, and manually configuring the image.Step 1: Download the imageDownload from Ubuntu’s cloud image cache.wget https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.imgOther bigger distros should offer a “cloud image” in some fashion.Enlarge Virtual DiskBy default it’s only 2.2G. That’s not very many Gs. :)$ qemu-img info bionic-server-cloudimg-amd64.imgimage: bionic-server-cloudimg-amd64.imgfile format: qcow2virtual size: 2.2G (2361393152 bytes)disk size: 322Mcluster_size: 65536Format specific information:    compat: 0.10    refcount bits: 16Resize it to 20G.$ qemu-img resize bionic-server-cloudimg-amd64.img 20GImage resized.$ qemu-img info bionic-server-cloudimg-amd64.imgimage: bionic-server-cloudimg-amd64.imgfile format: qcow2virtual size: 20G (21474836480 bytes)disk size: 322Mcluster_size: 65536Format specific information:    compat: 0.10    refcount bits: 16Once we boot it we’ll resize inside the OS.Set Root Passwordvirt-customize is an easy way to “edit” images.$ virt-customize -a bionic-server-cloudimg-amd64.img --root-password password:coolpass[   0.0] Examining the guest ...[  22.3] Setting a random seedvirt-customize: warning: random seed could not be set for this type ofguest[  22.3] Setting the machine ID in /etc/machine-id[  22.4] Setting passwords[  24.4] Finishing offRemove Cloud InitLet’s uninstall cloud-init as well.(NOTE: These two virt-customize steps could be put into one command.)$ virt-customize -a bionic-server-cloudimg-amd64.img --uninstall cloud-init[   0.0] Examining the guest ...[  12.1] Setting a random seedvirt-customize: warning: random seed could not be set for this type ofguest[  12.1] Uninstalling packages: cloud-init[  17.1] Finishing offBoot the VMCreate a vm definition file for virsh. Replace the various variables indicated by double braces with your choices of resources and file locations.Note that you could use virt-manager and load the backing image and do this graphically if you like that sort of thing.&lt;domain type='kvm'&gt;  &lt;name&gt;{{ vm_name }}&lt;/name&gt;  &lt;memory unit='KiB'&gt;{{ vm_required_memory }}&lt;/memory&gt;  &lt;currentMemory unit='KiB'&gt;{{ vm_required_memory }}&lt;/currentMemory&gt;  &lt;vcpu&gt;{{ vm_required_vcpu }}&lt;/vcpu&gt;  &lt;os&gt;    &lt;type arch='x86_64'&gt;hvm&lt;/type&gt;    &lt;boot dev='hd'/&gt;  &lt;/os&gt;  &lt;clock offset='utc'/&gt;  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;  &lt;on_crash&gt;destroy&lt;/on_crash&gt;  &lt;devices&gt;    &lt;emulator&gt;/usr/bin/qemu-kvm&lt;/emulator&gt;  &lt;disk type='file' device='disk'&gt;       &lt;driver name='qemu' type='qcow2'/&gt;       &lt;source file='{{ location_of_backing_image }}'/&gt;       &lt;target dev='vda' bus='virtio'/&gt;  &lt;/disk&gt;  &lt;interface type='bridge'&gt;    &lt;source bridge='virbr0'/&gt;    &lt;model type='virtio'/&gt;  &lt;/interface&gt;  &lt;serial type='pty'&gt;    &lt;target port='0'/&gt;  &lt;/serial&gt;  &lt;console type='pty'&gt;    &lt;target type='serial' port='0'/&gt;  &lt;/console&gt;  &lt;/devices&gt;&lt;/domain&gt;Define and Start the VMDefine and start.virsh define vm.xmlvirsh start vm_nameAccess ConsoleHandy dandy virsh console.virsh console vm_nameLogin with username: root password: coolpass (or whatever you set it to).Setup DHCP (Ubuntu Bionic only example)Networking will likely not be configured at all (b/c we removed cloud-init).Create the below. Note the interface name. Could probably add this with virt-customize.NOTE: This is Ubuntu Bionic/18.04 and it has new networking configuration.root@ubuntu:/etc/netplan# cat 01-netcfg.yamlnetwork:  version: 2  renderer: networkd  ethernets:    enp0s2:      dhcp4: trueApply it.root@ubuntu:/etc/netplan# sudo netplan --debug apply** (generate:930): DEBUG: 19:24:16.431: Processing input file //etc/netplan/01-netcfg.yaml..** (generate:930): DEBUG: 19:24:16.433: starting new processing pass** (generate:930): DEBUG: 19:24:16.434: enp0s2: setting default backend to 1** (generate:930): DEBUG: 19:24:16.435: Generating output files..** (generate:930): DEBUG: 19:24:16.436: NetworkManager: definition enp0s2 is not for us (backend 1)DEBUG:netplan generated networkd configuration exists, restarting networkdDEBUG:no netplan generated NM configuration existsDEBUG:device lo operstate is unknown, not repluggingDEBUG:netplan triggering .link rules for loDEBUG:replug enp0s2: unbinding virtio0 from /sys/bus/virtio/drivers/virtio_netDEBUG:replug enp0s2: rebinding virtio0 to /sys/bus/virtio/drivers/virtio_netNow interface should have an IP. (Assuming the hypervisor was setup with a default network that does DHCP.)Disk sizeNote that cloud-init usually does this. But b/c we don’t have it, we’ll perform the same basic steps manually.Growpart.root@ubuntu:/etc/netplan# growpart /dev/vda 1CHANGED: partition=1 start=227328 old: size=4384735 end=4612063 new: size=41715679,end=41943007Resize.root@ubuntu:/etc/netplan# resize2fs /dev/vda1resize2fs 1.44.1 (24-Mar-2018)Filesystem at /dev/vda1 is mounted on /; on-line resizing requiredold_desc_blocks = 1, new_desc_blocks = 3The filesystem on /dev/vda1 is now 5214459 (4k) blocks long.root@ubuntu:/etc/netplan# df -hFilesystem      Size  Used Avail Use% Mounted onudev            985M     0  985M   0% /devtmpfs           200M  460K  199M   1% /run/dev/vda1        20G  959M   19G   5% /tmpfs           997M     0  997M   0% /dev/shmtmpfs           5.0M     0  5.0M   0% /run/locktmpfs           997M     0  997M   0% /sys/fs/cgroup/dev/vda15      105M  3.4M  102M   4% /boot/efitmpfs           200M     0  200M   0% /run/user/0Should have a 20G / now.ConclusionOne should really use cloud-init properly, but if you are stuck or just plain don’t feel like it, then this is one way to go about getting a vm setup in KVM using a cloud image."
    } ,
  
    {
      "title"    : "An Introduction to 5G Network Slicing",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/06/14/5g-network-slicing.html",
      "date"     : "2018-06-14 00:00:00 -0400",
      "content"  : "A colleague and I gave a presentation entitled “5G Network Slicing and OpenStack” at the 2018 Vancouver OpenStack summit. We did a lot of research, reading 70 to 100 papers (whitepapers, standards body papers, academic papers, you name it–so many we published a bibliography) to try to determine what network slicing is and how OpenStack can participate.What is Network Slicing?After reading all of those documents, I ended up with a set of quotes that I particularly liked. Here are a couple of them.  “We define network slices as end-to-end(E2E) logical networks running on a common underlying (physical or virtual) network, mutually isolated, with independent control and management, and which can be created on demand.”- Network Slicing for 5G with SDN/NFV  “Network slicing is a key feature for the next generation network. It is about transforming the network/system from a static “one size fits all” paradigm, to a new paradigm where logical networks/partitions are created, with appropriate isolation, resources and optimized topology to serve a particular purpose or service category…” - 3GPP 28.801  “Slicing allows to concurrently support a large variety of use cases with diverging requirements on a common infrastructure platform…Examples of resources to be partitioned or shared, understanding they can be physical or virtual, would be: bandwidth on a network link, forwarding tables in a network element (switch, router), processing capacity of servers, processing capacity of network elements.” - Applying SDN Architecture to 5G SlicingWhether or not network slicing truly comes into existence, it’s fascinating to see how much change people think could happen. NS is nothing more than a total transformation of the network.What is Network Slicing?The main reason Network Slicing (NS) is required is to meet the need to support diverse 5G use cases. Typically the following use cases are provided as examples:  Enhanced mobile broadband  Massive machine type communications  Ultra reliable and low latency connectionsThe requirements of these networks are quite different. Some focusing on (much higher) bandwidth, and others latency. There is also a need to provide network access to 3rd parties and support MVNOs-like (Mobile Virtual Network Operator) business models. Safe, secure, and reliable critical communications also plays an important role. Slices could even exist across multiple operators.Simply stated, Network Slicing is the ability to provide APIs that, through the use of virtualization and quality of service, partition a network into diversified sections (AKA slices) across a set of heterogeneous domains. Effectively, the same concepts used in Infrastructure as a Service are applied to the network. By creating network slices we can make physical networks capable of carrying diverse traffic in a secure, multi-tenant fashion.Requirements(One of my favorite diagrams, care of Netmanias)Once all of these papers have been read, a set of common requirements starts to develop.  End to End - RAN, CN, Transport, edge clouds, core clouds–all of it :)  Orchestration - Manage heterogeneous network resources  On Demand - CRUD actions at any time  Elastic -  Grow/shrink network services resources based on need  Extensible - Expand network slice with additional functionality and characteristics  Safety - Failures in one slice not causing failures in others  Protection - Events in one slice do not have a negative impact on other slices  Recursion - Ability to build a new network slice out of other slices  Isolation - Guaranteed isolation and non-interference between network slices in data and control plane  Flexible -  Simultaneously accommodate diverse use cases  Exposure - Provide secure access to 3rd parties  Network Functions - Not just switches and routersNS with SDN and NFV(image care of metaswitch)SDN and NFV aren’t strictly required to perform NS, but it would be surprising if they were not involved.An important part of the definition of a slice is that it can include network functions. A slice isn’t just a partition, but also a series or chain of network functions. Sometimes these functions are part of multiple slices.SDN is also key because it provides the ability to program the network, and with functionality like Service Function Chaining (SFC) packets don’t necessarily have to follow a standard layer 3 path. (Note that I don’t define SDN as a centralized OpenFlow controller–it’s much more than that.)How does OpenStack fit in?Simply: it’s just a domain. By end-to-end I mean multiple domains are crossed, such as the Radio Access Network (RAN), edge clouds, MPLS core, data center networks (and intra-DC), WAN, etc. That said, as a domain OpenStack will have to support the required features to enable network slicing. The good news is that by supporting virtualized networks and having an API OpenStack already meets a large portion of the aforementioned requirements. However, some work, a discussion of which would not fit into this post, still needs to be done.ConclusionNetwork Slicing is a fascinating topic. Like any new, complex, paradigm shifting technology it’s easy to push back on (see NFV), but from my perspective the same value in IaaS would be gained from NS, and IaaS has shown to have considerable return on investment, especially in the area of automation. To what extent NS requirements are met remains to be seen, but overall NS in some fashion makes so much sense its likely unavoidable."
    } ,
  
    {
      "title"    : "The Four Major Components of NFV",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/06/10/four-components-of-nfv.html",
      "date"     : "2018-06-10 00:00:00 -0400",
      "content"  : "Network Function Virtualization (NFV) is a broad and complex topic. However, I think there are four major components to NFV and they are as follows:  NFV Infrastructure  Softwarization of the network and SDN  Management and Orchestration  Deploying Network ServicesCertainly the above is a simplification, but I think that these four pieces are the underlying structure of NFV.NFV InfrastructureThis is probably the best defined at this time, because, generally speaking, OpenStack is the defacto NFVi and Virtualization Infrastructure Manager (VIM). It’s not the only option, but it’s the most common.Regardless of whether it’s OpenStack or something else, there has to be a system that can act as Infrastructure as a Service (IaaS) with APIs and schedulers and the like–something to manage compute, network, and storage, as well as other more advanced NFV resources (GPUs, Numa, etc).ETSI is doing an admirable job of working to standardize the NFVi and VIM (as well as other areas, of course).Softwareization of the Network and SDNWhile some people believe that SDN has failed, I’m not one of them. If we only consider SDN as a centralized controller that uses OpenFlow, then perhaps failed is correct (though I usually suggest that OpenFlow was an academic effort). But that is not the only way to define SDN.Regardless of the definition of SDN, to be successful with NFV we need the ability to automate the network, and the only way to do that is to have access to APIs that talk to schedulers and resource managers to control network components. When we talk about the NFVi, or IaaS, we often say that it will control compute, storage, and networking. And it does. However, we need to step outside the boundaries of the NFVi into the rest of the network–including the core network. We need to manage as much of the network as we can as though it is software…APIs, libraries, dependencies (ugh) , and code.Thus we end up with the somewhat clumsy phrase “softwareization of the network.” Clumsy but appropriate and workable. To be successful with NFV many traditionally separate network domains need to be combined together into a global interface.Certainly implementation of this concept will be challenging. Traditional, legacy networks will push back on any kind of softwareization, but, in my opinion, it has to happen to some degree in order to be able to deploy network services successfully.Unfortunately it does not seem like this is an area that can be standardized, at least not at this time. While networks tend to do the same thing, they all seem to have been designed differently, and may be unique.Management and OrchestrationThis is another area that ETSI is working to standardize. What we need is some kind of higher level system that can talk to all the other APIs that we have made available (VIM and the “softwareized network”) to actually be able to deploy network services that customers can use (and pay for).Currently it is exemplified by the rather complex system called ONAP–the Open Network Automation Platform. There are other similar systems, such as ETSI’s own OSM and many commercial products as well.By using a management and orchestration (MANO) system, we can abstract away the underlying complexity of the NFVi and network and move towards the real work of actually (for real) deploying network services.Deploying Network ServicesOnce we have the NFVi/VIM, a softwareized network, and a management and orchestration system (or systems), we can finally get to the work of onboarding VNFs, building network services, and selling them to customers.However, it’s important to note that this is not a layer that one can simply purchase. The other three layers are infrastructure. They are systems to be deployed, and theoretically could be completely outsourced. This final layer is where all the actual work should occur–all the investment in the other layers is to service this one.This layer is effectively a form of application development. I don’t want to go into too much detail in this particular post, but typically onboarding a VNF or Network Service requires writing some code (eg. TOSCA). Even if the code is provided by vendors or from open source, each Service Provider (SP) will have to do some customization of this code, as well as do their own testing and validation of the VNFs and Network Services it defines. Further, SPs will need to develop workflows for deployment of services that match their historical corporate organizational structure. While overall every SP is the same, the way they approach that sameness is different and often (unfortunately) unique, much like their network and security policies.ConclusionThe first three layers are infrastructure. They need to be commoditized and automated as much as possible in order to abstract away their complexity. Once those layers are available and trusted, the hard work to deploy customer facing network services can begin."
    } ,
  
    {
      "title"    : "OpenStack Vancouver Summit 2018",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/05/28/openstack-vancouver-2018-summit.html",
      "date"     : "2018-05-28 00:00:00 -0400",
      "content"  : "Last week I was at the OpenStack Summit in Vancouver. This is the second time I’ve been to a OpenStack summit in this amazing location. It is probably the best place in the world to hold a conference, as the conference center is beautiful and the view is absolutely unbeatable. I am sure that this will not be the last OpenStack conference held in BC.OpenStack Slows Down and MaturesHaving been at the previous Vancouver summit, I believe in 2015, I know that it was much larger. I’m sure that the attendance at the 2015 version was six or seven thousand. This summit I believe was about 2500 people. There is a good reason for the reduction in attendance in that there are not as many developers that go as they now, instead, attend another event called the “PTG” (Project Team Gathering) which usually happens in a less expensive city (such as Denver…but strangely also Dublin). So that drops attendance by about half.The other factor is that OpenStack is maturing. It’s coming through they hype cycle and settling down. Certainly there is a lot of work that needs to be done to keep OpenStack going, especially with regards to NFV, but overall it’s a bit less “exciting” at this point. For example Telecoms are putting it into production. Think about that. The most conservative organizations in the world and deploying thousands of OpenStack instances.For another perspective, Redmonk has a balanced blog post.  Many event organizers, OpenStack included, set themselves up for this criticism by using attendance themselves as a sign of health and progress, but that doesn’t change the fact that moving towards smaller events can be a boon for communities as it makes the events more navigable and weeds out less committed third parties. Attendance, in other words, is not a particularly useful metric if you’re trying to assess the OpenStack project’s health, and not just because it forked its developer and user events.Should there be a Linux Foundation, an OpenStack (or Open Infra) foundation, and a CNCF foundation? I think so. OpenStack’s community is really built on collaboration. I give them a hard time occasionally, but compared to other open source projects they really try to keep things civil. I think the triumvirate of these three foundations is a powerful substrate for open source software development. Two might not be enough, and four too many.Open InfrastructureAs OpenStack focuses on other open source projects under their umbrella (Zuul and Kata Containers) it’s becoming fairly obvious that OpenStack will rebrand in some fashion around “Open Infrastructure.” They are putting out signals that this might happen, and I think it makes sense. I help to run the Toronto OpenStack meetup, and as well I recently started an Open Source Networking meetup. Just from my own view it would make sense to rebrand our meetups as Open Infrastructure and start focusing on a broader topic base.Vendors like to try to pin OpenStack to just being a set of APIs into which any backend can be plugged into. However I don’t really view it like that as OpenStack is a Linux ecosystem integration platform. It really is open infrastructure. I prefer to consider what gets tested by the OpenStack CI system as being what “OpenStack” is, and add in some key components like Ceph and open networking, as well as automation systems such as ONAP.Network SlicingWe presented a couple of talks at the summit. The one that probably resonated most with attendees was our presentation on 5G Network Slcing and OpenStack. Like a lot of technologies, it’s not that easy to define what Network Slicing is, but I personally find it fascinating, if at the very least academically. Certainly Network Slicing will occur in some fashion, but it is possible we won’t call it that…could just be some form of end to end virtualization. Or we could go all the way and build tools and APIs that meet all requirements for network slicing and create amazingly diverse networks and applications running on top of them, all the while providing secure, safe access for emergency communications.Code Drops - Airship, StarlingX, Akraino…There were several code drops from larger companies:  Akraino  StarlingX  AirshipAkrainoAFAIK Akraino is essentially an integration project to create an edge stack. I’m not sure how much code actually exists at this time, , but I think it’s a good idea to do work in this area whether it’s successful or not. Certainly there is a good opportunity to get involved in the project right now as it’s still quite fluid.I personally still think that innovation occurs in open source, so this kind of project is needed.StarlingXWind River has an OpenStack distribution (Titanium) which is aimed at Network Function Virtualizion (NFV) infrastructure and thus telecoms (and to be fair, industrial use cases as well). They had many proprietary changes to OpenStack in order to improve it and for various reasons have decided to try to upstream that code.Wind River is a pretty interesting company. I’ve got to give props to any company that writes code that has to work or otherwise lives are in danger.  For over 20 years, Wind River® has provided NASA with the most proven software platform to bring dozens of unmanned systems to space, resulting in some of the most significant space missions in history. - Wind River: 20 Years in SpaceAirshipIt’s unfortunate that it seems like we need another way to deploy OpenStack, but apparently we do. From my limited introduction to Airship I like the overall tone of the project in terms of how it’s abstracting the layers, and I believe it is also taking care of Kubernetes deployments as well. But don’t quote me on that as I haven’t looked at it closely enough. It also relies heavily on OpenStack-Helm.ConclusionOverall, it was a nice summit. Times are a changin’. I fully expect the OpenStack Foundation to rebrand in some fashion, that is assuming once doesn’t already consider them as having rebranded with “Open Infra” and emphasis on Zuul and Kata Containers as new, separate projects. I also expect a push back on complexity in NFV, but we need infrastructure that has APIs and schedulers, so the complexity is unavoidable, we just have to get good at it.The foundation has changed the PTG so that now operators, such as myself, are invited. We used to have a separate operators meetup, but now that will be co-located with the five day PTG. So the next time I go to an OpenStack event will be the upcoming PTG event in Denver."
    } ,
  
    {
      "title"    : "Service Providers and Telecoms Must Accept Complexity",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/05/13/service-providers-accept-complexity-of-nfv.html",
      "date"     : "2018-05-13 00:00:00 -0400",
      "content"  : "tl;drNFV is complex. But in order to automate we need APIs. Something has to schedule compute, network, and storage. Something has to provide the APIs to orchestrate it all. That something is NFV. Telecoms can’t give up so easily on NFV, because to do so would be at worst a rejection of automation and at best a renaming of NFV.Accepting NFV ComplexityI’ve been working with OpenStack and SDN for a long time. As well, for the last 2+ years, I have also been working in the area of Network Function Virutalization (NFV). At this point NFV is a bit of a catch all term for all the new things that have been happening in and around the service provider or telecom industry. Automation. SDN. 5G. All of these things are complex and perhaps complicated.OpenStack has found a place in telecoms. I think it’s fair to say that every telecom in the world is thinking about OpenStack in some capacity. We are at the point now, I believe, where many telecoms are past the proof of concept phase and are really starting to try to put OpenStack, and more specifically NFV infrastucture (NFVi), into production. However, this is the point where we will also start to see telecoms give up on NFV (and SDN) because it’s too complex.Telecoms are good at deploying physical boxes and manually deploying required applications on top of them because they heavily rely on the vendors to do it. If a telecom buys a piece of software from a vendor, they then put the vendor through the wringer to get them to deploy it and ensure it works. Unfortunately, this is “set and forget.”In the case of NFV, however, it’s not just a single vendor that is involved, and what’s more, telecom staff are being asked to understand how NFV works and to be able to deploy network functions into NFV infrastructure via automation (eg. ETSI MANO). Instead of relying on vendor professional services internal telecom staff are being asked to deploy network functions. But doing that is not as easy as beating up on a vendor. It’s at this point we will start to see a rejection of NFV as operationalizing NFVi, SDN, and automation is challenging. Not impossible, just challenging. (Aside: the real issue often in telecoms is the complexity of the organization, not the technology.)Rejection of NFV?I read Tom Nolle’s CIMI Corp blog quite a bit. I enjoy his take on telecoms and NFV. He has a recent post entitled Have Operators Uncovered a New Transformation Path? from which I take the following quote.  The leading technologies in transformation, according to popular wisdom, are SDN and NFV.  Operators have been griping about both in both conversations with me and surveys I’ve done.  SDN, they say, is too transformational in terms of the network technical model.  Central control of forwarding, the classic ONF OpenFlow SDN model, may or may not scale far enough and may or may not handle large-scale faults.  NFV, they say, is simply too complicated to be able to control operations costs, and its capital savings are problematic even without opex complications.I have some issues with the above statement. (Again, I like Tom’s work, I just have an issue with this particular post.)First, I don’t think anyone really wants a “OpenFlow” model of SDN. I prefer to think of OpenFlow as an academic project that furthered our thinking about SDN, not as something we would actually want to put into production. We need SDN, absolutely, but SDN != OpenFlow.The second statement I disagree with is that “NFV is too complicated.” We need automation to drive down costs. In order to automate we need APIs. And what APIs? OpenStack has the defacto NFVi API. In order to automatically deploy network functions we need something to schedule compute, storage, and network resources. That something is NFV. The vast majority of NFVi will probably be based on OpenStack.If we were to reject NFV all that would happen is that the abstraction of compute, storage, and networking would be hidden by another API (example Mesosphere perhaps?) but that does not avoid the complexity, it just gives it another name or is another product. We can create as many abstractions as are necessary to provide the appearance of reduced complexity, but somewhere there is an NFVi, and somewhere there is SDN, and these things must be well understood and operationally perfect, and that is challenging. But not so challenging that we have to give up on NFV, because giving up on NFV means giving up on automation and staying with the status quo of manually deploying applications onto physical boxes and manually wiring them up.My suggestions to telecoms would be to understand that some complexity is unavoidable and to allow their staff to get up to speed on operationalizing management of NFVi and deployment of NFs with the understanding that it is definitely harder to do than a one-time, static, unrepeatable deployment handled by a vendor. If you want to see complexity try altering one of those deployments a year down the road. ;)"
    } ,
  
    {
      "title"    : "Open Networking Summit North America 2018",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/03/31/ons-na-2018.html",
      "date"     : "2018-03-31 00:00:00 -0400",
      "content"  : "Last week I was lucky enough to attend the 2018 North American Open Networking Summit put on by the Linux Foundation.Overall, the ONS summit was great. I learned a lot, had some good conversations, and was reminded of how much cool stuff is going on in the realm of networking.ThemesThere were a few themes I identified:  The amount of NFV related work being done is amazing. I was a bit overwhelmed the first day as I reminded of all the work that is happening, most, if not all, in the NFV area. It’s not a stretch to say that Linux networking = NFV, IMHO.  Optical networking was also often mentioned as a place for automation, which surprised me.  ONAP - The Open Networking Automation Platform is nearing the top of the hype cycle. I believe ONAP is extremely important. However, it’s quite complicated and has a long way to go in terms of stability. Some telecoms are using pieces of it in production right now. It’s a large project and not everyone will use every single sub project.  Canada has a many companies working in networking and telecom, especially in Ottawa and Montreal. Ottawa has Wind River, Ciena, and Amdocs to name a few.  Speaking of complexity - I complained several times to people about the level of complexity we are entering. People used to think OpenStack is complex. Well now you have to add SDN, ONAP and containers onto that. OpenStack is probably the easy part now (relatively speaking). Sure, Kubernetes is easy to deploy, but how many people who deploy it truly understand how it works.  Dependencies - Still on the complexity front, I see tons of issues with dependencies and managing them. All of these systems, OpenStack, SDN, ONAP, etc, will have to not only be deployed, but talk to one another. We are heading into some serious dependency issues. I took some ONAP training on the last day, and we had several dependency issues just in terms of deployment, and the trainers had deployed this particular set of ONAP modules hundreds of times.  Central Office - There are so many COs spread across NA…tens of thousands of them. This is a real key area, perhaps the most key, for NFV in the next few years.  ONF is doing a ton of work in this area around the idea of “Central Office Rearchitected as Data Center” or CORD for short.Additional NotesA few other small points:  OpenContrail has been renamed “Tungsten Fabric.” Bit of an awkward name, but I don’t mind it. Especially if you drop it down to just TF. Juniper really needs the community around TF to grow, as it’s really a single vendor project which is no good to anyone in terms of open source.  dNOS has been renamed DANOS. There are a few Network Operating Systems (NOS): DANOS, Sonic (from Microsoft), and the recently announced Stratum (thought I am not completely sure Stratum would consider itself a NOS). AT&amp;T plans to deploy 60k whitebox switches with DANOS. I am very excited to see more work being done in the NOS area. All we really need are some good drivers for switch hardware and some more work around common networking protocols (as well as the new advanced technologies) and we should be able to have a thriving Linux NOS.  The P4 programming language was mentioned quite a bit. Will be very interested to see where that goes.  AT&amp;T sees 200PB of mobile traffic per day, up 50% from a year ago.  Wind River is open sourcing some (all?) of their platform.  OpenStack is still  trying to figure out what it is going to look like on the edge. Will it be remote hypervisor, multi-region, cell based, or light weight. No one knows. Probably all of them I guess.  Network slicing was often mentioned, but no one seemed to know what it was actually going to look like in terms of implementation. I heard it in several presentations, but no real details.  Fd.io/vpp was mentioned quite often. I need to better understand that technology.Again, overall,  the amount of work being done in the networking area is astounding and it will be hard for people to keep up with the pace. It’s a great area to be working in. I’m going to try to attend the European ONS summit upcoming this year."
    } ,
  
    {
      "title"    : "Jinja2 Namespaces and Variable Scope",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/03/15/jinja2-namespaces.html",
      "date"     : "2018-03-15 00:00:00 -0400",
      "content"  : "I have been doing some Network automation lately, specifically using the Juniper vSRX and generating config templates via Ansible.I quickly found out about variable scope in Jinja.  Please keep in mind that it is not possible to set variables inside a block and have them show up outside of it. This also applies to loops. The only exception to that rule are if statements which do not introduce a scope. - Jinja DocsPutting a lot of logic into templates is probably not a good idea, but I’m doing it anyway in this case. :)Example. I have two friends. One has a car, one doesn’t. Does the group of friends have a car?#!/usr/bin/env pythonfrom jinja2 import Templatefriends = []friend1 = {    \"name\": \"Frank\",    \"car\": False}friend2 = {    \"name\": \"Anna\",    \"car\": True}friends.append(friend1)friends.append(friend2)t_example = \"\"\"{% set ns = namespace(hasCar=false) %}{% for f in friends %}Friend {{ f.name }}{% if f.car == true %}{% set ns.hasCar = true %}{% endif %}{% endfor %} {# for n in friends #}{% if ns.hasCar == true %}SOMEONE HAS A CAR: TRUE{% else %}SOMEONE HAS A CAR: FALSE{% endif %}\"\"\"template = Template(t_example, trim_blocks=True)render = template.render(friends=friends)print renderIf I run this in ipython I get:In [1]: run jinja-ns.pyFriend FrankFriend AnnaSOMEONE HAS A CAR: TRUEIn [2]:That’s it. Namespaces in jinja."
    } ,
  
    {
      "title"    : "Juniper vSRX MPLS Lab",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/02/19/juniper-vsrx-mpls-network.html",
      "date"     : "2018-02-19 00:00:00 -0500",
      "content"  : "(network diagram of the lab)I’ve wrote up a lab (available on github) on deploying a MPLS core network using Juniper vSRX routers. It’s completely based on this blog post, entitled “Simple Juniper MPLS Core.”My goal with replicating the simple MPLS network was to try to get a better understanding of how MPLS will work in NFV and 5G where there is a desire to do “network slicing.”I’m not sure who wrote the initial blog post, but I am thankful they did because I was able to completely replicate it. I’m indebted to them because I feel like I now have a better understanding of Juniper and MPLS networks, which was my goal. This lab provides a base that I can build off of.I don’t want to say much more in this blog post because I put a lot of work into the lab, so please take a look at the github repo where it resides.Here’s a listing of all the virtual routers running an a baremetal KVM node.$ virsh list Id    Name                           State---------------------------------------------------- 86    ce-ny                          running 120   pe-lo4                         running 122   p2-njh                         running 123   p1-ny8                         running 124   pe-nj2                         running 125   ce-uk                          runningHere’s a session of ce-ny pinging/tracerouting ce-uk.$ ssh root@ce-nyPassword:--- JUNOS 17.3R1.10 built 2017-08-23 06:47:03 UTCroot@ce-ny%root@ce-ny% cliroot@ce-ny&gt; ping 6.6.6.6PING 6.6.6.6 (6.6.6.6): 56 data bytes64 bytes from 6.6.6.6: icmp_seq=0 ttl=61 time=6.536 ms64 bytes from 6.6.6.6: icmp_seq=1 ttl=61 time=3.840 ms^C--- 6.6.6.6 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max/stddev = 3.840/5.188/6.536/1.348 msroot@ce-ny&gt; traceroute 6.6.6.6traceroute to 6.6.6.6 (6.6.6.6), 30 hops max, 40 byte packets 1  10.3.5.0 (10.3.5.0)  4.759 ms  4.345 ms  4.514 ms 2  10.2.3.0 (10.2.3.0)  6.033 ms  6.077 ms  5.291 ms     MPLS Label=299776 CoS=0 TTL=1 S=0     MPLS Label=299776 CoS=0 TTL=1 S=1 3  10.2.4.1 (10.2.4.1)  6.693 ms  6.012 ms  8.049 ms     MPLS Label=299776 CoS=0 TTL=1 S=1 4  6.6.6.6 (6.6.6.6)  4.826 ms  5.409 ms  5.222 msroot@ce-ny&gt;It works! :)NOTE: On the diagram each router has a .17x IP address (full address is the management network, 192.168.122.17x). The x for each is also their router ID and loopback address. So ce-uk, .176, is 6.6.6.6.ConclusionI’m really happy with how this worked out, and as mentioned, I now have a base to do more exploration around MPLS, BGP, OSPF and other protocols, hopefully ending with me having a good understanding of what network slicing could or should look like in a service provider.One of the major things I learned is that building a virtual lab like this is really about understanding nodes and edges, just like what one would do with graphviz dot diagrams. This is why I’ve listed the “links” (aka edges) and interfaces on the diagram, to help me automate and validate. Building a network diagram like the above should be completely automatable, based on well understood graphing. But that is another story… :)"
    } ,
  
    {
      "title"    : "Connecting to Juniper with Ansible",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/01/24/juniper-ansible.html",
      "date"     : "2018-01-24 00:00:00 -0500",
      "content"  : "Recently I wanted to test out using Ansible to connect to Juniper vSRX routers. Unfortunately this was not as easy as I had thought/hoped it would be.Juniper vSRXJuniper provides a virtual machine image for the virtual SRX firewall.The version I’m using here is:media-vsrx-vmdisk-17.3R1.10.qcow2I’m running it in kvm on Ubuntu 16.04…but that’s another story.Configure vSRXThis is the easy part.set system services netconf sshset system services ssh hostkey-algorithm no-ssh-ecdsaset system services ssh hostkey-algorithm ssh-rsaNOTE: In the future the ssh hostkey-algorithm settings might not be needed. More on this later in the post.Also, the known_hosts file for where ever Ansible is running would need to be setup. HINT: ssh-keyscan.Setup PlaybookHere’s an example playbook. Note the provider section. This took a little while to get right. The junos_facts module requires some connection information, and it uses a dictionary called provider that can be setup in the task. Obviously these variables could be pulled from somewhere else, like an Ansible Vault file, but I’m leaving it bare as an example here.Ansible’s default facts gathering doesn’t understand how to gather facts from Juniper, so the juniper_facts module is required to get facts. Note I’m also gathing a subset of information in the example.- name: test  hosts:    - routers  roles:    - Juniper.junos  connection: local  gather_facts: no  tasks:    - name: check is netconf port is listening      wait_for:        host: \"{{ ansible_host }}\"        timeout: 5        port: 830    - name: collect default set of facts and configuration      junos_facts:        gather_subset: \"config\"        provider:            username: \"root\"            password: \"coolpass\"            transport: \"netconf\"            timeout: 5            host: \"{{ ansible_host }}\"Issues with ECDSA keysIt seems that the ncclient code can’t handle, at this time, ECDSA keys. It just fails with an “unknown key” message.I had to disable ECDSA keys in the Juniper routers.I tried this super simple test code, prior to forcing ECDSA on the Junipers, and it errored out with hostkey_verify=True, and succeeded with it set to False.$ cat test-netconf.pyfrom ncclient import managerwith manager.connect(host=\"&lt;hostname&gt;\", port=830, username=\"root\", password=\"coolpass\", hostkey_verify=True) as m:    c = m.get_config(source='running').data_xml    print cAs far as I know at this time this is the latest ncclient code.$ pip freeze | grep ncclientncclient==0.5.3Maybe there is another issue hidden away somewhere. Worth some more digging when I have time.ConclusionOverall, I’m not happy with how this looks. I like Ansible, but sometimes the whole module thing, which exists with almost all configuration management tools, just seems hacky. Here I have to turn of fact gathering, and then turn on a task based Juniper fact gathering. Further, getting this to work with a recent vSRX node took like an hour due to the ECDSA issue. One simple little thing like that breaks connectivity via Ansible. Dependencies and versioning…these are hard and don’t seem to be getting any easier, this is too brittle.I know the Ansible and networking (vendors) have done a lot of work to get many devices supported, it just doesn’t seem quite right to me. That said, Ansible is probably one of the best options for managing many individual network devices."
    } ,
  
    {
      "title"    : "Setting Up Google Cloud DNS with gcloud",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/01/14/gcloud-dns-setup.html",
      "date"     : "2018-01-14 00:00:00 -0500",
      "content"  : "Of all the things to outsource, mail and DNS should be first. :)I’m going to migrate my serverascode.com domain DNS from an old hosting company to Google Cloud DNS. Finally.I also have some DNS hosting done with AWS Route 53, but I am trying to learn more about gcloud. Also, I think hosting DNS at Google is slightly cheaper, in that each “zone” is $0.20/month vs $0.50/month at AWS. Each charges $0.40 for some huge number of DNS requests.One thing I find that is a problem with gcloud is that there is very little documentation outside of gclouds official docs. I am not really a fan of any of the official docs, be it for AWS or gcloud…they always read like autogenerated API docs. Also I find gclouds look, feel, and organization to be difficult to grasp.I’m only going to use the gcloud command line.Setting Up the DNS ProjectFirst, setup a new project. I’m not sure if the best way to use gcloud is to setup multiple projects, but I’m going to setup a DNS hosting project.NOTE: Replace “some-uuid” with some kind of random string. Projects need to have a unique name.$ gcloud projects create dns-hosting-&lt;some-uuid&gt; --name \"DNS Hosting\"List your billing accounts. This assumes you have setup at least one account for billing.$ gcloud alpha billing accounts listID                    NAME                  OPEN&lt;billing account ID&gt;  My Billing AccountSwitch to the DNS project.$ gcloud config set project dns-hosting-&lt;some-uuid&gt;Updated property [core/project].Assign a billing account to the DNS project.$ gcloud alpha billing accounts projects link dns-hosting-&lt;some-uuid&gt; --account-id=&lt;billing account ID&gt;Now the project can be billed.Enable the DNS API on this project.$ gcloud services enable dns.googleapis.comDNS should be in the list of available services for this project.$ gcloud services listNAME                              TITLEbigquery-json.googleapis.com      BigQuery APIcloudtrace.googleapis.com         Stackdriver Trace APIservicemanagement.googleapis.com  Google Service Management APImonitoring.googleapis.com         Stackdriver Monitoring APIstorage-api.googleapis.com        Google Cloud Storage JSON APIdns.googleapis.com                Google Cloud DNS APIlogging.googleapis.com            Stackdriver Logging APIclouddebugger.googleapis.com      Stackdriver Debugger APIdatastore.googleapis.com          Google Cloud Datastore APIsql-component.googleapis.com      Google Cloud SQLcloudapis.googleapis.com          Google Cloud APIsstorage-component.googleapis.com  Google Cloud StorageNow for the actual DNS setup.Setting Up DNS ZoneNow that the project is created and has a billing account we can setup the DNS zone.$ gcloud dns managed-zones create --dns-name=\"serverascode.com.\" --description=\"serverascode\"List zones.$ gcloud dns  managed-zones listNAME          DNS_NAME           DESCRIPTIONserverascode  serverascode.com.  serverascodeThe process is:  Start the transaction  Make changes, add DNS records, etc  Execute the transactionStart a DNS zone editing transaction.$ gcloud dns record-sets transaction start --zone=serverascodeA transaction.yaml file will be created where ever you run this command. Further commands will edit this file, and then finally we will execute this file to push the changes up to gcloud.Add an A record. In this example I am pointing serverascode.com to Github’s page servers.$ gcloud dns record-sets transaction add --zone=serverascode --name=\"serverascode.com\" --ttl 3600 --type A 192.30.252.153 192.30.252.154Add a CNAME for www.$ gcloud dns record-sets transaction add -z=serverascode --type=CNAME --name=\"www.serverascode.com\" --ttl 3600 \"serverascode.com.\"If you host your mail somewhere for this domain, add MX records. Here I enter two mail hosts.gcloud dns record-sets transaction add --zone=serverascode --name=\"serverascode.com\" --ttl 3600 --type MX \"10 mail1.somemailhost.com.\" \"20 mail2.somemailhost.com.\"Finally execute those changes.$ gcloud dns record-sets transaction execute --zone serverascodeAnd once they have been pushed we can list them.$ gcloud dns record-sets list --zone=serverascodeNAME                   TYPE   TTL    DATAserverascode.com.      A      3600   192.30.252.153,192.30.252.154serverascode.com.      MX     3600   10 mail1.somemailhost.com.,20 mail2.somemailhost.com.serverascode.com.      NS     21600  ns-cloud-d1.googledomains.com.,ns-cloud-d2.googledomains.com.,ns-cloud-d3.googledomains.com.,ns-cloud-d4.googledomains.com.serverascode.com.      SOA    21600  ns-cloud-d1.googledomains.com. cloud-dns-hostmaster.google.com. 6 21600 3600 259200 300www.serverascode.com.  CNAME  3600   serverascode.com.If that looks good then go to your registar and change the nameservers to Google’s, which is what I did.If you are reading this post then it must have worked!"
    } ,
  
    {
      "title"    : "My Thoughts on Meltdown and Spectre",
      "category" : "",
      "tags"     : "",
      "url"      : "/2018/01/06/my-thoughts-on-meltdown-spectre.html",
      "date"     : "2018-01-06 00:00:00 -0500",
      "content"  : "First off, I don’t really expect anyone to care what I think about all this. That’s the beauty of the Internet, I can just write what I’d like on my blog and it’s easy to ignore this backwater HTML. :)But I do have some fairly strong thoughts on Meltdown and Spectre, which I consider to be a rather large mess and frankly fairly concerning for the future of humanity. (Certainly computing is only one of our problems though.)Computing powerI’m not an academic or a philosopher, but I’ve always had this strong feeling that humanity would continue to push computing to the farthest it can go, that we would build Matrioshka brains or other similarly massive computers, and maybe have to move them to cold parts of the universe to run them.But the Meltdown and Spectre issues have set me back a bit on that thinking. I’m sure it’s just a bump in the road, but it is a bit depressing. Computer security is so difficult. Here we are moving everything into the public cloud which is based on CPUs that were never meant to be multi-tenant. If we can’t understand the CPUs that we are building now, imagine the issues we will have in the future when we have to deal with even more complexity. Perhaps Meltdown/Spectre will bring about change.Computer securityWith the recent WIFI hacks, SSL being in a poor state, and now massive CPU issues, things are not looking well (and in fact may never have never been)–I may have to do online banking on a Raspberry Pi.This NYT article makes a good point:  As things stand, we suffer through hack after hack, security failure after security failure. If commercial airplanes fell out of the sky regularly, we wouldn’t just shrug. We would invest in understanding flight dynamics, hold companies accountable that did not use established safety procedures, and dissect and learn from new incidents that caught us by surprise.Who was part of the embargo?One thing that I hope comes out in the near future is a time line of who and what companies knew of the issue. I applaud Google for figuring this out, and letting people know. I think there was a lot of unfairness regarding how this all came about.Kernel devs working the HolidaysGreg Kroah-Hartman has a pretty good post on the status of kernel devs.  Right now, there are a lot of very overworked, grumpy, sleepless, and just generally pissed off kernel developers working as hard as they can to resolve these issues that they themselves did not cause at all. Please be considerate of their situation right now. They need all the love and support and free supply of their favorite beverage that we can provide them to ensure that we all end up with fixed systems as soon as possible.Canonical is a small companyI’ve seen a bit of complaining about Ubuntu not having a patch yet (as of me writing this they do not) but one thing people don’t know is that Canonical is a very small company, the best stats I could find suggest ~550 staff. RedHat has ~10,000. That’s a big difference. I have a lot of empathy right now for their kernel team.Google P0 TeamI’ve got to give Google props for having the Project Zero (P0) team. Google does provide some pretty useful things for the global community, and P0 is one of the more important. What other large companies or public clouds do the same? Not many.Spectre is not fixed…Something I think is getting a little lost is that Spectre is not fixed. It seems Meltdown is mitigated, with some workloads having considerable performance impact, but Spectre is not solved. Again I go back to Kroah-Hartman’s blog. Note the part about “claim.”  Again, if you are running a distro kernel, you might be covered as some of the distros have merged various patches into them that they claim mitigate most of the problems here. I suggest updating and testing for yourself to see if you are worried about this attack vector  For upstream, well, the status is there is no fixes merged into any upstream tree for these types of issues yet. There are numerous patches floating around on the different mailing lists that are proposing solutions for how to resolve them, but they are under heavy development, some of the patch series do not even build or apply to any known trees, the series conflict with each other, and it’s a general mess.Google argues that public clouds are better equipped to deal with these issues. Perhaps it’s true. Though live migration is available in other IaaS systems, and, of course, if your app is “cloud native” you don’t have to live migrate anything, just drain (perhaps using Kubernetes).  In many respects, public cloud users are better-protected from security vulnerabilities than are users of traditional datacenter-hosted applications. Security best practices rely on discovering vulnerabilities early, and patching them promptly and completely. Each of these activities is aided by the scale and automation that top public cloud providers can offer — for example, few companies maintain a several-hundred-person security research team to find vulnerabilities and patch them before they’re discovered by others or disclosed. Having the ability to update millions of servers in days, without causing user disruption or requiring maintenance windows, is difficult technology to develop but it allows patches and updates to be deployed quickly after they become available, and without user disruption that can damage productivity.Abstractions get all the hype, and yet we still need baremetal knowledgeMany technical people don’t know have much knowledge CPUs work. Or can’t compile a Linux kernel. We now have all these high level abstractions (lambda) but still have to have knowledge of low level things. People who I admire on twitter aren’t working on figuring out how to deploy to AWS better, and instead are figuring out gdb and writing baremetal code in Rust or Go. This is a curious state to be in.Diversity in TechnologyI believe that diversity in technology is a good thing, and by this I mean different kinds of CPUs and other hardware, as well as operating systems and network protocols, etc. Thus Intel’s effective monopoly is not positive. I don’t know if trading Intel for AWS is better. I think we need something more.I know that many organizations are seeking to reduce cost by using commodity hardware, which usually just means virtualization on x86. I think that may be an overly simplistic way to look at the problem of reducing cost.Perhaps investing in other CPUs is a good idea if only to achieve some diversity and avoid systemic risk. (Frankly it would be pretty cool to have a non-x86 CPU laptop, we should just do that anyways.)ConclusionThis is a pretty rambling post, but I think it makes sense given the what’s happening this week in technology.Bonus: fun tweetsThis is interesting. This vulnerability illustrates an interesting economic upside for multi-tenant cloud providers:  REVENUE.  The vulnerability which they patched quickly drives CPU utilization up and they get to charge more based on CPU use! Rename Meltdown to “Cache-ing” $$$ https://t.co/fzBrpFWY4q&mdash; Hoff (@Beaker) January 7, 2018It&#39;s impossible to reason about computer security in a meaningful manner anymore.  The gap between &quot;architectural behavior&quot; and &quot;micro-architectural implementation&quot; is so great, so dark, and is basically, &quot;Here be Dragons.&quot;  We cannot build solid structures on faulty foundations.&mdash; Bitweasil (@Bitweasil) January 5, 2018This means the 6 month embargo of #meltdown and #Spectre cost those that weren&#39;t in on the club one full year of time responding to it.  @intel and @Google decided who would get that advantage and who wouldn&#39;t.&mdash; John-Mark Gurney (@encthenet) January 5, 2018pic.twitter.com/KRKypYAEiw&mdash; 防毒面を着ているサイバーテロ狼 (@wolfniya) January 4, 20185 lines of JavaScript broke every single Intel processor made in the past 15 years. pic.twitter.com/fyQcHk6haJ&mdash; Mike Pan (@themikepan) January 4, 2018“A CPU predicts you will walk into a bar, you do not. Your wallet has been stolen”— The Internet&mdash; Mike Skalnik (@skalnik) January 4, 2018Don&#39;t panic y&#39;all!Step 1) Don&#39;t use Intel processors.Step 2) Don&#39;t use AMD or anything ARM based.Step 3) You know what? Just give up technology altogether.Step 4) Retreat to the woods and build a cabin out of derelict silicon.Step 5) You&#39;re now Ted Kaczynski, you psycho.&mdash; Josh Cincinnati (@acityinohio) January 4, 2018CERT brings the harsh truth. #Meltdown #Spectre pic.twitter.com/UFPiYA39hd&mdash; Sciuridae Hero (@attritionorg) January 4, 2018Update #7 - Due to the incomplete information provided by hardware manufacturers, we joined forces with other impacted cloud providers including @linode, @packethost and @ovh to share information and work all together. https://t.co/iVHi72nmFJ&mdash; scaleway (@scaleway) January 4, 20188/ @Canonical engineers have been working on this since we were made aware under the embargoed disclosure (Nov 2017) and have worked through the holidays, testing and integrating an incredibly complex patch set into a broad set of @ubuntu kernels and CPU architectures.&mdash; Dustin Kirkland (@dustinkirkland) January 4, 2018"
    } ,
  
    {
      "title"    : "Three Things About 2017",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/12/30/three-things-in-2017.html",
      "date"     : "2017-12-30 00:00:00 -0500",
      "content"  : "Well, the year 2017 is almost over. I wanted to take a few minutes to think about what happened this year and write at least three of them down.1) Moving to TorontoBy far the biggest thing that happened to me this year was the decision to move to Toronto. My wife is from Toronto, and she had spent many years with me in Edmonton. I was born in Edmonton, and, other than a short stint in Athabasca, have lived there all my life. Edmonton is a great place, better than most people think, but it was just time for me to move on and experience some other place in Canada. We almost moved to Ottawa, but for personal reasons ended up in Toronto.Unfortunately the housing market in Toronto is insane. I don’t think anyone understands just how bad it is. We initially decided to rent for a while and figure things out, but the small house near Yonge and Eglinton we had rented turned out to be filled with black mold (as we unfortunately discovered, our landlord was one of the scuzzy ones you always hear about). We opted not to move in and ended up living with family in a house in suburban Woodbridge while we continue to sort things out.If we had moved to Toronto 3 or 4 years ago, houses would be almost half of their cost now. In terms of moving to Toronto, it is bad timing, but we will do our best. Researching real estate has made me realize how the low interest rates have affected housing and the stock market. The 2017 stock market did not have a negative month which is the first time that has ever happened. I, like many others, personally believe we are due for a correction, ala 2008, but I’m not sure what will cause it, and it could take years before it occurs…or it could be tomorrow. Also, I think over time we will discover just how many homes are owned by foreign entities. I don’t have an issue with foreign investment, but homes are not supposed to be investments, they are places for Canadians to live. The same goes for speculators, no matter where they reside.Toronto and Ontario are also much more expensive than Alberta (as most know). However, what many don’t know is that while Toronto and Ontario have land transfer tax, Alberta does not. This tax adds up to tens of thousands of dollars which the buyer pays upon purchase of property. It is very, very expensive to live here compared to Edmonton. I’m surprised that there is not considerable migration from Toronto to Calgary (I say Calgary because Edmonton just doesn’t have the reputation Calgary does, nor is it as close to the Rocky Mountains) but I suppose the Alberta economy is scaring people off.Of course Toronto has many advantages, especially around food and entertainment, which is why we came here in the first place. It’s amazing to see companies like Google and Mozilla with offices here. In fact Google is investing heavily in Toronto. I look forward to seeing a lot of live music in 2018. In 2017 we only made it to a LCD Soundsystem concert, but already have a couple great events lined up for 2018, such as Destroyer. I am also a basketball fan and hope to make it out to some Raptors games, though they are quite expensive to attend. And, of course, how can we forget TIFF. Despite its obvious issues (housing, scuzzy landlords, car culture–I’ve never seen so many Maseratis), Toronto really is a world class city with much to offer.Technology2) MeetupsDuring the first few weeks I went to several technology meetups. I plan to continue that in 2018. I’m also helping organize the Toronto OpenStack Meetup. Further to that I’ve taken over the Toronto SDN Meetup and am starting the Toronto Open Source Networking Meetup which I may combine with the SDN meetup.I also have other grandiose plans (as usual) around some kind of group or event regarding The Future of Networking but have not ironed out all the details as of yet. Hopefully being in Toronto will help the organization of such a group or event.3) Network Function VirtualizationOver the last couple of years I have been doing a lot of work in this area. Despite not having a classical networking background (which I think is almost an advantage in some situations) I am fascinated by networking and all the recent advancements…things like automation, Software Defined Networking, and Intent Based Networking. I also believe that network simulation is an important area that has not been given sufficient attention. Virtual Network Function (VNF) on-boarding will also continue to be a focus as it combines networking and automation.I have also been working with a colleague on Service Function Chaining workshops. I am really enjoying the workshop model. We have been putting SFC into practice and are finding that there are many problems that still need to be figured out. I like SFC because it removes the standard route based model of packet movement, and in fact makes it somewhat arbitrary. Once you start doing SFC you realize why large organizations, usually telcos, are using things like MPLS to manage their core networks. Segment routing in some form is very important going forward. I believe (and hope) networking will look much different in the future, and in fact if 5G is really to come to fruition (which is debatable) it has to.In a previous post I put automation, SDN, and intent based networking as major themes in networking. To that I would now add network simulation and SFC/segment routing as well.Bonus: 4) MoviesI didn’t watch as many films this year as I have in the past, but I can give two films as recommendations:  Good Time  The Killing of a Sacred DeerConclusion2017 was an interesting and challenging year. I expect more of the same in 2018, especially if there is some kind of black swan event that causes a market correction."
    } ,
  
    {
      "title"    : "Fedora 27 Automatic Updates",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/11/22/fedora-27-automatic-updates.html",
      "date"     : "2017-11-22 00:00:00 -0500",
      "content"  : "I thought I’d write a quick short port on setting up automatic updates on Fedora 27.$ dnf install dnf-automaticI am only going to do security updates and have them applied. What you do is up to you. :)# diff /etc/dnf/automatic.conf /etc/dnf/automatic.conf.orig5c5&lt; upgrade_type = security---&gt; upgrade_type = default18c18&lt; apply_updates = yes---&gt; apply_updates = noWhat timers do we have right now? (Ah, systemd, let’s replace cron. Sigh.)$ systemctl list-timers *dnf-*NEXT                         LEFT       LAST                         PASSED       UNIT                ACTIVATESWed 2017-11-22 08:36:26 EST  58min left Wed 2017-11-22 07:36:24 EST  1min 47s ago dnf-makecache.timer dnf-makecache.service1 timers listed.Pass --all to see loaded but inactive timers, too.Hmm there are several dnf timers.root# ls -1 /usr/lib/systemd/system/dnf*.timer/usr/lib/systemd/system/dnf-automatic-download.timer/usr/lib/systemd/system/dnf-automatic-install.timer/usr/lib/systemd/system/dnf-automatic-notifyonly.timer/usr/lib/systemd/system/dnf-automatic.timer/usr/lib/systemd/system/dnf-makecache.timerWhat does automatic.timer do? Runs the dnf-automatic service.root# cat /usr/lib/systemd/system/dnf-automatic.service[Unit]Description=dnf automatic# See comment in dnf-makecache.serviceConditionPathExists=!/run/ostree-booted[Service]Type=oneshotNice=19IOSchedulingClass=2IOSchedulingPriority=7Environment=\"ABRT_IGNORE_PYTHON=1\"ExecStart=/usr/bin/dnf-automatic /etc/dnf/automatic.conf --timerOk, let’s start the timer.root# systemctl enable dnf-automatic.timer &amp;&amp; systemctl start dnf-automatic.timerCreated symlink /etc/systemd/system/basic.target.wants/dnf-automatic.timer → /usr/lib/systemd/system/dnf-automatic.timer.Status:root# systemctl status dnf-automatic.timer● dnf-automatic.timer - dnf-automatic timer   Loaded: loaded (/usr/lib/systemd/system/dnf-automatic.timer; enabled; vendor preset: disabled)   Active: active (waiting) since Wed 2017-11-22 07:41:30 EST; 6min ago  Trigger: Thu 2017-11-23 07:43:53 EST; 23h leftNov 22 07:41:30 comput0r systemd[1]: Started dnf-automatic timer.Let’s try running.root# dnf-automaticLast metadata expiration check: 0:13:23 ago on Wed 22 Nov 2017 07:36:25 AM EST.Running transaction checkTransaction check succeeded.Running transaction testTransaction test succeeded.Running transactionThe following updates have been applied on 'comput0r':============================================================================================================================================================================ Package                                    Arch                                 Version                                        Repository                             Size============================================================================================================================================================================Upgrading: git                                        x86_64                               2.14.3-2.fc27                                  updates                               1.1 M git-core                                   x86_64                               2.14.3-2.fc27                                  updates                               4.1 M git-core-doc                               x86_64                               2.14.3-2.fc27                                  updates                               2.3 M openssl                                    x86_64                               1:1.1.0g-1.fc27                                updates                               564 k openssl-libs                               x86_64                               1:1.1.0g-1.fc27                                updates                               1.3 M perl-Git                                   noarch                               2.14.3-2.fc27                                  updates                                68 k xen-libs                                   x86_64                               4.9.0-14.fc27                                  updates                               674 k xen-licenses                               x86_64                               4.9.0-14.fc27                                  updates                               117 kTransaction Summary============================================================================================================================================================================Upgrade  8 PackagesLogs of installed packages are kept.root# tail /var/log/dnf.rpm.log2017-11-22T12:49:56Z INFO Upgraded: xen-libs-4.9.0-14.fc27.x86_642017-11-22T12:49:56Z INFO Upgraded: openssl-1:1.1.0g-1.fc27.x86_642017-11-22T12:49:56Z INFO Cleanup: openssl-1:1.1.0f-9.fc27.x86_642017-11-22T12:49:56Z INFO Cleanup: git-2.14.3-1.fc27.x86_642017-11-22T12:49:56Z INFO Cleanup: git-core-doc-2.14.3-1.fc27.x86_642017-11-22T12:49:56Z INFO Cleanup: git-core-2.14.3-1.fc27.x86_642017-11-22T12:49:56Z INFO Cleanup: xen-libs-4.9.0-13.fc27.x86_642017-11-22T12:49:56Z INFO Cleanup: xen-licenses-4.9.0-13.fc27.x86_642017-11-22T12:49:57Z INFO Cleanup: perl-Git-2.14.3-1.fc27.noarch2017-11-22T12:49:57Z INFO Cleanup: openssl-libs-1:1.1.0f-9.fc27.x86_64So, there you go. Will this computer be more secure now? I hope so. :)"
    } ,
  
    {
      "title"    : "Three Toronto Tech Meetups",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/11/09/three-toronto-tech-meetups.html",
      "date"     : "2017-11-09 00:00:00 -0500",
      "content"  : "Recently I moved from Edmonton to Toronto. There were a lot of reasons to make the move, and I won’t get into them here. Suffice it to say I’m sure I’ll be living in Toronto for the next five to ten years, then maybe, if I’m really lucky, retire to Drumheller, AB or Nelson, BC, a nice small town somewhere, something like that. It’s good to have goals. (Yes the housing market is insane in Toronto, and Ontario is much more expensive overall compared to Alberta…financially speaking moving here was probably a mistake. But so is having dogs.)One of the things I felt would be valuable about living in Toronto is the large information technology community. I was part of a lot of meetups and the like in Edmonton, such as the Python meetup and the Golang meetup. There were several good meetups that I didn’t attend, so it’s not like there was a lack of technology, it just didn’t have the breadth of Toronto. Often it was hard to get enough people to come out to the events…there’s a certain number that is a critical mass of attendees and we often couldn’t hit that number. We had a good location to have those meetups in Startup Edmonton, but attendance could be challenging. Not so in Toronto. (Though getting space here might be next to impossible.)Meetup #1 - Toronto Artificial Intelligence and Deep Learning MeetupThe title of this talk was “How Machine Learning Saved $25 million!”I enjoyed the presentation, as it was more of a practical overview of how to do machine learning from a process standpoint, based on the example of an e-commerce site trying to predict the shipping weight of items, errors in which was costing millions of dollars per year. A real life issue to be sure.Ultimately, I believe the point of the presentation was not a technical one, rather that Machine Learning is really 75% data science, 10% deployment, and 15% actual ML. The ratios aren’t from the presentation, nor accurate, but my point is it’s mostly data science, and then you still have to deploy the ML code somehow (maybe as a H2O.io plain old java object?).I’d say about 120 people attended, and the audience was very diverse in terms of demographics, which was nice to see. It took place at the WeWork space in downtown Toronto, and was organized and sponsored by H2O.io which is some kind of ML and AI toolkit. They were very nice about being sponsors and tried not to tout their product too much. I’m sure it cost $1000 or more to rent the WeWork space, so thanks H2O.io.Meetup #2 - Toronto Pivotal User MeetupI don’t really know anything about the Pivotal product, other than it’s some kind of PaaS based on Cloud Foundry, and they have some kind of configuration management tool, BOSH, that is apparently magic, and that they are a very successful business unit of DELL/EMC or whatever that conglomerate is called. That sentence sounds a little flip, but I really do like that Pivotal exists and I’m happy that they are doing so well in the Global Fortune 500ish space. It says a lot about automation, containers, and just delivering applications. I would imagine that OpenShift is a competitor of sorts, and OpenShift is also doing very well in the enterprise.The talk was by SnappyData. First off, please take what I say here with a grain of salt because I don’t have any experience with Apache Spark. My impression of what Snappy does is allow data to be stored in Spark so that you don’t have to go out to an external cluster, say a Cassandra system, to get data, which reduces complexity and latency. I might be wrong about that but that is certainly my impression from the talk.Snappy is actually based on an existing product called GemFire which apparently has been around for more than a decade and is in production in 1400 customers. I get the impression that this is one of those pieces of software that is quite valuable to those who know they need it. It’s always amazing to me how valuable software can be. There are all these small, hidden companies making tens of millions of dollars of a bunch of C code that a small, long term team has been writing and refactoring for years. But that’s just me speculating based on previous experience.Anyways, a lot of features Snappy has come from Gemfire. But, one of the more interesting things that Snappy can do is statistically sample large data sets and query that instead of the whole data set. This makes the queries much faster. Less accurate, but faster. But often there are situations where we don’t need great accuracy to make decisions or do things, such as visualization. It’s called their “synopsis engine” and seems very promising for quick decision making.Also, I learned about Apache Zeppelin.Meetup #3 - Toronto Enterprise DevOpsNext, I went to the Toronto Enterprise DevOps meeting held at ObjectSharp, who also bought the pizza. (So far every single one has served pizza. This particular pizza was not great lol. But three days of pizza is two days too many.)I really wanted to go to this meetup because it was called “Hashistacking the Enterprise” and I thought this meant that there were enterprises in Toronto that were really into the Hashicorp stack, ie. Vault, Nomad, etc. But the talk, although well done, was really an introduction to Hashicorp projects.The presenter had recently worked at Chef and was a big fan of Inspec so that was also discussed. I like Inspec too but it is difficult to use across a large fleet because you have to replicate the inventory somehow, though that wouldn’t be too hard if you have a centralized CMDB of some kind. But still a pain.I have worked a little bit with Nomad, mostly using it as a distributed cron system which is not really what it is meant for. I have also used Vault a bit, but am having some issues understanding the security model around it handing out certificates. I used to be a big user of Vagrant, but I don’t run VMs on my local workstation any more.One thing I did learn is that Terraform does everything in order, much like Ansible, and opposed to say Puppet which has a compilation step. I should say I have not used Terraform yet. Another interesting point is that it can be used for multiple clouds, as opposed to Cloud Formation or Azure ARM. That point/feature is quite important and I think Terraform is a successful product simply based on that.Their recently released Sentinel product was discussed as well, and I think it is absolutely fascinating as a policy engine, but it is closed source so I doubt I will ever have a chance to use it. Maybe it will spur an open source project that provides similar functionality. If only I was a 10x coder… ;)OpenStack TorontoI am also helping out with the OpenStack Toronto meetup. Space is a challenge.ConclusionToronto has a diverse tech community, but I’m just basing that on three meetups. There are all kinds of people doing all kinds of technology, some advanced, some not, some with new technology, some not. This is great to see.Obviously space is a big issue for having meetups in TO. I am trying to help the Toronto OpenStack meetup, but there are challenges for getting a low cost space that meets all requirements. The meetup space market, just like the housing market, is very difficult to get into."
    } ,
  
    {
      "title"    : "Three Pillars of Modern Networking",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/11/05/three-pillars-of-modern-networking.html",
      "date"     : "2017-11-05 00:00:00 -0400",
      "content"  : "Computer networking is at an interesting state. We have many new interesting technologies and methodologies for building and managing networks, but ultimately those new technologies have not made much of a dent in how the vast majority of computer networking is architectured and operated.I don’t have a classical networking background, no CCIE or anything like that, but I am very interested in new networking technologies and I believe in some cases my lack of traditional training (dare I say “baggage”) allows me a bit more room to operate in terms of adopting new technologies.Ultimately we still do networks with things like ARP, STP, VLANs, LACP and BGP. We connect relatively large broadcast domains together with layer 3 routes managed by BGP. That’s really about it. It does work, but we aren’t really moving ahead, even something like Ipv6 is just not seeing adoption. (There may be valid reasons for that, but I mention it anyways.)In this post I want to talk about three major technologies that can help to improve modern networking:  Network Automation  Software Defined Networking  Intent Based NetworkingNetwork AutomationAt its base, Network Automation is the application of the same configuration management tooling and methodologies as have been applied to server infrastructure, but instead of servers we simply apply them to network switches and routers. That’s about it. Useful? Yes. Revolutionary? No.Ansible, the configuration management tool I usually reach for (which has as many shortcomings as it does valuable features) has probably taken the lead in having the most easily accessible network automation tooling as well as scope of devices it is able to manage. Ansible’s module based approach makes it pretty simple for even novice developers (like myself) to create new modules and thus there are many core modules related to networking in Ansible.Python is a go to language for many people who need to write code but being a developer isn’t necessarily in their job title, people such as network engineers. Python also has a substantial network library called Napalm that can help to abstract many network devices into code.Ultimately, configuration management is probably a stop-gap measure between current methodologies and something better, something that has not necessarily arrived in networking yet. In the area of “server administration” overall configuration management is still valuable, but containers have moved the cutting edge to immutable style deployments based on one-time images as opposed to continuous usually agent based configuration management.Will this happen to networking…unknown. Technically many switches and routers are already configured to utilize an image model of updates and monolithic configuration files, but convergence time could be an issue in devices that are expected never to slow a packet let alone stop accepting a flow, nevermind the absolute fear in the minds of network admins around changes. Some switches, like Arista, can run containers and I expect to see more of that. Further, many large organizations are realizing the millions and millions of lines of unused code on switches and routers are a liability and are requesting modularity from vendors or rolling their own simpler networking operating systems (though companies doing that are the likes of LinkedIn and Google). But I digress.Software Defined NetworkingSoftware Defined Networking (SDN), like many new technologies, was, for a while, considered a potential panacea. Unsurprisingly that did not turn out the way many expected or hoped. However, what is surprising is how little overall traction SDN was able to achieve, and how few SDN startups, if any, succeeded. The joke that SDN means “still don’t know” managed to become reality. Sure we have some successful deployments, but their functionality is limited in terms of the massive scope that SDN originally had.I have been working with SDN for quite some time at about the time when everyone was just starting to realize OpenFlow was not going to win out. Well over 3 years ago now, I designed and deployed one of the first production SDN deployments in Canada when I built a public cloud based on OpenStack which used Midokura’s Midonet product. Midonet is a good example of where the SDN ecosystem has gone as they never quite achieved the success they were looking for and have pivoted into another business model and, I’m afraid, ultimately might not exist.What I like about SDN is that is presented the possibility to create networks that can do anything. Unfortunately, what we ended up with SDN is a bunch of overly complex control planes that are essentially wrappers around VXLAN, BGP, and some kind of distributed state stored in a system like ZooKeeper et al (see OpenContrail, Nuage, Midonet, etc, etc). Because they rely on an underlying physical network (the “underlay”) and universally speak BGP at the edge, all the legacy networking deployments we are used to are still required and instead we just pile on SDN as another layer. (“So now you have two problems/networks.”) I do think overlays are valuable, but the complexity required to get them in this exact context are considerable.Further,  AFAIK, most SDNs are deployed in concert with either OpenStack or VMWare, though VMWare is essentially (practically speaking) enforcing only using their SDN product NSX. I am quite biased in terms of my experience mostly being related to OpenStack, but I have not met anyone doing SDN outside of the context of a virtualization system and usually SDNs are “boxed inside” of the virtualization platform, though they may interact with larger networks at the BGP level for North/South traffic.Frankly I think that we didn’t push hard enough on really rethinking networking as a whole. As well the traditional, dare I say “heritage”, networking is too embedded and difficult to remove, and thus we ended up just piling more layers on top of if. On one hand if it ain’t broke don’t fix it, but on the other we can’t stagnate at the network level, because the network really is a major component of  “the computer.” Are we losing value as a species with our inability to move past IPv4?I look at technologies like Service Function Chaining, which, while still based on the reliability of standard networks, make packets appear on ports as though transported through a wormhole, as a direction that SDN should have gone in. Networks don’t have to look like they do.I still have hope for SDN. :)Intent Based NetworkingIntent Based Networking (IBN) is where SDN was three to five years ago.  IBN is a lot of things, but I believe that first and foremost the idea is that some kind of consumer asks for something, such as transport from A to B, and IBN determines how that is done. This is pretty well described in the paper Intent NBI – Definition and Principles presented by the Open Networking Foundation.Practically speaking it’s an admission that organizations are unwilling to alter or replace their existing legacy infrastructure, or legacy vendor relationships, with SDN. IBN is a higher layer of abstraction working above network automation and seeks to utilize a methodology where the intent of the network (say, to run applications) is described in some kind of language or configuration, and the actual deployment and operations of the network is created from that “intent” but that the actual underlying physical and virtual devices are not relevant.In some situations this simply looks like a piece of software that can abstract the configuration of multiple brownfield systems (all your Juniper and Cisco and Arista routers and switches, different versions and variations thereof) and then (re-?)configure them in such a way as that the network will fulfil the obligations “suggested” by the intent or consumer requests. (See Apstra Networks.) Instead of human beings interpreting business and technical requirements and writing, often by hand, over long periods of time, thousands of configuration files for network devices, and what’s more/worse keeping the connections in their head, the IBN system automates that, in effect forcing us to forget about the how and just focus on the what and why. At least in theory anyways.Machine learning may also be applicable to IBN, so expect to see considerable fanfare around that. Maybe IBNs can the “hallucinate” the network design (ala letsenhance.io product that upscales any JPG to 4K by hallucinating detalis).  Or maybe we can have AI invent new network protocols like AlphaGo created new Go moves.ConclusionSo there you have it. Three technologies that I believe are the current pillars of modern networking. They all have issues, but promise as well. Maybe IBN and Machine Learning will take us to some kind of network nirvana, or maybe it will stagnate like SDN seems to have. Only time will tell."
    } ,
  
    {
      "title"    : "Forms of Service Functioning Chaining and a BigSwitch Example",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/09/25/service-function-chaining-and-bigswitch.html",
      "date"     : "2017-09-25 00:00:00 -0400",
      "content"  : "Service Function Chaining is an important concept in Network Function Virtualization (NFV). But it is also a powerful tool in more generic Software Defined Networking (SDN) as well.I quite like the definition the OpenStack Networking SFC project has/had:  Service Function Chaining is a mechanism for overriding the basic destination based forwarding that is typical of IP networks. It is conceptually related to Policy Based Routing in physical networks but it is typically thought of as a Software Defined Networking technology. It is often used in conjunction with security functions although it may be used for a broader range of features. Fundamentally SFC is the ability to cause network packet flows to route through a network via a path other than the one that would be chosen by routing table lookups on the packet’s destination IP address. It is most commonly used in conjunction with Network Function Virtualization when recreating in a virtual environment a series of network functions that would have traditionally been implemented as a collection of physical network devices connected in series by cables.Essentially, at a high level, what we want to do is replicate several physical network devices connected together in a chain, except do it virtually. A simple example would be a network gateway, firewall, and intrusion detection system, all physical and all connected to one another through direct connections, potentially without even a switch involved, except, and this is important, it’s not done at layer 3, it’s done at layer 1 as a port to port setup.I have been working with SFC for a while and have found that there are three major types:  Layer 3 SFC - Which isn’t what I would actually consider SFC, but often it is done this way, because it is easier  Single Switch Layer 1 SFC - This is what this blog post will discuss in terms of BigSwitch  Multi-switch Layer 1 SFC - This is the Holy Grail of SFC, so to speakBigSwitchBigSwitch is an SDN vendor with an interesting set of features. They provide an operating system that runs on baremetal whitebox switches (Switch Light OS) and a centralized controller. Their focus seems to be around security. I won’t get into the details here, but they have a couple of different product options and what we are using is Big Monitoring Fabric.SDN is an interesting area to work in mostly because there are so many different ways one can implement networking when it is done (almost) completely in software. Certainly most of SDN systems rely on the now nearly standard ASICs in a physical switch, but the network logic is all in software. As I like to say, if we want to do SDN that is Apple Talk over USB we probably could. ;)BigSwitch has an interesting layer 1 style SFC mode, which they call “big chains” or “inline security” or something to that effect. Using their technology one can dynamically insert devices into a chain of physical ports on a physical switch. This is not all that useful from an NFV perspective, as it is limited to a single physical switch and its physical ports, but in terms of a security chain, say one based on the connection of an insecure network (eg. Internet) to a secure one (a DMZ or other internal network), it is useful, certainly more useful than physically plugging and unplugging network cables. At least using BigSwitch we can do these things dynamically through a REST API.Below we can see a “big chain” service chain. To start with there is no service inserted into the chain.After inserting the service, we can see that some kind of device, in this case a firewall, has been dynamically inserted into the layer 1 port to port service chain.It’s important to note that this can be done manually from the web interface, or it can be done via the BigSwitch API. The ability to insert services directly into a layer 1 chain via a REST API is extremely powerful…it’s hard to indicate with text and some pictures exactly how powerful that is. So take my word for it!SwitchyTo accomplish the service insertion, I wrote a quick command line based tool that can access some of the BigSwitch features via the BigSwitch controller’s REST API.It’s written in Python and uses the fun Click library to accomplish most of the structure for the actual commands. BigSwitch has some Python examples which I used to create a little library for some of their Big Chain features. Click is also useful because it is smart enough to be able to use environment variables, so things like usernames and passwords don’t have to be set on each run of the command, but can still be made required by the CLI.$ switchyUsage: switchy [OPTIONS] COMMAND [ARGS]...  bigswitch command line interfaceOptions:  --version          Show the version and exit.  --controller TEXT  Controller IP or URL  [required]  --switch TEXT      Switch ID  [required]  --username TEXT    Controller username  [required]  --password TEXT    Controller password  [required]  --help             Show this message and exit.Commands:  insert-service  insert a service into a chain  list-chains     list all bigswitch chains  list-instances  list all bigswitch instances  list-services   list all bigswitch services  remove-service  remove a service from a chain  show-chain      show a chain and servicesFor example we can list current chains, which show the chain name and which physical ports make up the ingress/egress points:$ switchy list-chainsTEST-CHAIN-03, ethernet1, ethernet2TEST-CHAIN-02, ethernet11, ethernet12TEST-CHAIN-01, ethernet5, ethernet6And list services:$ switchy list-servicesTEST-SERVICE-02TEST-SERVICE-01Or insert a service into a chain:$ switchy insert-service --chain TEST-CHAIN-01 --service TEST-SERVICE-01 --instance 1Inserted service into chainObviously anything one can do with the REST API can be done with Switchy, and of course we could add any logic that we would like into the command line application (which is the fun part of programming).ConclusionLayer 1 port to port SFC is extremely powerful. It’s important to note, however, that the BigSwtich example given here is not really one meant for NFV, as in NFV we have some kind of underlying infrastructure (termed the NFVi by ETSI), such as OpenStack, which manages many, many hypervisors, each with their own virtual switch. SFC that works in a multi-hypervisor/multi-switch environment looks much different than what is shown here, but this IS a stepping stone towards full blown SFC.PS.In this post I’ve taken a very narrow view of what BigSwitch can do. The point of this post was not to review BigSwitch but rather to use it as an example of various ways people implement different kinds of SFC. BigSwitch is a great product and if it sounds interesting to you I highly suggest trying it out. Further to that I have not completely explored everything BigSwitch does nor how it does it."
    } ,
  
    {
      "title"    : "Clean Keystone Catalog URLs",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/08/28/clean-keystone-urls.html",
      "date"     : "2017-08-28 00:00:00 -0400",
      "content"  : "I think the way OpenStack Keystone is deployed demands more consideration than people tend to give it. There are several different architectural models for Keystone and it would be well worth an organizations investment to take some time and think about the possibilities.I find that OpenStack distros (not that there are that many) limit the possibilities of Keystone models. I am not a huge fan of distros, but I can understand why organizations use them. All I mean is that distros tend to limit architectural possibilities. OpenStack is so flexible that it seems a shame to limit it. But enough ranting…Split KeystoneI tend to use a split keystone, one internal and one external. I’ve written about it before. I think this model can lead to a soft-center security issue, but its flexibility in terms of what catalog is presented to the end user makes up for it, IMHO.Clean URLsBecause I have an external Keystone I can provide a different catalog to external end users.$ os catalog list+--------------------+-----------+------------------------------------------------------------------------------------------+| Name               | Type      | Endpoints                                                                                |+--------------------+-----------+------------------------------------------------------------------------------------------+| Image Service      | image     | someregion                                                                                     ||                    |           |   admin: https://api.somecloud.com/image/                                           ||                    |           | someregion                                                                                     ||                    |           |   public: https://api.somecloud.com/image/                                          ||                    |           | someregion                                                                                     ||                    |           |   internal: https://api.somecloud.com/image/                                        ||                    |           |                                                                                          || Compute Service    | compute   | someregion                                                                                     ||                    |           |   admin: https://api.somecloud.com/compute/v2.1/7b61f0aece1b4aa896020f51fd724b1f    ||                    |           | someregion                                                                                     ||                    |           |   public: https://api.somecloud.com/compute/v2.1/7b61f0aece1b4aa896020f51fd724b1f   ||                    |           | someregion                                                                                     ||                    |           |   internal: https://api.somecloud.com/compute/v2.1/7b61f0aece1b4aa896020f51fd724b1f ||                    |           |                                                                                          || Network Service    | network   | someregion                                                                                     ||                    |           |   admin: https://api.somecloud.com/network/                                         ||                    |           | someregion                                                                                     ||                    |           |   public: https://api.somecloud.com/network/                                        ||                    |           | someregion                                                                                     ||                    |           |   internal: https://api.somecloud.com/network/                                      ||                    |           |                                                                                          || Identity Service   | identity  | someregion                                                                                     ||                    |           |   admin: https://api.somecloud.com/identity/v3                                      ||                    |           | someregion                                                                                     ||                    |           |   public: https://api.somecloud.com/identity/v3                                     ||                    |           | someregion                                                                                     ||                    |           |   internal: https://api.somecloud.com/identity/v3                                   ||                    |           |                                                                                          || Compute Service V3 | computev3 | someregion                                                                                     ||                    |           |   admin: https://api.somecloud.com/computev3/v3                                     ||                    |           | someregion                                                                                     ||                    |           |   public: https://api.somecloud.com/computev3/v3                                    ||                    |           | someregion                                                                                     ||                    |           |   internal: https://api.somecloud.com/computev3/v3                                  ||                    |           |                                                                                          |+--------------------+-----------+------------------------------------------------------------------------------------------+As can be seen above, there are no ports other than 443/https being used, nor any api-someserver.somecloud.com hostnames either. Everything is on the same hostname and then each service is identified using the /servicetype/ model. I think it makes for cleaner URLs.How Is This Accomplished?There are a few things that have to be altered to accomplish this:  Use an external Keystone that reports a different catalog  Setup keystone.conf to set the admin_endpoint and public_endpoint to the external identiy endpoint, but only on the external Keystone nodes  Disable catalog caching (not all caching just catalog) if both external and internal Keystones are using the same memcache  Configure your loadbalancer to recognize /servicetype/ and forward to a proper backend  Configure your loadbalancer to strip the /servicetype/ when forwarding to the backend serviceHaproxy ExampleForwarding is based on path_beg. Below Glance is used as an example.acl frontend-external-glance path_beg /image/use_backend backend-external-glance if frontend-external-glanceHere’s an example of haproxy config to strip the /servicetype/ in the backend definition:reqrep ^([^\\ ]*\\ /)image[/]?(.*)     \\1\\2The above would have to be setup for each service in the external catalog.ConclusionI’ve been testing this for a while and can’t find any issues (yet). There may be better ways to do this that I’m not aware of, and if so please let me know.I’m very happy with these clean URLs and I feel like it makes OpenStack look a bit less complicated to those who might be writing code using these APIs."
    } ,
  
    {
      "title"    : "Create an OpenStack Load Balancer",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/08/15/use-openstack-loadbalancer.html",
      "date"     : "2017-08-15 00:00:00 -0400",
      "content"  : "In a recent post I discussed the OpenStack Octavia project which provides a backend to Neutron’s LBaaS system.In this post I’ll just go over a quick example of creating and using a load balancer and using it in OpenStack.The official docs are pretty good on this, but it’s worthwhile to cover them in a real environment.Create Two WebserversI’ve already created two web servers, web-1 and web-2.$ os server list | grep web-| 05f0bb14-2895-4c7b-a493-ef2a1b57c721 | web-2 | ACTIVE  | vxlan1=10.50.0.15 | xenial     || 3ff621cd-46cb-4fcf-8d9d-2193718a11f5 | web-1 | ACTIVE  | vxlan1=10.50.0.35 | xenial     |I’ve also changed each of the /var/www/html/index.html pages to have only the hostname in it, so that if you connect to port 80 on either of them they respond with their hostname.Create a Load BalancerFirst, we create a load balancer and put it onto a specific network.$ neutron lbaas-loadbalancer-create --name web-lb vxlan1-subnetCreated a new loadbalancer:+---------------------+--------------------------------------+| Field               | Value                                |+---------------------+--------------------------------------+| admin_state_up      | True                                 || description         |                                      || id                  | 6b9c75c1-4af2-424d-8b20-681926de4e0d || listeners           |                                      || name                | web-lb                               || operating_status    | OFFLINE                              || pools               |                                      || provider            | octavia                              || provisioning_status | PENDING_CREATE                       || tenant_id           | 3b29434130cd487186f1da0b5831232f     || vip_address         | 10.50.0.11                           || vip_port_id         | b88dd055-e963-4227-86e1-558df52dc946 || vip_subnet_id       | 5ce133ce-cce6-4142-89d4-a71da87bbde6 |+---------------------+--------------------------------------+Show the newly created loadbalancer. Note the “octavia” provider, and the IP the load balancer received.$ neutron lbaas-loadbalancer-show web-lb+---------------------+--------------------------------------+| Field               | Value                                |+---------------------+--------------------------------------+| admin_state_up      | True                                 || description         |                                      || id                  | 6b9c75c1-4af2-424d-8b20-681926de4e0d || listeners           |                                      || name                | web-lb                               || operating_status    | ONLINE                               || pools               |                                      || provider            | octavia                              || provisioning_status | ACTIVE                               || tenant_id           | 3b29434130cd487186f1da0b5831232f     || vip_address         | 10.50.0.11                           || vip_port_id         | b88dd055-e963-4227-86e1-558df52dc946 || vip_subnet_id       | 5ce133ce-cce6-4142-89d4-a71da87bbde6 |+---------------------+--------------------------------------+There is also an existing security group called web that allows port 80.$ os security group list | grep web| db3f7e2d-2453-416d-8651-ba8544502d0f | web           | web                    | 3b29434130cd487186f1da0b5831232f |The security group must be added to the LB VIP port which was shown above in the LB show.$ neutron port-update --security-group web b88dd055-e963-4227-86e1-558df52dc946Updated port: b88dd055-e963-4227-86e1-558df52dc946Now to create a listener.$ neutron lbaas-listener-create --name web-lb-http --loadbalancer web-lb --protocol HTTP --protocol-port 80Created a new listener:+---------------------------+------------------------------------------------+| Field                     | Value                                          |+---------------------------+------------------------------------------------+| admin_state_up            | True                                           || connection_limit          | -1                                             || default_pool_id           |                                                || default_tls_container_ref |                                                || description               |                                                || id                        | 364c08e7-29a6-4c02-b6e6-7b0d18f3e10e           || loadbalancers             | {\"id\": \"6b9c75c1-4af2-424d-8b20-681926de4e0d\"} || name                      | web-lb-http                                    || protocol                  | HTTP                                           || protocol_port             | 80                                             || sni_container_refs        |                                                || tenant_id                 | 3b29434130cd487186f1da0b5831232f               |+---------------------------+------------------------------------------------+Create a pool.$ neutron lbaas-pool-create \\&gt;   --name web-lb-pool-http \\&gt;   --lb-algorithm ROUND_ROBIN \\&gt;   --listener web-lb-http \\&gt;   --protocol HTTPCreated a new pool:+---------------------+------------------------------------------------+| Field               | Value                                          |+---------------------+------------------------------------------------+| admin_state_up      | True                                           || description         |                                                || healthmonitor_id    |                                                || id                  | 19e86ff3-aa58-4582-8399-4ad3c9eaeb9d           || lb_algorithm        | ROUND_ROBIN                                    || listeners           | {\"id\": \"364c08e7-29a6-4c02-b6e6-7b0d18f3e10e\"} || loadbalancers       | {\"id\": \"6b9c75c1-4af2-424d-8b20-681926de4e0d\"} || members             |                                                || name                | web-lb-pool-http                               || protocol            | HTTP                                           || session_persistence |                                                || tenant_id           | 3b29434130cd487186f1da0b5831232f               |+---------------------+------------------------------------------------+Add the first member.$ neutron lbaas-member-create \\&gt;     --subnet vxlan1-subnet \\&gt;     --address 10.50.0.35 \\&gt;     --protocol-port 80 \\&gt;     web-lb-pool-httpCreated a new member:+----------------+--------------------------------------+| Field          | Value                                |+----------------+--------------------------------------+| address        | 10.50.0.35                           || admin_state_up | True                                 || id             | 38fb7a37-24f4-4fc5-a50b-4e21502d239e || name           |                                      || protocol_port  | 80                                   || subnet_id      | 5ce133ce-cce6-4142-89d4-a71da87bbde6 || tenant_id      | 3b29434130cd487186f1da0b5831232f     || weight         | 1                                    |+----------------+--------------------------------------+Add the second member.$ neutron lbaas-member-create \\&gt;     --subnet vxlan1-subnet \\&gt;     --address 10.50.0.15 \\&gt;     --protocol-port 80 \\&gt;     web-lb-pool-httpCreated a new member:+----------------+--------------------------------------+| Field          | Value                                |+----------------+--------------------------------------+| address        | 10.50.0.15                           || admin_state_up | True                                 || id             | 02074bde-90b2-4c0d-bfb1-2bb2e215d7fb || name           |                                      || protocol_port  | 80                                   || subnet_id      | 5ce133ce-cce6-4142-89d4-a71da87bbde6 || tenant_id      | 3b29434130cd487186f1da0b5831232f     || weight         | 1                                    |+----------------+--------------------------------------+Assign a Floating IP to the Load BalancerA floating IP has already been created.$ neutron floatingip-list+--------------------------------------+------------------+---------------------+---------+| id                                   | fixed_ip_address | floating_ip_address | port_id |+--------------------------------------+------------------+---------------------+---------+| 9e048a99-8533-4dc6-9e6f-1347c09d28e9 |                  | &lt;floating ip&gt;       |         |+--------------------------------------+------------------+---------------------+---------+Actually associate the floating IP. In this case we are associating a floating IP with a port instead of a “server.”neutron floatingip-associate 9e048a99-8533-4dc6-9e6f-1347c09d28e9 b88dd055-e963-4227-86e1-558df52dc946Associated floating IP 9e048a99-8533-4dc6-9e6f-1347c09d28e9Testing a Load balancerNow that the floating IP is up and attached to a working load balancer, and behind it is a couple of working web servers…$ while true; do curl &lt;floating ip&gt;; sleep 1; doneweb-2web-1web-2web-1web-2web-1web-2web-1^"
    } ,
  
    {
      "title"    : "Install the Ocatvia Loadbalancing System into an OpenStack Cloud",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/08/11/install-openstack-octavia-loadbalancer.html",
      "date"     : "2017-08-11 00:00:00 -0400",
      "content"  : "This post discusses OpenStack as Infrastructure as a Service (IaaS) and–although not completely–how to deploy the Octavia load balancing system into an OpenStack cloud. To be forthcoming, I don’t think you could get Octavia running just by reading this post, and it wasn’t necessarily the reason for it, but there might be some helpful notes and links somewhere in this text.The Usefulness of Load Balancers in IaaSMost public clouds have a load balancing service. In Amazon Web Services (AWS) it’s the Elastic Load Balancer service. In Google’s Cloud, it’s the Google Cloud Load Balancer. Azure also has a load balancer. Fortunatley so does OpenStack.Technically a load balancer service is not necessary to run an OpenStack cloud, but for all intents and purposes in order to deploy applications on top of OpenStack in a “cloud native” fashion, load balancers are a practical requirement.OpenStack and Load Balancer as a ServicePeople who run OpenStack clouds have some choices they can make in terms of how the LBaaS operates within their cloud. One of the newer methods for providing LBaaS in an OpenStack cloud is the Octavia project.  Octavia accomplishes its delivery of load balancing services by managing a fleet of virtual machines, containers, or bare metal servers—collectively known as amphorae—which it spins up on demand. This on-demand, horizontal scaling feature differentiates Octavia from other load balancing solutions, thereby making Octavia truly suited “for the cloud.”Service Tenant / Service VM ModelOne of the things that I like most about Octavia is the concept of the “service tenant” or “service virtual machine.” It’s important that the systems underpinning an OpenStack cloud be scalable. The way that Octavia approaches this is to create a specific virtual machine (or machines) for each load balancer that is created, and this VM is only used by a specific tenant. Essentially, hidden behind the scenes is a service tenant that is used to create virtual machines for end-user services.For example, if LBaaS + Octavia has already been deployed and we create a load balancer, we can see the virtual machine running in the Octavia service tenant. (This VM is not visible or accessible by the actual end user, other than as a load balancer endpoint.)# NOTE: As the octavia service tenant user...$ openstack server list+--------------------------------------+----------------------------------------------+--------+--------------------------------------------------+------------+| ID                                   | Name                                         | Status | Networks                                         | Image Name |+--------------------------------------+----------------------------------------------+--------+--------------------------------------------------+------------+| 9135a037-d67d-450e-b809-5a28a26b8b74 | amphora-3521b6fa-1824-448d-85d0-5f0cf1e87f67 | ACTIVE | test-vxlan=10.50.0.30; octavia-mgmt=172.16.31.20 |            |+--------------------------------------+----------------------------------------------+--------+--------------------------------------------------+------------+EnvironmentThe environment I’m working in is an OpenStack deployment based on Ubuntu Xenial/16.04 and the Ubuntu cloud archive packages (all though Octavia is not yet packaged and Octavia is installed from pip), and the version of OpenStack deployed is Ocata.Octavia and LBaaS  DocumentationAt the time I’m writing this blog post, the documentation for Octavia is not that detailed. In order to deploy Neutron LBaaS one would probably have to read the code for some deployment tools such as the devstack plugin for Octavia or the OpenStack Ansible projects role for Octavia. I would suggest that the Ansible role provides the most detailed information, especially their configuration file template.  Neutron documentation  Octavia Documentation  Devstack Octavia Plugin Script  OpenStack Ansible Octavia RolePre-deployment StepsTo use Octavia and Neutron a few things have to exist. Of course these could (and should) be provisioned through your automation system.The following needs to be done:  Create Octavia Neutron management network  Create Octavia service user  Create an Amphora image  Upload the Amphora image into Glance  Create Octavia security group  Create certificates  (Optional) Create SSH keys for admin level troubleshooting Amphora imagesOnce these tasks have been completed, Octavia and Neutron LBaaS can be deployed.Neutron ConfigurationBasically the process is to configure Neutron to provide the load balancing API, and then setup Octavia to be the back end for that API.There are three files that I have altered from a standard deployment. I believe some of them could be converged, but I kind of like the separation.  neutron.conf  neutron_lbaas.conf  services_lbaas.confI’ve also altered /etc/default/neutron-server to start neutron-server with the additional config files.neutron-api:/etc/neutron# cat /etc/default/neutron-server# defaults for neutron-server# path to config file corresponding to the core_plugin specified in# neutron.confNEUTRON_PLUGIN_CONFIG=\"/etc/neutron/plugins/ml2/ml2_conf.ini\"DAEMON_ARGS=\"$DAEMON_ARGS --config-file=/etc/neutron/neutron_lbaas.conf --config-file=/etc/neutron/services_lbaas.conf\"neutron-api:/etc/neutron# ps ax | grep neutron-server | head -1  538 ?        Ss     1:16 /usr/bin/python /usr/bin/neutron-server --config-file=/etc/neutron/neutron.conf --config-file=/etc/neutron/plugins/ml2/ml2_conf.ini --config-file=/etc/neutron/neutron_lbaas.conf --config-file=/etc/neutron/services_lbaas.conf --log-file=/var/log/neutron/neutron-server.logneutron.confThe first is to add the LBaaS service plugin to neutron.conf.service_plugins = router,neutron_lbaas.services.loadbalancer.plugin.LoadBalancerPluginv2neutron_lbaas.confAlso there is the addition of the neutron_lbaas.conf configuration file.[service_providers]service_provider = service_provider = LOADBALANCERV2:Octavia:neutron_lbaas.drivers.octavia.driver.OctaviaDriver:default[service_auth]auth_url = http://&lt;internal keystone endpoint&gt;:5000/v3admin_user = octaviaadmin_tenant_name = serviceadmin_password = &lt;octavia password&gt;admin_user_domain = defaultadmin_project_domain = defaultregion = tor1auth_version = 3endpoint_type = internalURL# NOTE(curtis): not sure of service name???#service_name = LBaaS# Disable server certificate verification (boolean value)#insecure = falseservices_lbaas.confneutron-api:/etc/neutron# grep -v \"^#\\|^$\" services_lbaas.conf[DEFAULT][haproxy][octavia]base_url = http://&lt;Octavia API or internal VIP&gt;:9876[radwarev2][radwarev2_debug]Update the Neutron DatabaseOnce Neutron has been configured to provide the LBaaS API, the database needs some “migrations.”neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade headEnsure neutron-lbaasv2-agent is Stopped and DisabledIt’s not used with Octavia and must not be running.Setup OctaviaBesides Neutron changes, Octavia also needs to be installed and configured.Ubuntu does not have packages for Octavia yet, so Octavia code will be installed via Pip. I’m using 0.10.0. (One can find all the release versions for OpenStack projects here.)# pip install ocatavia==0.10.0Pip installs Octavia services in /usr/local/bin/:# ls -1 /usr/local/bin/octavia-*/usr/local/bin/octavia-api/usr/local/bin/octavia-db-manage/usr/local/bin/octavia-health-manager/usr/local/bin/octavia-housekeeping/usr/local/bin/octavia-workerRunning those services would require some sort of init mechanism, such as systemd but that’s beyond the scope of this blog post. Suffice it to say, octavia-api, octavia-health-manager, octavia-housekeeping, and octavia-worker need to be running somewhere.Configure octavia.confThe octavia conifguration file is fairly complex. One of the best examples is the OpenStack Ansible Octavia Role’s octavia.conf template. Unfortunately it’s beyond the scope of this blog post to completely detail the Octavia config file, and it will take some consideration to get correct for your particular environment.As another example, I deployed a devstack instance based on the Pike release and put up the resulting octavia.conf in a Github gist.Create CertificatesIn the Octavia git repository there is a create_certificates.sh script which can be used to generate (example, non-prod) certificates. Production deployments would probably require some consideration in terms of certificate management.ConclusionHopefully this blog post has provided some useful information, and can get you on your way towards implementing Octavia and OpenStack LBaaS.Please feel free to email me (curtis at serverascode.com ) or perhaps add a comment to the post if you have any questions, notice any mistakes, or have improvements that could be made.Thanks for reading."
    } ,
  
    {
      "title"    : "Install easy-rsa on Ubuntu",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/07/28/easy-rsa.html",
      "date"     : "2017-07-28 00:00:00 -0400",
      "content"  : "At some point, usually multiple points, a sysadmin/operator/devops/whatever needs a certificate authority (CA). At first this seems easy, then it seems hard, then you think you know what you are doing but you don’t, and I’m not sure you ever do. But you still need that CA. Even if you are using some sort of fancy certificate managment system (such as Hashicorp Vault) you still probably need to manage your top level CA.Perhaps easy-rsa is the answer? Certainly it is “an answer.” :)InstallAs usual I am using Ubuntu 16.04/Xenial and there does seem to be a package for easy-rsaeasyrsa$ sudo apt install easy-rsaeasyrsa$ dpkg --list easy-rsaDesired=Unknown/Install/Remove/Purge/Hold| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)||/ Name                                  Version                 Architecture            Description+++-=====================================-=======================-=======================-===============================================================================ii  easy-rsa                              2.2.2-2                 all                     Simple shell based CA utilitySo we have version 2.2.2-2.Presumably we have an easyrsa command?easyrsa:~/vurt$ which easy-rsaeasyrsa:~/vurt$ which easyrsaoops no.Maybe there are some docs somewhere.easyrsa:~/vurt$ locate easy-rsa | grep README/usr/share/doc/easy-rsa/README-2.0.gz/usr/share/doc/easy-rsa/README.DebianAha!easyrsa:~/vurt$ cat /usr/share/doc/easy-rsa/README.Debianeasy-rsa for Debian-------------------easy-rsa is a set of scripts to easy the administration of a CertificateAuthority. For example to manage openvpn scripts.The effortless way to use it is calling \"make-cadir DIRECTORY\", which willcreate a new directory with symlinks to the scripts and a copy of theconfiguration files so you can edit them to suit your needs.i.e.~$ make-cadir my_ca~$ cd my_ca~/my_ca$ vi vars -- Alberto Gonzalez Iniesta &lt;agi@inittab.org&gt;  Mon, 12 Nov 2012 18:18:57 +0100Improving security of created certificates------------------------------------------easy-rsa defaults use 2048 bits for keylength and 10 years (3650 days) ascertificate lifetime.bettercrypto.org suggests increasing the keylength to 4096 bits and decreasingthe certificate lifetime. You can change those values in the 'vars' file of yourCA directory. -- Alberto Gonzalez Iniesta &lt;agi@inittab.org&gt;  Tue, 07 Jan 2014 12:36:35 +0100make-cadirSeems like make-cadir is the way to go.easyrsa:~$ make-cadir my_caeasyrsa:~$ cd my_ca/easyrsa:~/my_ca$ lsbuild-ca  build-inter  build-key-pass    build-key-server  build-req-pass  inherit-inter  openssl-0.9.6.cnf  openssl-1.0.0.cnf  revoke-full  varsbuild-dh  build-key    build-key-pkcs12  build-req      What’s in vars?ubuntu@vc-tor1-2-easyrsa:~/my_ca$ grep -v \"^#\\|^$\" varsexport EASY_RSA=\"`pwd`\"export OPENSSL=\"openssl\"export PKCS11TOOL=\"pkcs11-tool\"export GREP=\"grep\"export KEY_CONFIG=`$EASY_RSA/whichopensslcnf $EASY_RSA`export KEY_DIR=\"$EASY_RSA/keys\"echo NOTE: If you run ./clean-all, I will be doing a rm -rf on $KEY_DIRexport PKCS11_MODULE_PATH=\"dummy\"export PKCS11_PIN=\"dummy\"export KEY_SIZE=2048export CA_EXPIRE=3650export KEY_EXPIRE=3650export KEY_COUNTRY=\"US\"export KEY_PROVINCE=\"CA\"export KEY_CITY=\"SanFrancisco\"export KEY_ORG=\"Fort-Funston\"export KEY_EMAIL=\"me@myhost.mydomain\"export KEY_OU=\"MyOrganizationalUnit\"export KEY_NAME=\"EasyRSA\"Let’s change those.ubuntu@vc-tor1-2-easyrsa:~/my_ca$ diff vars vars.orig64,69c64,69&lt; export KEY_COUNTRY=\"CA\"&lt; export KEY_PROVINCE=\"AB\"&lt; export KEY_CITY=\"Edmonton\"&lt; export KEY_ORG=\"Serverascode\"&lt; export KEY_EMAIL=\"curtis@serverascode.com\"&lt; export KEY_OU=\"OpenStack\"---&gt; export KEY_COUNTRY=\"US\"&gt; export KEY_PROVINCE=\"CA\"&gt; export KEY_CITY=\"SanFrancisco\"&gt; export KEY_ORG=\"Fort-Funston\"&gt; export KEY_EMAIL=\"me@myhost.mydomain\"&gt; export KEY_OU=\"MyOrganizationalUnit\"After a bit of googling this seems to be a good set of docs for using this version of easy-rsa.Source vars and clean all.ubuntu@vc-tor1-2-easyrsa:~/my_ca$ . varsNOTE: If you run ./clean-all, I will be doing a rm -rf on /home/ubuntu/my_ca/keysubuntu@vc-tor1-2-easyrsa:~/my_ca$ ./clean-allBuild the CA.ubuntu@vc-tor1-2-easyrsa:~/my_ca$ ./build-caGenerating a 2048 bit RSA private key........................................+++............+++writing new private key to 'ca.key'-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [CA]:State or Province Name (full name) [AB]:Locality Name (eg, city) [Edmonton]:Organization Name (eg, company) [Serverascode]:Organizational Unit Name (eg, section) [OpenStack]:Certifcate AuthorityCommon Name (eg, your name or your server's hostname) [Serverascode CA]:Name [EasyRSA]:Email Address [curtis@serverascode.com]:That creates this directory and the files in it:ubuntu@vc-tor1-2-easyrsa:~/my_ca$ ls keys/ca.crt  ca.key  index.txt  serialBuild an intermediate key.ubuntu@vc-tor1-2-easyrsa:~/my_ca$ ./build-inter interGenerating a 2048 bit RSA private key.............................................+++........+++writing new private key to 'inter.key'-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [CA]:State or Province Name (full name) [AB]:Locality Name (eg, city) [Edmonton]:Organization Name (eg, company) [Serverascode]:Organizational Unit Name (eg, section) [OpenStack]:Intermediate CACommon Name (eg, your name or your server's hostname) [inter]:Name [EasyRSA]:Email Address [curtis@serverascode.com]:Please enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:An optional company name []:Using configuration from /home/ubuntu/my_ca/openssl-1.0.0.cnfCheck that the request matches the signatureSignature okThe Subject's Distinguished Name is as followscountryName           :PRINTABLE:'CA'stateOrProvinceName   :PRINTABLE:'AB'localityName          :PRINTABLE:'Edmonton'organizationName      :PRINTABLE:'Serverascode'organizationalUnitName:PRINTABLE:'Intermediate CA'commonName            :PRINTABLE:'inter'name                  :PRINTABLE:'EasyRSA'emailAddress          :IA5STRING:'curtis@serverascode.com'Certificate is to be certified until Jul 26 18:32:25 2027 GMT (3650 days)Sign the certificate? [y/n]:y1 out of 1 certificate requests certified, commit? [y/n]yWrite out database with 1 new entriesData Base UpdatedBuild a certificate request and sign it.ubuntu@vc-tor1-2-easyrsa:~/my_ca$ ./build-key mycertGenerating a 2048 bit RSA private key..+++.............................................+++writing new private key to 'mycert.key'-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [CA]:State or Province Name (full name) [AB]:Locality Name (eg, city) [Edmonton]:Organization Name (eg, company) [Serverascode]:Organizational Unit Name (eg, section) [OpenStack]:Common Name (eg, your name or your server's hostname) [mycert]:Name [EasyRSA]:Email Address [curtis@serverascode.com]:Please enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:An optional company name []:Using configuration from /home/ubuntu/my_ca/openssl-1.0.0.cnfCheck that the request matches the signatureSignature okThe Subject's Distinguished Name is as followscountryName           :PRINTABLE:'CA'stateOrProvinceName   :PRINTABLE:'AB'localityName          :PRINTABLE:'Edmonton'organizationName      :PRINTABLE:'Serverascode'organizationalUnitName:PRINTABLE:'OpenStack'commonName            :PRINTABLE:'mycert'name                  :PRINTABLE:'EasyRSA'emailAddress          :IA5STRING:'curtis@serverascode.com'Certificate is to be certified until Jul 26 16:34:19 2027 GMT (3650 days)Sign the certificate? [y/n]:y1 out of 1 certificate requests certified, commit? [y/n]yWrite out database with 1 new entriesData Base UpdatedConclusionThis was just a quick exploration of easy-rsa. I should note that the version available by default on Ubuntu 16.04 is a bit older. In another post I’ll explore a more recent version of easy-rsa."
    } ,
  
    {
      "title"    : "Setting up a Sensu Slack Handler",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/06/10/slack-handler-sensu.html",
      "date"     : "2017-06-10 00:00:00 -0400",
      "content"  : "I’ve been working on a Sensu install recently. I had some trouble getting the Slack handler working. The docs were a little off in terms of how to do this. But I can now say that it’s working. :)First, you will need the sensu-plugins-slack plugin.I have Sensu embedded, so I use that gem to install it. It’s already been installed below; I’m just showing the command.# /opt/sensu/embedded/bin/gem install sensu-plugins-slackYou can use the embedded Ruby by setting EMBEDDED_RUBY=true in /etc/default/sensuSuccessfully installed sensu-plugins-slack-1.0.0Parsing documentation for sensu-plugins-slack-1.0.0Installing ri documentation for sensu-plugins-slack-1.0.0Done installing documentation for sensu-plugins-slack after 0 seconds1 gem installedNext, login to your slack account and create a webhook.Now to configure it to be used.# cat /etc/sensu/conf.d/slack_handler.json{  \"handlers\": {    \"slack\": {      \"type\": \"pipe\",      \"command\": \"handler-slack.rb\",      \"severites\": [\"critical\", \"unknown\"]      }    },    \"slack\": {      \"webhook_url\": \"&lt;your slack webhook url&gt;\",      \"template\" : \"\",      \"channel\": \"#general\"    }}I have also setup Slack to be a default handler.# cat /etc/sensu/conf.d/default_handlers.json{  \"handlers\": {    \"default\": {      \"type\": \"set\",      \"handlers\": [        \"slack\"      ]    }  }}Once that is setup and Sensu restarted, you should be able to get alerts into a Slack channel.Happy Sensu Slacking!"
    } ,
  
    {
      "title"    : "Setting the Default MTU in Neutron VXLAN Networks to be 1500",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/06/06/neutron-vxlan-tenant-mtu-1500.html",
      "date"     : "2017-06-06 00:00:00 -0400",
      "content"  : "Dealing with MTU issues is no fun. They are hard to diagnose. One of the issues I have commonly had is when I create a Docker node in a tenant VXLAN based Neutron network in an OpenStack cloud, and the interface in the virtual machine gets a MTU of 1450 (default 1500 - 50 for VXLAN) but Docker sets up an interface with an MTU of 1500. This will fail in all kinds of ugly ways that aren’t obvious, unless you know what MTU issues look like.Below I show the docker0 interface on a docker node that is running in a default Neutron VXLAN. Note 1500 on the docker0 interface and 1450 on ens3.$ ip ad sh docker03: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default    link/ether 02:42:90:79:85:26 brd ff:ff:ff:ff:ff:ff    inet 172.17.0.1/16 scope global docker0       valid_lft forever preferred_lft forever    inet6 fe80::42:90ff:fe79:8526/64 scope link       valid_lft forever preferred_lft forever$ ip ad sh ens32: ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc pfifo_fast state UP group default qlen 1000    link/ether fa:16:3e:39:e5:c7 brd ff:ff:ff:ff:ff:ff    inet 10.50.0.18/24 brd 10.50.0.255 scope global ens3       valid_lft forever preferred_lft forever    inet6 fe80::f816:3eff:fe39:e5c7/64 scope link       valid_lft forever preferred_lft foreverSo how do we change this?  Ensure the physical network has an MTU &gt; 1550.  Make a few configuration changes to Neutron.Physical networkWhichever interface (usually a bond) that holds the “underlay” tunnel IP address, ie. the one that Open vSwitch listens on, needs to have an MTU &gt; 1550. Usually people set it to “jumbo frames”, ie. 9000 or higher.Say you have some Juniper switches, the MTU of the bond interface can be set to have an MTU of 9200. We’d want to do that for all the interfaces that are part of the tunnel/underlay setup.switch# set interfaces ae0 mtu 9200# commit etcswitch&gt; show interfaces ae0 | match MTU  Link-level type: Ethernet, MTU: 9200, Speed: 2Gbps, BPDU Error: None, MAC-REWRITE Error: None, Loopback: Disabled, Source filtering: Disabled, Flow control: Disabled,Now that the physical infrastructure will do &gt; 1550 we can configure Neutron.Configure Neutron to Set the MTU of VXLANs to 1500First, I should note this is an Otaka based cloud I’m working on.I want a couple things:  VXLAN interfaces in instances to have an MTU of 1500  Provider networks to also have a MTU of 1500 so that they can communicate with the InternetTo do this I set three variables in Neutron configuration files:  neutron.conf: [DEFAULT] - global_physnet_mtu = 1550  ml2_conf.ini: [DEFAULT] - path_mtu = 1550  ml2_conf.ini: [DEFAULT] - physical_network_mtus = provider:1500Where the “provider” in “provider:1500” is the name of your bridge mapping.Once this change is made and Neutron services restarted VXLAN networks should show an MTU of 1500.$ os network show test-vxlan | grep mtu| mtu                       | 1500     New instances will get an MTU of 1500. Old instances should get it if their dhcp client is restarted or they are rebooted.# After a reboot$ ip ad sh ens32: ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether fa:16:3e:39:e5:c7 brd ff:ff:ff:ff:ff:ff    inet 10.50.0.18/24 brd 10.50.0.255 scope global ens3       valid_lft forever preferred_lft forever    inet6 fe80::f816:3eff:fe39:e5c7/64 scope link       valid_lft forever preferred_lft foreverConclusionOverall, it would just be easier on everyone if the default MTU ended up being 1500 in Neutron VXLAN networks, so I think everyone should make a change like this to their OpenStack cloud. Otherwise inevitably things will fail in weird ways, if for no other reason then everything seems to expect an MTU of 1500."
    } ,
  
    {
      "title"    : "Build a Docker Private Registry with Self-Signed SSL",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/06/05/docker-private-registry-with-ssl.html",
      "date"     : "2017-06-05 00:00:00 -0400",
      "content"  : "Recently I’ve been getting back into Kubernetes, which, for the time being, uses Docker as the underlying container CRUD system. At some point when using k8s one will likely need a private Docker registry.Frankly the hardest part of this is getting the SSL certificates to work.Assumptions  Ubuntu 16.04  Docker is installed, in this post it’s 17.03.1~ce-0~ubuntu-xenialCreate SSL CertificatesI’m not 100% sure of the model I’m using to create the SSL certificates, but it is working with Docker. I don’t know if you’d want to use it in production. :)First, I create a OpenSSL configuration file called ca.conf. You might want to edit the distinguished name variables as well as the CN and alt_names.[req]distinguished_name = req_distinguished_namereq_extensions  = v3_reqx509_extensions = v3_caprompt = no[req_distinguished_name]C = CAST = AlbertaL = EdmontonO = Example.comOU = CACN = ca.example.com[v3_req]keyUsage = keyEncipherment, dataEncipherment, keyCertSignextendedKeyUsage = serverAuthsubjectAltName = @alt_names[ v3_ca ]subjectKeyIdentifier=hashauthorityKeyIdentifier=keyid:always,issuerbasicConstraints = CA:true[alt_names]DNS.1 = ca.example.comNext I also created a server.conf OpenSSL config file. You will want to change the CN and IP.1 or DNS.1. I am using an IP.[req]distinguished_name = req_distinguished_namex509_extensions = v3_reqprompt = no[req_distinguished_name]C = CAST = AlbertaL = EdmontonO = Example.comOU = DockerCN = registry.example.com[v3_req]keyUsage = keyEncipherment, dataEnciphermentextendedKeyUsage = serverAuthsubjectAltName = @alt_namesbasicConstraints = CA:FALSE[alt_names]#DNS.1 = registry.example.comIP.1 = &lt;ip of registry server&gt;Now that we have those configured, we can run this script. Note that this will update the local CA repository and restart Docker.#!/bin/bashset -eopenssl genrsa -out ca-privkey.pem 2048openssl req -config ./ca.conf -new -x509 -key ca-privkey.pem \\     -out cacert.pem -days 365openssl req -config ./server.conf -newkey rsa:2048 -days 365 \\     -nodes -keyout server-key.pem -out server-req.pemopenssl rsa -in server-key.pem -out server-key.pemopenssl x509 -req -in server-req.pem -days 365 \\      -CA cacert.pem -CAkey ca-privkey.pem \\      -set_serial 01 -out server-cert.pem  \\      -extensions v3_req \\      -extfile server.confecho \"INFO: print cacert.pem...\"openssl x509 -text -in cacert.pem -nooutecho \"INFO: print server-req.pem...\"openssl req -text -in server-req.pem -nooutecho \"INFO: print server-cert.pem...\"openssl x509 -text -in server-cert.pem -nooutopenssl verify -verbose -CAfile ./cacert.pem server-cert.pemecho \"INFO: updating local CA...\"# Have to use .crt file name for update command to worksudo cp cacert.pem /usr/local/share/ca-certificates/cacert.crtsudo update-ca-certificatesecho \"INFO: restarting docker\"sudo service docker restartThe cacert.pem file would need to be distributed to all hosts that would use the private repository.Create a Docker registryThe Docker documentation has an example of doing this.docker run -d -p 5000:5000 --restart=always --name registry \\  -v `pwd`/certs:/certs \\  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/server-cert.pem \\  -e REGISTRY_HTTP_TLS_KEY=/certs/server-key.pem \\  registry:2The above assumes the certs that were created with the ssl.sh script are in a certs directory.Push an ImageNow that a docker registry is running, I can push an image to it, and have done so. I find the Docker tagging and pushing system very awkward.$ docker imagesREPOSITORY                    TAG                 IMAGE ID            CREATED             SIZE10.70.0.28:5000/static-site   latest              fcaa5e0ee8f2        9 hours ago         109 MBstatic-site                   latest              fcaa5e0ee8f2        9 hours ago         109 MB10.70.0.28:5000/static-site   &lt;none&gt;              60ceae523ef0        14 hours ago        109 MBregistry                      2                   9d0c4eabab4d        3 weeks ago         33.2 MBWithout the right SSL setup I wouldn’t be able to push images.$ docker -D push 10.70.0.28:5000/static-siteThe push refers to a repository [10.70.0.28:5000/static-site]6ce8e637d806: Layer already existsa552ca691e49: Layer already exists7487bf0353a7: Layer already exists8781ec54ba04: Layer already existslatest: digest: sha256:eb52222d9a7e00426ad94eacaf442dd07e52243ecec7f328537515f0b4c035da size: 1155Now I have an internal, private repository that is using SSL so that I don’t have to reconfigure all the Docker nodes to use an insecure repository, though one would still have to ensure the cacert is installed on all the Docker nodes.ConclusionThe hardest part of this is SSL, which I’m sure I’ve done wrong but is working. Please let me know of any ways to do this better. :)"
    } ,
  
    {
      "title"    : "Installing Kubernetes with Kubeadm",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/06/02/kubeadm-openstack.html",
      "date"     : "2017-06-02 00:00:00 -0400",
      "content"  : "I have done a good amount of work with Kubernetes in the last year or so. I created a fairly substantial set of Ansible playbooks and workflows, called Sk8ts, which would deploy Kubernetes to AWS. It would create networks, gateways, instances, setup clusters, etc. But to be honest I only went about 85% as far as I should have and ran out of time to spend on it. Further, perhaps creating my own distribution is not a great idea, though I certainly learned a lot about Ansible, AWS, and k8s.I mention my work in Sk8ts because it was essentially a 3rd party installer or distribution. I also need to add to the context of this post the fact that I have spent years working on OpenStack, which does not have a specific, project led installer, and some people consider this to be a problem. Whether or not large, complicated infrastructure systems like OpenStack and Kubernetes have official installers is a bit of a conundrum.While OpenStack does not have an official installer, Kubernetes does: Kubedadm. So in this post I will look at deploying Kubernetes 1.6 with Kubeadm. Please note that Kubeadm is not production ready yet. But someday…Create hostsK8s needs somewhere to run. I have an OpenStack cloud that I can create networks and instances in.I’ve created four nodes to deploy k8s to. I’ve done this a few times so I kept the command around. For reference it’s below. I’ll use the first node as the master and the other three as the workers. The m1.medium flavor just has 4GB of memory, so they are not that large resource-wise.#!/bin/bashNET=cee24724-e062-4370-ba9f-57bed80f32cdopenstack server create \\--image xenial \\--key-name curtis \\--flavor m1.medium \\--min 4 \\--max 4 \\--nic net-id=$NET \\k8sJust note that that will boot four instances. :)Setup DockerOnce the instances have been created, we can install the k8s and docker packages.$ os server list+--------------------------------------+-------+--------+---------------------------------------------------+------------+| ID                                   | Name  | Status | Networks                                          | Image Name |+--------------------------------------+-------+--------+---------------------------------------------------+------------+| 5da0a8b9-9635-47ba-b381-f3f10b569523 | k8s-4 | ACTIVE | k8s-vxlan=10.50.0.16                             | xenial     || b033b2f6-b7b1-4f62-81c6-cc486320880a | k8s-3 | ACTIVE | k8s-vxlan=10.50.0.13                             | xenial     || 9a4f75d9-20ba-4be0-8daf-7b9a5b6ae289 | k8s-2 | ACTIVE | k8s-vxlan=10.50.0.17                             | xenial     || edfccc19-98da-463a-b0d4-a779ff19e12a | k8s-1 | ACTIVE | k8s-vxlan=10.50.0.11                             | xenial     |+--------------------------------------+-------+--------+---------------------------------------------------+------------+Above are the four k8s-x instances. Now I’ll ssh into k8s-1 and install the k8s and docker packages. To do the install I’ll just a script.ubuntu@k8s-1:~$ cat kube-install.sh#!/bin/bashapt-get updateapt-get install \\    apt-transport-https \\    ca-certificates \\    curl \\    software-properties-commonapt-add-repository \\   \"deb http://apt.kubernetes.io/ kubernetes-xenial main\"add-apt-repository \\   \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\   $(lsb_release -cs) \\   stable\"apt-get updateapt-get install docker-ce -y --allow-unauthenticatedapt-get install -y kubelet kubeadm kubectl kubernetes-cni --allow-unauthenticatedThat will insecurely install various packages. I’m not getting any GPG keys, etc.ubuntu@k8s-1:~$ sudo bash kube-install.shsudo: unable to resolve host k8s-1Get:1 http://security.ubuntu.com/ubuntu xenial-security InRelease [102 kB]SNIP!Setting up docker-ce (17.03.1~ce-0~ubuntu-xenial) ...Processing triggers for libc-bin (2.23-0ubuntu7) ...Processing triggers for systemd (229-4ubuntu17) ...Processing triggers for ureadahead (0.100.0-19) ...Reading package lists... DoneBuilding dependency tree       Reading state information... DoneThe following additional packages will be installed:  ebtables socatThe following NEW packages will be installed:  ebtables kubeadm kubectl kubelet kubernetes-cni socat0 upgraded, 6 newly installed, 0 to remove and 10 not upgraded.Need to get 43.2 MB of archives.After this operation, 323 MB of additional disk space will be used.WARNING: The following packages cannot be authenticated!  kubernetes-cni kubelet kubectl kubeadmAuthentication warning overridden.Get:5 http://nova.clouds.archive.ubuntu.com/ubuntu xenial/main amd64 ebtables amd64 2.0.10.4-3.4ubuntu1 [79.6 kB]Get:6 http://nova.clouds.archive.ubuntu.com/ubuntu xenial/universe amd64 socat amd64 1.7.3.1-1 [321 kB]Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubernetes-cni amd64 0.5.1-00 [5,560 kB]Get:2 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubelet amd64 1.6.4-00 [18.3 MB]Get:3 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubectl amd64 1.6.4-00 [9,659 kB]Get:4 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubeadm amd64 1.6.4-00 [9,234 kB]Fetched 43.2 MB in 4s (10.4 MB/s)    Selecting previously unselected package ebtables.(Reading database ... 54160 files and directories currently installed.)Preparing to unpack .../ebtables_2.0.10.4-3.4ubuntu1_amd64.deb ...Unpacking ebtables (2.0.10.4-3.4ubuntu1) ...Selecting previously unselected package kubernetes-cni.Preparing to unpack .../kubernetes-cni_0.5.1-00_amd64.deb ...Unpacking kubernetes-cni (0.5.1-00) ...Selecting previously unselected package socat.Preparing to unpack .../socat_1.7.3.1-1_amd64.deb ...Unpacking socat (1.7.3.1-1) ...Selecting previously unselected package kubelet.Preparing to unpack .../kubelet_1.6.4-00_amd64.deb ...Unpacking kubelet (1.6.4-00) ...Selecting previously unselected package kubectl.Preparing to unpack .../kubectl_1.6.4-00_amd64.deb ...Unpacking kubectl (1.6.4-00) ...Selecting previously unselected package kubeadm.Preparing to unpack .../kubeadm_1.6.4-00_amd64.deb ...Unpacking kubeadm (1.6.4-00) ...Processing triggers for systemd (229-4ubuntu17) ...Processing triggers for ureadahead (0.100.0-19) ...Processing triggers for man-db (2.7.5-1) ...Setting up ebtables (2.0.10.4-3.4ubuntu1) ...update-rc.d: warning: start and stop actions are no longer supported; falling back to defaultsSetting up kubernetes-cni (0.5.1-00) ...Setting up socat (1.7.3.1-1) ...Setting up kubelet (1.6.4-00) ...Setting up kubectl (1.6.4-00) ...Setting up kubeadm (1.6.4-00) ...Processing triggers for systemd (229-4ubuntu17) ...Processing triggers for ureadahead (0.100.0-19) ...Nice. Now we have all the k8s packages and Docker installed. I should note that the Docker version we are getting is perhaps not supported by k8s. I believe k8s is only validated on Docker 1.11 or 1.12. Frankly I’m not sure how to get that version any more, as Docker has split out into a community and enterprise versions. The k8s install does seem to work with this version though.ubuntu@k8s-1:~$ dpkg --list docker-ceDesired=Unknown/Install/Remove/Purge/Hold| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)||/ Name                                  Version                 Architecture            Description+++-=====================================-=======================-=======================-===============================================================================ii  docker-ce                             17.03.1~ce-0~ubuntu-xen amd64                   Docker: the open-source application container engineSo I’m getting 17.02-1-ce…??? Honestly, I don’t know what that version means.Installing the k8s masterNow I can use kubeadm.ubuntu@k8s-1:~$ sudo kubeadm initsudo: unable to resolve host k8s-1[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.[init] Using Kubernetes version: v1.6.4[init] Using Authorization mode: RBAC[preflight] Running pre-flight checks[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.03.1-ce. Max validated version: 1.12[preflight] WARNING: hostname \"k8s-1\" could not be reached[preflight] WARNING: hostname \"k8s-1\" lookup k8s-1 on 10.50.0.1:53: no such host[certificates] Generated CA certificate and key.[certificates] Generated API server certificate and key.[certificates] API Server serving cert is signed for DNS names [k8s-1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.50.0.11][certificates] Generated API server kubelet client certificate and key.[certificates] Generated service account token signing key and public key.[certificates] Generated front-proxy CA certificate and key.[certificates] Generated front-proxy client certificate and key.[certificates] Valid certificates and keys now exist in \"/etc/kubernetes/pki\"[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/admin.conf\"[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/kubelet.conf\"[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/controller-manager.conf\"[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/scheduler.conf\"[apiclient] Created API client, waiting for the control plane to become ready[apiclient] All control plane components are healthy after 23.025086 seconds[apiclient] Waiting for at least one node to register[apiclient] First node has registered after 4.505916 seconds[token] Using token: bdc910.dac015f93ad5a064[apiconfig] Created RBAC rules[addons] Created essential addon: kube-proxy[addons] Created essential addon: kube-dnsYour Kubernetes master has initialized successfully!To start using your cluster, you need to run (as a regular user):  sudo cp /etc/kubernetes/admin.conf $HOME/  sudo chown $(id -u):$(id -g) $HOME/admin.conf  export KUBECONFIG=$HOME/admin.confYou should now deploy a pod network to the cluster.Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:  http://kubernetes.io/docs/admin/addons/You can now join any number of machines by running the following on each nodeas root:  kubeadm join --token bdc910.dac015f93ad5a064 10.50.0.11:6443There are a bunch of containers running.ubuntu@k8s-1:~$ sudo docker pssudo: unable to resolve host k8s-1CONTAINER ID        IMAGE                                                                                                                            COMMAND                  CREATED             STATUS              PORTS               NAMESbf36a19d1d61        gcr.io/google_containers/kube-proxy-amd64@sha256:44cc08e7e8a2089eb8dfad6b692e9ece5994d6e6cff07fc9e9b1273cab0f6c6a                \"/usr/local/bin/ku...\"   2 minutes ago       Up 2 minutes                            k8s_kube-proxy_kube-proxy-jvdkl_kube-system_fbc037b7-4864-11e7-acb2-fa163ef42293_09bda7bb1a3f2        gcr.io/google_containers/pause-amd64:3.0                                                                                         \"/pause\"                 2 minutes ago       Up 2 minutes                            k8s_POD_kube-proxy-jvdkl_kube-system_fbc037b7-4864-11e7-acb2-fa163ef42293_0d5a926f598ef        gcr.io/google_containers/kube-scheduler-amd64@sha256:57661c79890b01ef2ff183ed4b467ca470efc4fb8d0517cd29abe49e72f6d904            \"kube-scheduler --...\"   2 minutes ago       Up 2 minutes                            k8s_kube-scheduler_kube-scheduler-k8s-1_kube-system_3145edd89dab0492bdacc0dd589d0e90_095faeb5d116b        gcr.io/google_containers/kube-controller-manager-amd64@sha256:a93d4c26d71de94861f78cf5ea62600e4952685d580e2774c630ea206b7c18ee   \"kube-controller-m...\"   2 minutes ago       Up 2 minutes                            k8s_kube-controller-manager_kube-controller-manager-k8s-1_kube-system_8d185204c4cf91dd9e76230d0642391b_0fc4c977e5061        gcr.io/google_containers/etcd-amd64@sha256:d83d3545e06fb035db8512e33bd44afb55dea007a3abd7b17742d3ac6d235940                      \"etcd --listen-cli...\"   2 minutes ago       Up 2 minutes                            k8s_etcd_etcd-k8s-1_kube-system_7075157cfd4524dbe0951e00a8e3129e_0c3d248897b53        gcr.io/google_containers/kube-apiserver-amd64@sha256:6d5aa429c2b0806e4b6d1d179054d6deee46eec0aabe7bd7bd6abff97be36ae7            \"kube-apiserver --...\"   2 minutes ago       Up 2 minutes                            k8s_kube-apiserver_kube-apiserver-k8s-1_kube-system_76f5cdc7dab34e6c8b32d96a42cc51e8_08482b6284833        gcr.io/google_containers/pause-amd64:3.0                                                                                         \"/pause\"                 2 minutes ago       Up 2 minutes                            k8s_POD_kube-scheduler-k8s-1_kube-system_3145edd89dab0492bdacc0dd589d0e90_04016d11d968d        gcr.io/google_containers/pause-amd64:3.0                                                                                         \"/pause\"                 2 minutes ago       Up 2 minutes                            k8s_POD_kube-controller-manager-k8s-1_kube-system_8d185204c4cf91dd9e76230d0642391b_0ebc0ef82e638        gcr.io/google_containers/pause-amd64:3.0                                                                                         \"/pause\"                 2 minutes ago       Up 2 minutes                            k8s_POD_kube-apiserver-k8s-1_kube-system_76f5cdc7dab34e6c8b32d96a42cc51e8_0045d7c8d75ba        gcr.io/google_containers/pause-amd64:3.0                                                                                         \"/pause\"                 2 minutes ago       Up 2 minutes                            k8s_POD_etcd-k8s-1_kube-system_7075157cfd4524dbe0951e00a8e3129e_0Install Networking PluginNow we need a networking plugin. By default kubeadm is ready to use weave. This is amazingly simple.root@k8s-1:/etc/kubernetes# kubectl --kubeconfig ./admin.conf apply -f https://git.io/weave-kube-1.6clusterrole \"weave-net\" createdserviceaccount \"weave-net\" createdclusterrolebinding \"weave-net\" createddaemonset \"weave-net\" createdThis will modify the networking on the host.root@k8s-1:/etc/kubernetes# ip ad sh1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc pfifo_fast state UP group default qlen 1000    link/ether fa:16:3e:f4:22:93 brd ff:ff:ff:ff:ff:ff    inet 10.50.0.11/24 brd 10.50.0.255 scope global ens3       valid_lft forever preferred_lft forever    inet6 fe80::f816:3eff:fef4:2293/64 scope link       valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default    link/ether 02:42:59:52:32:1d brd ff:ff:ff:ff:ff:ff    inet 172.17.0.1/16 scope global docker0       valid_lft forever preferred_lft forever4: datapath: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1376 qdisc noqueue state UNKNOWN group default qlen 1    link/ether a2:29:39:a0:df:49 brd ff:ff:ff:ff:ff:ff    inet6 fe80::a029:39ff:fea0:df49/64 scope link       valid_lft forever preferred_lft forever6: weave: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1376 qdisc noqueue state UP group default qlen 1000    link/ether 9a:80:36:0d:7c:64 brd ff:ff:ff:ff:ff:ff    inet 10.32.0.1/12 scope global weave       valid_lft forever preferred_lft forever    inet6 fe80::9880:36ff:fe0d:7c64/64 scope link       valid_lft forever preferred_lft forever7: dummy0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000    link/ether ee:ec:e0:cc:a1:9e brd ff:ff:ff:ff:ff:ff9: vethwe-datapath@vethwe-bridge: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1376 qdisc noqueue master datapath state UP group default qlen 1000    link/ether 2e:9b:d3:2f:66:21 brd ff:ff:ff:ff:ff:ff    inet6 fe80::2c9b:d3ff:fe2f:6621/64 scope link       valid_lft forever preferred_lft forever10: vethwe-bridge@vethwe-datapath: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1376 qdisc noqueue master weave state UP group default qlen 1000    link/ether 9e:1d:61:4f:c1:71 brd ff:ff:ff:ff:ff:ff    inet6 fe80::9c1d:61ff:fe4f:c171/64 scope link       valid_lft forever preferred_lft forever11: vxlan-6784: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 65485 qdisc noqueue master datapath state UNKNOWN group default qlen 1000    link/ether 8e:12:6f:d6:0c:1d brd ff:ff:ff:ff:ff:ff    inet6 fe80::8c12:6fff:fed6:c1d/64 scope link       valid_lft forever preferred_lft forever13: vethweplc205ec0@if12: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1376 qdisc noqueue master weave state UP group default    link/ether 62:6f:d0:66:4a:2b brd ff:ff:ff:ff:ff:ff link-netnsid 0    inet6 fe80::606f:d0ff:fe66:4a2b/64 scope link       valid_lft forever preferred_lft foreverNote the weave components.There are also weave containers created.root@k8s-1:/etc/kubernetes# docker ps | grep weavefa0eaddf9b6e        weaveworks/weave-npc@sha256:d4b37edd345b42fdc4cd4fdc9398233db035916c7ad04f2a99fb8230b1d2f6e9                                     \"/usr/bin/weave-npc\"     About a minute ago   Up About a minute                       k8s_weave-npc_weave-net-8n654_kube-system_889073fd-4865-11e7-acb2-fa163ef42293_0f3e22468fc86        weaveworks/weave-kube@sha256:0445da5b752a50133133e2d4d6383e622f4a06a3c744268740238c23ae05c594                                    \"/home/weave/launc...\"   About a minute ago   Up About a minute                       k8s_weave_weave-net-8n654_kube-system_889073fd-4865-11e7-acb2-fa163ef42293_03953f0b070dd        gcr.io/google_containers/pause-amd64:3.0                                                                                         \"/pause\"                 About a minute ago   Up About a minute                       k8s_POD_weave-net-8n654_kube-system_889073fd-4865-11e7-acb2-fa163ef42293_0Add K8s workersI’ll ssh into the other nodes and install the k8s and docker packages.ubuntu@k8s-2:~$ sudo bash kube-install.shSNIP!Now they can join via kubeadm.ubuntu@k8s-2:~$ sudo   kubeadm join --token bdc910.dac015f93ad5a064 10.50.0.11:6443sudo: unable to resolve host k8s-2[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.[preflight] Running pre-flight checks[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.03.1-ce. Max validated version: 1.12[preflight] WARNING: hostname \"k8s-2\" could not be reached[preflight] WARNING: hostname \"k8s-2\" lookup k8s-2 on 10.50.0.1:53: no such host[discovery] Trying to connect to API Server \"10.50.0.11:6443\"[discovery] Created cluster-info discovery client, requesting info from \"https://10.50.0.11:6443\"[discovery] Cluster info signature and contents are valid, will use API Server \"https://10.50.0.11:6443\"[discovery] Successfully established connection with API Server \"10.50.0.11:6443\"[bootstrap] Detected server version: v1.6.4[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)[csr] Created API client to obtain unique certificate for this node, generating keys and certificate signing request[csr] Received signed certificate from the API server, generating KubeConfig...[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/kubelet.conf\"Node join complete:* Certificate signing request sent to master and response  received.* Kubelet informed of new secure connection details.Run 'kubectl get nodes' on the master to see this machine join.We can see there are two nodes now.root@k8s-1:/etc/kubernetes# kubectl --kubeconfig ./admin.conf get nodesNAME      STATUS    AGE       VERSIONk8s-1     Ready     16m       v1.6.4k8s-2     Ready     5m        v1.6.4Now I’ll add the other nodes.root@k8s-1:/etc/kubernetes# kubectl --kubeconfig ./admin.conf get nodesNAME      STATUS     AGE       VERSIONk8s-1     Ready      21m       v1.6.4k8s-2     Ready      10m       v1.6.4k8s-3     Ready      1m        v1.6.4k8s-4     NotReady   7s        v1.6.4Great, now we have a k8s cluster of four nodes that was deployed by kubeadm.Deploy sock-shopSo how do we know this is even working? Lets deploy the socks shop app.root@k8s-1:/etc/kubernetes# kubectl --kubeconfig ./admin.conf create namespace sock-shopnamespace \"sock-shop\" createdroot@k8s-1:/etc/kubernetes# kubectl --kubeconfig ./admin.conf apply -n sock-shop -f \"https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true\"Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl applynamespace \"sock-shop\" configurednamespace \"zipkin\" createddeployment \"carts-db\" createdservice \"carts-db\" createddeployment \"carts\" createdservice \"carts\" createddeployment \"catalogue-db\" createdservice \"catalogue-db\" createddeployment \"catalogue\" createdservice \"catalogue\" createddeployment \"front-end\" createdservice \"front-end\" createddeployment \"orders-db\" createdservice \"orders-db\" createddeployment \"orders\" createdservice \"orders\" createddeployment \"payment\" createdservice \"payment\" createddeployment \"queue-master\" createdservice \"queue-master\" createddeployment \"rabbitmq\" createdservice \"rabbitmq\" createddeployment \"shipping\" createdservice \"shipping\" createddeployment \"user-db\" createdservice \"user-db\" createddeployment \"user\" createdservice \"user\" createdthe namespace from the provided object \"zipkin\" does not match the namespace \"sock-shop\". You must pass '--namespace=zipkin' to perform this operation.the namespace from the provided object \"zipkin\" does not match the namespace \"sock-shop\". You must pass '--namespace=zipkin' to perform this operation.the namespace from the provided object \"zipkin\" does not match the namespace \"sock-shop\". You must pass '--namespace=zipkin' to perform this operation.the namespace from the provided object \"zipkin\" does not match the namespace \"sock-shop\". You must pass '--namespace=zipkin' to perform this operation.the namespace from the provided object \"zipkin\" does not match the namespace \"sock-shop\". You must pass '--namespace=zipkin' to perform this operation.This might take a while to complete in terms of downloading docker images and such.We can ask for the port information.root@k8s-1:/etc/kubernetes# kubectl --kubeconfig ./admin.conf -n sock-shop get svc front-endNAME        CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGEfront-end   10.96.97.28   &lt;nodes&gt;       80:30001/TCP   55sWe can access the socks shop page…root@k8s-1:/etc/kubernetes# curl localhost:30001 | head  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;    &lt;meta charset=\"utf-8\"&gt;    &lt;meta name=\"robots\" content=\"all,follow\"&gt;    &lt;meta name=\"googlebot\" content=\"index,follow,snippet,archive\"&gt;    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;    &lt;meta name=\"description\" content=\"WeaveSocks Demo App\"&gt;100  8688  100  8688    0     0   314k      0 --:--:-- --:--:-- --:--:--  326kcurl: (23) Failed writing body (248 != 744)Issues  Initially I tried installing using kubeadm from behind an http proxy, but that brought all kinds of issues, so I gave up.  As mentioned, perhaps should be installing docker 1.12.  Not clear on the zipkin issue with socks-shop  I am confused with regards to how to setup access to deployed applications. With AWS it was straightfoward, configured K8s to create AWS loadbalancers. But in this situation, I’m not sure…yet. :)Conclusionk8s has changed a lot since I was using it in version 1.4. I’m quite behind. :)I’m curious to see if kubeadm will catch on and actually be the best way to deploy and manage k8s. There are many other (competing?) projects.I was inspired to try kubeadm by this heptio blog post in which they discuss how they don’t want to be k8s distribution.  …we need to be careful: distributions can be a dangerous path for a community. Each distributor has strong incentives to deliver differentiated experiences, and differentiated capabilities. As they develop a customer following their customers clamor for features. The community cannot move as fast as they could and so they deliver a patch. And somewhere a fairy dies. The community gets fragmented one really great customer request at a time. You end up with semantic divergence, and a community ‘dark ages’ period happens until a conquering empire emerges to pull it all together.I don’t know if kubeadm can deploy k8s in a way that every single organization will be happy with. But we shall see.At the very least, it’s an easy way to get a test/dev k8s install."
    } ,
  
    {
      "title"    : "Installing Weave Scope into Kubernetes",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/06/02/install-weave-scope.html",
      "date"     : "2017-06-02 00:00:00 -0400",
      "content"  : "In a previous post I installed a k8s cluster using kubeadm.Now I would like to add the Weave Scope application to the cluster so that I can visualize what is going on. I should note that I first saw Scope used in a presentation regarding OpenStack Helm. (Yeah, that’s right, using k8s to manage the OpenStack control plane.)Weave Scope allows you to:  See your Docker hosts, containers and services in real time. Easily identify and correct issues to ensure the stability and performance of your containerized applications.InstallationBasically it’s just a matter of getting the command correct. There are official docs on how to install Weave Scope.$ kubectl --kubeconfig ./admin.conf apply --namespace kube-system -f \"https://cloud.weave.works/k8s/scope.yaml?k8s-service-type=NodePort&amp;k8s-version=$(kubectl --kubeconfig ./admin.conf version | base64 | tr -d '\\n')\"Note that I added the k8s-service-type=NodePort.After a few seconds we can validate the deployment:root@k8s-1:/etc/kubernetes# kubectl --kubeconfig ./admin.conf -n kube-system get svcNAME              CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGEkube-dns          10.96.0.10      &lt;none&gt;        53/UDP,53/TCP   2hweave-scope-app   10.111.34.232   &lt;nodes&gt;       80:31863/TCP    1hAbove we can see what port it’s on, in this case 31863. Note that I’m not exporting this service in any fashion right now, to use Scope I’m connecting my browser directly to node:31863.sock-shopIn the previous post I deployed sock-shop.Here’s a view from Weave Scope of the sock-shop namespace.More ImagesThere are all kinds of handy tools.The Internet.Processes.I’m leaving a lot out.ConclusionI usually don’t reach for visualization tools, but with k8s it was helpful to get a picture of what’s going on. Plus, it’s so simple to install. I really like how services like this are installed into the kube-system namespace, a form of self-hosting. It’s a great model."
    } ,
  
    {
      "title"    : "OpenStack Boston Summit 2017",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/05/13/boston-openstack-summit-2017.html",
      "date"     : "2017-05-13 00:00:00 -0400",
      "content"  : "(A picture I took at the Boston Museum of Fine Arts)I’m not sure how many OpenStack summits I’ve been to, the first one I attended was the San Diego summit. I felt like this was one of the better summits I’ve been to, mostly because I had some colleagues to hang out with. Unfortunately, while other people from my current employer were supposed to attend, due to business reasons they had to cancel. But I was able to have some good discussions with some fellow Canadians (we have slack channel now) as well as my co-presenters for the multi-cloud/mult-site/what-have-you session.NFVI mostly work in the area surrounding “Network Function Virualization” (NFV). If you haven’t been to a summit in a while, I think I can sum up telecommunications companies participation in OpenStack by saying Verizon sent 100 people to the summit. As another example, AT&amp;T is leading the OpenStack Helm project. Yes, AT&amp;T, and according to their presentation on the helm project, they will be replacing all of their current OpenStack control planes with it (presumably moving away from Mirantis Fuel, which itself has been replaced by Mirantis MCP, but I digress).OpenStack Helm - Deploying the OpenStack Control Plane on KubernetesPreviously I wasn’t sure if the OpenStack control plane really needed a full Container Orchestration Engine (COE) like Kubernetes, because it’s hard enough to run OpenStack let alone an underlying (and complex) COE. Personally I usually use LXC based containers. But after seeing some of the work AT&amp;T on the helm project, I think my mind has changed. That said, I am not sure how they are provisioning the actual underlying Kubernetes infrastructure, which I presume is outside of the scope of the Helm project (ie. probably something like kubadm.I wanted to attend the hands on session, but it was at the same time as the telecom/NFV working group session. From this Calico blog post it sounds like they were using Calico in the workshop. I wonder if that is what they are using in production…that would be surprising:  Even more impressively, the AT&amp;T Integrated Cloud team hosted a hands-on workshop with over 100 participants in a packed room also performing the same task of deploying, and upgrading OpenStack on Kubernetes leveraging Helm, and Calico for networking, and completing this lab in mere minutes.Can/should Telecoms act like Facebook? Not everything hyperscalers like Facebook is doing is applicable, but if you are using containers then I don’t think everything should be dealt with on layer 2. We should be using IPv6 with containers. But Calico with IPv4 and treating everything as layer 3 is a good start.Treating infrastructure as software is difficult. It’s not the same as running a web application. There is a lot of hidden state in the underlying infrastructure. However, we cannot continue to treat infrastructure, such as private cloud, as hand-rolled bespoke, manually installed one-off systems. We have to find the right balance between running highly available infrastructure, and treating it like software, with continuous integration, and multiple deployments per day, as well as reconciling the fact that infrastructure will fail and applications have to be “cloud native.” Too far one way, and infrastructure is unstable, too far the other and we end up with set-and-forget.@ Most pop workshop @OpenStack Summit on OpenStack-helm with @v1k0d3n @portdirect. thanks to @kubernetesio  to move OpenStack to next level! pic.twitter.com/cRhoQFZYNO&mdash; Archy (@archyufa) May 11, 2017AT&amp;T: Zones and RegionsFirst, this is something I need to do more research into, because I only heard a tiny snippet of a conversation around this, but, it sounded to me like AT&amp;T was deploying clouds with a region and zone concept, where every region had two (availability?) zones. This would be very similar to what AWS, and any intelligent cloud provider, would do. Presumably the concept is that if one zone fails (which is probably an entire cloud) then the other (geographically close) zone is still running, and whatever application you deploy would exist over both zones.My point is not that this is a new concept, but the fact that a large telecom grasps, and has implemented, zones is impressive because it shows some “cloud building” (sorry) maturity.NFV: Nirvana StackNaming is hard. I don’t know if I like the name of this project, but I can understand where they are coming from. :)Telecoms and groups like OPNFV are coming together to establish some kind of reference stack with OpenStack and other open source systems, mostly around “Sofware Defined Networking” (SDN) plus OpenStack. I could not watch all the presentations, as they had an entire day dedicated to this in the OpenDayLight track, but my impression is that while using OpenStack has become fairly standard to do NFV, implementing an SDN solution is not part of the OpenStack project itself, and typically telecoms want the extra features and capabilities that an SDN solution can provide, but they are often difficult to integrate. So they want to perform some integration work on OpenStack + “Open Source SDN”, like Open Daylight, and this work is under the umbrella of the Nirvana Stack.It makes sense to me as while I have done a fair amount of work with ODL, installing it with OpenStack is not that straight forward. Also, part of the integration would be to identify and help to fix any issues upstream.ATT Lisa Fung gave introduction to Nirvana stack and objectives @OpenDaylightSDN day #OpenStackSummit pic.twitter.com/dW6AmumjzU&mdash; George Zhao (@georgeyzhao) May 11, 2017NFV: ONAPIf you work in the NFV area, you have probably heard about AT&amp;Ts ECOMP project. Basically it’s a massive layer of software (MANO+) that sits above OpenStack performing all kinds of automation. However, ECOMP has now transitioned into being a project named ONAP, and it’s in the process of merging another open source MANO system, Open-O. This transition is ongoing and will take some time, but at this point there is actual working ONAP code!AT&amp;T demoed ONAP (ECOMP) at the summit, and also provide Heat templates which will deploy the ONAP application itself into an OpenStack cloud. (However, I believe at this time the templates are only tested with Rackspace, though any OpenStack cloud (with Heat) would presumably work, perhaps with a little tweaking.)I’m really impressed with AT&amp;Ts willingness to release this code. Combined with their work on projects like OpenStack Helm, they are doing some really great things. Very impressive for such a huge telecom.Telco grade Orchestration ONAP for Openstack Orchestration pic.twitter.com/sBLsB7TeU7&mdash; Sudeep Batra (@sudeepbatra) May 10, 2017Open ContrailContrail is a SDN system from Juniper. They have an open source version called Open Contrail. On Wednesday night they had a Contrail user group meeting at Fenway park.Randy Bias recently joined Juniper to spearhead the Contrail product. For those of you not familiar with Randy, he is usually considered the progenitor of the “Pets vs Cattle” meme (which I don’t like, but sometimes it’s the easiest way to get the point across). So, you know, he like gets cloud and stuff and has for quite some time.He started off the Open Contrail night by saying they (Juniper) were going to totally reboot the community. Everything he said sounded great, and are things that need to happen with Open Contrail, with one exception…he mentioned the term “open core.” I personally don’t think open core really works…but then again I am not much of a business person. That said, I’m not actually clear on what he meant by open core, so we will just have to wait and see. Overall, the changes he mentioned during the talk were all things that need to happen. I’m not sure Juniper corp. will like it much, but he was certainly saying the right things.One of the major items Bias mentioned was that he wants to grow the number of Contrail deployments by an order of magnitude or two. I think that this is something that needs to happen if Contrail is going to be successful. They just don’t have enough deployments right now to get good community contributions and feedback…it’s a single vendor open source project and those rarely work out. It’s not so much that they need to make more money (ie. get more commercial deployments), which of course they do, it’s that they aren’t getting enough feedback and external contributions to improve the product, not enough critical mass.Another great #opencontrail meetup at the #OpenStackSummit by @emaganap good lineup of speakers and their stories at #fenwaypark pic.twitter.com/2xg8yowWUe&mdash; Amit Tank (@amitTank) May 11, 2017Massively Distributed CloudsPart of the presentation I worked on for the summit regarded “massively distributed clouds” (MD). As usual there are many issues around this in terms of naming and use-cases definition. I was in several sessions during the week regarding the phrase “massively distributed” and most of the sessions became quickly confusing to all in attendance. Often telecoms (somehow) know they need massively distributed clouds but they don’t actually know what it means, what is possible with OpenStack now, what isn’t, and what would require new architecture and designs…and most importantly new code.I heard three or four things in these meetings over and over again which are not related to what I would consider massively distributed clouds:  Some people just want massive single data center systems. This is possible now and is not massively distributed. It’s just a large cloud; a buncha compute nodes. If you have 5000 servers in a single data center I don’t think it matters if you have to dedicate 0.5% of them to the control plane. It’s massive, but not massively distributed.  Some organizations want a massive number of distinct, separate clouds. This is possible now. There are not many good options for managing a large number of clouds, and would likely involve some kind of Management and Orchestration (MANO) system and other software to manage and sync them, but I don’t really consider it a massively distributed use case. Large number yes, distributed no.  Another desire is for “zero touch provisioning” in which you have many small clouds or devices and you want a “zero touch” method install and update them. To me this is an important requirement, but it can be solved many ways outside of OpenStack, and would probably be a project of its own (or maybe using Ironic…I don’t know). Hopefully something is happening around this in the IoT area or we are in big trouble security-wise. The point is that we can provide this functionality outside of OpenStack if required.  I also encountered some people, more than one person, OK, two, that was quite sure that if you use Kubernetes or containerize the OpenStack control plane then it somehow gets smaller. I suppose it’s possible, but in general that is not the case. No matter if you run all the of the required control plane services in containers or not they are still taking up resources, need to be managed, etc.Finally we have what some people call “the retail use-case” in which an organization, say, for example, Walmart, has 10,000 stores with a few hypervisors in each. They don’t want to put a full, highly available OpenStack control plane in each and every store, ie. the cloud is made up of remote (or distributed) hypervisors. I believe that if we solve this use case we would solve most related problems, ie. reducing the size of the OpenStack control plane, dealing with latency, etc. To accomplish this OpenStack would have to change considerably. In my mind, this is what massively distributed really means. Sure, you have some other problems to solve (like “zero touch”) but overall if we could solve this problem then many other issues and use-cases fall into place.There’s an etherpad page (essentially online collaborative notes) from one of the sessions, the Birds of a Feather meeting, if you are interested in perusing some of the thoughts around MD.I should also mention that several people from the OpenStack foundation were in the room, including Sparky Collier, Jonathan Bryce, and Lauren Sell. It’s pretty clear to me that the foundation believes MD (or edge, fog, what have you) is important to the future of OpenStack.The ForumA major change to this summit was the creation of The Forum. Previously summits were also well attended by developers and there was an entire section of the summit dedicated to getting developers together discussing their projects. However that work has now been moved to a separate meeting, called the “Project Team Gathering” (PTG), which is not held at the same time and location as the summit. The first one was held in Atlanta in February, and I believe the next one is in Denver. What this means is that there are fewer developers at this summit. Three people mentioned to me that the summit felt more business like, and they didn’t know about the forum component how it had changed the makeup of the attendees.I attended many forum sessions, and they mostly turned out well. Certainly all positive discussions.OpenStack Operators Telecom/NFV Working Group SessionI have been involved with, and currently chair, and OpenStack working group called the OpenStack Operators Telecom/NFV Working Group. I won’t go into too much detail on what this group is about–it’s for people who build and operate NFV related clouds on a day to day basis, and the OpenStack foundation was kind enough to provide us some time and space to meet face to face.There is an etherpad page that was created for this session and it has some notes about what we discussed during the session. Have a look. :)Overall, I made a request to everyone in the room to go back to their respective organizations and seek out people who might fit well into this group and to let them know about it. Hopfully at the next meeting we have some new members!I should also mention another working group in the OpenStack community that has ties to Telecom and NFV: the Large Contributing OpenStack Operators group (otherwise known as the LCOO). I think they missed an opportunity to name themselves the Contributing OpenStack Operators who are Large, ie. COOL, but no one asked me. :)I’ve been working pretty closely with this group for a while, and they have been welcoming and accommodating, as they try to work with various large, usually telecom, companies who want to participate in the OpenStack community but maybe don’t quite know how…yet.ConclusionFor me, the summit was good. I missed hearing about what the developers are doing and working on by being able to sit in on their sessions, but I can understand why the OpenStack Foundation moved to creating the PTG concept. Only time will tell if it can improve the development of OpenStack, but I have a feeling it will. The worst part was that there wasn’t as many free drinks and candy around…only devs get that perk. ;) Devs, devs, devs.Because I work in the NFV area, I was extremely attuned to what is happening around it at the summit, and there is a lot. AT&amp;T, and soon Verizon (among others?) are really trying to participate in the open source community and putting resources into writing open source code. In contrast, other, usually smaller, telecoms are in over their head and will continue to rely on beating up vendors as their main technical strategy. My recommendation to all telecoms would be to invest in people and everything needed to make them successful.It’s going to take telecoms time to figure it all out, but if they do, then they will make a considerable impact on OpenStack over the next few years, and in fact it would not surprise me if NFV became the dominant use case for OpenStack…as telecoms around the world will be deploying thousands of clouds, which honestly I believe is thousands more than what has been deployed in the enterprise.Now I’m looking forward to the OpenStack Operators meetup in August in Mexico City, and the next summit which is in Sydney Australia. It’s a long way to go, but it should be fun."
    } ,
  
    {
      "title"    : "OpenStack Multi-site, Multi-clouds, and Distributed Clouds",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/05/09/openstack-multisite-multicloud.html",
      "date"     : "2017-05-09 00:00:00 -0400",
      "content"  : "One OpenStack cloud is rarely enough. Most organizations that deploy one OpenStack cloud will, at some point, at least think about deploying a second. Some organizations will not only want to depoy a couple clouds, but maybe 10, or 20, or 50…or even more. In some industries, such as telecommunications, organizations may want to deploy hundreds of OpenStack instances. And, as unbelievable as it sounds, some use cases require thousands. But what are the options available when deploying more than one OpenStack cloud? How would we deploy 2, 10, or 100+ clouds?In this post I will go over some options and their pros and cons, as well as consider some of the features and architectural changes OpenStack may need to implement in order to meet the requirements of use cases such as massively distributed clouds.I should also mention that this post is based on a talk I did at the 2017 Boston OpenStack summit with two colleagues, Adrien Lebre and Chaoyi Huang. Adrien is the chair of the “Fog, Edge, and Massively Distributed Working Group” and Chaoyi works on the Tricircle and Kingbird projects and is active in many areas of Network Function Virtualization (NFV). I learned a lot from the two of them while putting together the presentation. I won’t link to the video here, because I’m not a huge fan of being recorded, but a simple Youtube search will find it. :)DefinitionsTerminology for multiple OpenStack clouds is difficult. Some people use different terms for the same thing, and use the same terms for different things. I’ll make a couple of definitions, but be aware that other people might not use them the same way I do, or might disagree with my choices.Regions/Multi-region: When I talk about multi-region, I essentially mean that the OpenStack Keystone database is shared in some fashion.Mult-site: I define multi-site as having multiple OpenStack clouds, but no shared Keystone.Fog/Edge/Massively Distributed: I will use these interchangeably. Essentially it means that we have many small “data centers” (which may not be datacenters at all) and there will be only a few hypervisors located there–the OpenStack control plane will be remote, ie. in a different datacenter. This is for use cases like retail and such where an organization has thousands of locations, and many one or two hypervisors in each location.OpenStack Regions - The Historical MethodI used to work at a public cloud in Canada which was based on OpenStack. We planned on having two OpenStack regions in Canada, one in Vancouver and one in Toronto. OpenStack regions are a good way to manage authentication and authorization for two or three OpenStack clouds (or more, depending on your risk profile). The fact that I mention authorization (authz) and authentication (authn) is important, because in general this is all regions provide–shared identity. Only Keystone (and potentially Glance and Horizon) are suited for being shared across regions. Most of the other services, such as Nova and Neutron, will have completely separate installations in each region, and are not shared.We’ve established that with regions we share Keystone in some fashion. In the image above, we can see a simplistic diagram of a Keystone that is shared across datacenters of a secure, private link. Each datacenter has MySQL Galera nodes that are part of a Galera cluster that exists across multiple datacenters.To share the Keystone across mutliple datacenters you have three main options:Centralized Keystone DB: In this model there is only one centralized Keystone database, and the other regions access the database directly via the WAN link; they have no local Keystone DB.Asyncronous Keystone DB: Here you would have database instances in each region/DC, but only one would be the “master” version (ie. read/write) and the other regions would have their DBs asynchronously updated by the master, ie. they would be secondary databases.Syncronous (Clustered) Keystone DB: Using MySQL/MaraiDB Galera clustering, the Keystone database would be synchronously shared across all regions. This is the model that is usually given as an example of regions.All of these options have pros and cons. Asynchronous is probably the most scalable model, but overall my opinion is that none of these are all that useful. Essentially regions, as I define them, are no longer a good option.The reality is that all a shared Keystone model buys you is shared authz/authn. We still have all kinds of other issues to deal with, from keys to images to determining how to share Neutron tenant networks across clouds. There are things that every OpenStack multi-cloud deployer wants, and regions don’t help with any of that. Further to this, using a shared Keystone has some additional cons, such as operational complexity, and the fact that the Keystone version would probably have to be the same across all clouds, making upgrades slightly more difficult.Nova Cells V2Nova has had a cell model (V1) for quite some time. Essentially it allows the creation of reduced (Nova) failure domains and allows for better scalability. In V2 this is accomplished by adding a nova_api DB to the control plan, and a nova DB in the cell. Also each Cell has its own messaging queue (aka RabbitMQ). However, V1 is not widely used, and in fact I was recently at a OpenStack summit where Nova developers mentioned that they are surprised V1 even works.However, V2 is is now default in OpenStack Ocata+. If you deploy Ocata you will have one cell. However, Cells V2 is not yet done, and in fact you can’t add additional cells. Right now you can have as many cells as you want as long as it’s one. This is expected to change in Pike or Queens. The point is that V2 will now be widely used, and at some point will be usable, and will enable Nova to be highly scalable.Unfortunately, this is just for Nova. I would love to see a similar model applied to all major OpenStack projects. But even if they all decided they wanted to do that, it would probably take a couple years for the changes to make their way into the releases.I’d also like to mention that, at least when Cells V2 is production ready, using it in combination with something like routed provider networks would be extremely powerful. I would consider routed provider networks complimentary to cells. Imagine doing Nova Cells V2 on a per rack basis, and also routed provider networks.Shared LDAPI should note that Keystone can authenticate to a LDAP backend, and that backend could be shared/synced across many clouds. Often this is done with Active Directory (which I know nothing about). But, of course, this just provides shared authn.Shared-Nothing Multi-cloudMirantis has a good blog post on shared-nothing OpenStack multi-cloud. On first blush, it seems overly simplistic. We just deploy a bunch of completely separate clouds.The reality is that you can’t just stop there. In a multi-cloud deployment with shared-nothing we are going to have to rely on some kind of 3rd party abstraction layer that sits above the clouds and manages users, groups, keys, images, etc. There is a lot of work required to implement that kind of model, as at least the 3rd party “cloud broker” must be implemented or developed. But, as Mirantis mentions, there are a lot of pros to using this model.Realistically, if you are deploying 10s or 100s of clouds, this is probably the only model that will work (at least at this time).ETSI MANOOver the last year or so I have been doing a lot of work within the realm of “Network Function Virtualization” (NFV). The European Telecommunications Standards Institution (ETSI) has defined some NFV components. One of those complements is the “Management and Orchestration” (MANO) system. I’m sure there is a proper, specific definition for MANO, but I tend to see it as an amorphous blob that can take any shape and perform any function (ie. the ultimate lock-in).There are several MANO vendors and open source projects. Suffice it to say that overall it seems like the MANO layer would not mind taking complete control of all OpenStack clouds. I’m not so sure that is a good idea, and may not be in OpenStack’s best interest. However, that doesn’t mean it’s not the best solution for telecommunications companies, such as AT&amp;T. That said, MANO systems won’t be able to do absolutely everything without some help from additional projects, especially around networking.OpenStack FederationFederation is a powerful technology that essentially allows one cloud to trust the users of another. The canonical use case for federation is when non-profit or academic institutions would like to enable their cloud users to utilize resources in another organizations cloud (and potentially vice-versa). Further to this, it could also be used as a form of centralized identity.For this blog post I just want to go over some potential solutions, and don’t want to go in depth into any of the solutions in particular. In future blog posts I hope to go into federation more deeply.Tricircle and KingbirdThere are a couple of projects I would like to mention that could be used to help deploy multiple clouds. As mentioned, it’s often desirable to have networking in OpenStack understand multi-cloud. Tricircle helps with this. Further, as mentioned, even if we used regions, we would still have to manage keys, images, and such. This is where Kingbird enters the picture.I can’t comment much more on either of those projects as I have not used them. But, suffice it to say that they are in active development and are very important for organizations such as telecommunications companies who would like to deploy tens or hundreds of clouds.Fog, Edge, and Massively distributedAs previously mentioned, there are use cases, such as retail stores with a couple hypervisors in each store, which we would consider as requiring some kind of massively distributed deployment model. This model does not currently exist, but there is a working group within the OpenStack community that is defining the surrounding use-cases, and over time will begin to define a recommended implementation.It’s also fairly apparent to me that the OpenStack Foundation would like to see this use case supported. It is going to take a considerable amount of work to enable this use case, but over the next few releases I would expect to see it being earnestly discussed and worked on at some level. I’m excited to see increasing interest in the edge model, as to enable it we’ll have to make substantial changes to OpenStack.ConclusionOpenStack has some fairly rich authn/authz models. Unfortunately, my go to multi-cloud model of yesteryear, regions, doesn’t work (IMHO) when you have more than a handful of clouds, and doesn’t provide all that many features anyways. NFV deployments, and deployments requiring massively distributed hypervisors, won’t be able to use regions (not that they would help anyways), so we have to look at other models, some of which don’t yet exist.Likely we will require a combination of tools such as software defined networking, perhaps Tricircle, Kingbird and massively distributed in order to deploy and operate large numbers of clouds. Generally speaking the telecommunications industry is going to drive these requirements as they are the ones that need thousands of tiny clouds or hundreds of small/medium ones.It’s an exciting time in OpenStack, partially because I’m not sure what is going to happen. Will OpenStack pivot to support large NFV deployments? I think they will have to in order to continue to grow, assuming that is a goal. If not, then it is hard to say. Perhaps I’m just in denial that OpenStack just has to be able to provide shared-nothing clouds and some magical MANO or other higher layer abstraction along with SDN solutions and a couple cell-like models will be enough. What’s more, there does seem to be a lot of interest in creating an Open Source MANO solution, for example ONAP."
    } ,
  
    {
      "title"    : "SDN on All Interfaces",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/01/24/sdn-all-host-interfaces.html",
      "date"     : "2017-01-24 00:00:00 -0500",
      "content"  : "Recently I have been involved in some discussions surrounding network segmentation, specifically around the concept of PVLAN…private VLAN. Note that this is different from the idea of a private network, or a tenant network in a virtualization system, and is instead a form of network isolation, and I believe is described in RFC 5517.  Private VLAN, also known as port isolation, is a technique in computer networking where a VLAN contains switch ports that are restricted such that they can only communicate with a given “uplink”. The restricted ports are called “private ports”. Each private VLAN typically contains many private ports, and a single uplink. The uplink will typically be a port (or link aggregation group) connected to a router, firewall, server, provider network, or similar central resource. -  WikipediaPVLAN is based on physical switch ports, and only works on access ports, not trunk ports. This is a problem in environments where we have bonds on hosts that are accessing multiple VLANs and, of course, virtual switches in a virtualization environment. (That said, some distributed virtual switches support PVLANs, but not OVS which is common in most open source virtualization systems, such as OpenStack.)Micro-segmentationWhat we are really talking about, as far as I’m concerned, is micro-segmentation. This concept has become popular recently, mostly due to a desire for zero-trust networks and the proliferation of containers.One way to deploy containers is to give them all an IP, be it IPv4 or IPv6, and to not be concerned as to what IP they get, and where this IP is actually located in the datacenter. This suggests that we need powerful network policies to control what these containers need to connect to, and thus we get micro-segmentation. Frankly I’m not sure how layer 2 networks make sense in a situation where we have zero-trust, but if it does, then the virtualization hosts will have to have their interfaces, especially virtual switches, controlled by SDN, or perhaps some kind of centralized firewall management (where firewall means managing flows, not blocking ports).See this fascinating Facebook presentation on Internet-scale Virtual Networking Using Identifier-Locator Addressing for some mindblowing ideas. cough IPv6 cough. There is a video as well.Zero-trust NetworksEast/west traffic has grown considerably.  The threat from inside is bigger than ever before and is further exacerbated by the fact that around 80 percent of traffic in data centers is now of east-west nature - SDX CentralOrganizations of all kinds are getting cracked left and right, or…er…east and west. Clearly security is broken, most often because we build networks that are secure on the perimeter but wide open internally (also humans make mistakes and write software).  The Zero Trust Model is simple: cybersecurity professionals must stop trusting packets as if they were people. Instead, they must eliminate the idea of a trusted network (usually the internal network) and an untrusted network (external networks). In Zero Trust, all network traffic is untrusted. - ForresterI believe Ronald Reagan popularized the phrase “trust but verify” which unfortunately has become a common refrain, and one I have never liked. Clearly we cannot automatically trust any network activity, and frankly I don’t know how much network verification has ever occurred.Some organizations use PVLAN to try to deter lateral movement of bad actors, but in a modern, virtualized, or even worse, containerized, datacenter PVLAN is of no use. So we need another technology to achieve zero trust.Separation of DutiesClearly one area PVLAN may have an edge is that it is performed by physical network hardware. If micro-segmentation is managed by the local vswitch on a physical host, or (ugh) firewall rules in a VM, then there is not as much separation of duty. Further these systems are typically managed out of band of the network (though things like routing protocols would not be).That said, virtualization is so important that it likely cancels out any security issues; it’s just too useful. Even OpenBSD is getting a hypervisor. If we are going to do large scale virtualization, ie. IaaS, then we have no choice but to trust the hypervisors, and thus the vswitches. If you work in a place that believes hypervisor breakouts are too risky, then I would imagine your infrastructure is quite different than most, and you have unique requirements.Further, it could be that the fact that all hypervisors are secured via micro-segmentation is helpful should some of them become compromised. Unless the centralized “micro-segmentation” provider is also compromised, then all of the other micro-segmentation rules would still be in place, making it hard to hop from one compromised node to the other because the security polices are highly distributed and exist in some fashion on all nodes at all times.Routing on the HostI want to mention “routing on the host” because it is a model that is becoming more popular, though still not that common. The idea is to allow hosts to announce what IPs they own, ie. typically become BGP speakers. This certainly comes with its own set of issues and benefits, which I won’t get into here. Certainly this model would have an effect on implementing micro-segmentation.Some network companies, such as Cumulus Linux, are making routing on the host easier. I should note that I am a big fan of Cumulus, though no one I’ve ever worked with seems to like them. Cumulus, at least started out, trying to make routing on the host more commonplace. But I am not sure how well they have succeeded. They have gone so far as to create a custom Quagga package to enable routing on the host.Project Calico is similar:  Based on the same scalable IP network principles as the Internet, Calico leverages the existing Linux kernel forwarding engine without the need for virtual switches or overlays. Each host propagates workload reachability information (routes) to the rest of the data center – either directly in small scale deployments or via infrastructure route reflectors to reach Internet level scales in large deployments.Calico seems to be picking up steam regarding Kubernetes (k8s), where k8s has made the major architectural decision to give each container its own IP.Virtual Switches are Part of Your Network InfrastructureI think if there is a valuable idea in this post it is to consider virtual switches as part of your network infrastructure. For example in a small OpenStack deployment you might have four physical switches, but then you will have one virtual switch for every OpenStack Neutron DHCP/L3 server and of course all the compute nodes. That adds up to a lot of switches. If you have 100 hypervisors then you have 100+ switches.These vswitches can break–they can get hit by cosmic rays, be attacked by viral switches (I made that up) and run into all kinds of other issues.Interfaces (and Bonds) are Part of Your Network InfrastructureHosts often have bonds that are configured to accept multiple VLANs. We have Link Layer Discovery Protocol (LLDP) to help with this, and to show that the interfaces and bonds are really part of the network infrastructure. Perhaps OVS sits on top of the bonds. Perhaps the bonds are only used for underlying IaaS infrastructure. Perhaps the bonds are Linux bonds or they are OVS bonds or both types exist. I have not tried as of yet, but apparently OVS supports LLDP.Presumably, in a post-VLAN micro-segmentation enabled world, we would need something like OVS to control all interfaces in a physical host, even ones that aren’t supporting virtual machines. Linux bridge isn’t going to cut it. Don’t get me started on MLAG… :)Open vSwitch is Pretty ImportantDoes Open vSwitch support PVLANS? Not right now. Perhaps if OVS did then this whole thing would not even be a question. But to me, even if it did, how would it do it? It would be done with flows. Which you can do now. Further, as discussed in this blog post, in my opinion PVLAN is a heavy handed way to try to enable micro-segmentation, meaning that if we are to do micro-segmentation with OVS then it would likely be considerably more sophisticated than simply emulating PVLAN.In the case of IaaS, virtual switches are in effect distributed and are controlled by a centralized system (for example OpenStack Neutron) or with an SDN controller (perhaps in conjunction with Neutron). If you have hundreds of OVS instances, then they need to be centrally managed. It’s not enough to push a one-time configuration into each vswitch as obviously workloads are being constantly created and destroyed or (yuck) moved.Potential Micro-segmentation SolutionsSecurity Groups for Tenant WorkloadsOne potential solution to micro-segmentation is firewall rules on the host. Or, if some form of segmentation is required in tenant workloads in an IaaS system, then usually they support security groups.Generally speaking, security groups in IaaS are used to specifically allow communication on a layer 2 network, just like any normal network would do, and are typically used to stop external layer 3 ingress connections. Usually the default is to allow all traffic on the local network, which makes sense in most cases. Depending on their implementation they may be able to implement a form of micro-segmentation but occasionally security group functionality is somewhat limited.Host-based FirewallsFor physical hosts, we could setup Iptables rules as a way to achieve micro-segmentation. Perhaps some 3rd party system would manage and deploy Iptables rules. It could be as simple as configuration management tooling, such as Ansible, but the question is what happens on changes, and what is the “single source of truth” for the config tooling to access? For example if a container is created, and it can come up with any IP on any hypervisor, then how will this 3rd party Iptables configurator know what to do and when to do it, unless that system is what is managing the life cycle of containers.However, even without containers, things start to get rapidly ugly, even in a non-hypervisor environment, basically around heterogeneous systems, where there is a lot of variation in things like operating systems. Managing firewall rules on multiple platforms will not be easy.Also, Iptables is global and you will take a performance hit.  There is no concept of per-interface rules in iptables, iptables rules are global. This means that for every packet, incoming interface needs to be checked and execution of rules branched to the set of rules appropriate for the particular interface. This means linear search using interface name matches which is costly, especially with a high number of VMs. - RedHatFirewall Vendors, Service Function ChainingFirewalls will probably never go away. I wish they would…all these middleboxes getting in the way, slowing things down, costing money, etc. Typically firewalls just have two sides: inside and outside. With zero-trust and micro-segregation we are trying to get rid of that false dichotomy.Firewall vendors love the idea of micro-segmentation though. Palo Alto is a popular firewall provider; they have managed to step into the role that Checkpoint used to fill. They have a neat demo using service function chaining and OpenStack to dynamically insert a firewall into a service chain. Very interesting and quite powerful, but also resource intensive. We can’t insert an entire firewall into every flow on the network or drop one in front of every container…maybe one on every host? If the firewall is on every host, then it might as well be the vswitch. Perhaps companies like Palo Alto could either write their own OVS compatible vswitch or there is some kind of a plugin system.We likely need something less resource intensive. While SFC is an important concept for providing tenant security and functionality, it is, however, not as useful for securing the underlying infrastructure, ie. it is an NFV technology as opposed to a datacenter security technology. That said in situations where attacks are detected inserting a powerful firewall or IDS may be valuable even in the DC.Firewalls in the NICsWe are likely to see a surge in specialized hardware, partially due to the slowdown in Moore’s law. This could include having “smart NICs.” VXLAN offloading is common, so why not flows as well.Netronome has some interesting products, though I have not used them. And of course now we’d be relying on firmware which is notoriously painful. However, this could be beneficial in that it is potentially separate(ish) hardware. But that said, I’m not sure how they are programmed or what their control plane looks like…perhaps from the host. :0  Compounding the problem is heightened security, which is moving from a firewall on the perimeter of the network to policy-based security with network rules governing access to virtual machines and now software containers. Das says that it is common to require 1,000 security policy rules per VM in clouds these days, which works out to 48,000 rules for a two-socket server with a total of 48 VMs, and moreover that at some infrastructure running inside of the datacenters in China, a single server might have to juggle as many as 1 million – yes, more than a factor of 20X more – security policy rules. Shifting from VMs to containers, where a single server might have thousands of containers and yet a large number of security policy rules, will make the situation even worse. - The Next Platform1000 rules per VM sounds crazy. But maybe.If we could get higher performance with specialized NICs, regain CPU cores, and still have the ability to easily update and install software on them, then this might be a good way to go about obtaining micro-segmentation.Software Defined NetworkingMy thesis is that if we want micro-segmentation, then they way to do this is in the network using some form of SDN which manages the local virtual switch as well as the underlying physical infrastructure, and hopefully it would utilize IPv6. (Surprisingly IPv6 is probably the hardest part, because few seem to support it.)The prior solutions, mostly utilizing firewall rules in some fashion, were really just ideas/brainstorming if a true SDN solution is not possible for whatever reason.I don’t want to get into what SDN systems might be able to meet these requirements in this post, so for now I will just leave it as a statement to come back to in future writing, especially once I have done some work in the lab.Various Paradoxes, Twiddling with VLANs, and Let’s Do as We Say TooTypically the underlying IaaS infrastructure will be deployed just like we have deployed “enterprise” systems for the last 20 years (minus virtualization). VLANS. Bonds. MLAG. Middleboxes. Manual configuration. Servers or server pairs that can never be down. Hand crafted and bug ridden. Expensive. Instance high availability. Time consuming. Waterfall. Complicated. Tribal knowledge. Security choke points. Hard-candy coating with soft milk chocolate inside. On and on.We put IaaS on top of this old-school enterprise architecture and deployment and force the tenants to do “cloud native.” It just doesn’t seem good or fair to me. If anything this is why private clouds fail. Why do IaaS if it is just going to look like VMWare deployments of the last decade or two? Most of the time the answers are circular: we deploy it this way because nobody else is doing it the other way, so we deploy it this way. See IPv6.PVLAN is a great example of VLAN twiddling. In an era where security is failing, we really need to inhibit malware  and lateral movement in DCs where east/west traffic is growing. Thus, we need to push SDN–not Iptables rules–all the way to host interfaces in order to enable micro-segmentation…somehow…I’m not sure if anyone will actually read this post. I’m sorry it’s kind of long (I left some things out, honestly, like Intent Based Networking for example) but I find this area is fascinating and there are many options. It’s not a solved space. We really need to do something about at least two things 1) enabling zero-trust networks and 2) not continuing to deploy underlying IaaS infrastructure the way we have for the last 20 years: we need to deploy the systems that support the IaaS using cloud native designs as well.Certainly I mention containers a lot in this post, and that makes it easy to dismiss much of what I’m saying because how many organizations are really going to run containers and be successful at it. Not many. Most organizations will still have a majority of workloads that will not fit into the container paradigm. However, they will certainly be using virtualization (really are they that different), and in some cases perhaps be looking at a technology like OpenStack, and in that situation it would be completely possible to work on enabling zero-trust networks and micro-segmentation, likely through and SDN solution. My thesis is that that SDN should not just live only in OpenStack, but should also be part of the underlying IaaS infrastructure. I think that is completely possible. Further to that, it is likely, if not inevitable, and if not already, that the OpenStack control plane will be containerized, meaning that if you run OpenStack you will be running containers, whether you want to or not."
    } ,
  
    {
      "title"    : "OpenDaylight Boron, OpenStack, and Networking-ODL",
      "category" : "",
      "tags"     : "",
      "url"      : "/2017/01/09/opendaylight-boron-openstack-networking-odl.html",
      "date"     : "2017-01-09 00:00:00 -0500",
      "content"  : "Software Defined Networking (SDN), like many other technical terms (cough devops cough) is often considered a panacea, but is also fraught with peril, etc, etc…you can see where I’m going, feel free to complete this thought on your own.Anyways, the point of this post is that I have recently been able to spend some time getting the OpenDaylight Boron release to work as the SDN controller for an OpenStack cloud, and I thought I would relay some of the issues I encountered as well as some of the required software and steps to complete the integration.tl;dr - Official DocumentationAs I write this there is a page that details how to integrate ODL Boron with OpenStack.I only encountered two issues with this documentation, the first is that the DLUX web GUI feature/plugin mentioned is incorrect and you would want the DLUX-core plugin. But that wouldn’t affect actually using ODL, just access to a simple GUI.Next, due to a bug in OpenStack which was fixed in master and back-ported to Newton, I needed to deploy Newton Neutron from the stable branch. I would imagine that that back-port will now be in most OS packaging though, so you probably won’t have to worry about it, unless you are deploying a pre-Newton OpenStack. Not sure if it was back ported to release before Newton.Otherwise, besides showing first deploying OpenStack and then deleting all the neutron state (not sure why all the examples do that), the documentation is pretty good and I’m sure will improve.This is BoronMost of the blog posts and documentation you will find on the Internet are related to previous releases of OpenDaylight, and most of these instructions will only be about 75% correct.Boron has a new ODL feature, Netvirt which has taken over some of the functionality for supporting an OpenStack cloud. Essentially, instead of using the odl-ovsdb-openstack feature/plugin, you will use the odl-netvirt-openstack plugin.Other than Netvirt and requiring Java 8, Boron seems to be quite similar from a systems administration perspective.Networking-ODLNetworking-ODL is required code, perhaps driver is the right term, to allow the Neutron API to work with and OpenDaylight controller.  OpenStack networking-odl is a library of drivers and plugins that integrates OpenStack Neutron API with OpenDaylight Backend. For example it has ML2 driver and L3 plugin to enable communication of OpenStack Neutron L2 and L3 resources API to OpenDayLight Backend.It does not come with Neutron packaging at this time, and needs to be installed separately, either via Pip as most examples show or via source. Typically I will install from source using stable/&lt;openstack version&gt; just to get the latest code.Configuration Management of Open vSwitchThere are a couple of settings that need to be made to each Open vSwitch before it can be used by ODL. Typically this will be done with your configuration management tooling, ie. what ever is managing OpenStack’s configuration.The first is the “manager”. Most documentation will show how to set that.$ sudo ovs-vsctl show | head7f7084b9-f0d8-449a-97fa-06476303d3cf    Manager \"tcp:10.15.0.190:6640\"        is_connected: true    Bridge br-int        Controller \"tcp:10.15.0.190:6653\"            is_connected: true        fail_mode: secure        Port \"tap35617dae-c8\"            Interface \"tap35617dae-c8\"        Port \"tun18897aa9c4e\"Above the manager is connected to the ODL controller.Next is the local_ip.$ sudo ovs-vsctl list Open_vSwitch | grep other_configother_config        : {local_ip=\"10.15.0.11\", provider_mappings=\"provider:bond1\"}That IP will become the endpoint for VXLAN tunnels.Finally, as can be seen above as well, there is a provider_mappings key/value pair. This will list out the various Neutron networks and the physical interfaces they are supposed to use. This will be what floating IP or provider networks (ie. “external networks”) will use. ODL will use the value to setup OVS on each node.All three of these settings would be put in place by your config mgmt tool, eg. Ansible. :)(Example of ODL Netvirt Pipeline)Issues  The NATP component, currently at least, does not support ICMP. So if you have an instance on VXLAN and there is a router with an external gateway on on external network, ping, for example, won’t work, but UDP and TCP will. I recieved the errors below on the ODL controller when pinging from an instance. Apparently it is because OpenFlow doesn’t support ICMP on flows. This can be confusing while testing…it certainly confused me for a while.2017-01-10 19:13:03,772 | ERROR | pool-21-thread-1 |NaptPacketInHandler              | 345 -org.opendaylight.netvirt.natservice-impl - 0.3.2.Boron-SR2 | IncomingPacket is neither TCP or UDP packet2017-01-10 19:13:03,772 | ERROR | pool-21-thread-1 |NaptPacketInHandler              | 345 -org.opendaylight.netvirt.natservice-impl - 0.3.2.Boron-SR2 | IncomingPacket is neither TCP or UDP packet  I wonder about having a tunnel from each host to every other host. Not sure how well that will scale. Something to look into.  Like many other SDN systems, sometimes the initial setup of an unknown flow can be slow, but once it is in place on the OVS switch obviously it’s faster. I’ve experienced this with Midokura’s Midonet as well.Otherwise everything seemed to work fine.Why OpenDaylight?What does one get from deploying ODL?My opinion is that it provides the potential to perform fairly advanced network configurations that without an SDN controller would be difficult or impossible. Frankly at this state ODL + OpenStack is still being worked on. Most of the basic features are there, but where it really starts to get interesting is once we are past that stage, and want to do things like connecting tenants across multiple clouds, connecting to hardware VTEPs, things like that. It is going to take a while for all of this to become ready for production use.There were a fair number of features delivered in ODL Boron (3):  Merge of NetVirt and VPNService projects  L2, L3  Auto-bridge creation  Auto-tunnel creation  Floating IP’s  VLAN and Flat provider network support for multiple internal and external networks  Security Groups: Stateful using conntrack, Stateless, Learn (for OVS-DPDK)  NAPT, SNAT  IPv6  Layer 2 GatewayMy thoughts on the above, as well as additional capability not mentioned:  No more br-int/iptables. Ok, br-int still exists, but security groups are NOT done by iptables. This was something that always bugged me, and a lot of people, but it was a good solution. However, now, with ODL, iptables is taken out of the stack and security groups are done with OVS rules. This is very powerful and important, as it will “remove the many layers of bridges/ports required in iptable implementation.” (1)  ODL takes care of layer 3 with a “distributed virtual router” that is on par with Neutron DVR (2), though no ICMP with NATP.  Ability to do Service Function Chaining (SFC). This functionality may come to “vanilla” Neutron, but most existing software that supports deploying SF chains will support ODL.What’s next?  Troubleshooting ODL - This is not easy, as the paths through tables are somewhat convoluted, but the flow map above certainly helps.  High Availability - ODL can be clustered, but I have not ventured into that area as of yet. Distributed systems are tough.  L2 Gateway - Wanting to get your VXLAN based workloads out into a physical network is an oft requested feature. It seems ODL Boron has the ability to use L2 Gateway. This needs to be explored.  Service Function Chaining - This might not be as useful for organizations who are not interested in Network Function Virtualization (NFV), but that said, this is still quite powerful and useful in many “cloudy” situations. :)  OVS-DPDKFootnotes  OVSDB Netvirt  OpenDaylight PDF  NetVirt Basic Tutorial"
    } ,
  
    {
      "title"    : "What OpenStack Distros?",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/12/21/what-openstack-distros.html",
      "date"     : "2016-12-21 00:00:00 -0500",
      "content"  : "Oops that’s Destro not Distro. Note the OpenStack logo though :)On the surface it seems like there are many ways to install an OpenStack cloud; many OpenStack distributions.OpenStack requires services and infrastructure and the only way to get those all co-oridinated is to use some kind of automated tooling, which is typically what these “distros” provide.However, my definition of “distro” may not line up with most. Essentially I would consider a “distro” a OpenStack installer, and, more importantly something that helps manage OpenStack after the initial installation. Often the installation process is called “Day 1” and managing OpenStack after the installation “Day 2.” It’s not enough just to install OpenStack, distros need to provide Day 2 help as well.tl;drThere are way fewer OpenStack distros than one would think. Also, even if you have an OpenStack distro you will still need internal staff and/or professional services from the distro vendor. I’m aware that is not that helpful a conclusion. :)To quote Boris Renski of Mirantis:  Everybody’s OpenStack software is equally badBut let’s go through this process anyways… :)Official Distro ListOpenStack provides a list of most of the distros. Here’s what is listed on there as of today:  Stratosphere  Appformix  Dell EMC RedHat Solution  StackBuffet  Cisco Metacloud  HPE Helion OpenStack  T2 Cloud OS  Hyper-C  Enovance Service Provider Cloud  Suse OpenStack Cloud  H3 Cloud OS  Dell EMC VXRack with Neutrino  Aqorn Thunder  Redhat OpenStack Platform  Platform9  Ubuntu OpenStack  Bright Computing  Mirantis OpenStack  Oracle OpenStack  TransCirrus Cloud Appliance  Mirantis  IBM Spectrum Scale for Object Storage  AWCloud OpenStack Distribution  Oracle OpenStack for Oracle Linux  RackSpace Private Cloud  Vmware VIO  EasyStack ESCloud  Animbus CloudOS  Huawei Fusionsphere OpenStack  Ultimum Cloud Platform  IBM Cloud Manager with OpenStack  ZTE TECS OpenStack  Debian  SwiftStack  Breqwatr Cloud ApplianceThat seems like a long list with 35 entries; let’s go through it, examine the entries and see what can, and should, be removed.Not DistrosA few are not OpenStack distros: Appformix (which was just bought by Juniper and is really a metrics system), IBM Spectrum (just object storage), SwiftStack (also just object storage), as well as Debian (which provides invaluable packaging, but not actually an installation and/or management system). I’m a big fan of Swift and thus SwiftStack, but it is not a full fledged OpenStack distro (ie. does not provide compute, networking).DefunctThen there are a few that are essentially defunct: eNovance was bought by RedHat some time ago, and Suse just bought HPE Helion. Oracle just cancelled Solaris. I think IBM Cloud Manager is dead too.Biased RemovalsA few I am going to remove just because I don’t see them as real entries, such as Oracle and VMware VIO, also the VXRack entry. Add them back in if you prefer, best of luck. :)APACSome of these companies I just can’t find much information about, mostly because I am not familiar with APAC companies or languages. I wish I knew more. T2 Cloud. H3Cloud OS. AWCloud. Animbus Cloud. EasyStack. If your company is an APAC or works with APAC customers, then perhaps one of these entries would be a great choice. I would certainly be interested to know how these companies OpenStack distro works.Small CompaniesNext up are small companies. I think it would be very difficult to recommend a small company for an OpenStack distro as who knows if they will be around in a couple years. I suppose the same can be said for larger companies, given HPEs sad retreat. Bright Computing (Amsterdam). Aqorn (US). Ultimum (Prague).Further, many companies, large and small, actually use someone elses distro, eg. RedHat or Mirantis.AppliancesPersonally, I would toss any appliance vendor. What is an appliance in 2016? It’s an x86 box with Linux on it. Which is what you would deploy the OpenStack control plane on anyways. So out go Breqwatr and Transcirrus (both of which I’m guessing are quite small companies). Mirantis Unlocked Appliances isn’t even a distro, it’s just certified hardware for the Mirantis plaform, not sure why it’s on the list at all.SolutionsSome options are “solutions” as opposed to distros. Example Dell + EMC + RedHat.  Presumably I could put my own solution up, say Mirantis + SolidFire + OpenDaylight or any combination of OpenStack, Storage, and SDN. Not a distro.UnclearI’m not sure what Aptira StackBuffet is, but based on a quick look it seems to be a system to generate OpenStack packages for the operating system layer. I would not consider that a distro. I’m sure Aptira provides OpenStack clouds, but I’m not sure how.OverspecializedI’d put Hyper-C into this category as it seems to be directed at Windows only shops. Presumably you can run any workload on their distro, but it does seem to have a specific marketing orientation. I would put Stratoscale here as well as they are a hyperconverged entry, apparently with their own hypervisor.So what are we left with?  SUSE/HPE Helion  Cisco Metacloud  RedHat OpenStack Platform  Platform9  Ubuntu OpenStack (does this mean BootStack?)  Mirantis  RackSpace Private Cloud  Huawei FusionSphere  ZTE TECS OpenStackBefore going further with that list, lets checkout some open source “distros.”OpenSource DistrosThe OpenStack project hosts several distros, or libraries:  Puppet OpenStack  OpenStack Ansible  OpenStack Kolla  TripleoPuppet OpenStack is probably the most commonly used of the three I mention above, but it is more like a libary of Puppet manifests that can be used to deploy OpenStack. However, you need to create your own “composition layer” that will actually utilize those manifests. Thus I would not consider it a distro, though again, it is commonly used and well tested, it just requires some additional work.OpenStack Ansible and Kolla are distros though. OpenStack Ansible is quite mature and stable, and is also, I believe, the basis for the RackSpace Private Cloud. Koll, on the other hand, is relatively new. It is getting a lot of press, so to speak, because it uses Docker images as a deployment mechanism, and, of course, it is also embroiled in the k8s hype.I am quite familiar with Tripleo. It is the basis for RedHat’s Director installer. I am not it’s biggest fan, but it does work. It uses the aforementioned OpenStack Puppet as well as Ironic and Heat to deploy OpenStack. I personally have a concern with Day 2 and Tripleo, however.Also, and this might be lesser known, the Open Platform for Network Function Virutalization (OPNFV) project has a few installers. Some are based on existing systems like Tripleo. I don’t know if you could use any of these in production, as they are mostly meant to be part of OPNFV’s continuous integration system, but they do exist.ConclusionWhether you agree or disagree with my inclusions and removals, I think it’s safe to say that there is not a massive number of OpenStack distributions, and the ones that do exist are quite different in what they support and how they work after Day 1.But…what distros do we have left?  SUSE/HPE Helion  Cisco Metacloud  RedHat OpenStack Platform  Platform9  Ubuntu OpenStack (does this mean BootStack?)  Mirantis  RackSpace Private Cloud  Huawei FusionSphere  ZTE TECS OpenStack  OpenStack Ansible  OpenStack Kolla  OpenStack TripleoI don’t know anything about Huawei Fusionsphere or ZTE TECS OpenStack. They may very well be based on an existing distro. Can’t comment.Platform9 is a unique entry due to its SaaS design, which may not be suitable for all deployments, but I do think it is quite innovative.RackSpace Private Cloud is based on OpenStack Ansible.RedHat OpenStack Platform is based on Tripleo.Mirantis is/was a “pure-play” distro which is now moving quickly to use Kubernetes to manage the OpenStack control plane. (1)As far as Cisco Metacloud, I don’t know much about it, so can’t really comment, though it does seem t o be a valid option, assuming you like dealing with mega-sales-companies.Next up, the Ubuntu/Canonical entry. I have not used it, but it seems to be a valid choice. I am hesitant regarding the Juju configuration management tool, but I have not used it so that is unfair. The visualization of components is interesting.The sale of the HPE Helion intellectual property to SUSE is weird. There would have to be some period of instability here. I can’t see it being a viable option right now. Basically HPE has completely removed its involvment in OpenStack.So, in my extremely biased opinion, what are we left with? Not much.What’s left? Recommendations…The Ubuntu/Juju distro seems like it would be Ok, but I have not used it.RedHat OpenStack and Tripleo are good too. I have concerns about Tripleo on Day 2.Rackspace Private Could is probably a great option given it is based on OpenStack-Ansible. Or OpenStack-Ansible by itself is great, if you want to use the open source version and support it yourself with help from the community.Mirantis is still viable, but their k8s direction is concerning. Also, they are themselves questioning the value of a distro:  …very few companies can manage a complex, distributed and fast moving piece of software such as OpenStack…Therefore, most customers end up utilizing extensive professional services from the distribution vendor. - Mirantis BlogSo even with a distro you are still using professional services. That said, their recent focus on Day 2 is powerful, though to be honest I have not used Fuel.If you are a company that wants to use OpenStack and is hoping to make it simpler by using a distro, then I think you are in for a surprise. OpenStack is not cheap as it usually requires a team of fairly sophisticated infrastructure engineers to deploy and maintain, whether you are using a distro or not. Now, those engineers may be employees of your company, or they may be employees of a professional services firm. Either way, OpenStack requires people to manage it. Frankly distros may get in the way.Footnotes  Kubernetes (k8s) has really thrown a wrench into distros because everyone is scrambling to keep up with the hype. Kubernetes is great, but I think using k8s simply to manage the OpenStack control plane is overkill, and the additional complexity is not needed."
    } ,
  
    {
      "title"    : "Tracing or Logging RabbitMQ",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/12/17/tracing-logging-rabbitmq.html",
      "date"     : "2016-12-17 00:00:00 -0500",
      "content"  : "Messaging queues like RabbitMQ are widely used. For example it is a key component in an OpenStack deployment.Sometimes you need to peek into the queue and see what is happening and I thought I would write a quick post on one way to do that.Turn on the firehoseRabbit has an option to turn on “tracing” so that you can see every message that is coming through the queue.  RabbitMQ has a “firehose” feature, where the administrator can enable (on a per-node, per-vhost basis) an exchange to which publish- and delivery-notifications should be CCed. - RabbitMQIf you have a default install and just want the “/” vhost, you can simply turn on tracing.root@rabbit:~# rabbitmqctl trace_on Starting tracing for vhost \"/\" ...Now you need to read that queue.Reading messages on the firehose queueYou will need a script to read the queue.I just grabbed one off of github, but it would be straight forward to write your own.This particular python script uses the pika library.ubuntu@rabbit:~$ virtualenv venvubuntu@rabbit:~$ . venv/bin/activateubuntu@rabbit:~$ pip install pikaNow you can turn on the firehose. I’m going to redirect to a file because there are a lot of messages.(venv) ubuntu@rabbit:~$ time ./trace.py &gt; trace.out^CTraceback (most recent call last):  File \"./trace.py\", line 37, in &lt;module&gt;    channel.start_consuming()  File \"/home/ubuntu/venv/local/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 1681, in start_consuming    self.connection.process_data_events(time_limit=None)  File \"/home/ubuntu/venv/local/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 647, in process_data_events    self._flush_output(common_terminator)  File \"/home/ubuntu/venv/local/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 410, in _flush_output    self._impl.ioloop.poll()  File \"/home/ubuntu/venv/local/lib/python2.7/site-packages/pika/adapters/select_connection.py\", line 590, in poll    events = self._poll.poll(self.get_next_deadline())KeyboardInterruptreal    0m2.421suser    0m0.096ssys 0m0.024s(venv) ubuntu@rabbit:~$ wc -l trace.out 99 trace.out(venv) ubuntu@rabbit:~$ head trace.out  [*] Waiting for logs. To exit press CTRL+C [x] 'publish.nova':u'rabbit@rabbit':[u'conductor']:'{\"oslo.message\": \"{\\\\\"_context_domain\\\\\": null, \\\\\"_msg_id\\\\\": \\\\\"91deacb97e0948069f234db946d661ae\\\\\", \\\\\"_context_quota_class\\\\\": null, \\\\\"_context_read_only\\\\\": false, \\\\\"_context_request_id\\\\\": \\\\\"req-e282aa6c-ccd0-4a23-aa1c-e818a339c56a\\\\\", \\\\\"_context_service_catalog\\\\\": [], \\\\\"args\\\\\": {\\\\\"objmethod\\\\\": \\\\\"save\\\\\", \\\\\"args\\\\\": [], \\\\\"objinst\\\\\": {\\\\\"nova_object.version\\\\\": \\\\\"1.20\\\\\", \\\\\"nova_object.changes\\\\\": [\\\\\"report_count\\\\\"], \\\\\"nova_object.name\\\\\": \\\\\"Service\\\\\", \\\\\"nova_object.data\\\\\": {\\\\\"binary\\\\\": \\\\\"nova-compute\\\\\", \\\\\"deleted\\\\\": false, \\\\\"created_at\\\\\": \\\\\"2016-12-13T21:53:45Z\\\\\", \\\\\"updated_at\\\\\": \\\\\"2016-12-17T15:31:28Z\\\\\", \\\\\"report_count\\\\\": 32164, \\\\\"topic\\\\\": \\\\\"compute\\\\\", \\\\\"host\\\\\": \\\\\"server03\\\\\", \\\\\"version\\\\\": 15, \\\\\"disabled\\\\\": false, \\\\\"forced_down\\\\\": false, \\\\\"last_seen_up\\\\\": \\\\\"2016-12-17T15:31:28Z\\\\\", \\\\\"deleted_at\\\\\": null, \\\\\"disabled_reason\\\\\": null, \\\\\"id\\\\\": 7}, \\\\\"nova_object.namespace\\\\\": \\\\\"nova\\\\\"}, \\\\\"kwargs\\\\\": {}}, \\\\\"_unique_id\\\\\": \\\\\"3d3aaa47d8b94fc8bf025a797e788d87\\\\\", \\\\\"_context_resource_uuid\\\\\": null, \\\\\"_context_instance_lock_checked\\\\\": false, \\\\\"_context_is_admin_project\\\\\": true, \\\\\"_context_user\\\\\": null, \\\\\"_context_user_id\\\\\": null, \\\\\"_context_project_name\\\\\": null, \\\\\"_context_read_deleted\\\\\": \\\\\"no\\\\\", \\\\\"_context_user_identity\\\\\": \\\\\"- - - - -\\\\\", \\\\\"_reply_q\\\\\": \\\\\"reply_0949e6a10e0345c4ba494eb121edc1f1\\\\\", \\\\\"_context_auth_token\\\\\": null, \\\\\"_context_show_deleted\\\\\": false, \\\\\"_context_tenant\\\\\": null, \\\\\"_context_roles\\\\\": [], \\\\\"_context_is_admin\\\\\": true, \\\\\"version\\\\\": \\\\\"3.0\\\\\", \\\\\"_context_project_id\\\\\": null, \\\\\"_context_project_domain\\\\\": null, \\\\\"_context_timestamp\\\\\": \\\\\"2016-12-16T16:05:25.637936\\\\\", \\\\\"_context_user_domain\\\\\": null, \\\\\"_context_user_name\\\\\": null, \\\\\"method\\\\\": \\\\\"object_action\\\\\", \\\\\"_context_remote_address\\\\\": null}\", \"oslo.version\": \"2.0\"}' [x] 'deliver.conductor':u'rabbit@rabbit':[u'conductor']:'{\"oslo.message\": \"{\\\\\"_context_domain\\\\\": null, \\\\\"_msg_id\\\\\": \\\\\"91deacb97e0948069f234db946d661ae\\\\\", \\\\\"_context_quota_class\\\\\": null, \\\\\"_context_read_only\\\\\": false, \\\\\"_context_request_id\\\\\": \\\\\"req-e282aa6c-ccd0-4a23-aa1c-e818a339c56a\\\\\", \\\\\"_context_service_catalog\\\\\": [], \\\\\"args\\\\\": {\\\\\"objmethod\\\\\": \\\\\"save\\\\\", \\\\\"args\\\\\": [], \\\\\"objinst\\\\\": {\\\\\"nova_object.version\\\\\": \\\\\"1.20\\\\\", \\\\\"nova_object.changes\\\\\": [\\\\\"report_count\\\\\"], \\\\\"nova_object.name\\\\\": \\\\\"Service\\\\\", \\\\\"nova_object.data\\\\\": {\\\\\"binary\\\\\": \\\\\"nova-compute\\\\\", \\\\\"deleted\\\\\": false, \\\\\"created_at\\\\\": \\\\\"2016-12-13T21:53:45Z\\\\\", \\\\\"updated_at\\\\\": \\\\\"2016-12-17T15:31:28Z\\\\\", \\\\\"report_count\\\\\": 32164, \\\\\"topic\\\\\": \\\\\"compute\\\\\", \\\\\"host\\\\\": \\\\\"server03\\\\\", \\\\\"version\\\\\": 15, \\\\\"disabled\\\\\": false, \\\\\"forced_down\\\\\": false, \\\\\"last_seen_up\\\\\": \\\\\"2016-12-17T15:31:28Z\\\\\", \\\\\"deleted_at\\\\\": null, \\\\\"disabled_reason\\\\\": null, \\\\\"id\\\\\": 7}, \\\\\"nova_object.namespace\\\\\": \\\\\"nova\\\\\"}, \\\\\"kwargs\\\\\": {}}, \\\\\"_unique_id\\\\\": \\\\\"3d3aaa47d8b94fc8bf025a797e788d87\\\\\", \\\\\"_context_resource_uuid\\\\\": null, \\\\\"_context_instance_lock_checked\\\\\": false, \\\\\"_context_is_admin_project\\\\\": true, \\\\\"_context_user\\\\\": null, \\\\\"_context_user_id\\\\\": null, \\\\\"_context_project_name\\\\\": null, \\\\\"_context_read_deleted\\\\\": \\\\\"no\\\\\", \\\\\"_context_user_identity\\\\\": \\\\\"- - - - -\\\\\", \\\\\"_reply_q\\\\\": \\\\\"reply_0949e6a10e0345c4ba494eb121edc1f1\\\\\", \\\\\"_context_auth_token\\\\\": null, \\\\\"_context_show_deleted\\\\\": false, \\\\\"_context_tenant\\\\\": null, \\\\\"_context_roles\\\\\": [], \\\\\"_context_is_admin\\\\\": true, \\\\\"version\\\\\": \\\\\"3.0\\\\\", \\\\\"_context_project_id\\\\\": null, \\\\\"_context_project_domain\\\\\": null, \\\\\"_context_timestamp\\\\\": \\\\\"2016-12-16T16:05:25.637936\\\\\", \\\\\"_context_user_domain\\\\\": null, \\\\\"_context_user_name\\\\\": null, \\\\\"method\\\\\": \\\\\"object_action\\\\\", \\\\\"_context_remote_address\\\\\": null}\", \"oslo.version\": \"2.0\"}' [x] 'publish.reply_0949e6a10e0345c4ba494eb121edc1f1':u'rabbit@rabbit':[u'reply_0949e6a10e0345c4ba494eb121edc1f1']:'{\"oslo.message\": \"{\\\\\"_msg_id\\\\\": \\\\\"91deacb97e0948069f234db946d661ae\\\\\", \\\\\"failure\\\\\": null, \\\\\"_unique_id\\\\\": \\\\\"fbb674f006f54c939dd3df7b8e234748\\\\\", \\\\\"result\\\\\": [{\\\\\"last_seen_up\\\\\": \\\\\"2016-12-17T15:31:38Z\\\\\", \\\\\"updated_at\\\\\": \\\\\"2016-12-17T15:31:38Z\\\\\", \\\\\"obj_what_changed\\\\\": []}, null], \\\\\"ending\\\\\": true}\", \"oslo.version\": \"2.0\"}' [x] 'deliver.reply_0949e6a10e0345c4ba494eb121edc1f1':u'rabbit@rabbit':[u'reply_0949e6a10e0345c4ba494eb121edc1f1']:'{\"oslo.message\": \"{\\\\\"_msg_id\\\\\": \\\\\"91deacb97e0948069f234db946d661ae\\\\\", \\\\\"failure\\\\\": null, \\\\\"_unique_id\\\\\": \\\\\"fbb674f006f54c939dd3df7b8e234748\\\\\", \\\\\"result\\\\\": [{\\\\\"last_seen_up\\\\\": \\\\\"2016-12-17T15:31:38Z\\\\\", \\\\\"updated_at\\\\\": \\\\\"2016-12-17T15:31:38Z\\\\\", \\\\\"obj_what_changed\\\\\": []}, null], \\\\\"ending\\\\\": true}\", \"oslo.version\": \"2.0\"}' [x] 'publish.nova':u'rabbit@rabbit':[u'conductor']:'{\"oslo.message\": \"{\\\\\"_context_domain\\\\\": null, \\\\\"_msg_id\\\\\": \\\\\"1db1384294174e8d83ba30ffae7912e2\\\\\", \\\\\"_context_quota_class\\\\\": null, \\\\\"_context_read_only\\\\\": false, \\\\\"_context_request_id\\\\\": \\\\\"req-e8d0d0c9-35b1-4d47-9ab9-b0d6f08283eb\\\\\", \\\\\"_context_service_catalog\\\\\": [], \\\\\"args\\\\\": {\\\\\"objmethod\\\\\": \\\\\"save\\\\\", \\\\\"args\\\\\": [], \\\\\"objinst\\\\\": {\\\\\"nova_object.version\\\\\": \\\\\"1.20\\\\\", \\\\\"nova_object.changes\\\\\": [\\\\\"report_count\\\\\"], \\\\\"nova_object.name\\\\\": \\\\\"Service\\\\\", \\\\\"nova_object.data\\\\\": {\\\\\"binary\\\\\": \\\\\"nova-compute\\\\\", \\\\\"deleted\\\\\": false, \\\\\"created_at\\\\\": \\\\\"2016-12-13T21:50:18Z\\\\\", \\\\\"updated_at\\\\\": \\\\\"2016-12-17T15:31:29Z\\\\\", \\\\\"report_count\\\\\": 32176, \\\\\"topic\\\\\": \\\\\"compute\\\\\", \\\\\"host\\\\\": \\\\\"server02\\\\\", \\\\\"version\\\\\": 15, \\\\\"disabled\\\\\": false, \\\\\"forced_down\\\\\": false, \\\\\"last_seen_up\\\\\": \\\\\"2016-12-17T15:31:29Z\\\\\", \\\\\"deleted_at\\\\\": null, \\\\\"disabled_reason\\\\\": null, \\\\\"id\\\\\": 6}, \\\\\"nova_object.namespace\\\\\": \\\\\"nova\\\\\"}, \\\\\"kwargs\\\\\": {}}, \\\\\"_unique_id\\\\\": \\\\\"d484f67b6dc34bc88b26c7a011015c36\\\\\", \\\\\"_context_resource_uuid\\\\\": null, \\\\\"_context_instance_lock_checked\\\\\": false, \\\\\"_context_is_admin_project\\\\\": true, \\\\\"_context_user\\\\\": null, \\\\\"_context_user_id\\\\\": null, \\\\\"_context_project_name\\\\\": null, \\\\\"_context_read_deleted\\\\\": \\\\\"no\\\\\", \\\\\"_context_user_identity\\\\\": \\\\\"- - - - -\\\\\", \\\\\"_reply_q\\\\\": \\\\\"reply_eab5dc5ef70a480881932f27d8d2157b\\\\\", \\\\\"_context_auth_token\\\\\": null, \\\\\"_context_show_deleted\\\\\": false, \\\\\"_context_tenant\\\\\": null, \\\\\"_context_roles\\\\\": [], \\\\\"_context_is_admin\\\\\": true, \\\\\"version\\\\\": \\\\\"3.0\\\\\", \\\\\"_context_project_id\\\\\": null, \\\\\"_context_project_domain\\\\\": null, \\\\\"_context_timestamp\\\\\": \\\\\"2016-12-16T16:05:22.325724\\\\\", \\\\\"_context_user_domain\\\\\": null, \\\\\"_context_user_name\\\\\": null, \\\\\"method\\\\\": \\\\\"object_action\\\\\", \\\\\"_context_remote_address\\\\\": null}\", \"oslo.version\": \"2.0\"}' [x] 'deliver.conductor':u'rabbit@rabbit':[u'conductor']:'{\"oslo.message\": \"{\\\\\"_context_domain\\\\\": null, \\\\\"_msg_id\\\\\": \\\\\"1db1384294174e8d83ba30ffae7912e2\\\\\", \\\\\"_context_quota_class\\\\\": null, \\\\\"_context_read_only\\\\\": false, \\\\\"_context_request_id\\\\\": \\\\\"req-e8d0d0c9-35b1-4d47-9ab9-b0d6f08283eb\\\\\", \\\\\"_context_service_catalog\\\\\": [], \\\\\"args\\\\\": {\\\\\"objmethod\\\\\": \\\\\"save\\\\\", \\\\\"args\\\\\": [], \\\\\"objinst\\\\\": {\\\\\"nova_object.version\\\\\": \\\\\"1.20\\\\\", \\\\\"nova_object.changes\\\\\": [\\\\\"report_count\\\\\"], \\\\\"nova_object.name\\\\\": \\\\\"Service\\\\\", \\\\\"nova_object.data\\\\\": {\\\\\"binary\\\\\": \\\\\"nova-compute\\\\\", \\\\\"deleted\\\\\": false, \\\\\"created_at\\\\\": \\\\\"2016-12-13T21:50:18Z\\\\\", \\\\\"updated_at\\\\\": \\\\\"2016-12-17T15:31:29Z\\\\\", \\\\\"report_count\\\\\": 32176, \\\\\"topic\\\\\": \\\\\"compute\\\\\", \\\\\"host\\\\\": \\\\\"server02\\\\\", \\\\\"version\\\\\": 15, \\\\\"disabled\\\\\": false, \\\\\"forced_down\\\\\": false, \\\\\"last_seen_up\\\\\": \\\\\"2016-12-17T15:31:29Z\\\\\", \\\\\"deleted_at\\\\\": null, \\\\\"disabled_reason\\\\\": null, \\\\\"id\\\\\": 6}, \\\\\"nova_object.namespace\\\\\": \\\\\"nova\\\\\"}, \\\\\"kwargs\\\\\": {}}, \\\\\"_unique_id\\\\\": \\\\\"d484f67b6dc34bc88b26c7a011015c36\\\\\", \\\\\"_context_resource_uuid\\\\\": null, \\\\\"_context_instance_lock_checked\\\\\": false, \\\\\"_context_is_admin_project\\\\\": true, \\\\\"_context_user\\\\\": null, \\\\\"_context_user_id\\\\\": null, \\\\\"_context_project_name\\\\\": null, \\\\\"_context_read_deleted\\\\\": \\\\\"no\\\\\", \\\\\"_context_user_identity\\\\\": \\\\\"- - - - -\\\\\", \\\\\"_reply_q\\\\\": \\\\\"reply_eab5dc5ef70a480881932f27d8d2157b\\\\\", \\\\\"_context_auth_token\\\\\": null, \\\\\"_context_show_deleted\\\\\": false, \\\\\"_context_tenant\\\\\": null, \\\\\"_context_roles\\\\\": [], \\\\\"_context_is_admin\\\\\": true, \\\\\"version\\\\\": \\\\\"3.0\\\\\", \\\\\"_context_project_id\\\\\": null, \\\\\"_context_project_domain\\\\\": null, \\\\\"_context_timestamp\\\\\": \\\\\"2016-12-16T16:05:22.325724\\\\\", \\\\\"_context_user_domain\\\\\": null, \\\\\"_context_user_name\\\\\": null, \\\\\"method\\\\\": \\\\\"object_action\\\\\", \\\\\"_context_remote_address\\\\\": null}\", \"oslo.version\": \"2.0\"}' [x] 'publish.reply_eab5dc5ef70a480881932f27d8d2157b':u'rabbit@rabbit':[u'reply_eab5dc5ef70a480881932f27d8d2157b']:'{\"oslo.message\": \"{\\\\\"_msg_id\\\\\": \\\\\"1db1384294174e8d83ba30ffae7912e2\\\\\", \\\\\"failure\\\\\": null, \\\\\"_unique_id\\\\\": \\\\\"366886d2234e49649665576f43ae3f72\\\\\", \\\\\"result\\\\\": [{\\\\\"last_seen_up\\\\\": \\\\\"2016-12-17T15:31:39Z\\\\\", \\\\\"updated_at\\\\\": \\\\\"2016-12-17T15:31:39Z\\\\\", \\\\\"obj_what_changed\\\\\": []}, null], \\\\\"ending\\\\\": true}\", \"oslo.version\": \"2.0\"}' [x] 'deliver.reply_eab5dc5ef70a480881932f27d8d2157b':u'rabbit@rabbit':[u'reply_eab5dc5ef70a480881932f27d8d2157b']:'{\"oslo.message\": \"{\\\\\"_msg_id\\\\\": \\\\\"1db1384294174e8d83ba30ffae7912e2\\\\\", \\\\\"failure\\\\\": null, \\\\\"_unique_id\\\\\": \\\\\"366886d2234e49649665576f43ae3f72\\\\\", \\\\\"result\\\\\": [{\\\\\"last_seen_up\\\\\": \\\\\"2016-12-17T15:31:39Z\\\\\", \\\\\"updated_at\\\\\": \\\\\"2016-12-17T15:31:39Z\\\\\", \\\\\"obj_what_changed\\\\\": []}, null], \\\\\"ending\\\\\": true}\", \"oslo.version\": \"2.0\"}' [x] 'publish.nova':u'rabbit@rabbit':[u'conductor']:'{\"oslo.message\": \"{\\\\\"_context_domain\\\\\": null, \\\\\"_msg_id\\\\\": \\\\\"06d003ee9aa0445e8947c27a92275f0c\\\\\", \\\\\"_context_quota_class\\\\\": null, \\\\\"_context_read_only\\\\\": false, \\\\\"_context_request_id\\\\\": \\\\\"req-dc8f9ccf-151e-4f9b-9fd9-52350c39fd6f\\\\\", \\\\\"_context_service_catalog\\\\\": [], \\\\\"args\\\\\": {\\\\\"object_versions\\\\\": {\\\\\"PciDevicePoolList\\\\\": \\\\\"1.1\\\\\", \\\\\"ServiceList\\\\\": \\\\\"1.19\\\\\", \\\\\"PciDevicePool\\\\\": \\\\\"1.1\\\\\", \\\\\"Service\\\\\": \\\\\"1.20\\\\\", \\\\\"TagList\\\\\": \\\\\"1.1\\\\\", \\\\\"InstancePCIRequests\\\\\": \\\\\"1.1\\\\\", \\\\\"VirtCPUModel\\\\\": \\\\\"1.0\\\\\", \\\\\"MigrationContext\\\\\": \\\\\"1.1\\\\\", \\\\\"SecurityGroup\\\\\": \\\\\"1.1\\\\\", \\\\\"DeviceBus\\\\\": \\\\\"1.0\\\\\", \\\\\"Instance\\\\\": \\\\\"2.3\\\\\", \\\\\"KeyPair\\\\\": \\\\\"1.4\\\\\", \\\\\"KeyPairList\\\\\": \\\\\"1.3\\\\\", \\\\\"DeviceMetadata\\\\\": \\\\\"1.0\\\\\", \\\\\"InstancePCIRequest\\\\\": \\\\\"1.1\\\\\", \\\\\"EC2Ids\\\\\": \\\\\"1.0\\\\\", \\\\\"InstanceInfoCache\\\\\": \\\\\"1.5\\\\\", \\\\\"VirtCPUFeature\\\\\": \\\\\"1.0\\\\\", \\\\\"Flavor\\\\\": \\\\\"1.1\\\\\", \\\\\"SecurityGroupList\\\\\": \\\\\"1.0\\\\\", \\\\\"PciDevice\\\\\": \\\\\"1.5\\\\\", \\\\\"VirtCPUTopology\\\\\": \\\\\"1.0\\\\\", \\\\\"InstanceNUMACell\\\\\": \\\\\"1.3\\\\\", \\\\\"InstanceList\\\\\": \\\\\"2.1\\\\\", \\\\\"ComputeNode\\\\\": \\\\\"1.16\\\\\", \\\\\"InstanceFault\\\\\": \\\\\"1.2\\\\\", \\\\\"Tag\\\\\": \\\\\"1.1\\\\\", \\\\\"HVSpec\\\\\": \\\\\"1.2\\\\\", \\\\\"InstanceDeviceMetadata\\\\\": \\\\\"1.0\\\\\", \\\\\"InstanceNUMATopology\\\\\": \\\\\"1.2\\\\\", \\\\\"PciDeviceList\\\\\": \\\\\"1.3\\\\\"}, \\\\\"objmethod\\\\\": \\\\\"get_by_host\\\\\", \\\\\"args\\\\\": [\\\\\"server03\\\\\"], \\\\\"objname\\\\\": \\\\\"InstanceList\\\\\", \\\\\"kwargs\\\\\": {\\\\\"use_slave\\\\\": true, \\\\\"expected_attrs\\\\\": []}}, \\\\\"_unique_id\\\\\": \\\\\"42f089b774994641a15621df44b4493e\\\\\", \\\\\"_context_resource_uuid\\\\\": null, \\\\\"_context_instance_lock_checked\\\\\": false, \\\\\"_context_is_admin_project\\\\\": true, \\\\\"_context_user\\\\\": null, \\\\\"_context_user_id\\\\\": null, \\\\\"_context_project_name\\\\\": null, \\\\\"_context_read_deleted\\\\\": \\\\\"no\\\\\", \\\\\"_context_user_identity\\\\\": \\\\\"- - - - -\\\\\", \\\\\"_reply_q\\\\\": \\\\\"reply_0949e6a10e0345c4ba494eb121edc1f1\\\\\", \\\\\"_context_auth_token\\\\\": null, \\\\\"_context_show_deleted\\\\\": false, \\\\\"_context_tenant\\\\\": null, \\\\\"_context_roles\\\\\": [\\\\\"admin\\\\\"], \\\\\"_context_is_admin\\\\\": true, \\\\\"version\\\\\": \\\\\"3.0\\\\\", \\\\\"_context_project_id\\\\\": null, \\\\\"_context_project_domain\\\\\": null, \\\\\"_context_timestamp\\\\\": \\\\\"2016-12-17T15:31:43.661237\\\\\", \\\\\"_context_user_domain\\\\\": null, \\\\\"_context_user_name\\\\\": null, \\\\\"method\\\\\": \\\\\"object_class_action_versions\\\\\", \\\\\"_context_remote_address\\\\\": null}\", \"oslo.version\": \"2.0\"}'Once you’ve got the messages you want, turn off tracing.root@rabbit:~# rabbitmqctl trace_offStopping tracing for vhost \"/\" ...You might need to empty out the firehost queue.root@rabbit:~# rabbitmqctl list_queues | grep firehosefirehose    450root@rabbit:~# rabbitmqctl purge_queue firehosePurging queue 'firehose' in vhost '/' ...root@rabbit:~# rabbitmqctl list_queues | grep firehosefirehose    0"
    } ,
  
    {
      "title"    : "OpenStack Bifrost",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/12/11/openstack-ironic-bifrost.html",
      "date"     : "2016-12-11 00:00:00 -0500",
      "content"  : "No matter how you deploy OpenStack, you need some kind of software that manages the operating system that runs directly on the baremetal. Sure, maybe you have all kinds of exotic container management systems, but one of the problems with running your own private cloud is that you still need to manage the physical hosts and their base operating system. Those containers have to run somewhere!So most OpenStack deployment systems come with some kind of baremetal installer. For example Mirantis uses Cobber and Tripleo uses Ironic (within the concept of an undercloud).There are other baremetal installers too, such as Ubuntu MaaS, I think Foreman also, and if you feel like it you can even roll your own based on PXE booting with dnsmasq or similar. The point is you want to automatically deploy the baremetal OS (BOS).BifrostI already mentioned OpenStack Ironic as a way to manage the BOS. Bifrost also uses Ironic:  Bifrost (pronounced bye-frost) is a set of Ansible playbooks that automates the task of deploying a base image onto a set of known hardware using ironic. It provides modular utility for one-off operating system deployment with as few operational requirements as reasonably possible.Basically it installs a standalone Ironic system, and then also uses Ansible playbooks to generate OS images and deploys them to baremetal nodes. It is a combination of Ironic and Disk Image Builder with Ansible to install and orchestrate them.InstallationInstallation is straight forward. I would repeat the docs here, but they are quite good in terms of installation. The only thing to really remember is to set the network_interface variable to the correct interface on the bifrost node. You will need at least one server to run this from. I’m using a virtual machine. One of the interfaces on the bifrost node must be on the same network as the DHCP interfaces of the physical nodes you want to manage with bifrost. Like most BOS installers, Ironic installs via PXE booting.Once the install completes you should be able to run Ironic commands.ubuntu@p3-bifrost-01:~$ ironic node-list+--------------------------------------+----------+---------------+-------------+--------------------+-------------+| UUID                                 | Name     | Instance UUID | Power State | Provisioning State | Maintenance |+--------------------------------------+----------+---------------+-------------+--------------------+-------------++--------------------------------------+----------+---------------+-------------+--------------------+-------------+But of course you won’t have any Ironic nodes because you have not enrolled servers into Ironic.Disk ImageThe installation process also creates a disk image that will be deployed to the physical node.This image is in /httpboot.ubuntu@p3-bifrost-01:~$ ls /httpboot/*.qcow2/httpboot/deployment_image.qcow2By default the image is named deployment_image.qcow2.If you don’t change any defaults it will be Debian Jessie.You can change what OS is build using a couple of options. For example if you wanted Ubuntu Trusty:dib_os_release: \"trusty\"dib_os_element: \"ubuntu\"Note that I had some issues with Ubuntu Xenial and Bifrost which I will discuss later in the post.But just be aware that part of the installation process also creates this image, and the image is obviously very important as it is what gets deployed to the physical node. Likely you will want to customize that image.EnvironmentI’ve added sourcing a couple of bifrost files to the .profile of the local user:# Bifrost. ~/bifrost/env-vars. /opt/stack/ansible/hacking/env-setupJust makes it a bit easier to use Bifrost.Hardware EnrollmentAgain, the docs are good on this.Basically you need to setup a file that details your inventory of physical servers. The inventory can be defined in the old CSV manner, or in a newer method with either JSON or YAML.Here’s an example of a single server in YAML. Obviously your real inventory should include tens or hundreds of physical servers.---  node01:    uuid: \"00000000-0000-0000-0000-000000000001\"    driver_info:      power:        ipmi_username: \"PXE_USER\"        ipmi_address: \"10.10.0.10\"        ipmi_password: \"PXE_PASSWORD\"    nics:      -        mac: \"48-8C-83-E7-A5-D5\"    driver: \"agent_ipmitool\"    properties:      cpu_arch: \"x86_64\"      ram: \"128375\"      disk_size: \"400\"      cpus: \"48\"    name: \"node01\"When you start a deployment, the dnsmasq server running on the node will wait to see DHCP requests from the above mac address, and when it receives them proceed to install a specific OS image to that physical node.DeploymentDeployment of nodes is a single Ansible command. Again the docs are good on this.Here’s an example deployment of a single node. I’ve put it into a deploy script just to make it a bit easier to use. :)ubuntu@p3-bifrost-01:~$ cat deploy.sh #!/bin/bashexport BIFROST_INVENTORY_SOURCE=~/node01.ymlpushd ~/bifrost/playbooks  ansible-playbook -vvvv -i inventory/bifrost_inventory.py deploy-dynamic.yamlpopd Once the deployment completes, you should have some nodes listed as active in Ironic.ubuntu@p3-bifrost-01:~$ ironic node-list+--------------------------------------+----------+---------------+-------------+--------------------+-------------+| UUID                                 | Name     | Instance UUID | Power State | Provisioning State | Maintenance |+--------------------------------------+----------+---------------+-------------+--------------------+-------------+| 00000000-0000-0000-0000-000000000001 | node01 | None          | power on    | active             | False       |+--------------------------------------+----------+---------------+-------------+--------------------+-------------+Hopefully in your deployment you will have more than one node. :)What IP address did the node get?You can specify what IP address the node will get on the DHCP/PXE network as a static entry in the nodes inventory information, but I have not done that. Thus it will get a random IP from the DHCP pool.ubuntu@p3-bifrost-01:~$ cat /var/lib/misc/dnsmasq.leases 1481507199 48-8C-83-E7-A5-D5 10.100.0.31 node01 01:48-8C-83-E7-A5-D5I can ssh into that node:ubuntu@p3-bifrost-01:~$ ssh 10.100.0.31Welcome to Ubuntu 16.04.1 LTS (GNU/Linux 4.4.0-53-generic x86_64) * Documentation:  https://help.ubuntu.com * Management:     https://landscape.canonical.com * Support:        https://ubuntu.com/advantage  Get cloud support with Ubuntu Advantage Cloud Guest:    http://www.ubuntu.com/business/services/cloud0 packages can be updated.0 updates are security updates.Last login: Sun Dec 11 13:54:29 2016 from 10.100.0.3ubuntu@node01:~$ Bwoop!Ubuntu Xenial and building your own imagesBifrost will try to help out and generate a working image for you. This is nice. However, it did not work with Ubuntu Xenial. I ran into some kind of Python related error which I will investigate further. I believe it’s related to the default use of simple-init instead of cloud-init, but I’m not completely sure at this time.But, you an also generate your own image using Disk Image Builder (dib).Dib is very easy to install. Using it takes a bit more work as there are many options.I am building a working Xenial image like so:ubuntu@p3-bifrost-01:~/dib$ cat build.sh #!/bin/bashcd /home/ubuntu/dibexport PATH=$PATH:/home/ubuntu/dib/diskimage-builder/bin:/home/ubuntu/dib/dib-utils/binexport DIB_RELEASE=xenialexport DIB_DEV_USER_PASSWORD=\"SOME_USER\"export DIB_DEV_USER_USERNAME=\"SOME_PASS\"export DIB_DEV_USER_PWDLESS_SUDO=\"Yes\"export DIB_DEV_USER_PASSWORD=\"/bin/bash\"export DIB_CLOUD_INIT_DATASOURCES=\"ConfigDrive\"# dib elements to use in creating the imageEL=\"ubuntu vm cloud-init devuser serial-console cloud-init-datasources\"disk-image-create -a amd64 -t qcow2 -o xenial.qcow2 \\-p python2.7,python-simplejson \\$ELThis creates a Xenial image that has cloud-init installed but set to only use ConfigDrive as a data source. The bifrost installation does not have a metadata service that the instance can call back to, and instead uses ConfigDrive to add things to the instance dynamically, such as ssh keys and the like.Then I just copy the resulting image to /httpboot/deployment_image.qcow2 and that is what will be deployed to the physical node.I should note that:  serial-console does not seem to work in Xenial  Adding a devuser in production is probably not a great ideaI also add python2.7 and python-simplejson for use with Ansible.ConfigDriveThe drive is attached to the physical node and has a json file that defines some dynamic changes to the host. Below it is just hostname and an ssh key for the ubuntu user’s authorized_keys file.ubuntu@node01:~$ sudo mount /dev/sda2 /mntmount: /dev/sda2 is write-protected, mounting read-onlyubuntu@node01:~$ find /mnt/mnt/mnt/openstack/mnt/openstack/2012-08-10/mnt/openstack/2012-08-10/meta_data.json/mnt/openstack/content/mnt/openstack/latest/mnt/openstack/latest/meta_data.jsonubuntu@node01:~$ cat /mnt/openstack/latest/meta_data.json {    \"availability_zone\": \"\",    \"files\": [],    \"hostname\": \"node01\",    \"name\": \"node01\",    \"meta\": {},    \"public_keys\": {        \"mykey\": \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0tdpS1S83ZQPMzFVwJ603CiKyapIOSfjmofqlYExYm+UCcFuC1eUF+xYA/dwFucKbd6shdLxC/cSLtHilQjolyg32jKw8G0LwittPH7Npi1BSCmLg5xnwUML6Hh/g/r3Xjj0NfuqIMBiiwQR/XkCyWHt5tE8ztGCm14Mz4SSL8qFhPdZXF0r5O9iBsWCJ6OsabuPK3lZQUqeMnKynARocnqXa4KHUr1yEOM/VHMNnUuJbRPEJoFHPrqS+vHOwKPWBvfv8Eia6GxsCXt3Z+ioWYA4Ejed/Csv1kRAWLDA4xuExD4VjgHdpHPoPt1M3nv3BdhwJpDzhSrXumGdFZz79 ubuntu@p3-bifrost-01\"    },    \"uuid\": \"00000000-0000-0000-0000-000000000001\"}If you have not used OpenStack or any IaaS that uses metadata it can be a bit confusing. Suffice it to say that in image based deployment systems there is often some dynamic configuration you need to make, at least inject ssh keys, and this is done with a combination of information from some datasource, and cloud-init in the image.I should note that Bifrost generates this ConfigDrive automatically.ConclusionThis is a basic overview of using Bifrost. I have quite a bit more work to do in terms of understanding and customizing Bifrost, but in a few hours I had a basic system going that works well. If you are using Ubuntu Trusty you will probably have a bit of an easier time as opposed to Xenial, but I expect that will either improve or I will discover that I was doing something silly. :)The ability to automatically deploy a custom image to many baremetal servers is a basic requirement for a successful deployment of OpenStack or any other large, complicated system."
    } ,
  
    {
      "title"    : "Moore&#39;s Law and the Datacenter",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/12/03/moores-law-and-the-datacenter.html",
      "date"     : "2016-12-03 00:00:00 -0500",
      "content"  : "I’m a big fan of Packet Pushers and their various podcasts. Recently I listened to a podcast with Pradeep Sindhu who is the CTO and co-founder of Juniper Networks. I quite enjoyed the podcast, and while I think perhaps the hosts found Pradeep’s desire to explain and define a baseline for everything a bit long winded, I enjoyed his obviously science-driven attitude towards discussing technology. We should all strive to be more precise.Many people are aware that Moore’s Law, which Pradeep redefined as an observation, not a law, is ending. For decades we haven’t had to do much to get continual, massive (exponential) increases in computing power. But this is no longer. Instead, currently, we get access to more cores instead of faster CPUs. Clock speeds have maxed out. Now in order to achieve increases in computing power we have to look to parallelism, specialized hardware, and ways to deal with latency.NetworkingWhat I thought was interesting in this particular podcast is that this also applies to networking technology. A good example that was given is the fact that a 100 GB network connection is actually 4 25GB connections which are tied together (1). So even in networking we have hit the end of Moore’s Law and have to deal with parallelism, which is more complex. As Pradeep mentions in the podcast, most, if not all, applications will have to be (re)written to use parallelism, and those that do not will be left behind.That said there does seem to be some debate if Moore’s Law applies to networking at all, given it’s really bounded by the speed of light, and that managing latency is going to be what is important.  …the future of telecoms in general is firmly centred on managing latency due to contention between flows created by competing applications and users. This means scheduling resources appropriately to match supply and demand. - Five reasons why there is no Moore’s Law for networksBut then again, if we are going to use software to manage networks, then indeed that software will hit Moore’s law.I suppose the point is that in order to keep moving forward and gaining performance, networks will have to become smarter, and, likely, less predictable.  The path forward requires that the bandwidth our networks provide becomes smarter. The networks themselves need to become programmable platforms. The infrastructure needs to be as real-time, flexible and dynamic as our smartphones have become. The answer to the problem of increased demand on the network is to flip that phrase around and evolve to what can be called network-on-demand. Network topology, connectivity, service class and quality of service all need to be on-demand services that can be customized to suit the needs of the end user. - When Moore Is Not Enough – Why Our Growing Networks Require More SoftwareThis reminds me of the requirement of 5G to have very low latency. In fact I believe the target is 1ms (though I don’t see how that is possible, frankly).  Physics stands a bit in the way of this. The speed of light and electricity is limited and in 1 ms, light can only travel around 200 km through an optical fibre. So even if network equipment does not add any latency whatsoever, the maximum round trip distance is 100 km. - 5G Latency - 1ms - Is it Possible?Some of Pradeep’s suggestions are discussed in a Network World article:  The next step will be to use the network to interconnect multiple data centers with low latency connections. In this case, think of the cloud as a collection of pooled resources, like processing and storage, over geographic distance and being inter-connected with a high-performance network. In this scenario, its unlikely that network managers can continuously update the network fast and efficient enough to meet the needs of an environment that must scale over distance. This brings up step two of Pradeep’s Principle: network automation. This doesn’t mean simple automation like the provisioning of QoS or VLANs. Sindhu made it clear he was talking about a network that was almost autonomic and self learning in nature and could quickly adapt to any kind of environmental changes. - Pradeep’s Principle: Give up on Moore’s Law and embrace automationMany network engineers are unsure if software defined networking (SDN) is really all that useful. Pradeep suggests that in order to increase performance we are not going to have much choice but to look to SDN, network automation and even machine learning. Things are going to get a lot more complex in the network.Hardware is more important than everAnother interesting point made in the podcast is that hardware is more important than ever. Because we are nearing the end of Moore’s law, and thus general computing is no longer receiving the easy gains of the last few decades, we will likely see more specialized, single-purpose hardware being developed. This specialized hardware can realize the large performance gains we require to move ahead in computing as a species. Thus, while in many ways we have moved to commodity hardware in the datacenter, it’s quite likely we will also see a movement towards specialized hardware, especially in the realm of artificial intelligence, machine learning, and networking, among others. Certainly we see that already in terms of TCP offloading in network interface cards and chips like Barefoot Networks P4 processor.Further, given the recent advances in machine learning, there will likely be specialized hardware developed to increase performance in this area:  Strong demand for silicon tuned for algebra that’s crucial to a powerful machine-learning technique called deep learning seems inevitable, for example. Graphics chip company Nvidia and several startups are already moving in that direction… - Moore’s Law is Dead: Now WhatCo-processorsAs a simple systems administrator, I can see the value in “co-processors” and the easiest example I have is of a network card that is doing TCP off-loading or other intelligent network processing. One company working in this area is Netronome.These co-processors concern me a bit because that will mean a lot of the functionality will be embedded into the firmware of the network card, and there is not much worse for a systems administrator than firmware and it’s related issues, but there is probably no way around that if we are to continue improving intelligence and speed in the network. To enable performance we give up the ability to easily change software. That will be more costly, however.FPGAThis is not an area I understand well yet. However, recently AWS released a developer preview where users can rent large FPGAs. It does seem that they are currently difficult to program.  Amazon’s AWS cloud computing service today announced that it is launching a new instance type (F1) with field-programmable gate arrays (FPGAs). The promise of FPGAs is that they are essentially chips that you can reprogram on the fly and tune them for your specific applications, making them faster than traditional CPU/GPU combinations. - TechCrunchParadoxesThese thoughts lead me to a couple of paradoxes:1) Hardware is more important than everMost startups are based on software. Software is awesome. It’s (relatively) easy and cost effective. But as discussed so far in this post, given Moore’s law and our lost of easy performance gains with general computing, we will need specialized hardware.Futher, there has been a rapid movement to commodity hardware which essentially means many x86 servers. Thanks to virtualization, most workloads are heading to x86. I don’t expect to see AWS offer Power CPU based virtual machines, though Google always threatens with Power processors, most likely to be able to have some kind of bargaining…er chip…with Intel.That said, perhaps we just need some new inventions to get us past the limits of silicon CPUs. Certainly novel CPU technology is inevitable, but will it be enough, who will do it, and when?Also, co-processors have/will become much more important.2) Lower latency means datacenters and/or caching closer to end-usersIt seems to me that telecoms are selling off datacenters. Further, the public cloud is concentrating computing in fewer, much larger datacenters. If we are really going to be limited by latency, this seems to be a paradox. Perhaps there will simply be a point where we either can’t get faster networking, or we don’t need it. Then again people are still moving into cities, perhaps we’ll just have an AWS region per large city and they will be large data caches. Isn’t caching one of those hard computer science problems? :) That said, I believe Akamai is doing well.ConclusionThere is a lot happening in the IT world, and some of it is paradoxical. On one hand we have a serious movement towards the concentration of computing power in large, regional data centers (ie. AWS), but on the other the speed of light is a serious boundary and will require (some? most?) data and computing to be closer to end users. Further, software development gets most of the attention, but we are going to need considerable material and engineering breakthroughs in hardware to continue to increase computing performance. What’s more, despite the movement towards commodity hardware (ie. x86 + virtualization), we will need specialized physical systems in networking and machine learning.LinksHere are a few random links that I’ve read while writing this blog post.  After Moore’s Law  Barefoot Network’s Chips Will Transform the Tech Industry  Wikipedia - 100 Gigabit Ethernet  Google’s Tensor Processing Unit could advance Moore’s Law 7 years into the future  Google supercharges machine learning tasks with TPU custom chip  CenturyLink Reaches $2 Billion Deal to Sell Data Centers  Even if Moore’s Law is “running out,” there’s still plenty of room at the bottom  Five reasons why there is no Moore’s Law for networks  The Death of Moore’s Law Will Spur Innovation  AWS Developer Preview of FPGAFootnotes1) 4 25 gigabaud lanes"
    } ,
  
    {
      "title"    : "The First OpenStack Days Canada",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/11/23/openstack-canada-day.html",
      "date"     : "2016-11-23 00:00:00 -0500",
      "content"  : "On November 22nd, 2016 the first OpenStack Days Canada (OSDCA) was held in Montreal. OpenStack Days are regional events that bring together people using and operating OpenStack clouds. They are slightly bigger in size and geographic pull than your typical meetup, but not nearly as big as the OpenStack Summits. I believe the attendance at the first OpenStack Days Canada was around 190, which is very good. You can find a few tweets about the event with the #OSDCA hashtag.Basically they are a small, one or two day conference. There are speakers, panels, and some of them do workshops and such, and perhaps some kind of social evening event.Montreal a Hotbed of OpenStackThere is a ton of OpenStack related activity in Montreal. Actually, technology in general. Montreal is an extremely wired city. There are many data centers, service providers, and service providers for service providers. There are great schools here, relatively low cost of living, and many interesting companies. It’s probably the only place a boring anglophone like myself could live in Quebec, as English is pretty common, though I still feel like an doofus even ordering a coffee.Some of the companies that were at OpenStack days and which are based in the Montreal area:  Vexxhost  Inocybe  Internap  Ormuco  CloudOps  and more I’m sureThe TalksThere were two tracks of talks at OSDCA, a “user” track and a “fan” track. I jumped around a lot between the two tracks.Below are the talks I attended. I can’t remember what company everyone was from, but I remember a few.1 - Running Production Ready Kubernetes Cluster on Openstack by Ayrat KhayretdinovIt’s impossible to attend any kind of OpenStack technical event without hearing about Kubernetes from either the standpoint of 1) running the OpenStack control plane in k8s, or 2) running k8s in OpenStack on instances as part of a project. This talk was regarding option #2 (and I assure you the two models are completely different and almost not related at all).One of his main points is that currently k8s does not do multi-tenancy well, so one way to do that would be to deploy multiple k8s in different OpenStack projects.He also reminded me of the Murano application catalog system which I should look into again, as it does manage TOSCA templates (I believe), which is related to the work I do in Network Function Virtualization (NFV).2 - Ceph OSD hardware – a pragmatic guide by Piotr Wachowicz of Bright ComputingThis talk gave real examples of what kind of hardware and configuration is best for a Ceph cluster. There are real considerations to make in terms of the sizing of the Ceph nodes, especially in regards to the number and size of disks, the amount of SSD caching, and finally the size and utilization of the networking components. NUMA is also a concern, and frankly it seemed that one socket systems match up very well for Ceph storage nodes. It also sounded like PCIe SSDs would make a lot of sense in Ceph nodes as they can cache for a larger number of spinning disks.3 - Tales From OpenStack’s Gate: How Debugging the Gate Helps Your Enterprise by Matthew TreinishThis was part of a talk that was done at the Vancouver summit. I was quite interested in their use of Elastic Search, specifically their elastic-recheck setup. Hunting for bugs using Elastic Search is a pretty interesting subject.4 - InfraCloud, a Community Cloud Managed by the Project Infrastructure Team - by Paul BelangerThe OpenStack foundation is now running its own OpenStack cloud, called infracloud. The OpenStack CI system uses many, mostly public, OpenStack-based clouds to run millions of tests. Now they can also rely on their own internal cloud as one of those regions. Infracloud does not need to be highly available because if it is down they can just use one of their other clouds. But they are learning what it is like to actually run an OpenStack cloud, other than DevStack, which I think is very valuable to operators such as myself, as they will likely find bugs that other testing may not.Paul also reminded me of bifrost which is a way to deploy a stand alone Ironic system to manage baremetal servers. Basically you could use this instead of something like Cobbler.Finally, Paul went through their configuration management tooling which is a combination of Puppet and Ansible. They no longer use a true “puppet master” system, and instead run Puppet via Ansible, which is surprisingly quite common.5 - OpenStack and Opendaylight integration by Rashmi Pujar and Mohamed Elserngawy from InocybeI’m a big fan of OpenDaylight. I’m hoping to add OpenDaylight to my OpenStack lab environment, which is currently running “vanilla” OpenStack networking, and only using provider networks.The talk was quite dense, and they had a demo as well, and frankly they just ran out of time. It’s great to see a Canadian company, Inocybe, trying to lead the way in an important open source technology like OpenDaylight.Many of the advanced networking features being worked in the OpenStack ecosystem are first developed using OpenDaylight, so if you want to be on the cutting edge of things like “service function chaining” then running OpenDaylight can be important.6 -Store All the Things: Storage options for your OpenStack Cloud by Sean McGinnisSean is the Primary Technical Lead (PTL) of Cinder. He discussed Cinder (block storage), Manila (shared file systems) and Swift (object storage). I often forget about Manila, and it would be an important and useful system for applications that need shared storage.His talk would be useful to people who aren’t sure how to use storage in an OpenStack cloud.7 - OpenStack adoption stories: Linux Foundation &amp; Opta Insurance Services a panel with with Konstantin Ryabitsev (Linux Foundation) and Jin Lee (Opta Insurance, a division of SCM insurance) moderated by Mohammed Nasser (Vexx Host).This was probably the most interesting discussion for me as Jin Lee from Opta is a mathematician with the company and they do machine learning. He was very positive about their use of OpenStack and how easy it was for him to start writing basic code to spin up hundreds of instances to run their machine learning algorithms. The OpenStack foundation would have been very happy to hear his comments. If I remember correctly they use a combination of in-house OpenStack and Vexxhost to obtain computing resources.He also mentioned speed as being important, in terms of their ability to run algorithms, discover information, and then create microservices that can provide that information. If they can provide services to their customers faster than their competitors, that is an advantage.Toronto is NextThe 2017 OpenStack Days Canada will be in Toronto. I look forward to attending and perhaps next year I will submit a talk.Thanks to the organizersI’m sure a lot of work went into this event. Thanks to the sponsors and organizers of the event, and I look forward to the 2017 OpenStack Days Canada."
    } ,
  
    {
      "title"    : "Demystifying Kubernetes and OpenStack",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/11/23/demystifying-kubernetes-and-openstack.html",
      "date"     : "2016-11-23 00:00:00 -0500",
      "content"  : "OpenStack on k8s and/or k8s on OpenStackThere seems to be some confusion around using Kubernetes (k8s) and OpenStack together. As an OpenStack Operator, it bugs me a little bit that two very different models of using OpenStack and Kubernetes together are often conflated, occasionally for marketing purposes. In this post I want to identify the two major ways they work together and which is which.##tl;drThe two main ways of using OpenStack and k8s together are:1) Use k8s to manage the OpenStack Control PlaneYou can use k8s to manage the OpenStack control plane. Many people don’t realize (especially VMWare administrators), that OpenStack has a fairly complex and resource intensive control plane. For example, if you are buying hardware for an OpenStack deployment, you need to buy hardware for the compute nodes AND the control plane. Typically OpenStack Operators use (n/2)+1 nodes for the control plane due to the use of distributed systems for messaging and state.This would be a methodology used by OpenStack Operators.2) Install k8s into an OpenStack ProjectOnce you have deployed an OpenStack cloud, you can then deploy k8s into a project within that cloud, ie. you, as an OpenStack end-user, provision virtual machines within OpenStack and deploy k8s onto those virtual machines.This would be a methodology used by OpenStack end-users.k8s and the OpenStack Control PlaneOpenStack is a complex system. It requires many API servers and other services in order to work. APIs and other OpenStacks services are typically Python based servers. Supporting infrastructure such as MySQL/MariaDB and RabbitMQ, among others, are also required. These systems all have to run somewhere, and this somewhere makes up the OpenStack control plane.For example, Nova, which provides compute in OpenStack, is made up of several servers, a few of which I list below:/usr/bin/nova-api/usr/bin/nova-cert/usr/bin/nova-conductor/usr/bin/nova-consoleauth/usr/bin/nova-novncproxy/usr/bin/nova-schedulerEach of those services could be run on a monolithic “controller” server, ie. a single physical server, or they could each be split out into their own container (or virtual machine).Using containers can make an OpenStack Operators life easier, especially when it comes to upgrading, or when you would like to run different versions of particular services which can often have dependency issues.If you decide to use containers to deploy OpenStack services, then you will either have to manually schedule them, or you will need some kind of Container Orchestration Engine (COE) such as k8s, Docker Swarm, or something else. (I should note that in talking with other OpenStack Operators, it is not actually that common to use a COE to manage containers. Quite often containers are “manually” scheduled using some kind of configuration management tool such as Puppet or Ansible, both of which can provision containers.)If you would like to use a COE, or have additional feature requirements, then using k8s may be of value to you. You could use k8s to manage each OpenStack service. Not only can k8s schedule containers, it also has some additional features that helps to keep those containers running. But it also brings additional complexity. k8s still requires care and feeding like any complex system. If an OpenStack Operator is running 60 containers on 3 physical servers, it may not be worth the additional complexity to run k8s.In this example, k8s would only run the services required to provide the OpenStack control plane.Certain OpenStack distributions, such as Mirantis’, are working to alter their deployment system to use k8s as the underlying COE for the OpenStack control plane.Running k8s in OpenStackThis is likely the more common use of k8s and OpenStack together. OpenStack would be used like any other Infrastructure as a Service system (IaaS), be it public or private cloud. You would somehow provision virtual machines within and OpenStack project/tenant and then deploy k8s to those virtual machines. k8s would then be running in OpenStack.There are many ways to install k8s into OpenStack, from using kubeup.sh (which I believe supports OpenStack) to using the OpenStack Magnum project, and many others. I don’t want to list them all here, but suffice it to say that there are many ways to get k8s installed into an OpenStack cloud. Likely one would use some kind of automated k8s-onto-OpenStack deployment system.ConclusionHopefully I’ve demonstrated that there are two major ways that k8s and OpenStack can work together, and that they are quite different. By far the most common way of using k8s and OpenStack together will be to deploy a k8s cluster into an OpenStack tenant/project. That said, managing the OpenStack control plane with k8s could also be beneficial, but it does bring additional complexity.Finally, to make things confusing, one could run the OpenStack control plane on k8s, and then also install another k8s cluster, or multiple k8s clusters, into the OpenStack cloud. Fun!"
    } ,
  
    {
      "title"    : "Rsyslog to Elasticsearch",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/11/11/ryslog-to-elasticsearch.html",
      "date"     : "2016-11-11 00:00:00 -0500",
      "content"  : "For decades systems administrations have known that it’s important to centralize logs, be it for troubleshooting or security reasons. In my case, not only do I want to centralize logs, but I also want them to be searchable. (Not that grep on a centralized log file isn’t powerful, but that’s not the solution I’m looking for at this time.)ElasticsearchElasticsearch has also been around for a while.Elasticsearch:  … is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected.Bascially it allows storing, indexing, and searching data. Typically it’s part of a stack, the “ELK” stack, made up of Elasticsearch, Logstash, and Kibana.As a systems administrator, I think it’s also important to note that Elasticsearch is, when used as a cluster, a distributed system, which means Aphyr has tested it. One shouldn’t take the addition of yet another distributed system lightly. As an example, I have MySQL Galera, RabbitMQ, Nomad, and now Elasticsearch for distributed systems in production right now.Rsyslog directly to ElasticsearchThe point of this post is to show how to use rsyslog to send logs directly into an Elasticsearch cluster. Currently I am not using the L part of the stack, meaning I have no Logstash. I’m just using rsyslog to send log messages directly into Elasticsearch, and I use Kibana as a graphical interface to search logs. I would imagine that in the future I will use Logstash, but for now I am not.I’ve put up a very simple Ansible role to configure rsyslog to send logs to Elasticsearch. I run Haproxy in front of the Elasticsearch cluster so I send all the logs to the Haproxy virtual IP, which loadbalances across the Elasticsearch cluster. But other than that, this is what I use, and it has been taking in millions of logs, probably 30/second, for a few weeks.omelasticsearchomelasticsearch does all the work. You just need a rsyslog version recent enough to have that module. Then add in a configuration file which points to your Elasticsearch cluster, and you’re done. Quite simple actually.Here is the package:$ dpkg --list rsyslog-elasticsearchDesired=Unknown/Install/Remove/Purge/Hold| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)||/ Name                                  Version                 Architecture            Description+++-=====================================-=======================-=======================-===============================================================================ii  rsyslog-elasticsearch                 8.16.0-1ubuntu3         amd64                   Elasticsearch output plugin for rsyslogAnd here is the configuration file:$ cat templates/etc/rsyslog.d/elasticsearch.conf module(load=\"omelasticsearch\") # Elasticsearch output module# this is for index names to be like: logstash-YYYY.MM.DDtemplate(name=\"logstash-index\"  type=\"list\") {    constant(value=\"logstash-\")    property(name=\"timereported\" dateFormat=\"rfc3339\" position.from=\"1\" position.to=\"4\")    constant(value=\".\")    property(name=\"timereported\" dateFormat=\"rfc3339\" position.from=\"6\" position.to=\"7\")    constant(value=\".\")    property(name=\"timereported\" dateFormat=\"rfc3339\" position.from=\"9\" position.to=\"10\")}# template to generate JSON documents for Elasticsearch in Logstash formattemplate(name=\"plain-syslog\"  type=\"list\") {    constant(value=\"{\")    constant(value=\"\\\"@timestamp\\\":\\\"\")         property(name=\"timereported\" dateFormat=\"rfc3339\")    constant(value=\"\\\",\\\"host\\\":\\\"\")            property(name=\"hostname\")    constant(value=\"\\\",\\\"severity-num\\\":\")      property(name=\"syslogseverity\")    constant(value=\",\\\"facility-num\\\":\")        property(name=\"syslogfacility\")    constant(value=\",\\\"severity\\\":\\\"\")          property(name=\"syslogseverity-text\")    constant(value=\"\\\",\\\"facility\\\":\\\"\")        property(name=\"syslogfacility-text\")    constant(value=\"\\\",\\\"syslogtag\\\":\\\"\")       property(name=\"syslogtag\" format=\"json\")    constant(value=\"\\\",\\\"message\\\":\\\"\")         property(name=\"msg\" format=\"json\")    constant(value=\"\\\"}\")  }action(type=\"omelasticsearch\"  server=\"{{ elastic_search_ip }}\"  serverport=\"9200\"  template=\"plain-syslog\"  # use the template defined earlier  searchIndex=\"logstash-index\"  dynSearchIndex=\"on\"  searchType=\"events\"  bulkmode=\"on\"                   # use the Bulk API  queue.dequeuebatchsize=\"5000\"   # ES bulk size  queue.size=\"100000\"   # capacity of the action queue  queue.workerthreads=\"5\"   # 5 workers for the action  action.resumeretrycount=\"-1\"  # retry indefinitely if ES is unreachable  errorfile=\"/var/log/omelasticsearch.log\") CuratorYou will probably want to prune the logs that are entered into your ES cluster, that is unless you have a lot of storage space. I am using Curator to do that. The omelastic module configuration show above adds indices with names like “logstash-YYY-MM-DD” and you can use that pattern with a curator action file to delete indices older than a certain number of days. I have a daily job that is run out of a Nomad cluster to prune indices over 30 days old.$ curl 'elasticsearch-vip:9200/_cat/indices?v'health status index               pri rep docs.count docs.deleted store.size pri.store.size SNIP!green  open   logstash-2016.11.04   5   1     158184            0     55.4mb         27.7mb green  open   .kibana               1   1          7            1     72.1kb           36kb green  open   logstash-2016.11.05   5   1     538415            0      181mb         90.5mb green  open   logstash-2016.11.06   5   1     538909            0    181.1mb         90.5mb green  open   logstash-2016.11.07   5   1     591699            0    219.6mb        109.9mb green  open   logstash-2016.11.08   5   1     574186            0      207mb        103.5mb green  open   logstash-2016.11.09   5   1     626842            0    231.8mb        115.9mb green  open   logstash-2016.11.10   5   1     570544            0    203.8mb        101.9mb green  open   logstash-2016.11.11   5   1     350968            0      145mb         69.9mb PerformanceI should also note that this configuration may not be as performant as you would like. There are certainly considerations to make in terms of the number of indices, and shards, and their size, and how that relates to performance of the ES cluster over time. I would doubt that this example will be useful in large systems and would require a fair amount of tuning. Please keep that in mind. :)ConclusionThis is a pretty simplistic example of Elasticsearch use. But I think it’s helpful because it can get you up and running, logging to Elasticsearch, quickly, and from this point you can work on improving things as you learn about ES. That’s my plan anyways. :)"
    } ,
  
    {
      "title"    : "OpenStack and ETSI MANO",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/11/02/openstack-etsi-mano.html",
      "date"     : "2016-11-02 00:00:00 -0400",
      "content"  : "The summit for the Newton release of OpenStack was held in Barcelona. I attended and found it to be a very interesting summit, one that, as usual, marks change for OpenStack.OpenStack has always had more existential crisis than other open source projects. Previous crisis have regarded things like CloudStack (remember that?), the “public cloud” (which is still partially ongoing), the “big tent,” the continued–but improving–separation of developers and users, whether or not it is simply a vendor dumping ground, etc, etc.The current crisis would be around its growing adoption by telecommunications companies. As we enter a phase of OpenStack’s lifecycle in which it sees serious adoption by extremely large, conservative, (cough inefficient) enterprises I would imagine that we could very well see OpenStack slowly change into something that might be unrecognizable to early adopters.TelecommunicationsOne of the biggest changes that has occurred in the past year for OpenStack is that telecommunication companies have (finally) begun adoption. And they are going all in, so to speak.Telecommunications is potentially a 1 trillion dollar industry. Even a small percentage of that is a huge sum of money, and given the relatively minimal size of the OpenStack market share, is considerable in terms of how it could sway OpenStack’s development and progress. (1)Jay Pipes, someone who has been around the community for some time, has thoughts, or at least a tweet, on where he thinks OpenStack is going:Prediction: in less than 2 years, Telcos will have fully taken over #OpenStack and converted it into a purpose-built NFVi+MANO solution&mdash; Jay Pipes (@jaypipes) October 25, 2016MANO - Management and OrchestrationFor those of you unfamiliar with the acronym MANO, it stands for “Management and Orchestration.” The standards group ETSI has defined a large part of the way that telecommunications companies are going to perform Network Function Virtualization (NFV).At this point OpenStack mostly makes up the NFV infrastructure (NFVi) and Virtualization Infrastructure Manager (VIM) components, and, in my opinion, currently stops there. The ETSI MANO layer, however, sits on top of all of this infrastructure, which would likely include many OpenStack clouds in many datacenters as well as other components such as Software Defined Networking (SDN) controllers, and magically manages and orchestrates it. (4)To suggest that OpenStack will also own the MANO component is surprising. So far, other than relatively unused multi-region support, OpenStack has not had many projects dealing with managing multiple clouds, or external systems outside of its own particular realm. Adding a MANO system would be a big change for the OpenStack project, one I’m not sure will happen. There are several open source MANO projects (2) and I have difficulty seeing them being moved within the OpenStack namespace. But, it could happen.An example of an OpenStack project within reach of the MANO layer is the Tacker (3) project. Tacker is a generic Virtual Network Function Manager (VNFM) and a Network Function Orchestrator (NFO) but (I guess) not a full fledged MANO. But in effect it can act as one, as one of Tacker’s features is the “[a]bility to orchestrate VNFs across Multiple VIMs and Multiple Sites (POPs).” So in some respects there is already evidence of OpenStack working (somewhat?) at the MANO level. Once could see a MANO project coming into existence (if it’s not there already) and use Tacker as the VNFM component, and the rest of OpenStack as the NFVi and VIM blocks.However, these MANO layers are going to be extremely important to the success of telecoms as they migrate/evolve to NFV. Further, these MANO layers are going to be extremely complicated and feature rich. My impression is that they will attempt to be all-encompassing.(5,6) They will have to talk to all kinds of complex systems. The MANO layer might simply be too big to be part of the OpenStack project and may more likely end up beneath the Linux Foundation (which is where Jay Pipes says OpenStack will end up eventually anyways).Common sense suggests that OpenStack remain a set of primitives that higher level abstractions can use. So this MANO layer can use Neutron, Nova, Heat, Telemetry, etc, etc, to perform its job across multiple, complex systems, and not just OpenStack clouds. But, perhaps instead of common sense this is really just opinion, and other people and organizations will have a differing one.Footnotes  “Revenue from OpenStack, the open source software platform for cloud computing, will likely top $5 billion by 2020 and grow at a 35 percent compounded annual growth rate, according to 451 Research. While that growth rate is strong, the platform’s overall revenue is still fairly small compared to that of market leaders like VMware and Amazon Web Services (AWS), the firm said.” - via SDXCentral and 451 Research  Eg. OSM and Open-O  Tacker  Should have called it ETSI MAGI. ;)  Video: OpenStack and the Orchestration Options for Telecom NFV  I would expect all-encompassing MANO systems, ones that try to take over everything (hint vendor lock-in hint) to fail."
    } ,
  
    {
      "title"    : "Distributed Cron With Nomad",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/10/18/distributed-cron-with-nomad.html",
      "date"     : "2016-10-18 00:00:00 -0400",
      "content"  : "Everybody loves Hashicorp! Most technologists have probably used Vagrant at some point. Besides Vagrant, Hashicorp has other great technologies, however, in this post I’d like to talk about Nomad.NomadNomad is:  A Distributed, Highly Available, Datacenter-Aware SchedulerNomad has many use cases. For example it has a Docker driver which means it can handle running Docker containers. I prefer to think of it as a job scheduler. You want something done, you create a job in Nomad and Nomad handles it for you. In this blog post, I describe a specific example: using Nomad to handle cron jobs, such as backing up a MySQL database.Caveats  I should note that the example discussed is in a lab setting. But it is backing up an OpenStack MySQL Galera cluster in a permanent lab deployment, so it’s important data.  Instead of running Nomad as root, I run it as the nomad user. This might get in the way if you are planning on using it to run docker workloads. I don’t know because I haven’t done that.  Because I’m not running Nomad as root, I’m allowing the use of raw_exec.  I’m not using a Consul cluster. Instead I’m using Nomad’s built in clustering capability. I would imagine in a large production environment you would have a Consul cluster running.Setting Up a Nomad ClusterOne thing I like about Nomad, and various Go-based systems, is that they are distributed in a single binary. Add a user, and a systemd startup file, and you are done. Well, installing anyways.I have some Ansible that takes care of this, but in way of illustration, below is the configuration on a server member.First, the base.hcl file.ubuntu@NODE-1-nomad-server:/etc/nomad$ cat base.hcl # Increase log verbositylog_level = \"DEBUG\"# Setup data dirdata_dir = \"/var/lib/nomad\"bind_addr = \"172.17.5.129\"# Enable debug endpoints.enable_debug = trueNow the server.hcl file. The retry join IP is the IP of the first Nomad cluster member.ubuntu@NODE-1-nomad-server:/etc/nomad$ cat server.hcl server {    enabled = true    bootstrap_expect = 3    retry_join = [\"172.17.5.129\"]}The nice thing about Nomad configuration is that it can be additive, so to speak, in that we can have a base file and then a server configuration and a client configuration. The server configuration would only be deployed to servers, and the client with the clients, but the base configuration to all nodes. It’s good for configuration management.Once the server members come up you can check their status:ubuntu@NODE-3-nomad-server:~$ nomad server-members -address http://172.17.5.126:4646Name                           Address       Port  Status  Leader  Protocol  Build  Datacenter  RegionNODE-1-nomad-server.global  172.17.5.129  4648  alive   true    2         0.4.1  dc1         globalNODE-2-nomad-server.global  172.17.5.127  4648  alive   false   2         0.4.1  dc1         globalNODE-3-nomad-server.global  172.17.5.126  4648  alive   false   2         0.4.1  dc1         globalAnd we are done with the cluster.Setting Up a Nomad ClientSame binary, same startup, same user, same base.hcl, remove server.hcl and add in client.hcl.The server IPs are those of the three Nomad cluster servers.ubuntu@NODE-1-mysql-backup:/etc/nomad$ cat client.hcl client {    enabled = true    servers = [ \"172.17.5.129\",\"172.17.5.127\",\"172.17.5.126\" ]    # Enable raw_exec. We are not running nomad as root.    options = {        \"driver.raw_exec.enable\" = \"1\"    }}Once the clients startup you can check their status:ubuntu@NODE-3-nomad-server:~$ nomad node-status -address http://172.17.5.126:4646ID        DC   Name                    Class   Drain  Status306dcf6b  dc1  NODE-3-mysql-backup  &lt;none&gt;  false  ready6a3109d5  dc1  NODE-1-mysql-backup  &lt;none&gt;  false  ready8e3abc4a  dc1  NODE-2-mysql-backup  &lt;none&gt;  false  readyPeriodic JobNomad supports periodic jobs. To my simplistic layperson mind that means cron jobs…but distributed cron jobs!job \"mysql-backup\" {  datacenters = [\"dc1\"]  type = \"batch\"  constraint {    attribute = \"${attr.unique.hostname}\"    regexp = \".*mysql-backup\"  }  periodic {    // Launch every hour    cron = \"0 * * * * *\"    // Do not allow overlapping runs.    prohibit_overlap = true  }  task \"run-mysql-backup\" {    driver = \"raw_exec\"    config {      # When running a binary that exists on the host, the path must be absolute      command = \"/usr/local/bin/mysql-backup.sh\"    }    resources {      # defaults    }  }}Certainly the job shown above is overly simplistic. I haven’t put in many jobs or read through the jobs documentation properly. But the above job seems to be working and it is using at least one constraint in that the hostname of the nomad client must end in “mysql-backup.” I have three mysql-backup nodes running, thus on an hourly basis the job will run on one, and only one, of the three nodes. Each of the three mysql-backup nodes are LXC containers running on a different physical hosts, so the idea is that if one of the containers, or the host, becomes unavailable, the job will still continue to run on the surviving nodes.Also I should read up more on resources. I don’t think the default resources provide much in the way of CPU and memory.Once the job is added to the cluster, it looks like this:ubuntu@NODE-3-nomad-server:~$ nomad status -address http://172.17.5.126:4646 mysql-backupID                   = mysql-backupName                 = mysql-backupType                 = batchPriority             = 50Datacenters          = dc1Status               = runningPeriodic             = trueNext Periodic Launch = 10/18/16 23:00:00 UTC (37m51s from now)Previously launched jobs:ID                                Statusmysql-backup/periodic-1476824400  deadmysql-backup/periodic-1476828000  deadIn this example there have only been two runs of the job created, and strangely they are listed as having a status of “dead”, but I believe that is actually the correct status as the jobs have completed. Slightly confusing terminology.ubuntu@NODE-3-nomad-server:~$ nomad status -address http://172.17.5.126:4646 mysql-backup/periodic-1476824400ID          = mysql-backup/periodic-1476824400Name        = mysql-backup/periodic-1476824400Type        = batchPriority    = 50Datacenters = dc1Status      = deadPeriodic    = falseSummaryTask Group        Queued  Starting  Running  Failed  Complete  Lostrun-mysql-backup  0       0         0        0       1         0AllocationsID        Eval ID   Node ID   Task Group        Desired  Status    Created At5e661d1e  373600be  306dcf6b  run-mysql-backup  run      complete  10/18/16 21:00:00 UTCConclusionHigh performance computing experts are probably used to being able to run batch jobs like this, certainly the idea has been around for a long time, but I am not an HPC specialist. I just want a simple distributed (lol) system that can run a backup script on any available node. Plus I like trying out “new stuff” even if it partially implements older ideas.More to come in future posts!"
    } ,
  
    {
      "title"    : "Getting an AWS Instance&#39;s Region with Boto",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/10/01/boto-get-region.html",
      "date"     : "2016-10-01 00:00:00 -0400",
      "content"  : "This is a quick post on how to get the region an AWS instance is in using Boto.tl;drI’m using a virtualenv with boto installed in an instance in AWS. To use boto.ec2.connect_to_region() I need to provide the region. But how do I know that without having to hard code it or get it from a config file? I can use the instances AWS metadata.(venv) ubuntu@ip-172-31-14-171:~$ cat region.py #!/opt/venv/bin/python2.7import boto.utilsdata = boto.utils.get_instance_identity()region_name = data['document']['region']print region_name(venv) ubuntu@ip-172-31-14-171:~$ ./region.py us-west-1Now I know what region the instance is in programmatically.A bit moreAWS instances can have an IAM role that allows them to do many things in AWS, such as creating instances or uploading files to S3 or creating “virtual private clouds” (VPCs, ie. networking). They don’t have to have API IDs or keys, they can just be given roles on creation. Obviously, once they are deleted they no longer have that access.Further, the Boto library is smart enough to know when it is running in an AWS instance.This way we can list all ec2 instances from within a particular instance without having to provide it specific credentials. However, in order to connect to ec2 using Boto we need to provide the region. As mentioned, thankfully Boto provides a way to do that prior to connecting using the instances metadata.(venv) ubuntu@ip-172-31-14-171:~$ cat connect.py #!/opt/venv/bin/python2.7import boto.utilsimport boto.ec2data = boto.utils.get_instance_identity()region_name = data['document']['region']conn = boto.ec2.connect_to_region(region_name)for i in conn.get_only_instances():   print i(venv) ubuntu@ip-172-31-14-171:~$ ./connect.py Instance:i-5ed52deaThat’s it! Now depending on the roles and permissions the instance has in terms of AWS, it can use those APIs to manage all kinds of instructure and tools.Hat tip to this Stackoverflow post."
    } ,
  
    {
      "title"    : "Kubernetes the Hard Way in AWS with Ansible",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/09/05/kubernetes-the-hard-way-in-aws-with-ansible.html",
      "date"     : "2016-09-05 00:00:00 -0400",
      "content"  : "Even though I mostly work with OpenStack, I also quite like Amazon Web Services (AWS). Further I am doing a lot of work with containers, and have been doing so for a while–I was messing around with Mesos two years ago.Kubernetes has a lot of attention right now. Docker has slightly cooled off, but is moving rapidly forwards technology-wise. Kubernetes has recently reached some kind of front-runner status in terms of container management systems, though I should note that Docker is being used as the container runtime in all the examples discussed in this blog post. Kubernetes does not provide containers directly, and instead uses an underlying system such as Docker, or more recently, rkt for that layer. Kubernetes focuses on managing the deployment of containerized applications.tl;drI’ve released some Ansible playbooks and documents that will deploy Kubernetes into AWS, and configure Kubernetes to manage some AWS features like route tables and elastic load balancers.Kubernetes the Hard WayA couple months ago Kelsey Hightower released a set of documents entitled Kubernetes the Hard Way (KtHW) which details how to manually deploy k8s. k8s comes with a kube-up.sh script that will deploy it to several IaaS providers, but it hides much of the deployment complexity, which is considerable. I don’t see how you could run k8s in production with just the kube-up.sh script.I took the example that Mr. Hightower had provided and altered it to use AWS (instead of Google Compute Engine) and also use Ansible instead of running individual commands. But I still kept the “one step at a time” mentality, which combines perfectly with Ansible when using multiple playbooks for each infrastructure layer.If you’d like to try out Kubernetes the Hard Way in AWS with Ansible (KtHWAA) then you can take a look at this Github repository. If you run into any errors, please don’t hesitate to submit an issue via Github and I will work on getting it fixed right away.k8s in AWSk8s integrates well with AWS. For example, using the networking methodology that is shown in KtHW, route tables need to be setup in the VPC to point the node networks to particular nodes. k8s can do this automatically.Further, when you create a service in k8s, it can automatically configure an AWS EC2 load balancer to expose the service externally. This includes creating appropriate security groups.There are a few other integrations that I have yet to configure in my KtHWAA repository, but the two important ones, 1) pod routes and 2) ELB are completed and work great.The ability of k8s to configure the IaaS around it is quite amazing. I’m sure seasoned AWS users understand that IAM roles and policies are powerful, but to me it means something special because the infrastructure I create in AWS, such as k8s, can also configure AWS features. It seems obvious, but that is extremely dynamic. This capability is something that is lacking in OpenStack–the ability to create roles and policies and provide components, such as a virtual machine, the ability to alter the tenant infrastructure around it.I’m also impressed that the k8s project has put so much work in AWS integration. Given k8s is a Google led project, I’m happy to see them supporting other IaaS providers. So kudos to Google and the other members of the k8s project team.Using AWS with AnsibleI ran into a few funny issues with Ansible modules and AWS, but overall I’m happy with how things turned out. I could do everything I needed to with Ansible, though certainly there are some improvements to be made. Even though Ansible has been around for a while the modules are constantly improving and as AWS continues to grow at an unprecedented pace I’m sure the Ansible will get better and better in terms of managing AWS, as will my ability to properly use both.One example is that, at this time, is not an ec2_group_facts module, so you can only get information about a security group if you create it with the ec2_group module. But that will be easily fixed.Forward Looking StatementI plan on doing a lot more work with k8s in AWS. There are at least of couple of areas, 1) volumes and 2) auto scaling, that I would like to investigate. k8s is powerful but complicated, and the real goal isn’t to be someone running k8s infrastructure, but rather to use k8s to run actual applications.AWS is extremely powerful and granular. Through creating KtHWAA I learned a lot about AWS Virtual Private Cloud (ie. networking) as well as the load balancer service. I’ve always been a big fan of AWS spot instances, so I use those as well (though I wouldn’t use them as much in a production instance). As mentioned, IAM roles and policies are a huge feature and differentiator, though I haven’t used other IaaS systems as extensively.Regarding networking in k8s, I don’t completely understand the Container Networking Interface (CNI) setup, so there is some reading and experimentation to do in that area as well.Thanks!Thanks to Kelsey Hightower for releasing KtHW, as well as the team that put together KtHW with AWs, which replaces GCE commands with similar AWS ones."
    } ,
  
    {
      "title"    : "Installing ZFS in an AWS EC2 Instance Using User-Data",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/09/05/aws-zfs-user-data.html",
      "date"     : "2016-09-05 00:00:00 -0400",
      "content"  : "Quick post on installing ZFS into an AWS instance using user-data and cloud-init.I’m doing some work with Kubernetes, which uses Docker, and Docker can use ZFS as a backing store. I want ZFS configured before I do anything else, and the best way to do that is to either create a specific Amazon machine image (AMI) or to use user-data. I chose the former.Note that this is an instance-store AMI and has two block devices, xvdb and xvdc.#cloud-config packages: - zfsutils-linuxruncmd: - umount -f /mnt - zpool create -f zpool-docker /dev/xvdb /dev/xvdc - zfs create -o mountpoint=/var/lib/docker zpool-docker/dockerThat’s it.Once the instance is booted up, you can login and see that there is a /var/lib/docker file system mounted from the zpool-docker zpool.root@docker0:/home/ubuntu# zfs list /var/lib/dockerNAME                  USED  AVAIL  REFER  MOUNTPOINTzpool-docker/docker   265M  29.0G  1.59M  /var/lib/docker"
    } ,
  
    {
      "title"    : "Ansible and Ubuntu 16.04 Xenial - Get Python 2.7",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/08/16/ansible-python2-xenial.html",
      "date"     : "2016-08-16 00:00:00 -0400",
      "content"  : "Quick post on getting Ansible working with an Ubuntu 16.04/Xenial server.I have a bunch of nodes running in Amazon Web Services (AWS) and I need to put them under configuration management. This is a default Xenial image which does not have Python 2.7 apparently. No problem!I can use the raw module to get Python 2.7.ansible_host$ ansible --sudo -m raw -a \"apt-get install -y python2.7 python-simplejson\" some_groupSNIP!Setting up python2.7 (2.7.12-1~16.04) ...Setting up libpython-stdlib:amd64 (2.7.11-1) ...Setting up python (2.7.11-1) ...Setting up javascript-common (11) ...Setting up libjs-jquery (1.11.3+dfsg-4) ...Setting up python-simplejson (3.8.1-1ubuntu2) ...Now I can run ansible ping.$ ansible -m ping some_group10.20.1.30 | success &gt;&gt; {    \"changed\": false,     \"ping\": \"pong\"}10.20.1.25 | success &gt;&gt; {    \"changed\": false,     \"ping\": \"pong\"}10.20.1.113 | success &gt;&gt; {    \"changed\": false,     \"ping\": \"pong\"}Done!Of course it would probably be a better idea to create an AWS image that has Python 2.7 already installed.Futher, if you want to do it from a playbook, just make sure to set gather_facts to false.---- hosts: all  gather_facts: False  tasks:    - name: ensure python 2.7 is installed      raw: apt-get install -y python2.7 python-simplejson       # try pinging now    - ping:"
    } ,
  
    {
      "title"    : "RedHat OpenStack Director - Part 1 - Overview",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/07/23/redhat-openstack-director-part-1.html",
      "date"     : "2016-07-23 00:00:00 -0400",
      "content"  : "Due to the complexity, number of services, clustered systems, and other components that comprise an OpenStack cloud, some kind of tooling is required to manage its deployment (and hopefully life-cycle over time). There are several systems available to manage an OpenStack cloud, such as Mirantis’ Fuel, OpenStack Ansible, upstream Tripleo, to name only a few.RedHat also offers such a product: Director. At least I think it’s called Director, and I believe is part of the RedHat OpenStack Platform.In this blog post I’ll do a quick overview of RedHat Director 8 with regards to using it to install an OpenStack cloud. I should note that I am still working to understand some functionality of Director, especially around custom modification and post-deployment changes–though things are not looking well on those two points at this time. However, more investigation and experimentation is required. So there will be a second post on Director in the near future.I should also note that I usually prefer to deploy OpenStack myself, most commonly using Cobbler and Ansible, though at some point I’d prefer to try to use Ironic over Cobbler. I find that I can deploy a complex OpenStack cloud in a couple thousand lines of Ansible. Whether or not to use an OpenStack deployment tool is a difficult discussion, though I can understand why deployment tools such as Director are chosen. It’s similar to discussions on web frameworks. Some people like them, some people don’t. Occasionally it’s not possible to use a framework, but most of the time it’s a good decision–as long as they are flexible.Features of OpenStack Deployment ToolsMost OpenStack deployment systems do at least the following in some manner:  Baremetal operating system deployment  Network design/deployment  Configuration management  Orchestration of services  Vetted OpenStack configuration options  OpenStack version upgrades  Monolithic or container based deployment  Plugin based architecture  Engineering decisions made for you  Reduction of complexity through abstraction layer  Occasionally Day 2 operational tooling  HA deployment using clustered/distributed systemsEach of these systems will make different, though similar decisions. For example RedHat Director uses Ironic to manage baremetal operating systems, whereas Mirantis Fuel uses Cobbler. While Director’s configuration management component is mostly based on upstream OpenStack Puppet modules, OpenStack Ansible uses…you guessed it…Ansible. Regarding OpenStack control services, Director currently takes a monolithic approach and deploys all control services onto a single operating system instance (though can have multiple controllers), OpenStack Ansible deploys tens of LXC based containers with each major service running in its own container.##What I like about DirectorThere are a few things that I like about RedHat’s Director:  It’s command line and template based and overall quite sparse  It uses Ironic for baremetal OS management  It works in terms of a repeatable installation  That it’s based on midstream and upstream opensource projects  Undercloud/overcloud modelSparseDirector is quite sparse. There is no fancy web interface, or at least not one that I’m aware of. I don’t mind because I prefer to use the command line, and more specifically, if there is no web GUI, and the deployment is based on templates and the openstack overcloud command, then the deployment definition can be stored in a source code repository. This makes it much easier to have repeatable installations.IronicOpenStack Ironic is an interesting project used to manage baremetal (ie. physical servers) instead of virtual machines.  Ironic is an OpenStack project which provisions bare metal (as opposed to virtual) machines by leveraging common technologies such as PXE boot and IPMI to cover a wide range of hardware, while supporting pluggable drivers to allow vendor-specific functionality to be added.As mentioned, I typically use Cobbler for this part of an OpenStack deployment, but I’m enjoying using Ironic with Director. That said, Ironic is much more complicated than Cobbler, and things that are easy with Cobbler are not so easy with Ironic. However, they are much different systems and even though they effectively do the same thing, I’m not sure it’s possible to compare them. Given my preference for an undercloud, it would make sense to incorporate Ironic there.RepeatableRepeatable deployments are important but perhaps not for the reason you would expect. All of these deployment tools need to be developed using modern continuous integration tools. And in fact all of the examples I provided, Director, Fuel, and OpenStack Ansible, do just that. This improves their quality because every change to their code is checked via CI. Though, I’m not sure how many of them test an HA deployment, as they typically only have the resources for some kind of all-in-one virtual deployment.During my own investigation I have probably deployed the Overcloud 40 times. Each time make slight changes to the Director templates, gradually improving my deployment. It’s important that a deployment be based in code, not in clicking buttons in a graphical interface.For example, a new deployment requires these steps:director$ heat stack-delete overcloud -y #delete the current clouddirector$ #edit templates if requireddirector$ ./bin/overcloud-deploy.sh #this contains the rather long deployment command referencing several templatesdirector$ #wait for it to completedirector$ heat stack-list+--------------------------------------+------------+-----------------+---------------------+--------------+| id                                   | stack_name | stack_status    | creation_time       | updated_time |+--------------------------------------+------------+-----------------+---------------------+--------------+| e24a92a9-dc9f-462b-af16-c8329714f238 | overcloud  | CREATE_COMPLETE | 2016-07-22T19:18:11 | None         |+--------------------------------------+------------+-----------------+---------------------+--------------+director$ #run validation tests, if everything looks good, check in the changesRepeatable deployments are extremely important, and that is one area Director (and Tripleo) shines.I should note that the Director node itself is quite “pet-like” and unless you do some work to automate its deployment, it is a system of record (in the parlance of our times…lol).Based on Opensource CodeDirector is mostly Tripleo. In fact I’m not completely sure where it differs from the upstream code. Specifically, Director consumes the output of the RDO project. In turn, RDO consumes upstream Tripleo–thus it’s “midstream”, in between RedHat and Tripleo.One issue is that the upstream code is considerably farther ahead than Directors. There are features in Tripleo trunk right now that I would love to have, and in fact seem completely necessary, but are not in Director because it’s a few releases behind upstream. If I had my choice I would probably use the RDO output instead of Director, or perhaps even upstream.Undercloud/OvercloudTripleo stands for “OpenStack on OpenStack.”  TripleO is a project aimed at installing, upgrading and operating OpenStack clouds using OpenStack’s own cloud facilities as the foundation - building on Nova, Ironic, Neutron and Heat to automate cloud management at datacenter scale.When you use Tripleo you have two clouds:  Undercloud, ie. Director  OvercloudThe Undercloud is an OpenStack instance where Ironic and Heat (as well as all the other required services) run so that you can deploy the Overcloud.Most people don’t grasp, at the beginning anyways, that an OpenStack cloud requires considerable infrastructure to run. You will probably need three or four potentially highly available or clustered virtual machines to run services such as Jenkins and other operational components, perhaps software defined network controllers (SDN), virtual machine management nodes for other traditional systems (SolidFire I’m looking at you), and more.You will need an Undercloud whether you give it that name or not. In fact in my lab I have at least three clouds: first, a cloud to run Director(s) in, then Director, then finally the Overcloud. It’s fairly involved, and over and above the hardware you need for the HA plane of the overcloud. This is why container based deployments make a lot of sense, and unfortunately that is not something Tripleo supports at this time.##Things I don’t like  The Undercloud instance is a pet  Tripleo does not have much in the way of “life-cycle” management  Seemingly inflexible  Limited documentation  Hundreds of thousands of lines of Puppet  Orchestration?  Scaling controller services  Lack of containerizationUndercloud: System of RecordMy impression of the Director system itself is that it is not “cloud native” for lack of a better phrase and would take some work to make it highly available in a fashion that would be typical of an application in an OpenStack cloud. Whether you call it a “pet” or a “system of record” or “not cloud native” it is an awkward system to operate.Lack of Life-cycleThis is the biggest issue I have with Director. At this time I can find little having to do with operating a cloud over time, what some people call Day 2 (where Day 1 is the installation). Certainly there is some mention of upgrades and the like, but I have been working with Director for a few weeks and still really have no idea what it does in this area, despite looking through all the documentation. I have also heard that other organizations, HPe specifically, started out with Tripleo but dropped it because they realized the need to provide a life cycle for OpenStack deployments. Setting and forgetting an OpenStack installation is the easiest way to fail.Currently I would be concerned at having to run a long-term cloud using Director, and I’m not even sure if you could. That said I have more to learn, and also Director is a young product. But so far it doesn’t look good for Day 2 operations, let alone day 300.Lots of PuppetThere is a ton of puppet.[stack@director-01 ~]$ sudo find / -name \"*.pp\" | xargs wc -lSNIP!   1952 /home/stack/openstack-tripleo-heat-templates-0.8.14-14/puppet/manifests/overcloud_controller_pacemaker.pp     57 /home/stack/openstack-tripleo-heat-templates-0.8.14-14/puppet/manifests/overcloud_object.pp     61 /home/stack/openstack-tripleo-heat-templates-0.8.14-14/puppet/manifests/overcloud_volume.pp     96 /home/stack/openstack-tripleo-heat-templates-0.8.14-14/puppet/manifests/ringbuilder.pp      0 /opt/stack/selinux-policy/ipxe.pp 115712 totalEep. Certainly not all that Puppet is getting executed with every deploy, but even if it’s 50%, that is still a lot of Puppet to understand. I can deploy a production OpenStack cloud with 3-5K lines of Ansible. If you don’t understand it, then it’s an abstraction.Seemingly InflexibleI have tried to update an overcloud twice post-deploy. Both have failed and in fact broke the overcloud. This is pretty concerning.Further, on one deploy I attemped to disable Swift. The default deployment of swift configures a storage device on each of the controllers.[heat-admin@overcloud-controller-2 swift]$ swift-ring-builder object.builder object.builder, build version 31024 partitions, 3.000000 replicas, 1 regions, 1 zones, 3 devices, 0.00 balance, 0.00 dispersionThe minimum number of hours before a partition can be reassigned is 1The overload factor is 0.00% (0.000000)Devices:    id  region  zone      ip address  port  replication ip  replication port      name weight partitions balance meta             0       1     1    172.17.19.12  6000    172.17.19.12              6000        d1 100.00       1024    0.00              1       1     1    172.17.19.13  6000    172.17.19.13              6000        d1 100.00       1024    0.00              2       1     1    172.17.19.11  6000    172.17.19.11              6000        d1 100.00       1024    0.00 In hopes of reducing the complexity of the deployment somewhat, and not relying on a somewhat unusual Swift deployment, I attempted to turn it off by setting ControllerEnableSwiftStorage to false.director$ grep -A 2 \"  ControllerEnableSwift\" overcloud.yaml   ControllerEnableSwiftStorage:    default: true     description: Whether to enable Swift Storage on the ControllerDisabling that still had Swift deployed, it just didn’t work because the storage component was not configured. lol. It seems that in Tripleo trunk there is the ability to select services which I could really use…like right now.Further, I can’t find a way to disable Ceilometer. By default Ceilometer is deployed with a MongoDB backend, all on the controllers. In a busy OpenStack cloud that will surely fail.Limited DocumentationThe RedHat provided documentation is limited. Further, it seems that the suggestions made in the documentation are not really what is used in production. Also, by default the Director templates implement things like bonding using Open vSwitch, but then in the documentation it suggests not using OVS bonding. The upstream documentation is much better, and you could then ask questions on the RDO or Tripleo mailing lists or IRC channels…not so much for RedHat.In fact, in order to really understand how Tripleo works, I think you’d have to run upstream in the lab and learn from that. Then perhaps you could drop down to Director.OrchestrationOrchestration is an overloaded term. In this context I mean the ability to perform complex actions on a cloud once it’s already deployed. For example upgrades. Or a simpler example, restart a clustered service. I am not clear on how to performan these kinds of actions using Director. Fuel performs these kind of actions using MCollective, and OpenStack Ansible can just use Ansible. But for Director? I’m not exactly sure. My guess is that it is done via the various custom os-* commands the project has created. More investigation is required.Scaling Controller ServicesHow do I add a controller? Or provide more nova-conductor services? According to the RedHat docs you cannot scale up, or down, the controller nodes.Lack of ContainerizationMonolithic OpenStack controllers have largely been left behind. Most people are using containers to split out OpenStack services. I would not, if I had the choice, deploy monolithic controllers. I’ve made that mistake before. At the very least LXC should be used. I believe Director/Tripleo is starting to support Docker, but RedHat lists it as a tech preview.It seems counter-intuitive, but using something like Kubernetes makes a lot of sense for the OpenStack control plane. Or, as mentioned, at least LXC, which I am using in my own undercloud. LXC 2.0 recently came out and while I have issues with it, overall I’m happy.##ConclusionI’ve deployed large, production, high SLA OpenStack clouds before, and run them over time. Deploying is easy, operating is bloody difficult. I know what is required…what works and what doesn’t. Today my impression of Director is that it is fairly behind in terms of what successful, modern OpenStack operators are doing to manage their clouds. I have a lot more to learn about Director and Tripleo, but three or four weeks in I feel I have a good idea of what they offer.Hopefully in the next post I’ll know more about Day 2 capabilities, and perhaps I’ll be more comfortable with it. Certainly there is no denying that behind the scenes Director, based on Tripleo, is quite complicated and has many components to learn and understand. Perhaps those components allow for operational proficiency. Or perhaps not."
    } ,
  
    {
      "title"    : "Glance with Multiple Backend Stores",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/07/08/glance-multiple-backend-stores.html",
      "date"     : "2016-07-08 00:00:00 -0400",
      "content"  : "Every OpenStack cloud needs a Glance deployment. Thus, I’ve deployed it many times–but every time I’ve only put one backend into use: either simple file based storage or object storage using Swift.Recently I was surprised to learn that you can have multiple backends, or stores as Glance calls it, and can upload images into whichever store you prefer using an option on the Glance command line.DevstackIf you want to test it out it’s very easy–just deploy a DevStack instance. All you need is vm with a few gigs of ram.Here is the local.conf I used. It enables Swift.ubuntu@devstack:~/devstack$ cat local.conf [[local|localrc]]# CredentialsADMIN_PASSWORD=passwordDATABASE_PASSWORD=passwordRABBIT_PASSWORD=passwordSERVICE_PASSWORD=passwordSERVICE_TOKEN=passwordSWIFT_HASH=passwordSWIFT_TEMPURL_KEY=password# Enable Neutrondisable_service n-netdisable_service n-novncenable_service q-svcenable_service q-agtenable_service q-dhcpenable_service q-l3enable_service q-metaenable_service neutron# Enable Swiftenable_service s-proxyenable_service s-objectenable_service s-containerenable_service s-account# Disable Horizondisable_service horizon# Disable Heatdisable_service heat h-api h-api-cfn h-api-cw h-eng# Disable Cinderdisable_service cinder c-sch c-api c-vol# Swift temp URL's are required for agent_* drivers.SWIFT_ENABLE_TEMPURLS=True# By default, DevStack creates a 10.0.0.0/24 network for instances.# If this overlaps with the hosts network, you may adjust with the# following.NETWORK_GATEWAY=10.1.0.1FIXED_RANGE=10.1.0.0/24FIXED_NETWORK_SIZE=256# Log all output to filesLOGFILE=$HOME/devstack.logLOGDIR=$HOME/logsAh, good old DevStack.Glance configOnce DevStack is done, the /etc/glance/glance-api.conf file will contain this section:[glance_store]stores = file, http, swiftdefault_swift_reference = ref1swift_store_config_file = /etc/glance/glance-swift-store.confswift_store_create_container_on_put = Truedefault_store = swiftfilesystem_store_datadir = /opt/stack/data/glance/images/Note the “stores” option, and that “file, http, swift” are configured.Glance API V1One caveat at this time is that the Glance V2 API does not seem to allow for setting an option when uploading an image as to which backend to use. Thus, with Glance V2 you will always be using the default_store, which in this example is Swift.But, if you use V1, you can set the “–store” option and choose a backend.ubuntu@uc-osclient-01:~$ glance --os-image-api 2 help image-create | grep \"\\-\\-store\" # No optionubuntu@uc-osclient-01:~$ glance --os-image-api 1 help image-create | grep \"\\-\\-store\" usage: glance image-create [--id &lt;IMAGE_ID&gt;] [--name &lt;NAME&gt;] [--store &lt;STORE&gt;]  --store &lt;STORE&gt;       Store to upload image to.Perhaps in the future V2 will also have a backend store option.Uploading images to multiple backend storesOk, lets use the Glance CLI to upload images to the file and Swift backends.First, I downloaded the Cirros image.ubuntu@devstack:~$ wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img--2016-07-09 03:04:11--  http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.imgResolving download.cirros-cloud.net (download.cirros-cloud.net)... 64.90.42.85Connecting to download.cirros-cloud.net (download.cirros-cloud.net)|64.90.42.85|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 13287936 (13M) [text/plain]Saving to: ‘cirros-0.3.4-x86_64-disk.img’100%[==================================================================================================================================&gt;] 13,287,936  3.61MB/s   in 4.2s   2016-07-09 03:04:15 (3.04 MB/s) - ‘cirros-0.3.4-x86_64-disk.img’ saved [13287936/13287936]Cubswin:).As a note, I’m using the Glance CLI version 2.1.0.ubuntu@devstack:~$ glance --version2.1.0Next, upload an image to the Swift backend.ubuntu@devstack:~$ glance --os-image-api-version 1 image-create --store swift --name swift-cirros --disk-format qcow2 --container-format bare --is-public True --file cirros-0.3.4-x86_64-disk.img +------------------+--------------------------------------+| Property         | Value                                |+------------------+--------------------------------------+| checksum         | ee1eca47dc88f4879d8a229cc70a07c6     || container_format | bare                                 || created_at       | 2016-07-09T03:07:09.000000           || deleted          | False                                || deleted_at       | None                                 || disk_format      | qcow2                                || id               | a8dc5259-9579-4cff-be0a-f175278f60f3 || is_public        | True                                 || min_disk         | 0                                    || min_ram          | 0                                    || name             | swift-cirros                         || owner            | fc42df31d6c84c04b55cf116e06c2b38     || protected        | False                                || size             | 13287936                             || status           | active                               || updated_at       | 2016-07-09T03:07:11.000000           || virtual_size     | None                                 |+------------------+--------------------------------------+Now upload another image, this time using the file backend.ubuntu@devstack:~$ glance --os-image-api-version 1 image-create --store file --name file-cirros --disk-format qcow2 --container-format bare --is-public True --file cirros-0.3.4-x86_64-disk.img +------------------+--------------------------------------+| Property         | Value                                |+------------------+--------------------------------------+| checksum         | ee1eca47dc88f4879d8a229cc70a07c6     || container_format | bare                                 || created_at       | 2016-07-09T03:07:55.000000           || deleted          | False                                || deleted_at       | None                                 || disk_format      | qcow2                                || id               | e361ad97-6336-45d9-9c6a-e461ed91126d || is_public        | True                                 || min_disk         | 0                                    || min_ram          | 0                                    || name             | file-cirros                          || owner            | fc42df31d6c84c04b55cf116e06c2b38     || protected        | False                                || size             | 13287936                             || status           | active                               || updated_at       | 2016-07-09T03:07:55.000000           || virtual_size     | None                                 |+------------------+--------------------------------------+We can peek into the Glance database to see where the image_location is.mysql&gt; select value from image_locations where image_id = \"e361ad97-6336-45d9-9c6a-e461ed91126d\";+---------------------------------------------------------------------------+| value                                                                     |+---------------------------------------------------------------------------+| file:///opt/stack/data/glance/images/e361ad97-6336-45d9-9c6a-e461ed91126d |+---------------------------------------------------------------------------+1 row in set (0.00 sec)mysql&gt; select value from image_locations where image_id = \"a8dc5259-9579-4cff-be0a-f175278f60f3\";+-----------------------------------------------------------------+| value                                                           |+-----------------------------------------------------------------+| swift+config://ref1/glance/a8dc5259-9579-4cff-be0a-f175278f60f3 |+-----------------------------------------------------------------+1 row in set (0.01 sec)As can be seen above, the swift-cirros image is stored in the Swift backend, and the file-cirros image is stored in the file based backend. I was quite surprised to learn this was possible, and even more surprised to see that the option does not seem to be in the Glance V2 API…and even more surprised that the Glance CLI has different options based on the API used.There’s always something to learn about OpenStack. :)"
    } ,
  
    {
      "title"    : "Split OpenStack Keystone Catalog",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/06/24/split-keystone-catalog.html",
      "date"     : "2016-06-24 00:00:00 -0400",
      "content"  : "OpenStack Keystone is aptly named–it’s a service almost everyone who deploys OpenStack need to run. From a high level it provides authentication.  Keystone is an OpenStack project that provides Identity, Token, Catalog and Policy services for use specifically by projects in the OpenStack family. It implements OpenStack’s Identity API. – Keystone documentationHowever, as the above quote mentions it also provides a catalog of OpenStack service endpoints, where endpoints essentially means where you make REST API calls to.Listing the catalogWhen Keystone is deployed, you can use the OpenStack command line client to list the endpoints.Below I just show the endpoints for the compute service. Note that the endpoint starts with https, and thus these endpoints are TLS enabled. (Note that I alias openstack to os.)ubuntu@uc-external-osclient-01:~$ os catalog show compute+-----------+-----------------------------------------------------------------------+| Field     | Value                                                                 |+-----------+-----------------------------------------------------------------------+| endpoints | yeg-uc-1                                                              ||           |   admin: https://uc-                                                  ||           | api.lab.example.com:8774/v2/9a6a0815d6e74146bb76f19fd580bc31     ||           | yeg-uc-1                                                              ||           |   public: https://uc-                                                 ||           | api.lab.example.com:8774/v2/9a6a0815d6e74146bb76f19fd580bc31     ||           | yeg-uc-1                                                              ||           |   internal: https://uc-                                               ||           | api.lab.example.com:8774/v2/9a6a0815d6e74146bb76f19fd580bc31     ||           |                                                                       || name      | Compute Service                                                       || type      | compute                                                               |+-----------+-----------------------------------------------------------------------+But if I run the same command from an internal client we see slightly different endpoints.ubuntu@uc-internal-osclient-01:~$ os catalog show compute+-----------+-----------------------------------------------------------------------------------------+| Field     | Value                                                                                   |+-----------+-----------------------------------------------------------------------------------------+| endpoints | yeg-uc-1                                                                                ||           |   admin: http://uc-api.lab.example.com:8774/v2/9a6a0815d6e74146bb76f19fd580bc31    ||           | yeg-uc-1                                                                                ||           |   public: http://uc-api.lab.example.com:8774/v2/9a6a0815d6e74146bb76f19fd580bc31   ||           | yeg-uc-1                                                                                ||           |   internal: http://uc-api.lab.example.com:8774/v2/9a6a0815d6e74146bb76f19fd580bc31 ||           |                                                                                         || name      | Compute Service                                                                         || type      | compute                                                                                 |+-----------+-----------------------------------------------------------------------------------------+Note that it is http not https.Split CatalogIn this example there are actually two Keystone servers running, and they are identical except for their default_catalog.template file. In fact that file is identical except that the internal Keystone server will use http as the protocol, and the external Keystone server provides a catalog of endpoints with TLS enabled.Why would we do this?Well, in some cases we want internal OpenStack services to simply use http/plaintext, perhaps for performance reasons, but obviously external access, even if it’s only accessible from a companies internal network, should be TLS enabled. Separating the Keystone catalog using two Keystone servers and perhaps a load balancer makes this split Keystone setup possible.In short, external is encrypted and internal is not.default_catalog.templateThere are two major ways to define the Keystone catalog: 1) via the database and 2) default_catalog.template.The template file is just that–a configuration file. Using this configuration file, different Keystone servers can return different catalogs. This is, in my opinion, the easiest way to manage the catalog, and what’s more, is much easier to maintain using configuration management tools. (To all developers: don’t put configuration information into the database if you can avoid it. LXC 2.0 I’m looking at you.)Split DNSIn this particular example internal and external access uses the same Keystone endpoint host name: uc-api.lab.example.com but the internal systems have a different IP address associated with that hostname than the external servers do.The load balancer in front of the Keystone servers directs traffic to the correct Keystone backend based on the IP address that is being accessed by the client. So external systems access the external Keystone server and vice-versa for internal.Configuration managementTypically I use Ansible to manage configurations. The Keystone servers are separated into two groups: internal and external.The external servers have their protocol set to https.[ansible-openstack]$ cat group_vars/external-keystone-api ---keystone_endpoint_protocol: \"https\"By default the protocol in the default_catalog.template file is http.[ansible-openstack]$ head roles/keystone/templates/default_catalog.templates # keystonecatalog.{{ region }}.identity.publicURL = {{ keystone_endpoint_protocol | default('http') }}://{{ api_fqdn }}:$(public_port)s/v2.0catalog.{{ region }}.identity.adminURL = {{ keystone_endpoint_protocol | default('http') }}://{{ api_fqdn }}:$(admin_port)s/v2.0catalog.{{ region }}.identity.internalURL = {{ keystone_endpoint_protocol | default('http') }}://{{ api_fqdn }}:$(public_port)s/v2.0catalog.{{ region }}.identity.name = Identity Service# novacatalog.{{ region }}.compute.publicURL = {{ keystone_endpoint_protocol | default('http') }}://{{ api_fqdn }}:8774/v2/$(tenant_id)scatalog.{{ region }}.compute.adminURL = {{ keystone_endpoint_protocol | default('http') }}://{{ api_fqdn }}:8774/v2/$(tenant_id)scatalog.{{ region }}.compute.internalURL = {{ keystone_endpoint_protocol | default('http') }}://{{ api_fqdn }}:8774/v2/$(tenant_id)sCertainly there are many other things that the configuration management tool will have to do, but the basics are shown above. There has to be some way to set the external Keystone template to use https and http for internal. This is just one way to accomplish that.Issues with this modelUsing this model we are classifying traffic into internal and external. Further, we are suggesting that internal traffic for the OpenStack services does not require encryption. This is a big decision. Certainly it’s a common model, but it creates a “chewy center” so to speak, meaning that external access is a hard shell, but that once you get through that hard shell, everything within is easier to crack. Sometimes people thing that “defense in depth” means that it’s okay for each ring to be less secure, but that is not necesarily the goal.Another way to secure the API endpoints is to assume that all traffic, regardless of origin, may be malicious. With that assumption splitting Keystone would not be desirable, and instead it would be worthwhile to secure Keystone as much as possible regardless of the origin of the traffic, and in fact there would only be one API endpoint used by external end users and internal OpenStack services. Something to consider.That said, it’s sometimes difficult to get internal OpenStack services to use https, or at least it has been previously."
    } ,
  
    {
      "title"    : "OPNFV Summit 2016",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/06/24/opnvf-summit-2016.html",
      "date"     : "2016-06-24 00:00:00 -0400",
      "content"  : "I was fortunate to be able to attend the Open Platform for Network Function Virtualization (OPNFV) 2016 summit in Berlin.First OPNFV eventThis was my first OPNFV related event. To be honest prior to attending I wasn’t quite sure what OPNFV really was. I mean, I knew it was related to NFV in general, but what were the actual goals of the organization? After attending I feel like I have a better idea of what OPNFV’s purpose is, and also the realization that it is still early times with regards to this entire area of cultural, business and technological change related to NFV.Design SummitThe first two days of the summit were the design summit. Like OpenStack, OPNFV uses its summit to also provide a place for the contributers involved in OPNFV to get together face-to-face and discuss the technical aspects of various projects.OpenStack Operators Telecom/NFV Working GroupRecently I have been trying to start up a new working group within OpenStack specifically dealing with OpenStack Operators and how they are designing, deploying, maintaining and operating OpenStack NFV clouds. For those that don’t know, there is an OpenStack Operators mailing list, and this group has meetups between OpenStack summits, and as well a portion of the OpenStack design summit time is dedicated to it.During the OPNFV design summit we were privileged to be given a time slot to have a “birds of a feather” meetup for OpenStack operators. Unfortunately the BoF was lightly attended. Like I mentioned previously, this is a new group and it’s going to take some time to get going. I feel like over the next year or so we will see an explosion of NFV systems installed which are based on OpenStack and thus there will be a considerable number of OpenStack operators managing these clouds, and they are going to want a place to discuss issues with their peers. (I hope.)My hope is that like other OpenStack working groups we will be able to shepherd positive changes back up through the OpenStack community, be it through working with OpenStack projects to meet NFV related requirements, or via documenting common deployment designs, and similar items.If you, or anyone you know, would like to participate in this working group we have bi-weekly IRC meetings.Tacker and Service Function ChainingOpenStack Tacker is an interesting project that initially started under the OPNFV umbrella, but has now migrated to be beneath OpenStack. Tacker is a “generic virtual network functions (VNF) manager.”I had the chance to interact with the project technical lead (PTL) for Tacker a couple of times and it was fascinating to hear their story of how the project is maturing and navigating the various open source communities it interacts with. Time and time again I hear how development is largely a social interaction, not just writing code in your basement or cubicle. It takes a lot of work to understand various communities guidelines and cultures. In fact a large component of OPNFV is around understanding how best to work with upstream projects like OpenStack.One of the major requirements Tacker has from OpenStack is the need for service function chaining (SFC). There is a project within OpenStack now, called networking-sfc which will enable SFC in Neutron, and thus it will be usable by Tacker. With SFC being enabled in Neutron the ability to move workloads from one cloud to another irrespective of the underlying networking technology will be possible. Interoperability is important.  Fundamentally SFC is the ability to cause network packet flows to route through a network via a path other than the one that would be chosen by routing table lookups on the packet’s destination IP address. It is most commonly used in conjunction with Network Function Virtualization when recreating in a virtual environment a series of network functions that would have traditionally been implemented as a collection of physical network devices connected in series by cables. – networking-sfcTacker, as well as SFC, provide important NFV-related functionality and it will be great to see how they are used by the community.Open vSwitch, OpenDaylight, and OpenStack PerformanceThere was a good presentation by HPE regarding Open vSwitch (OVS) performance when using OpenDaylight (ODL) in OpenStack.The two main points in terms of performance were:  Use Open vSwitch DPDK  Using ODL allows for 110x improvement in layer 3 routing versus standard OpenStack layer 3 (ie. DVR)The 110x improvement is pretty amazing. From my impression of the presentation, it wouldn’t be possible to use DVR if performance is a requirement for your particular use-case. Note that this was layer 3 only, not layer 2. That said there was an improvement in layer 2 performance, just not as drastic.In the presentation some reasons for this lack of performance in “vanilla Neutron” were listed as:      Linux qRouter performance issues     Low performance of additional bridges and iptables     Connection from OVS to the qrouter is not accelerated by DPDK.  More to look into here.Main SummitAfter the first two days of the design collaboration, the summit moved on to a more standard presentation and panel model.OpenStack Community PanelI was lucky to be on the OpenStack community panel session and we talked a bit about where NFV is going in terms of OpenStack and what we see happening, or would like to happen, in the future.I spoke a bit about the OpenStack Operators Telecom/NFV working group, and I’m hoping that participating in the panel helps to get the word out, so to speak, about the group. The Neutron and Tacker PTLs were the other members of the panel.CENGNI would be remiss if I didn’t mention the Canadian non-profit CENGN which has been involved with OPNFV from early on. They were the first associate member of the OPNFV. For some reason Canada is usually well behind the technology curve, especially around networking, perhaps due to the void left by the utter (and sad) collapse of Nortel. With CENGN it seems there may be some recovery on the horizon in Canada, or at least an example of an organization designed to push the bleeding edge into Canadian business.Mikko Hypponen - Complexity is the Enemy of Security  How many of the Fortune 500 are hacked? 500. – Mikko HypponenWhile the talk was titled with regards to complexity, he didn’t really get into the complexity around NFV, other than calling this summit “the acronym conference”which is both funny and accurate. He did mention how Heartbleed and Shellshock have led to the creation of the Core Infrastructure Initiative which irregardless of the silly names given large vulnerabilities is an extremely positive change.The Hitchhiker/Hacker’s Guide to NFV BenchmarkingThere was a short and information packed presentation on benchmarking. There is just a ton of information in the slides and I heavily suggest going through them if you need to benchmark your NFV deployments.NetReady, Gluon, Service Function Chaining (SFC)NetReady is a OPNFV project to identify gaps in networking requirements (at this point mainly dealing with OpenStack Neutron) and write prototype code to fill those gaps.Gluon is a project that allows those prototypes to be easily created, but it does so outside of Neutron by manipulating Nova port binding and, as far as I can tell, avoiding Neutron entirely, which is not great, but I can understand the need to be able to prototype and fail fast.SFC was all over the OpenStack summit in Austin, and it’s no surprise to see it highlighted at the OPNFV summit as well. I watched a good presentation/demo on it, and there were a lot of questions about competing implementations, from Huawei doing something in ONOS, Redhat doing something with ODL, networking-sfc, etc, etc.Machine learningThere was a great, but short, presentation on applying machine learning to network scaling strategies.One thing I realized from this presentation, because they specifically stated it, is that reactive scaling is not enough–you need predictive and reactive in order to scale properly, and one way to get predictive is to use machine learning.Another important point made was the lack of production data available for use in research and development of predictive scaling. They are hoping the OPNFV can help with that somehow.BerlinBerlin is an amazing place. Fortunately it’s still one of the less expensive cities in Europe which means artists can afford to live here and do interesting things. The weather was great. I went around and, of course, visited many tourist attractions, but my favourite thing was a fascinating bicycle race called the Fixie 42 which is a 42km race on fixie bikes. The average speed for the fixie race was 49 km/h; they were practically flying. Also while in Berlin I was fortunate enough to see Beck live!ConclusionOverall, I feel attending the OPNFV summit was extremely valuable. OPNFV is only in year two–it’s early times. Most telecoms are only now getting started with NFV. The learning curve, both cultural and technological, is considerable. Recently I heard a quote along the lines of “telecommunications is too important to leave to Telecoms” and in some respects this is what OPNFV is doing.Further, I’d like to get involved in OPNFV more. Unfortunately I missed the two security presentations, but there is an OPNFV Security group and that might be something I look into helping out with. OPNFV is still a small organization, and while I expect it to grow over time, much like OpenStack has, now seems like a good time to be involved."
    } ,
  
    {
      "title"    : "Getting a Data Simcard while Travelling in Germany",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/06/22/buying-simcard-in-germany.html",
      "date"     : "2016-06-22 00:00:00 -0400",
      "content"  : "Recently I travelled to Berlin for a technology conference. Strangely I don’t have a data plan for my “smart phone” at home in Canada, but when travelling there is nothing that makes getting around a new city easier than having a smart phone with a data plan. I don’t know how I did it before smart phones. I guess I just got lost a lot.I did some research on how to get a data plan while in Germany, but nothing really came up that was all that recent. So I thought I’d detail how I found an inexpensive data plan.tl;drHave an unlocked phone. Buy a BlauWorld SIM card at the Alexanderplatz Saturn store. Go back to your hotel and use your hotel wifi to register the SIM card using your hotel’s address in the Blau online activation form. Once the form is completed, wait about 30 minutes, then put the SIM into your phone, enter the SIM card’s pin number from the letter in your BlauWorld package into the phone when requested. Done! Now you can access the internet via your phone.BlauWorldAfter doing a bit of research I decided (rightly I believe) that a Blau card would be easiest, mostly because they have an English version of their website. Specifically I was looking for a BlauWorld card, which is apparently a card often used by foreign workers in Germany. Further, it doesn’t seem to have much in the way of restrictions–just a simple online signup. Also, in theory they are widely available, though I have only seen them at Saturn stores.Saturn is a technology store–they sell TVs, computers, cell phones, and, of course, BlauWorld SIM cards. The first Saturn I tried did not have a BlauWorld SIM card package, but they suggested I to go to another location–the Saturn at Alexanderplatz. So I did, and they had a BlauWorld card for about 10 euros, which is quite low-cost. It comes with a SIM card and a 10 euro credit. With the credit the BlauWorld SIM card is essentially free.The BlauWorld SIM card can be resized to nano, micro, and regular SIM card sizes so it should fit in all phones.Activation/RegistrationThere is an English version of the Blau website which is quite helpful if, like myself, you can’t read German. Click on the Activate tab and follow the instructions. You’ll need the phone number from the letter in the BlauWorld package to start the activation. Enter your name and a German address. I used my hotel’s address. It might take a couple tries to get the right information into the right fields.During the activation I was able to select a 1GB data plan with the 10 euro credit that came with the SIM card. Actually the 1GB option was the only option during the initial activation. There are other options you can pick once you’ve registered the card completely if you want SMS and/or voice, but it would cost extra. The SMS and voice plans were fairly expensive, but not unreasonable. I didn’t need voice or SMS. Data was just fine.Once the registration is finished, wait 30 minutes or so then put the SIM card into your phone and start it up. Enter the pin number from the letter.At this point you have 1GB of data, which is perfect for a weeks stay, and it only cost 10 euros total, for both the SIM card and the 1GB plan.Now, knowing you can use your phones map application, you can venture out and explore Germany."
    } ,
  
    {
      "title"    : "OpenStack Provider Networks",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/06/11/openstack-provider-networks.html",
      "date"     : "2016-06-11 00:00:00 -0400",
      "content"  : "There are many ways to deploy networking in OpenStack. I’ve deployed it old-school with nova-network, new-school with Neutron and Midokura’s Midonet, and just recently I put up a lab deployment of Neutron + provider networks.To me, provider networks are kind of like nova-network in terms of their simplicity of deployment, where simplcity means your network team probably doesn’t have to do anything new, and can rely on their (potentially) tried and true network designs. I mean, let’s face it–many network architects are going to dislike SDN and/or overlays, etc. Using provider networks will at least allow OpenStack to be deployed in somewhat hostile network environments.From the OpenStack Networking Guide:  Provider networks generally offer simplicity, performance, and reliability at the cost of flexibility. Unlike other scenarios, only administrators can manage provider networks because they require configuration of physical network infrastructure…In many cases, operators who are already familiar with network architectures that rely on the physical network infrastructure can easily deploy OpenStack Networking on it.The goal of this post is to go over the deployment and include some snippets of configuration to give a cursory example of how this deployment is working and what it looks like while running. Please note this is a lab deployment and is not necessarily meant for production use. Oh, and I’m not a “network architect” by any stretch, but I have deployed some fairly complicated networks in relation to OpenStack.The Stack  Ubuntu 14.04  Edgecore 5712  Cumulus Linux 2.5.7  Open vSwitch 2.5.0 from Ubuntu’s Cloud Archive  OpenStack Mitaka from Ubuntu’s Cloud Archive  A single controller running LXC 2.0 and a bunch of containers  A couple of baremetal compute nodes  100% managed by AnsibleCumulus LinuxIn my case I control the physical network and it consists of an Edgecore 5712 with Cumulus Linux loaded on it.cumulus@oc-sw-02$ cat /etc/lsb-release DISTRIB_ID=\"Cumulus Linux\"DISTRIB_RELEASE=2.5.7DISTRIB_DESCRIPTION=2.5.7-753304d-201603071654-buildRecently Cumulus released 3.0 so I should load that on soon, but for now I’m back on 2.5.7.Cumulus has an interesting feature called VLAN aware bridge mode and that is what I’m using.Here’s a snippet of my interfaces file, which is managed by Ansible. VLANs 12 and 13 are meant to be the provider networks.# VLANSauto bridge0iface bridge0    bridge-ports controller-01-bond0 compute-01-bond0 compute-02-bond0    bridge-vlan-aware yes    bridge-vids 10 12 13 11    bridge-pvid 1    bridge-stp onauto bridge0.10iface bridge0.10    address 172.17.3.1/24auto bridge0.12iface bridge0.12    address 172.16.5.33/27auto bridge0.13iface bridge0.13    address 172.16.5.65/27auto bridge0.11iface bridge0.11    address 172.16.5.1/28I quite like working with Cumulus linux. The Edgecore 5712 + Cumulus is a compelling offer.NeutronI deployed Neutron exactly like the Open vSwitch and provider networks is shown in the networking guide. One difference from the guide is that neutron-server by default doesn’t use the ml2_conf.ini file, only the openvswitch_agent.ini file.My deployment has the physical network providing layer 2 and layer 3, but Neutron is handling DHCP. So on the neutron-api node it sets up some namespaces where the DHCP server listens.root@uc-neutron-api-01:/etc/neutron/plugins/ml2# cat openvswitch_agent.ini [ml2]type_drivers = flat,vlan# empty because we don't support project/private networkstenant_network_types = mechanism_drivers = openvswitchextension_drivers = port_security[ml2_type_flat]flat_networks = provider[ml2_type_vlan]network_vlan_ranges = provider [securitygroup]firewall_driver = iptables_hybrid enable_security_group = True[ovs]bridge_mappings = provider:br-providerThere are a couple of networks setup in neutron.ubuntu@uc-osclient-01:~$ neutron net-list+--------------------------------------+------------+-----------------------------------------------------+| id                                   | name       | subnets                                             |+--------------------------------------+------------+-----------------------------------------------------+| 5e73a2f5-a52b-4833-9978-531fc98cd783 | vlan_13 | 725c3dbe-65f2-4dc5-b9ec-596309bc2229 172.16.5.64/27 || eb175122-2500-4234-becb-030da538893a | vlan_12 | 7b799b92-ba55-454c-8db7-558476559b4e 172.16.5.32/27 |+--------------------------------------+------------+-----------------------------------------------------+ubuntu@uc-osclient-01:~$ neutron subnet-list+--------------------------------------+-------------------+----------------+------------------------------------------------+| id                                   | name              | cidr           | allocation_pools                               |+--------------------------------------+-------------------+----------------+------------------------------------------------+| 725c3dbe-65f2-4dc5-b9ec-596309bc2229 | vlan_13-subnet | 172.16.5.64/27 | {\"start\": \"172.16.5.66\", \"end\": \"172.16.5.94\"} || 7b799b92-ba55-454c-8db7-558476559b4e | vlan_12-subnet | 172.16.5.32/27 | {\"start\": \"172.16.5.34\", \"end\": \"172.16.5.62\"} |+--------------------------------------+-------------------+----------------+------------------------------------------------+We can see their network namespaces.root@uc-neutron-api-01:~# ip netns listqdhcp-5e73a2f5-a52b-4833-9978-531fc98cd783qdhcp-eb175122-2500-4234-becb-030da538893aAnd with a couple of virtual machines deployed we can see what interfaces OVS has created.I should note that the neutron-api server is an lxc container. eth2 in the container is bridged to a bonded interface in the baremetal OS.root@uc-neutron-api-01:~# ovs-vsctl show58669c80-e22a-4ea8-9b2d-2d21d1c95163    Bridge br-provider        Port phy-br-provider            Interface phy-br-provider                type: patch                options: {peer=int-br-provider}        Port \"eth2\"            Interface \"eth2\"        Port br-provider            Interface br-provider                type: internal    Bridge br-int        fail_mode: secure        Port br-int            Interface br-int                type: internal        Port int-br-provider            Interface int-br-provider                type: patch                options: {peer=phy-br-provider}        Port \"tapd404a48d-df\"            tag: 3            Interface \"tapd404a48d-df\"                type: internal        Port \"tapa02b5817-23\"            tag: 2            Interface \"tapa02b5817-23\"                type: internal    ovs_version: \"2.5.0\"On the compute node OVS looks like this. br-provider sits on bond0 and adds VLAN tags.root@compute-01:/home/ubuntu# ovs-vsctl show543d058b-8452-4902-bc4c-77bd62ae3e07    Bridge br-int        fail_mode: secure        Port int-br-provider            Interface int-br-provider                type: patch                options: {peer=phy-br-provider}        Port br-int            Interface br-int                type: internal        Port \"qvo60f707e9-0d\"            tag: 3            Interface \"qvo60f707e9-0d\"        Port \"qvoe77743d9-e1\"            tag: 2            Interface \"qvoe77743d9-e1\"        Port \"qvoaad386ab-2f\"            tag: 2            Interface \"qvoaad386ab-2f\"    Bridge br-provider        Port \"bond0\"            Interface \"bond0\"        Port phy-br-provider            Interface phy-br-provider                type: patch                options: {peer=int-br-provider}        Port br-provider            Interface br-provider                type: internal    ovs_version: \"2.5.0\"If I tcpdump bond0 on the compute node and ping an instance on that provider network…root@compute-01:/home/ubuntu# tcpdump -n -e -ttt -i bond0 | grep \"vlan 12\"tcpdump: WARNING: bond0: no IPv4 address assignedtcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on bond0, link-type EN10MB (Ethernet), capture size 65535 bytes00:00:00.000848 fa:16:3e:c3:6f:17 &gt; cc:37:ab:2c:9c:92, ethertype 802.1Q (0x8100), length 102: vlan 12, p 0, ethertype IPv4, 172.16.5.35 &gt; 172.16.4.4: ICMP echo reply, id 19867, seq 1, length 6400:00:00.136563 cc:37:ab:2c:9c:92 &gt; fa:16:3e:c3:6f:17, ethertype 802.1Q (0x8100), length 102: vlan 12, p 0, ethertype IPv4, 172.16.4.4 &gt; 172.16.5.35: ICMP echo request, id 19867, seq 2, length 6400:00:00.000225 fa:16:3e:c3:6f:17 &gt; cc:37:ab:2c:9c:92, ethertype 802.1Q (0x8100), length 102: vlan 12, p 0, ethertype IPv4, 172.16.5.35 &gt; 172.16.4.4: ICMP echo reply, id 19867, seq 2, length 6400:00:00.999631 cc:37:ab:2c:9c:92 &gt; fa:16:3e:c3:6f:17, ethertype 802.1Q (0x8100), length 102: vlan 12, p 0, ethertype IPv4, 172.16.4.4 &gt; 172.16.5.35: ICMP echo request, id 19867, seq 3, length 64Looks good to me. :)Ansible OpenStack network modulesAnsible 2.1 has modules for OpenStack networks and subnets that work quite well. The modules have been around for a while, but 2.1 added a couple required features for adding a provider network.Here’s a snippet of creating a provider network. The networks dict is a yaml dict that I’m working on to define the entire network stack.    - name: ensure provider networks exist      os_network:        name: \"{{ item.key }}\"        provider_network_type: \"vlan\"        provider_physical_network: \"provider\"        provider_segmentation_id: \"{{ item.value.vlan_id }}\"        shared: True      delegate_to: uc-osclient-01      when: item.value.type == \"provider\"       with_dict: networks    - name: ensure provider subnets exist      os_subnet:        network_name: \"{{ item.key }}\"        name: \"{{ item.key }}-subnet\"         cidr: \"{{ item.value.network }}{{ item.value.cidr }}\"         gateway_ip: \"{{ item.value.address }}\"      delegate_to: uc-osclient-01      when: item.value.type == \"provider\"       with_dict: networksIt’s so nice to be able to automate the creation of the entire network stack, from Cumulus and bonds all the way up to neutron subnets.Interfaces down?One thing that really confused me when working on this deployment was that the interfaces are marked down.root@compute-01:/home/ubuntu# ip ad sh | grep DOWN | grep \"ovs\\|br-\"15: ovs-system: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default 16: br-int: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default 17: br-provider: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default I’m not clear on why these are down, but everything is working fine. This is something I would like to understand.root@compute-01:/home/ubuntu# ovs-ofctl show br-providerOFPT_FEATURES_REPLY (xid=0x2): dpid:000090e2babace6cn_tables:254, n_buffers:256capabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS ARP_MATCH_IPactions: output enqueue set_vlan_vid set_vlan_pcp strip_vlan mod_dl_src mod_dl_dst mod_nw_src mod_nw_dst mod_nw_tos mod_tp_src mod_tp_dst 1(bond0): addr:90:e2:ba:ba:ce:6c     config:     0     state:      0     speed: 0 Mbps now, 0 Mbps max 2(phy-br-provider): addr:ea:dc:cb:d8:23:25     config:     0     state:      0     speed: 0 Mbps now, 0 Mbps max LOCAL(br-provider): addr:90:e2:ba:ba:ce:6c     config:     PORT_DOWN     state:      LINK_DOWN     speed: 0 Mbps now, 0 Mbps maxAgain, not sure why the LOCAL(br-povider) is down, but it is, and everything works fine.InstancesHere’s the routing table of a virtual machine running in the OpenStack cloud using a provider network.ubuntu@t4:~$ netstat -rnKernel IP routing tableDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface0.0.0.0         172.16.5.33     0.0.0.0         UG        0 0          0 eth0169.254.169.254 172.16.5.34     255.255.255.255 UGH       0 0          0 eth0172.16.5.32     0.0.0.0         255.255.255.224 U         0 0          0 eth0Note the metadata server route at 169.254.169.254.ConclusionIf you are deploying a private cloud into an environment where software defined networking and/or overlays are not welcome, then provider networks might be your only option. If you are old-school OpenStack and liked nova-network, then provider networks will seem similar. I do like their simplicity.Not every OpenStack deployment is going to require hundreds or thousands of private tenant networks. If you have an OpenStack-hostile network environment, which is quite common I assure you, then this might help. Hopefully they’ll at least allow you to have neutron manage DHCP on the provider networks.Next up I need to look into IPv6 and Open vSwitch DPDK as well as performance testing and a host of other items."
    } ,
  
    {
      "title"    : "Austin 2016 OpenStack Summit",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/05/01/openstack-summit-2016-austin.html",
      "date"     : "2016-05-01 00:00:00 -0400",
      "content"  : "Random shopping cart in Austin##Or, You Know What’s Cool? A Trillion DollarsThe OpenStack summit is a large, multi-faceted event. It brings vendors, operators, developers and users (among others) all together in one massive conference. At this summit there were ~7500 people in attendance [1]. The scale wasn’t just based on the number of people though–the number of conference tracks and talks was almost overwhelming.##Niche CloudsFirst I have to say that I felt that, even before leaving for the event, I would be more interested in what was going on at this conference than the Tokyo summit. During that summit I worked on operating a small public cloud in Canada. There are not that many, in my opinion, OpenStack-based public clouds. Thus it was hard to find some kind of operations solidarity in that particular niche. While I believe that there are in total somewhere around 18 public OpenStack-based public clouds [0], some much larger than others, their influence, especially the smaller ones, pales in comparison to the number of private OpenStack cloud deployments.At any rate, my point is that I no longer work in this particular OpenStack niche, and have moved into a new niche, one that is getting a considerable amount of attention.##Network Function Virtualization (NFV)I knew coming into this summit that NFV would be pretty high on the hype cycle. The sleeping telco giants have all been startled awake and their millions of employees and billions of dollars are yawning and stretching. Despite the hype of NFV and influence of telco (TELCO SMASH!) it is an exciting time in technology, especially around networking, automation, containers, and I would argue, operations. Change is good, but it’s hard.In one of the sessions it was suggested that telecommunications is a trillion dollar market. That is a massive number, and most of these organizations are only getting started with OpenStack. However, I have no doubt in saying all of them are looking to deploy OpenStack clouds. (Certainly, some will be implementing OpenStack clouds in hopes they fail, and they probably will, but others will be putting in an honest effort to succeed.) This is a sea change in IT if there ever was one.Almost every networking related design session I went to dealt with, in some capacity, the need to improve support for telecommunications requirements–especially around technologies and workflows like service function chaining (SFC) and monitoring/fault management. I went to several network component design sessions and SFC was discussed in nearly all of them. Networks are about to get a lot more complex, but also easier to automate and program.##Ops Telco/NFV Working GroupI moderated a session in the Ops summit on Telco/NFV. Quite a few people showed up and we had a good discussion on Telco/NFV. [2] I think there is so much interest in NFV that people are looking anywhere for help and information. That’s why we ended up with so much interest in the working group. However, it’s an ops group, so if there is an operators need for the Telco/NFV group then we need to make sure we’re meeting that need, and not overlapping with other working groups and entities. So it will take a bit of time to figure out what we can do.One thing I can say is that I quite like the approach the Ops large deployment team has taken, which is to select one or two changes they would like to see in OpenStack and really shepherd those changes through the…uh stack over a long period of time to make sure they get into the project. More on this later.##Open Platform for NFV - OPNFVOPNFV is a really interesting project. I realized at this summit that OPNFV is essentially a group of OpenStack operators. They are doing the same work a large operators group would do, specifically around getting OpenStack and NFV up and running and nearly ready for production. They are modelling and improving, as best they can, OpenStack + NFV in production-like environments. I watched a couple great presentations by people working within OPNFV, and I’m hoping to go to their summit in June in Berlin.##Ops Informal MeetupAt the Tokyo summit there was an informal ops meetup on the Friday, basically hanging out in the dev summit lunch area. I didn’t attend that meetup because there wasn’t quite room at the time for me even to sit down. I didn’t mind because I just went out into Tokyo for the day. But it apparently was a great meetup because the foundation scheduled a large room for the entire Friday for operators to meetup and have an informal discussion about being OpenStack operators. The result was a wide-ranging discussion on almost all facets of OpenStack. We ended up with a 700 line etherpad. [3]Some items I took out of the informal meetup:  Maybe there is a need in Horizon to have some sort of “simplistic” interface option, where simple means less options. After having worked in a public cloud I have no problem saying that most customers are using it to create long-lived virtual machines, and are not using much of the other functionality, at least not yet. Having some kind of operator option to…er reduce options would be a good idea. See Digital Ocean.  Redfish - I’m still not sure what this is, but if it’s about making IPMI/ILOM/BMC/out-of-band management easier then I’m all in. [4]  ZFS is in Ubuntu Xenial, but there is no OpenStack Cinder driver that I’m aware of. That’d be cool. Someone should do that. I would if I was good enough to do it quickly, but I’m not.  OCP hardware - There were a couple of people deploying OCP hardware. OpenStack Ops discussions never seem to talk much about hardware, perhaps because it’s so vendor centric. One would think OCP would be a different story. That said, it’s not always easy to buy small amounts of OCP hardware.Those are just a few things that I took from the informal session. I had hoped to split out the Telco/NFS working group in the afternoon, but I had the feeling most of the Telco/NFVers left after the morning session and the facilities just weren’t quite setup for splitting off. Perhaps the next ops meetup or summit will be better suited.##OpenStack Mission StatementI went to one BoF session on the mission statement. It seemed like the goal of the session was to try to ascertain how well OpenStack was doing in particular areas mentioned in the relatively new mission statement:  To produce a ubiquitous Open Source Cloud Computing platform that iseasy to use, simple to implement, interoperable between deployments,works well at all scales, and meets the needs of users and operators ofboth public and private clouds.They broke out pieces of the statement:  Ubiquitous  Open Source Cloud Computing Platform  Easy to use  Simple to implement  Interoperable  Scales  (Can’t remember how they broke out the rest)and we talked about how well we were meeting these objectives.We got stuck on easy to use and simple to implement for a while. I personally don’t think those are actually OpenStack goals, especially around “simple to implement” because OpenStack is not a singular product, rather it’s a framework to build “Open Source Cloud Computing Platforms.” OpenStack doesn’t really have any artifacts of its build process–it’s just a collection of code and some documentation, and of course, a community around that code. (Actually I think they publish tgzs somewhere…)The tone in the room felt unusual, in that the people writing and thinking about the mission statement are pretty far away from actually using it or operating it. I understand that these phrases are long term goals for OpenStack, but at least one is not possible to meet.I would probably write something more like:  To produce an ubiquitous Open Source framework for cloud computing that is easy to use, interoperable between deployments, works well at all scales, and meets the needs of users and operators of both public and private clouds.and just get rid of the whole “simple to implement” component.##AustinI’ve been to Austin twice now. Austin is a great city, very eccentric. My home town of Edmonton always wants to be Austin. I took the bus from the Alamo Drafthouse downtown to Book People, and it was the happiest bus ride I’ve ever been on. An entire bus load of strangers was just laughing and having a great time. I thought I had stepped onto some kind of roaming festival, but nope, just a normal Austin bus ride. Oh, and I had a quick discussion with an Austinite who had recently watched Strange Brew. If it wasn’t so damn humid in Austin I could probably live there. Humidity is only 40% in Edmonton today.Also–Cheer Up Charlies was a great bar. People were bringing their dogs in! Crazy. Also, Argo’s foodtruck at Cheer Up Charlies had the best vegetarian bacon burger I’ve ever had. It was perfect. I think my biggest personal expense for the trip was eating there.##ConclusionOpenStack, public clouds like AWS, automation, containers, software defined networking–I was wondering where we’d go after innovation in these areas. At this point it seems like Telecos are now entering these relatively new spaces and bringing their considerable clout to bear. Perhaps we flounder around here a bit and then move on to whatever comes next. There is some danger of the massive resources of telcos putting pressure in the wrong places for OpenStack, and that things might break (though I didn’t see any obvious signals at the summit).I would imagine the OpenStack foundation is working hard to determine how to best direct those resources and ensure that the OpenStack project stays healthy, but it will be a difficult job. Telcos are strange beasts, but I think it’s important to remember that they, as organizations, have more change to implement to use OpenStack than vice-versa, mostly around their culture. Relatively speaking it’s much easier to deploy OpenStack than it is to adapt your culture to use it. OpenStack does offer a tremendous opportunity for telcos, and I suppose I should really be worried if telcos can handle the change, not the other way around.[0] https://twitter.com/e_monty/status/725482310879997953[1] Note that the summit will be changing over the next year or so. I believe the design portion of the summit is going to be split out into its own event.[2] https://etherpad.openstack.org/p/AUS-ops-NFV-Telco [3] https://etherpad.openstack.org/p/AUS-ops-informal-meetup [4] http://redfish.dmtf.org/redfish/v1"
    } ,
  
    {
      "title"    : "Analysis of OpenStack Austin 2016 NFV/Telco Track",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/04/14/openstack-austin-2016-summit-nfv-telco-track.html",
      "date"     : "2016-04-14 00:00:00 -0400",
      "content"  : "I went through all the presentations in the Telco/NFV track at the summit to find out what companies were presenting, and how many presenters each had.Please note this is not a scientific look at the numbers by any stretch of the imagination:  This analysis is occurring before the summit.  I’m not sure about a couple of companies in terms of their size.  I’m not counting unique presenters, just the occurrences. So if the same person did three presentations I’d count them as three different presenters.  I wrote this up on 2016-04-14 and things may have changed by then.  A presentation can have multiple people presenting, or it could be a panel which also has multiple people presenting.  I took the number of employees each company has from a simple Google search, so not accurate, but ball park. A recent article in the New York Times suggested AT&amp;T has 280000 employees.  Some companies are owned by larger companies, for example Nuage is owned by Alcatel-Lucent which I think is now owned by Nokia. Or something.  I did not submit a talk to the Austin 2016 summit.Information about Telco/NFV track presentations and presentersSome basic numbers on the presentations.            Item      #                  Presentations      26              Companies presenting      22              Total Presenters      75              Presenters with no affiliations      2              Presenters with unknown affiliations      2              Presenters from companies with &gt; 1000 employees      62              Presenters from companies with &gt; 100,000 employees      40              Presenters from non-profit foundations      5              Presenters from startups      4      TopicsThe topics are fairly varied. I was hoping to be able to categorize them, but it does seem diverse, at least within the NFV realm.Below are some keywords I pulled out of the blurb for each talk.            Keyword                                              VNF      Distributed NFV      Service Chaining      Open Source      vCPE              Panel      VM Placement      Uptime      Orchestration      OPNFV              Deployment      Service Chaining      IPv6      OpenStack Congress      Multi-site Distributed              OpenStack Tacker      Performance      SR-IOV      Monitoring/Telemetry      Latency              Platform Aware Scheduling      OPNFV Doctor      CPU Pinning      Huge Pages      NUMA      Some of them were mentioned more than once, such as OPNFV, service chaining, and technologies like SR-IOV/CPU Pinning/NUMA/Huge Pages. Multi-site or distributed was mentioned a couple of times. Both OpenStack and OPNFV have ongoing Telecom/NFV related projects such as Congress and Tacker as well as Doctor that should be well represented.Number of employeesBelow is a table of the larger companies and how many people each employs. It’s a considerable sum of over two million employees! That’s a lot of people.                   Company      # of Employees                  1      Juniper      9500              2      AT&amp;T      240000              3      HP(e?)      240000              4      BT      99000              5      Intel      107000              6      Redhat      8000              7      Ericsson      118000              8      Cisco      72000              9      Canonical      700              10      PlumGrid      250*              11      451 Research      250*              12      Midokura      100*              13      Nuage/Nokia      62000              14      Verizon      178000              15      Big Switch      100*              16      China Mobile      246000              17      NTT      242000              18      Orange      157000              19      Brocade      4000              20      Huawei      170000              21      NEC      102000              22      99Cloud      Unknown      Again, more than two million employees represented from all over the world. Amazing stuff.Please note for the “*“-ed companies I’m pretty much just guessing.ConclusionsThe vast majority of presenters in the Telco/NFV track are from extremely large companies. In fact most have more than 100,000 employees. 40 of the 75 presenters are from companies with more than 100,000 employees.I’m amazed at how large these companies are. As someone who is employed at a small company that works on NFV projects, I’m not sure what to make of this. Either small companies don’t submit presentations, or they are not accepted. Is NFV only important to large Telecoms? Are the only telecoms interested in OpenStack very large companies? Do small telecom companies even exist? Will they in the future?  Who are the people working on NFV in these companies…are they large teams or small, R&amp;D type groups? Do smaller companies help telecoms to deploy OpenStack-based NFV solutions?On one hand, the results are somewhat obvious: the majority of the companies doing presentations at the OpenStack summit on NFV related topics are very large because telecoms are very large. But on the other I’m surprised there aren’t more startups and the like in this mix. There is so much work to be done–so much software to be created–that there is, I believe, considerable opportunity. Perhaps this says something about the ability of a small company to able to participate in the OpenStack ecosystem."
    } ,
  
    {
      "title"    : "Edgecore 5712 Switch",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/03/25/edgecore-5712.html",
      "date"     : "2016-03-25 00:00:00 -0400",
      "content"  : "Slightly blurry picture of our 5712s in our labBy now most people in IT, especially if involved in networking, have heard the term “whitebox,” mostly in relation to switches. Whitebox switches are essentially commodity network devices that come without an operating system. For various reasons it’s now possible to purchase a fast, relatively low-cost switch based on merchant silicon [1] and run any of several (usually Linux based) network operating systems on it. This is quite powerful in terms of commoditization as well as providing access to a large open source ecosystem (again, Linux).My use caseCurrently my use case if fairly small or simplistic at this point. Basically our two 5712s will be used to provide the underlying physical network for an OpenStack lab. While I hope to get into larger network designs, our basic case will be to run Cumulus Linux on the switches and use their CLAG feature,ie. multi-chassis link aggregation, to provide network high availability to our hosts. Nothing too fancy, and in fact keeping it simple is one of the “features” I like about whitebox switching. Sometimes less is more. Further, because Cumulus Linux is Linux, it lends itself well to automation. I’ll be able to manage all our OpenStack lab network infrastructure using Ansible, which is the same tool I use to manage OpenStack itself.SpecsThe unboxeningThe Edgecore 5712 is a 10GB switch with 6 40GB uplink ports.  48 x SFP+ switch ports, supporting 10GbE (DAC, 10GBASE-SR/LR) or 1GbE (1000BASE-T/SX/LX).  Switch Silicon: Broadcom BCM56854 Trident II 720 Gbps.  CPU: Intel Rangeley C2538.  Network design approved by Open Compute Project (OCP)The Intel Rangeley CPU is an Atom chip. The switch also comes with 8GB of memory.Trident IIFrom my layperson perspective the important part is the Trident II chip. This would be the “merchant silicon” that is the engine of this device, and it is a very powerful engine. A large number of switches will use this silicon so you will see the features that it provides in many whitebox switches.From the Broadcom site;  First switch to support VMWare VXLAN and Microsoft NVGRE tunneling protocols supported by SmartNV technology  Enables spanning-tree-free and CLOS-style network topologies through TRILL, SPB and ECMP with SmartHash technology  SmartTable and SmartBuffer technologies enable large-scale data centers with 10,000+ end user nodes  Up to 128x 10G integrated SerDes with Energy Efficient Ethernet for maximum port density per RU  Standards-compliant 10GbE/40GbE switch with support for up to 32 ports of 40GbE or up to 100+ ports 1GbE/10GbEThe same chip is used in Edgecore’s 32 port 40GB switch, which I would have preferred to purchase because you can split out each 40GB port into four 10GB ports, and thus have 128 10GB ports from one switch! Most organizations could get away with only two switches in their data center. You could run a lot of virtual machines with 128x2 10GB ports. But I digress…The VXLAN feature will also be useful, as I have deployed software defined networking systems such as Midokura’s Midonet that rely heavily on VXLAN overlays.Available Network Operating systemsFortunately the 5712 is one of the best supported whitebox switches. That said there are only a handful of network operating systems available right now. These are the ones that I know of:  Cumulus Linux  Pico8  OpenSwitch (not the same as Open-V-Switch)Right now I have only installed OpenSwitch and Cumulus Linux. OpenSwitch is open source and is mostly backed by HP (I believe). However it’s not quite ready for production, as far as I know. I’ve yet to try out Pica8’s PicOS so I can’t really comment on it. Both Cumulus and OpenSwitch installed perfectly on the 5712. [2] I believe, as of right now, the only switch that OpenSwitch supports is the 5712, whereas Cumulus supports several makes and models.PurchasingWe fluked out and one of the resellers we deal with had two in stock. They didn’t even know what they were. It was weird. But were like “hey, can we get these two switches off you” and after a while they sold them to us. We also ordered a 1GB Edgecore switch that was not in stock, and it still hasn’t come in yet (this was several weeks ago). Being in Canada doesn’t help, as everything has to get imported from the US.This is part of why IaaS like AWS/Google/Azure is going to win a majority share: because it’s hard to even order physical equipment. Don’t even get me started on the process of buying licenses for Cumulus. The money wasn’t a problem, as we are getting good value, rather it’s the process…but that’s another blog post. A big part of open source software use is convenience–it’s only a download away. Certainly this convenience comes with its own set of issues, but location, location, location. I am on month three of waiting for hardware for my OpenStack/OPNFV lab, and still have another month or two to go.While I try not to focus on how much hardware costs in terms of commoditization, as I feel that companies tend to focus on that too much, I can say that these switches are very cost effective, even including a Cumulus Linux license. The 40GB versions are even better value.ConclusionIn my opinion, merchant silicon has paved the way for disaggregation of the switch hardware from the operating system which has thus enabled commoditization. Whitebox switches plus an OS like Cumulus makes for a powerful network platform. The trickle down of technology from the “hyperscalers” has allowed people like myself and the organizations we work for access to advanced network systems. This shift is very similar to what happened, or is happening, with x86 commodity servers and Linux/*BSD. That said we are still in the early days but I expect it to unfold much like commodity x86. [3]Further, I believe that the somewhat limited feature set of the network operating systems is actually a feature in itself…when buying network gear from larger vendors you often pay for features you don’t need, so it allows some choice. That said, if you know exactly what you need and this whitebox model doesn’t provide it, then you’re still free to purchase from whatever vendor you would like. There will be many situations where that is the case, however, maybe I don’t need a huge network infrastructure, maybe all I need is two 32 port 40GB switches with MLAG to support 6000 virtual machines, or maybe I want to implement a relatively large-but-simple ECMP-based CLOS design. Choice is good.1: Merchant silicon is a fancy phrase for “off the shelf chips”, ie. the vendor doesn’t have to spend money on developing and making their own network device chips. Cisco, for example, spends a considerable amount of money developing their own switching silicon. 2: Actually I did have one problem. I had OpenSwitch installed on one of the two switches, then I installed Cumulus over top. Something happened during the install that broke Cumulus, but it still completed the install and booted up. For some reason my ability to boot back into ONIE from the Cumulus operating system was broken. I had to use the console to boot into ONIE so I could continue testing my install automation setup. It was fairly unusual…as though the install had crashed part way through, just enough to boot the OS but break some functionality. I could not replicate the issue.3: A recent Human Infrastructure Magazine entry briefly discussed merchant silicon, ECMP designs, etc."
    } ,
  
    {
      "title"    : "Packaged OpenVSwitch with DPDK on Ubuntu Trusty",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/03/24/trusty-openvswitch-dpdk.html",
      "date"     : "2016-03-24 00:00:00 -0400",
      "content"  : "Murano by Mathieu St-PierreThis is just a quick post on getting OpenVSwitch with DPDK installed on Ubuntu 14.04/Trusty using a package instead of compiling it on your own.tl;drInstall the Ubuntu Liberty Cloud Archive. Then you can install openvswitch-switch-dpdk. However I have not completed the configuration component because there are some bugs, as well I’m lacking a baremetal server at the moment as I was working in AWS. So really this post is just to say that there is a DPDK OpenVSwitch for Ubuntu Trusty. I’ll follow up with more information in a later post.InstallationFirst ensure python3-software-properties is installed.$ apt-get install python3-software-propertiesThen install the cloud archive.$ add-apt-repository cloud-archive:libertyNow apt-get update and check the policy/version of the openvswitch-switch-dpdk package.$ apt-cache policy openvswitch-switch-dpdkopenvswitch-switch-dpdk:  Installed: 2.4.0-0ubuntu1~cloud0  Candidate: 2.4.0-0ubuntu1~cloud0  Version table: *** 2.4.0-0ubuntu1~cloud0 0        500 http://ubuntu-cloud.archive.canonical.com/ubuntu/ trusty-updates/liberty/main amd64 Packages        100 /var/lib/dpkg/statusNow it can be installed.$ apt-get update$ apt-get install openvswitch-switch-dpdkConfiguring to use DPDKSet alternatives.$ update-alternatives --set ovs-vswitchd /usr/lib/openvswitch-switch-dpdk/ovs-vswitchd-dpdkupdate-alternatives: using /usr/lib/openvswitch-switch-dpdk/ovs-vswitchd-dpdk to provide /usr/sbin/ovs-vswitchd (ovs-vswitchd) in manual modeAt this point you could follow the rest of the instructions for using OVS however I will warn you that it’s not that straightforward at this time, though I’m sure that will improve in the future. This bug report has some hints. I expect this will be much easier soon. At any rate, at least OVS with DPDK is installed and you can start messing around with getting it working."
    } ,
  
    {
      "title"    : "Infrastructure Zero",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/03/11/infrastructure-zero.html",
      "date"     : "2016-03-11 00:00:00 -0500",
      "content"  : "Image by David Szaudertl;drIn terms of Infrastructure as a service (IaaS) the point of open source software and hardware IMHO the point is to:  innovate faster than closed source, and,  drive down the cost of IaaS to near zero to focus on deploying applications.Perhaps #2 is merely a by-product, but I think there is a considerable distinction between what enterprise organizations think they are supposed to do with Open Source Software (OSS), ie. not pay for it and fire staff (lol), and what they are really doing: using highly collaborative, world-wide software and hardware projects to implement  and manage new technology and reduce the costs of particular layers (such as, right now, the infrastructure layer) so that we can spend money on higher level systems, if not higher level abstractions, and thus do more with the same resources.There’s a difference between getting something for free, and using that something to commoditize systems and move up the value chain.Props to Randy BiasIn this blog post Randy Bias, now of EMC, formerly of Cloudscaling (which was purchased by EMC) mentions that infrastructure has no value.  All value is from applications that consume [infrastructure].In this video he talks a bit more about infrastructure zero, likening infrastructure to utilities like power and water, and how it is not a differentiating part of your business in that we all need CPU, storage, and networking–that what is differentiating is the applications, and custom code, that utilizes those primitives.I’ve always liked what Randy says about OpenStack and infrastructure, because I feel that he’s usually right, but the market tends to disagree with him and go on to screw everything up. Cloudscaling had some good ideas regarding infrastructure choices for OpenStack, but even though the company “exited,” ultimately it failed, and as far as I know the Cloudscaling product no longer exists.In an Appformix blog post, Mr. Dunnigan rephrases Randy’s theory:  …the sole purpose of infrastructure is to serve the applications they run.  Infrastructure running zero workloads has zero value.Infrastructure ZeroImage: The facade 2010, Robert OverwegCurrently IaaS is going through a period in which we are attempting to use OSS and hardware, as well as automation, to drive down the cost of operating IaaS as close to zero as possible. The reality is that IaaS has little value unless it’s not working, in which case it has negative value.But in general its value is zero or close to zero, and the real goal is to get the cost of operating IaaS as close to zero as possible. But there is a long way to go, and doing so requires resources and talent given that important components of IaaS, such as OpenStack, are not as operationally mature as we would require to fully commoditize. Further, while we have whitebox network switches the operating systems available for them are relatively new.This is not to say that OpenStack is not “production ready” (it is) rather that it doesn’t necessarily cost less to operate than other traditional virtualization solutions because of the human resources required to, for example, upgrade it. That said it does come with what is now considered a standard API which most other IaaS-like systems do not. So if you consider the costs a wash then just having an API will improve things. But I digress…Public CloudI don’t want to get into private cloud vs public cloud debate in this post. That said I feel comfortable saying a large majority of IT organizations will be using public clouds like Amazon, Azure, and Google. But it’s unlikely to be 100%, and will take many years, if not decades. There will still be tens of billions of dollars available for private clouds, just not hundreds of billions.Further, some people believe that the bigger Amazon Web Services gets, the bigger OpenStack (ie. private clouds) will be. However, based on who is saying that there may be bias in that thinking.Suffice it to say that I am a believer in public clouds, but also a realist in terms of the fact that businesses have to sell what people want, and many organizations–at least right now–want private clouds.It’s from this standpoint that I make most of my generalizations on infrastructure zero, in that work that public clouds, and other large IaaS users (eg. Facebook) are are doing is trickling down into private clouds (see Open Compute or the Xeon-D processor), and in conjunction with software like OpenStack we hope to drive the cost to zero so that we can work on deploying applications and systems that provide differentiation and ultimately make money.Nobody wants IaaS, but everyone needs itI have been working with OpenStack for several years, including deploying a public cloud in Canada based on OpenStack. I can say without reservation that no one wants just OpenStack.Deploying an OpenStack cloud and handing over credentials is a surefire way to fail, because, most often, end users will have no idea what to do with it. Users want to deploy applications. Simply having access to OpenStack APIs is (usually) not enough. There needs to be another layer of abstraction in between IaaS and most users–another layer of utility.Recently Werner Vogels, of AWS fame, posted an entry on his blog that detailed ten things he has learned building a public cloud (perhaps the public cloud). One of his points regards “primitives”:  One of the most important mechanisms we provided was to offer customers a collection of primitives and tools, where they could pick and choose their preferred way to engage with the AWS cloud…What I take that to mean is that building a cloud requires ensuring certain primitives exist so that layers of abstraction and workflows can be defined and overlaid on those blocks. The primitives are obvious (compute, network, storage, etc) and have to exist, but they do no directly provide value as we still need abstraction and automation above them.In many ways I am surprised that AWS has grown so quickly given that it is a complex system of primitives sans frameworks. In this post I am suggesting that primitives are not enough, and yet AWS is thriving, and is, I believe, one of the fasted technology companies to a 10 billion run rate.However, as AWS matures, and the (somewhat creepy?) socio-techo organization behinds it learns what frameworks and workflows its customers want, I expect it to begin identifying layers of abstraction that will continue to drive growth and enable simpler, faster, more powerful deployments of technologies, making it easier to develop, deploy, scale, and operate new applications. (Such as games.) They’ll develop frameworks that will quickly become standards.Another point of note: I recently listened to a podcast interview with someone from Pivotal (a platform as a service product based on Cloud Foundry). Unfortunately I can’t find it so this is anecdotal. Occasionally when Pivotal talks to prospective customers, these customers have technical staff who ask what operating system their code is running on, and Pivotal pushes back on that question, because why do they need to know? Is the application running properly? If yes, then it’s irrelevant what the underlying virtual machine operating system is, just like it’s irrelevant as to what the IaaS is. The abstraction layer will (hopefully) deal with it. The user doesn’t need to know. The PaaS can ask the IaaS for CPU, storage, and network. Push your code up, it’s running properly…that’s all that matters.controller.controllerBy “controller.controller” I mean a “controller of controllers” or “manager of managers.” In the Vogel post I mentioned about he also uses the term “framework” which is probably similar. Right now this area is rather messy. Everyone is working on building these systems and it is a difficult thing to do. It reminds me of how many Javascript frameworks there are. I don’t know if we want as many IaaS abstraction frameworks as there are Javascript frameworks. Then again most attempts at a “single pane of glass” fail. So somewhere between two and several.From my perspective a controller of controllers could be as simple as using OpenStack Heat or Hashicorp’s Terraform, or as complex as the ETSI defined MANO layer. The point is that when an OpenStack system cloud is deployed, so to should systems that enable higher level abstractions, and likely more than one.These systems will be incredibly important in terms of not only managing infrastructure but also in terms of driving business profitability, on both sides of the equation.The Infrastructure|Software Paradox(Image from Bloomberg)I recently ordered the book The Software Paradox but it hasn’t arrived yet so I haven’t read it. But I can’t wait to dig in. The premise:  The Software Paradox explores the counterintuitive idea that software’s realizable commercial value is headed in the opposite direction of its market importance.Essentially “software is eating the world” but it’s getting harder to make money off of it. How can that be?In many ways this post regards “The Infrastructure Paradox” in which infrastructure is more important than ever but it’s become less profitable. Or is it simply that it has become commoditized and has lower margins which requires a different business model? To be honest I don’t know, but the reality is that the change is unstoppable.What do we do now?From my standpoint as a long-time systems administrator, I feel like my best path is to work to aggressively reduce the cost of IaaS (ie. automation, monitoring, ect), make deploying applications as easy and automatable as possible, and understand the dynamics of the public and private cloud marketplace. Further–as a systems administrator of all things–understanding the business model of a product that is simultaneously incredibly important but has limited positive financial value seems paramount.1: controller.controller was a Toronto indie rock band. I don’t know if I’ve ever even heard one of their songs, but I remember their name because it always seemed like a tech company rather than a band. &lt;br/ &gt;2: Not sure where to put this but perhaps this is all just a rehash of “utility computing” that I believe originated in 1961!"
    } ,
  
    {
      "title"    : "OpenStack Operators Midcycle, OpenStack Ansible Midcycle, and Ansible Fest",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/02/23/osops-ansible-meetups.html",
      "date"     : "2016-02-23 00:00:00 -0500",
      "content"  : "This is a brief post on my recent trip to the UK to attend:  The OpenStack Operators Midcycle meetup in Manchester  The OpenStack Ansible project’s midcycle meetup at Rackspace’s UK HQ in London Hayes  Ansible Fest LondonOpenStack Operators Midcycle Meetup (Manchester, UK)For the first leg of the trip I spent two days in Manchester’s Media City attending the OpenStack Operators midcycle meetup. Originally this meetup was for European operators. At the time I registered for the meetup, a few months ago, I was working at a Canadian OpenStack-based regional public cloud. Europe has several similar clouds and I was interested in meeting up with the operators of those clouds to exchange ideas. So that was my original intention in terms of attending the meetup. After registering, the meetup turned into the official midcycle operators event, and I also moved on to a new job that didn’t involve running a public cloud. So both the event and my purposes for attending the event changed, but in such a way that it made sense for me to participate.The most important thing I learned from the midcyle event is that OpenStack is the perfect cloud system for Europeans. The European Union recently put out a RFP (I have been looking but cannot find the actual document) requesting vendors to propose how they would run a series of cloud systems (IaaS, PaaS, etc) for the EU. In some parts of that document OpenStack was called out as a standard.OpenStack fits much better into the mentality of the EU than it does with North America. In my opinion, the EU wants OpenStack because OpenStack is an open project, because it’s open source, it’s an open community…all the values that the project stands for and stands on. In fact OpenStack is based on “the four opens.” This is opposed to what I typically see in North America which is reducing costs and trying to avoid the dreaded “vendor lock-in.” (Neither of which are likely to be achieved when adopting OpenStack.) Further, European operators are interested in federation, and the private clouds of NA are not.Other notes from the midcycle:  I met a couple of people who are using whitebox switches and network OSes like Cumulus Linux. This is good because when I returned to my desk in Edmonton there were a couple of Edgecore 5712s on my desk that are due to go into our lab.  Ceph continues to be a popular storage system to go along with OpenStack installations. I’m finally setting up a ceph cluster, for the first time, in the near future as we have a small cluster of ceph nodes on order as well, more lab fodder. There are large companies heavily relying on ceph storage for production systems.  People are finding that Chef and Puppet are not completely working out. It’s becoming common to utilize Ansible in combination, especially around orchestration (depending on how you define the word).  OpenStack Ansible was used to deploy a large cluster (500+ compute nodes). Anecdotally people have said that once you hit a couple hundred compute nodes that it’s time to start looking at cells. That does not seem to be the case with more recent versions of OpenStack. 500+ compute nodes is a lot of virtual machines.  Speaking of cells…in Mitaka everyone will use cells V2, but to start you can only have one cell.  Apparently a lot of “ceph enablement” has been done in MitakaManchester’s Media City reminded me a lot of the business area of Santa Clara. It’s a vast business desert. Very few places to eat, no drug store, not much to do. In short not a great place to visit. I’m sure Manchester itself has a lot to offer, but if I was staying longer to enjoy the city I’d definitely be moving to different hotel downtown. I don’t usually like to have breakfast in the hotel, but I had little choice in Media City. I find it amazing that hotels can charge $25-30 for a buffet lunch made up of oily eggs and weird tasting hashbrowns. At least the coffee had caffeine in it.OpenStack Ansible MidcyleThe midcyle ocurred at Rackspace’s UK HQ, and there were about 12 people there, with a few more coming in and out online via video conferencing. Notably four members of the HPE Helion lifecycle management team were also in attendance.I have been following the OpenStack-Ansible project since it started. At my previous job I was able to make use of some of the components of the project, such as the roles to deploy and manage MySQL Galera and RabbitMQ, but that was over a year ago. Much has changed. I would imagine the code has completely rolled over as the project has seen impressive growth. At my current position I’m hoping to make use of the OpenStack-Ansible project to deploy our OpenStack lab, and in doing so be able to commit some code back to the project. For the purposes of our lab I’ll need OpenVSwitch (OVS) support, which does not quite exist in the project yet, but is on the way. Fortunately OpenStack-Ansible already supports using a ceph cluster (though does not deploy one, rather ceph-ansible does that).Topics for discussion ranged from patterns and workflows that should be in use in OpenStack-Ansible to how to manage the life cycle of an OpenStack cloud. HPE (now with an E) recently released a snapshot of their Ansible-based deployment and management system for their Helion project. Several HPE employees were in attendance as it seemed like there was the possibility of some collaboration, or at least shared learning (misery?) in terms of what each group has done with OpenStack and Ansible. The HP team had moved from Chef to TripleO and are now using Ansible, specifically because they found the previous two difficult to use in terms of managing (mangling?) an OpenStack system over time. Certainly Ansible is not the perfect tool, but, at least in HPE’s experience, it is a more appropriate configuration management system for their purposes. Unfortunately the reality is that it is hard to maintain a life cycle for any system, especially complex ones like OpenStack. As I am often saying, even today in fact: it’s easy to deploy OpenStack, but managing it over time (eg. upgrades) is much, much more difficult.One thing I learned from the midcycle is that code talks. In many cases it’s easier to discuss the results of a proof of a concept, based on code, than it is to discuss that concept without any code. It’s easy to lapse into a two hour discussion about what could happen if a pattern or workflow was followed, as opposed to a five minute discussion when you have actual code to look at. Code can talk much faster.It was certainly nice to meet a good portion of the core OpenStack-Ansible team and to put IRC nicks to faces. I hope to be able to contribute back to the project in the near future as the OpenStack lab I’m working on is deployed. I’m appreciative of the OpenStack-Ansible team for allowing me to attend the midcycle, especially given my limited ability to contribute to the conversation.Ansible Fest(Ansible Fest from the middle row, before the talks started)First off it’s important to note that I was at a previous Ansible Fest, the 2014 event in NYC (I think that’s when it was). It was a small event and one that I felt was technical. It’s possible that I’m remembering incorrectly. However, I felt it was technical, and that the Ansible Fest London 2016 event was not. London was much larger, which is to be expected given Ansible’s popularity and growth. According to the event staff there were about 530 people in attendance. Growth is good. RedHat buying Ansible is probably good. Certainly more developers have been added to the project and if that is the only thing that RedHat adds then that is positive just by itself.However, I don’t think I would go to AnsibleFest again, mostly because the event seemed to be focused on telling people to do automation, devops, immutable infrastructure and the like, as opposed to really showing what Ansible can do, ie. telling vs showing. Certainly there were some attempts to provide examples of powerful uses of Ansible, especially around networking and winops (a term I first heard at this event, which I like), as well as a presentation by jimi-c on how (and why) to write Ansible modules, but it just wasn’t quite enough. If I went to an Ansible event again it would have to be more workshop-like or perhaps have multiple tracks with less showy presentations. In the end I think they did their best and catered to the crowd they knew they would have.Another nit: for a day long conference it was quite expensive. Certainly cost is a relative thing, but compared to other events I’ve been to AnsibleFest was costlyA few things I learned from the conference:  I was speaking with a company that works in the gambling industry, and they are using AWS as their main site host, but also have a secondary physical site (“luke warm” was the term used) in a co-location center so the government can have something physical to seize in case of some sort of legal or regulatory issue.  Ansible is often used in continuous integration or deployment (CI/CD) pipelines. These pipelines are important to organizations because if the pipeline stops working, perhaps due to Ansible, then they can’t deploy their code, can’t get fixes or features out, and that costs money. So they want support from Ansible so they can get their pipelines back up and running as soon as possible. Basically everything in that pipeline requires support of some kind.  Speaking of support, most of the questions in the question/answer periods were about support. Many organizations want Ansible support.  Through a “raise your hand if” question from a presenter, it was noted that the large majority of people in the audience work at companies with more than 1000 employees.  Ansible Galaxy runs on donated resources from Microsoft Azure.  One of the main announcements from the conference is that Ansible is dedicated to supporting network automation, and released several modules for various networking companies such as Cisco and Cumulus Linux. I’ve been using the Cumulus modules for a couple weeks, so in effect they just moved the modules from one github repository into another, but it’s great to see a focus on networking for Ansible. It’s dearly needed.  Ansible is in some ways a better fit for network automation because it’s agentless. You can run Ansible over SSH, or you can write Ansible modules that use APIs that the networking system provides.  CLOS networks are becoming the norm.  Some organizations have to manage millions of lines of network configuration that are cut and pasted into network devices. I can’t see how that works.All of the presentation videos are available online.Europe in general(Shakespeare’s Globe Theatre, under construction for a new show)Other than getting the flu, which seems to happen on every trip, I had a great time traveling from Edmonton to Amsterdam to Manchester to London and home again. Edmonton has a direct flight to Amsterdam via KLM, so that makes travel to Europe easier for me. Unfortunately it’s currently the only direct flight to Europe. We used to have a direct flight to NYC as well, but this isn’t a “Edmonton Direct Flight Blog.” :)I was in five hotel rooms in nine days but everything worked out 0K, and I made it home in almost one piece. However, I did get lucky in a couple instances. At one point I was quite lost after getting off the train from Manchester (forgot about a transfer) and had to take an expensive cab from the train station to my hotel. However, the cab driver could not find the hotel and we drove around for an extra 30,40 minutes with the meter off as he tried to locate it. I did eventually get to my hotel and he was apologetic. In general I found Londoners to be a pleasant group of people no matter how many of them we crammed onto the underground. Once I was looking at a map of Canary Wharf and a policeman actually gave me directions. This has never happened to me anywhere else, not even in Edmonton.Probably the biggest thing that I learned is that Europe (OK, just Amsterdam and London) is cold and rainy at this time of year. Edmonton is quite a dry place, and that makes it feel warmer than it is. Zero degrees in Amsterdam felt like -10 in Edmonton, and one of the first things I had to do was buy a scarf. I was freezing. Also it rains a lot in Amsterdam and London. It rarely rains in Edmonton, and my home town is one of the sunniest places in the world. We get over 2300 hours of sunshine per year versus 1662 for Amsterdam and 1481 for London [1]. Even notoriously rain-soaked-everyone-has-an-umbrella Vancouver has more sunshine than London. Poor London. (Apparently Yuma, Arizona is the sunniest place in the world with 4000 hours of sunshine a year…11 hours a day.)My next trip should be to Austin for the OpenStack summit. I love Austin, but am a bit concerned about the recent Texas open carry laws. However, Austin is such an amazing place that it might be worth the risk.1: Sunshine Hours"
    } ,
  
    {
      "title"    : "GRE point to point and AWS VPC",
      "category" : "",
      "tags"     : "",
      "url"      : "/2016/01/25/aws-gre.html",
      "date"     : "2016-01-25 00:00:00 -0500",
      "content"  : "This is a quick, and simple, post on setting up a GRE tunnel between two instances in an Amazon VPC. Not all that useful in the real world, but it is part of exploring creating overlay and other network models within an EC2 VPC. I can’t find the original link I was working off of, but there are several blog posts out there that discuss setting up a simple GRE point to point tunnel on Linux, such as this one.First: AWS spot instancesI like spot instances, so that’s what I used in this example. You put in a bid and you can get EC2 instances for a fraction of the “regular” price. These instances can go away at any time depending on market forces, but I have found that they are perfect for testing and so far have not had one disappear out from under me–I’ve always terminated them myself after a few hours. Usually I start up a couple instances in the morning and shut them down at the end of the day. Costs about $0.02 for 2 or 3 m3.medium instances, if not less. Great for labs or training. Also I like the fact that it forces me to get used to destroying and rebuilding instances, and automating as much as possible to make the new instances ready as quickly as possible.So create two AWS instances in the same VPC. In the example that follows they have the IPs of 172.22.1.{76,238}.Configure interfacesFirst we’ll manually create network interface files. The OS here is Ubuntu 14.04.root@ip-172-22-1-238:/etc/network/interfaces.d# cat gre1.cfg auto gre1iface gre1 inet tunnel  mode gre  netmask 255.255.255.255  address 10.0.0.2  dstaddr 10.0.0.1  endpoint 172.22.1.76  local 172.22.1.238   ttl 255And on the other node:root@ip-172-22-1-76:/etc/network/interfaces.d# cat gre1.cfg auto gre1iface gre1 inet tunnel  mode gre  netmask 255.255.255.255  address 10.0.0.1  dstaddr 10.0.0.2  endpoint 172.22.1.238  local 172.22.1.76   ttl 255Then start the interfaces.$ # both instances$ ifup gre1And now you should see something like…root@ip-172-22-1-76:/etc/network/interfaces.d# netstat -rnKernel IP routing tableDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface0.0.0.0         172.22.1.1      0.0.0.0         UG        0 0          0 eth010.0.0.2        0.0.0.0         255.255.255.255 UH        0 0          0 gre1172.22.1.0      0.0.0.0         255.255.255.0   U         0 0          0 eth0root@ip-172-22-1-76:/etc/network/interfaces.d# ip ro shdefault via 172.22.1.1 dev eth0 10.0.0.2 dev gre1  proto kernel  scope link  src 10.0.0.1 172.22.1.0/24 dev eth0  proto kernel  scope link  src 172.22.1.76 IperfWithout GRE overlay:# iperf -c 172.22.1.238------------------------------------------------------------Client connecting to 172.22.1.238, TCP port 5001TCP window size:  325 KByte (default)------------------------------------------------------------[  3] local 172.22.1.76 port 52187 connected with 172.22.1.238 port 5001[ ID] Interval       Transfer     Bandwidth[  3]  0.0-10.0 sec   464 MBytes   388 Mbits/secWith GRE:# iperf -c 10.0.0.2------------------------------------------------------------Client connecting to 10.0.0.2, TCP port 5001TCP window size:  325 KByte (default)------------------------------------------------------------[  3] local 10.0.0.1 port 47969 connected with 10.0.0.2 port 5001[ ID] Interval       Transfer     Bandwidth[  3]  0.0-10.0 sec   462 MBytes   387 Mbits/secSurprisingly close. Perhaps too surprisingly.tcpdumpIf I ping one node from the other, and then tcpdump GRE on eth0…root@ip-172-22-1-76:/etc/network/interfaces.d# sudo tcpdump -n -e -ttt -i eth0 proto gretcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes00:00:00.000000 02:28:2a:9e:bb:03 &gt; 02:f4:ec:79:c6:7b, ethertype IPv4 (0x0800), length 122: 172.22.1.238 &gt; 172.22.1.76: GREv0, proto IPv4 (0x0800), length 88: 10.0.0.2 &gt; 10.0.0.1: ICMP echo request, id 2533, seq 1, length 6400:00:00.000043 02:f4:ec:79:c6:7b &gt; 02:28:2a:9e:bb:03, ethertype IPv4 (0x0800), length 122: 172.22.1.76 &gt; 172.22.1.238: GREv0, proto IPv4 (0x0800), length 88: 10.0.0.1 &gt; 10.0.0.2: ICMP echo reply, id 2533, seq 1, length 6400:00:01.000261 02:28:2a:9e:bb:03 &gt; 02:f4:ec:79:c6:7b, ethertype IPv4 (0x0800), length 122: 172.22.1.238 &gt; 172.22.1.76: GREv0, proto IPv4 (0x0800), length 88: 10.0.0.2 &gt; 10.0.0.1: ICMP echo request, id 2533, seq 2, length 6400:00:00.000038 02:f4:ec:79:c6:7b &gt; 02:28:2a:9e:bb:03, ethertype IPv4 (0x0800), length 122: 172.22.1.76 &gt; 172.22.1.238: GREv0, proto IPv4 (0x0800), length 88: 10.0.0.1 &gt; 10.0.0.2: ICMP echo reply, id 2533, seq 2, length 64^C4 packets captured4 packets received by filter0 packets dropped by kernelListening on gre1 for icmp:root@ip-172-22-1-76:/etc/network/interfaces.d# sudo tcpdump -n -e -ttt -i gre1 icmptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on gre1, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes00:00:00.000000  In ethertype IPv4 (0x0800), length 100: 10.0.0.2 &gt; 10.0.0.1: ICMP echo request, id 2534, seq 1, length 6400:00:00.000024 Out ethertype IPv4 (0x0800), length 100: 10.0.0.1 &gt; 10.0.0.2: ICMP echo reply, id 2534, seq 1, length 6400:00:00.998982  In ethertype IPv4 (0x0800), length 100: 10.0.0.2 &gt; 10.0.0.1: ICMP echo request, id 2534, seq 2, length 6400:00:00.000023 Out ethertype IPv4 (0x0800), length 100: 10.0.0.1 &gt; 10.0.0.2: ICMP echo reply, id 2534, seq 2, length 64^C4 packets captured4 packets received by filter0 packets dropped by kernelGRE overheadGRE adds 24 bytes of overhead. So the GRE tunnel MTU is 8977, 24 less than eth0’s 9001 MTU. (Some EC2 instances support jumbo frames, and in this case my spot instances came up with an MTU of 9001, but this is not always the case, and is not always desirable.)ubuntu@ip-172-22-1-76:~$ ip ad sh eth02: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000    link/ether 02:f4:ec:79:c6:7b brd ff:ff:ff:ff:ff:ff    inet 172.22.1.76/24 brd 172.22.1.255 scope global eth0       valid_lft forever preferred_lft forever    inet6 fe80::f4:ecff:fe79:c67b/64 scope link        valid_lft forever preferred_lft foreverubuntu@ip-172-22-1-76:~$ ip ad sh gre15: gre1@NONE: &lt;POINTOPOINT,NOARP,UP,LOWER_UP&gt; mtu 8977 qdisc noqueue state UNKNOWN group default     link/gre 172.22.1.76 peer 172.22.1.238    inet 10.0.0.1 peer 10.0.0.2/32 scope global gre1       valid_lft forever preferred_lft forever    inet6 fe80::5efe:ac16:14c/64 scope link        valid_lft forever preferred_lft foreverConclusionThis was just a simple exercise for me to get used to networking within AWS VPC, and to gradually work towards more complicated overlay (or other) models and software defined solutions."
    } ,
  
    {
      "title"    : "A Year with Midokura&#39;s Midonet and OpenStack",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/12/31/a-year-with-midokura-midonet-and-openstack.html",
      "date"     : "2015-12-31 00:00:00 -0500",
      "content"  : "One of the first things to do when deciding to deploy OpenStack is figure out what the Neutron network is going to look like. For that, one needs the project requirements, what your network administrators are willing to deploy (allow), and what the operators are comfortable using. Maybe those are all the same person, I don’t know. :) There are all the general Neutron deployment options, as well as plugins like Project Calico, Plumgrid, and many others to choose from. One might say you have a plethora of choices.Before I get too far, I should note that I don’t have a lot of experience with non-Midonet deployments of Neutron, so I have “strong opinions, lightly held” in terms of how to deploy Neutron.My feeling when deciding to use Midonet was that the more generic Neutron deployment model, something like HA with DVR, sounded complicated and in some cases limited. Tales of debugging sounded awful. OpenStack operators love to anecdotally complain about standard Neutron deployment models, so I thought I’d just skip all that and implement Midonet. I needed a highly available system that also supported a VPC style of tenant networking. Also I like software defined networking (whatever that actually means). Midonet seemed like a perfect fit, and after a year in production, I feel it was a good choice. Now if I complain about networking in OpenStack I’d be really be complaining about Midonet as opposed to Neutron (which is essentially just the front-end API).However, other networking models should be considered. I think it’s safe to say I have experience operating Enterprise Midonet 1.8, but after that things get fuzzy. So complete whatever due-diligence seems necessary for your project.What is Midonet?From my perspective as an OpenStack operator, Midonet is a Neutron plugin and software defined networking system. It falls into the category of “overlay” models. It creates a logical network topology over top of the physical one, essentially using VXLAN (or GRE) encapsulation. The network state is distributed and stored mostly in Zookeeper. The Midolman agent runs on API, gateway, and compute nodes. Midonet has good documentation, so I suggest taking at look at that to get a better understanding of how it all works.Here’s the standard Midonet diagram. It shows how Midonet “overlays” on top of the physical network.Things I like about MidonetThere are many things to like about Midonet.  It is open source, though I’m not sure how many non-Midokura developers contributeNote: I have not deployed the open source version, only the commercial 1.8 series  BGP-based HA on the gateway nodes for north/southThis means you can drop a gw node and only lose about 10 packets before the IP fails over. I really like this feature. Requires your network admin to do some work setting up BGP sessions. Midonet takes care of them on its side when BGP ports are configured. Also requires stateful port groups to be configured, but that is not hard. I’m not sure why anyone would deploy Midonet gateways without doing that.  Good support from MidokuraI don’t recommend many commercial systems, but I do recommend Enterprise Midonet. The support team has been fantastic.  Midokura is a modern, fast moving company with new ideasThere are lots of interesting things that could/can be done with a system that enables SDN. Think about anycast with floating IPs across WAN-connected regions, cool stuff like that. An overlay is certainly not less complicated, but the features and capabilities it adds are worth it in my opinion.  Because it is software features can be added without requiring new hardware  Has an API  Gets rid of standard Neutron network nodes, though north/south goes through the gw nodesSome Neutron deployments require dedicated network nodes.  Your network admins can probably ignore it and just provide L3 connectivity  Security groups are not iptables based  Potential for service chaining, NFV, all that hyped up stuff  Does have a web interface, but I have never used it  _Midokura is working hard on important OpenStack projects like Kuryr  Layer-3 forwarding in the hypervisor  LBaaS driver does work, though has some caveatsThings that aren’t as much fun  Configuring ports, BGP sessions, and stateful port groupsThis is a bit painful for me, only because I didn’t have time to write an Ansible module for it. This process would be easily automated, but if it isn’t, then it is fairly complicated. In the end I wrote a terrible bash script to do it for me, which made me feel bad.  Does have the extra requirements of Zookeeper and CassandraMany people dislike Zookeeper. I’m not sure why. I’m not a big fan of Java-based systems, but I can tell you that Zookeeper has been running fine for over a year with absolutely no issues. That said it’s recommended to run the network service database (NSDB) nodes, ie. Zookeeper and Cassandra, on baremetal servers that are separate from the rest of the infrastructure. Nodes with 32GB or more is fine. Certainly these clusters require some care and feeding.Then again, the network state has to go somewhere. Other solutions have to store it as well. Physical routers store network state too, just in config files or other local databases. Every SDN system has to store state, and usually that will be some kind of distributed database (perhaps they rolled their own).  Knowing that Midokura’s exit plan likely involves being bought outI would imagine they will eventually be bought by a large conglomerate like Cisco. One of my other favorite tools, Ansible, was recently purchased by RedHat. Sometimes the exit turns out well, sometimes not.Issues I encountered  Memory resource sizing for midolmanThe Midolman agent can use a fair amount of memory. The package comes with some example sizing configurations. It is definitely a good idea to use the correct configuration file for the particular service and size of server that is being deployed, then make sure Nova knows to hold back that memory on compute nodes. This is for the deployer to do.  Clean up CassandraEventually the Cassandra database can become large and needs to be cleaned up.  Not being able to keep up with releasesI believe Enterprise Midonet is in a 1.9 release now. It’s not easy to keep up, and doing so would be something to take into consideration. Like OpenStack itself, you will want to upgrade as frequently as you can, which in the end…is never enough. Again, this is a deployer issue not a Midonet issue. New code is a good thing and we need to get it deployed.Related things I have not (yet) worked with  VXLAN off-loading  VTEP integration  vhost-net driver (should increase performance)ConclusionI chose to deploy Midonet based on several assumptions and project requirements. In the end, Midonet works as advertised, and, I feel, makes my life as an OpenStack operator easier. I definitely recommend Midonet for OpenStack deployments, or anyone who is interested in software defined networking."
    } ,
  
    {
      "title"    : "Easy VPN or Proxy for Firefox with SSH",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/12/23/ssh-vpn-firefox.html",
      "date"     : "2015-12-23 00:00:00 -0500",
      "content"  : "I thought I would write a quick post on using OpenSSH as a simple vpn for use with Firefox. There are quite a few blog posts on how to do this already, but I think it’s so simple and easy that it’s worth repeating. It’s really easy to setup, and if you use Firefox profiles you can have a profile that is pre-configured to use with any ssh socks connection. I will often use Firefox over an SSH connection to a remote host when using wifi at airports, on planes, in coffee shops, etc.Also–I use it to gain access to internal web services remotely, as opposed to using some awful vpn system, one where the vpn server hasn’t been updated in X number of years. Who wants to maintain a complex vpn system when all one needs is a safe and secure ssh server? Not me. :)When using Firefox over an ssh vpn as discussed in this post, all your web and dns traffic from Firefox will be tunneled through the ssh socks proxy, and come out through the remote server. What’s more, the tunnel is encrypted with ssh.Of course, in order to use this you need a remote ssh server. But those are easy to come by, and you could easily startup a server in a public cloud, such as Digital Ocean. The instance could be quite small…512MB should be fine. $5 a month, start it up only when you know when you are going to need it, very inexpensive.Setup SSH SessionIt can be this easy:your-laptop$ ssh -D 127.0.0.1:8080 you@some.remote.hostsome.remote.host$ # now logged in to remote host, ssh proxy is setupYou can see that ssh is listening on localhost 8080.your-laptop$ lsof -n -P -i :8080 -sTCP:LISTENCOMMAND   PID   USER   FD   TYPE  DEVICE SIZE/OFF NODE NAMEssh     27005 curtis    4u  IPv4 1769809      0t0  TCP 127.0.0.1:8080 (LISTEN)You could also use the -N and -T switches to not execute a remote command and disable getting a tty.As long as that ssh session is up the proxy will be working.Configuring FirefoxIt’s straightforward to configure Firefox. To access the network settings in Firefox, go to Preferences -&gt; Advanced -&gt; Network.  Manual proxy  Socks host: 127.0.0.1  Port 8080 (or any you want, just has to be the same as you set in the ssh command)  Select “Socks V5”  Use remote DNS (which I prefer, but you don’t have to)Firefox profilesFirefox can have profiles. I have a specific profile setup to use with an ssh-based vpn on port 8080. So when I want to use Firefox over the ssh vpn I just open up Firefox using that profile. Unless you use Firefox with the ssh vpn all the time I prefer to use a profile so I don’t have to go into the settings every time and make changes. I believe there might be some plugins that make it easier to have proxies setup, turned off and on and such, but I haven’t used one so can’t make a recommendation.ConclusionWe all need to use wifi, especially when travelling. However, wifi networks can have security issues. I definitely recommend using a vpn when working remotely over unknown wifi, and one of the easiest ways to do that is with ssh."
    } ,
  
    {
      "title"    : "Docker Machine, OpenStack, and Neutron LBaaS",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/12/22/docker-machine-openstack-neutron-lbaas.html",
      "date"     : "2015-12-22 00:00:00 -0500",
      "content"  : "In this post I’ll setup a docker swarm using OpenStack as the IaaS provider and then boot some containers that are loadbalanced by a Neutron-based LBaaS.I was re-introduced to docker via this interesting post that discusses docker-machine and the binpack strategy, which I must look into more, as I’m intersting in the area of binpacking. However this post just deals with setting up a small docker-machine cluster and loadbalancing it.tl;drUsing an OpenStack cloud that has a loadbalancer-as-a-service (LBaaS) feature, I created a swarm of docker machines using…er docker-machine. I also created a private docker image repository server that the swarm machines use. Next up I added an index.php file to the php:5-apache image, ie. a new dockerfile with FROM php:5-apache, and pushed that image to the local private repository. Then I created three instances of that image and added the appropriate ports and IPs to the LBaaS pool. Finally I could curl the LBaaS public virtual IP (VIP), and would be “round-robinned” to the docker instances.curtis-laptop$ while true; do curl http:///; sleep 1; doneversion 2 51166cb9c64cversion 2 adc5bdca0cbaversion 2 3ee186d80986^C&lt;/code&gt;&lt;/pre&gt;As you can see in the above output, the index.php on the webservers outputs the docker instance's hostname, which is part of its ID. With three containers as members of the lb, we see three unique hostnames, which means the traffic to the VIP is getting loadbalanced across the three containers.## Mistakes I madeAs a note, this blog post came from a couple hours of me messing around with docker-machine for the first time. It's been a while since I've used docker, and I'm behind on the new systems and features that are available. This means take anything I do here with a grain of salt, because I don't necessarily know what I'm doing. Especially around the management of images.While working on this blog post I made several silly mistakes and false starts, such as:- Using --swarm-master for every swarm node I created (not just the first one)- Could not find a good webserver/php docker image, first one I tried was [broken](https://hub.docker.com/r/eboraas/apache-php/). Finally realized I could use the php:5-apache image as a base.- Spent quite a while figuring out how to run an insecure private repository.- If I restarted the swarm nodes the swarm-agent container on each node did not startup automatically. Due to this I received TLS errors, but in reality it's failing to connect to port 3376 for docker machine, and that port will not be up if the swarm-master container isn't running.- Have to specify port for insecure-registry option.- Made some mistakes with local and remote (?) repositories with Docker...still working on this.The rest of this post will detail the steps I followed to setup this particular infrastructure.## OpenStack rc fileI prefer to have an openstackrc file to source to use with docker-machine instead of setting up the command line switches.#!/bin/bashexport OS_USERNAME=export OS_PASSWORD=export OS_TENANT_NAME=export OS_AUTH_URL=export OS_REGION_NAME=# basic trusty image from ubuntu cloud imageexport OS_IMAGE_ID=export OS_FLAVOR_ID=# if you have VPC style openstack neutron to accessexport OS_NETWORK_ID=# the user for docker-machine to ssh in withexport OS_SSH_USER=ubuntu&lt;/code&gt;&lt;/pre&gt;Source that file in order to provide the environment variables to docker-machine.docker-machine$ source openstackrc## OpenStack instancesI created a \"docker-machine\" instance where I installed docker-machine and also setup a private repository on that node.Then using docker-machine and a proper OpenStack env file I created several swarm instances (more later on that). Don't create those instances yet, you just need a place to run docker-machine from and it's probably easiest to do that from an instance with the OpenStack tenant network.docker-machine$ os server list+--------------------------------------+-----------------------------+--------+---------------------------------+| ID                                   | Name                        | Status | Networks                        |+--------------------------------------+-----------------------------+--------+---------------------------------+| 164cac1c-1762-417a-8f0c-397e1965db51 | swarm-3                     | ACTIVE | test-network=10.0.33.30            || 62e0f05f-4818-427e-8be7-ae998304d1ec | swarm-2                     | ACTIVE | test-network=10.0.33.28            || 65aa5c10-590a-4a0b-a2a3-debb8b53d1ca | swarm-1                     | ACTIVE | test-network=10.0.33.27            || 0c60c7fe-2e19-4e39-bee2-2b09a79d9d17 | docker-machine              | ACTIVE | test-network=10.0.33.7, |+--------------------------------------+-----------------------------+--------+---------------------------------+&lt;/code&gt;&lt;/pre&gt;## Install docker machineInstall the docker-machine CLI on the docker-machine instance. Doesn't seem to be a package for docker-machine.docker-machine$ curl -L https://github.com/docker/machine/releases/download/v0.5.3/docker-machine_linux-amd64 &gt;/usr/local/bin/docker-machine &amp;&amp; \\&gt;     chmod +x /usr/local/bin/docker-machine  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed100   599    0   599    0     0   3760      0 --:--:-- --:--:-- --:--:--  3791100 14.1M  100 14.1M    0     0  18.5M      0 --:--:-- --:--:-- --:--:-- 35.8Mdocker-machine$ which docker-machine/usr/local/bin/docker-machinedocker-machine$ docker-machine --versiondocker-machine version 0.5.3, build 4d39a66## Bash completionI also setup the bash completion scripts for docker-machine.First I cloned the docker-machine repository.docker-machine$ git clone https://github.com/docker/machineCloning into 'machine'...remote: Counting objects: 14693, done.remote: Compressing objects: 100% (10/10), done.remote: Total 14693 (delta 3), reused 0 (delta 0), pack-reused 14683Receiving objects: 100% (14693/14693), 10.94 MiB | 11.63 MiB/s, done.Resolving deltas: 100% (7937/7937), done.Checking connectivity... done.Then I sourced the bash completion scripts in my bashrc file.source ~/machine/contrib/completion/bash/docker-machine.bashsource ~/machine/contrib/completion/bash/docker-machine-prompt.bashsource ~/machine/contrib/completion/bash/docker-machine-wrapper.bashPS1='[\\u@\\h \\W$(__docker_machine_ps1 \" [%s]\")]\\$ '## Install docker on docker-machine instance Just to be confusing I also installed docker on the docker-machine virtual machine. I added my user to the docker group and then re-logged in so that I could run docker commands without having to sudo to root.docker-machine$ docker -vDocker version 1.9.1, build a34a1d5I run the private docker image registry on the docker-machine host, not in the swarm. More on that later.docker-machine$ docker psCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES49a78c2588f7        registry:2          \"/bin/registry /etc/d\"   2 hours ago         Up About an hour    0.0.0.0:5000-&gt;5000/tcp   registry## Create a private docker image repositoryIt seems that if I want custom images on my docker swarm then they have to come from somewhere. I decided to setup a local registry so that each docker node in the swarm can obtain custom images from it.The [documentation](https://docs.docker.com/registry/) for creating your own internal, private repository is quite good and it's easy to do. Well, easy to create an \"insecure\" repository, ie. one that isn't properly protected by TLS and other measures. But, as usual, for the purposes of exploration, I created an insecure image repo.In this case the repo ends up on the IP of my docker-machine instance (not part of the swarm). Below is an example of an image in the 10.0.33.7:5000 repo.docker-machine$ docker images | grep 10.0.3310.0.33.7:5000/apache-php-5   latest              ef50e992d38e        2 hours ago         480.5 MB## Create a php:5-apache image that will echo hostnameFor the example, I want each host to print its hostname in the php page that apache gives out, so that I can see that each container is actually being used. That means putting a custom index.php file into the php:5-apache image, building it, and uploading it into the private repository.docker-machine$ cat Dockerfile FROM php:5-apacheCOPY index.php /var/www/html/index.phpdocker-machine$ cat index.php The process is (afaik):1. Build image2. Tag image3. Push to private repositoryExample:docker-machine$ docker build -t apache-php-5 .Sending build context to Docker daemon 4.096 kBStep 1 : FROM php:5-apache ---&gt; cb016a201e95Step 2 : COPY index.php /var/www/html/index.php ---&gt; 916c65313b03Removing intermediate container 609b66516729docker-machine$ docker tag -f apache-php-5 10.0.33.7:5000/apache-php-5docker-machine$ docker push 10.0.33.7:5000/apache-php-5The push refers to a repository [10.0.33.7:5000/apache-php-5] (len: 1)916c65313b03: Pushed cb016a201e95: Image already exists bf61c2e22863: Image already exists 1e2f9e5b3fb5: Image already exists e70a54e5b8fe: Image already exists 869ca7daafc8: Image already exists d5b98059a0c3: Image already exists 98990d4b1772: Image already exists f90b4bdd0fe0: Image already exists 8cdb621c15a4: Image already exists 08d3d3dae3d4: Image already exists 619689ca2bd1: Image already exists 44f9dadf58c0: Image already exists a53e1776c44e: Image already exists 9ee13ca3b908: Image already exists latest: digest: sha256:c5044670f7a8e325791bd818826d1f1b7f3fbc6f62ab839232fdb0dffd289efc size: 49053## Create the swarmNow using docker-machine we can create the swarm.docker-machine$ docker run swarm createUnable to find image 'swarm:latest' locallylatest: Pulling from library/swarmd681c900c6e3: Pull complete 188de6f24f3f: Pull complete 90b2ffb8d338: Pull complete 237af4efea94: Pull complete 3b3fc6f62107: Pull complete 7e6c9135b308: Pull complete 986340ab62f0: Pull complete a9975e2cc0a3: Pull complete Digest: sha256:c21fd414b0488637b1f05f13a59b032a3f9da5d818d31da1a4ca98a84c0c781bStatus: Downloaded newer image for swarm:lateste922e4c46e469b4eb29d96f7d1723f08Now that the swarm token is created we can boot some swarm instances. Start with the *swarm-master*. Note the \"--engine-insecure-registry 10.0.33.7:5000\" option. Also note that I'm using the token that the \"docker run swarm create\" command returned.docker-machine$ docker-machine create -d openstack --swarm --swarm-master --swarm-discovery token://$TOKEN --engine-insecure-registry 10.0.33.7:5000 swarm-1# this will take a few minutes...Now create two more swarm members.Second swarm member.docker-machine$ docker-machine create -d openstack --swarm --swarm-discovery token://$TOKEN --engine-insecure-registry 10.0.33.7:5000 swarm-23rd swarm member.docker-machine$ docker-machine create -d openstack --swarm --swarm-discovery token://$TOKEN --engine-insecure-registry 10.0.33.7:5000 swarm-3Eval the environment for the swarm, so that when we use the docker command we are connecting to the swarm not the local docker host.docker-machine$ eval $(docker-machine env --swarm swarm-1)docker-machine ~ [swarm-1]$Now we can connect to the swarm. :)docker-machine$ docker infoContainers: 7Images: 11Role: primaryStrategy: spreadFilters: health, port, dependency, affinity, constraintNodes: 3 swarm-1: 10.0.33.27:2376  └ Status: Healthy  └ Containers: 3  └ Reserved CPUs: 0 / 1  └ Reserved Memory: 0 B / 2.053 GiB  └ Labels: executiondriver=native-0.2, kernelversion=3.13.0-74-generic, operatingsystem=Ubuntu 14.04.3 LTS, provider=openstack, storagedriver=aufs swarm-2: 10.0.33.28:2376  └ Status: Healthy  └ Containers: 2  └ Reserved CPUs: 0 / 1  └ Reserved Memory: 0 B / 2.053 GiB  └ Labels: executiondriver=native-0.2, kernelversion=3.13.0-74-generic, operatingsystem=Ubuntu 14.04.3 LTS, provider=openstack, storagedriver=aufs swarm-3: 10.0.33.30:2376  └ Status: Healthy  └ Containers: 2  └ Reserved CPUs: 0 / 1  └ Reserved Memory: 0 B / 2.053 GiB  └ Labels: executiondriver=native-0.2, kernelversion=3.13.0-74-generic, operatingsystem=Ubuntu 14.04.3 LTS, provider=openstack, storagedriver=aufsCPUs: 3Total Memory: 6.158 GiBName: swarm-1## Start some containersWith the swarm setup and the apache-php-5 container image ready, containers can be started up.docker-machine ~ [swarm-1]$ docker run -p 80:80 -p 443:443 -d 10.0.33.7:5000/apache-php-5fd30a72b4c112374c70b1609f5ffe773ce7121f69da2005f066fb151a89f100bdocker-machine ~ [swarm-1]$ docker psCONTAINER ID        IMAGE                         COMMAND                CREATED             STATUS              PORTS                   NAMES51166cb9c64c        10.0.33.7:5000/apache-php-5   \"apache2-foreground\"   12 hours ago        Up 11 hours         10.0.33.27:80-&gt;80/tcp   swarm-1/stupefied_wrightadc5bdca0cba        10.0.33.7:5000/apache-php-5   \"apache2-foreground\"   12 hours ago        Up 12 hours         10.0.33.30:80-&gt;80/tcp   swarm-3/loving_jepsen3ee186d80986        10.0.33.7:5000/apache-php-5   \"apache2-foreground\"   12 hours ago        Up 12 hours         10.0.33.28:80-&gt;80/tcp   swarm-2/serene_boothSo we have three containers with port 80 on the swarm node forwarding to port 80 in the container(s).## Create OpenStack loadbalancerThe cloud I'm using has a loadbalancer feature. It's somewhat limited, in that it can only do TCP and round robin, but that's still useful, especially given the public VIP is handled at the OpenStack IaaS layer, and is not something I have to manage. Nor do I have to create any complicated methods to move a floating IP in case of the failure of a node.First the pool is created.docker-machine$ neutron lb-pool-create --lb-method ROUND_ROBIN --protocol TCP --name docker-lb --subnet-id 832fc99e-42ab-4234-8a5f-acacde56713fCreated a new pool:+------------------------+--------------------------------------+| Field                  | Value                                |+------------------------+--------------------------------------+| admin_state_up         | True                                 || description            |                                      || health_monitors        |                                      || health_monitors_status |                                      || id                     | e2e38662-a994-4e86-bc68-05bbe95b95ce || lb_method              | ROUND_ROBIN                          || members                |                                      || name                   | docker-lb                            || protocol               | TCP                                  || provider               |                                      || router_id              | 16ce36bd-94a2-4203-8e30-871134c47272 || status                 | ACTIVE                               || status_description     |                                      || subnet_id              | 832fc99e-42ab-4234-8a5f-acacde56713f || tenant_id              | 0b33fce4b64d4c288098344b3b443370     || vip_id                 |                                      |+------------------------+--------------------------------------+Next members are added to the pool.docker-machine$ neutron lb-member-create --address 10.0.33.27 --protocol-port 80 docker-lbCreated a new member:+--------------------+--------------------------------------+| Field              | Value                                |+--------------------+--------------------------------------+| address            | 10.0.33.27                           || admin_state_up     | True                                 || id                 | 6a187959-49ea-4570-bbab-27c9427fd030 || pool_id            | e2e38662-a994-4e86-bc68-05bbe95b95ce || protocol_port      | 80                                   || status             | ACTIVE                               || status_description |                                      || tenant_id          | 0b33fce4b64d4c288098344b3b443370     || weight             | 1                                    |+--------------------+--------------------------------------+docker-machine$ neutron lb-member-create --address 10.0.33.28 --protocol-port 80 docker-lbCreated a new member:+--------------------+--------------------------------------+| Field              | Value                                |+--------------------+--------------------------------------+| address            | 10.0.33.28                          || admin_state_up     | True                                 || id                 | 3ae848e0-8d96-4b9a-afd2-b07ead463f78 || pool_id            | e2e38662-a994-4e86-bc68-05bbe95b95ce || protocol_port      | 80                                   || status             | ACTIVE                               || status_description |                                      || tenant_id          | 0b33fce4b64d4c288098344b3b443370     || weight             | 1                                    |+--------------------+--------------------------------------+docker-machine$ neutron lb-member-create --address 10.0.33.30 --protocol-port 80 docker-lbCreated a new member:+--------------------+--------------------------------------+| Field              | Value                                |+--------------------+--------------------------------------+| address            | 10.0.33.30                           || admin_state_up     | True                                 || id                 | a2c6a6c7-d300-4e76-9110-0ca22d4e5b76 || pool_id            | e2e38662-a994-4e86-bc68-05bbe95b95ce || protocol_port      | 80                                   || status             | ACTIVE                               || status_description |                                      || tenant_id          | 0b33fce4b64d4c288098344b3b443370     || weight             | 1                                    |+--------------------+--------------------------------------+Now a virtual IP for the loadbalancer. The subnet ID in this case is a public external network, not a VPC-style tenant network.docker-machine$ neutron lb-vip-create --name docker-lbvip --protocol-port 80 --protocol TCP --subnet-id 832fc99e-42ab-4234-8a5f-acacde56713f docker-lbCreated a new vip:+---------------------+--------------------------------------+| Field               | Value                                |+---------------------+--------------------------------------+| address             | 10.0.33.19                           || admin_state_up      | True                                 || connection_limit    | -1                                   || description         |                                      || id                  | 61f5c2b1-1af9-4c0a-8e26-c7130ede58a8 || name                | docker-lbvip                         || pool_id             | e2e38662-a994-4e86-bc68-05bbe95b95ce || port_id             | ca71900c-10a1-4e22-8f48-0b2ce0d04ee2 || protocol            | TCP                                  || protocol_port       | 80                                   || session_persistence |                                      || status              | ACTIVE                               || status_description  |                                      || subnet_id           | 832fc99e-42ab-4234-8a5f-acacde56713f || tenant_id           | 0b33fce4b64d4c288098344b3b443370     |+---------------------+--------------------------------------+Create a monitor.docker-machine$ neutron lb-healthmonitor-create --delay 6 --type TCP --max-retries 3 --timeout 2Created a new health_monitor:+----------------+--------------------------------------+| Field          | Value                                |+----------------+--------------------------------------+| admin_state_up | True                                 || delay          | 6                                    || id             | c6b2ebbf-fa31-48f1-9861-96400ec2dcba || max_retries    | 3                                    || pools          |                                      || tenant_id      | 0b33fce4b64d4c288098344b3b443370     || timeout        | 2                                    || type           | TCP                                  |+----------------+--------------------------------------+Associate that monitor to the loadblancer pool.docker-machine$ neutron lb-healthmonitor-associate c6b2ebbf-fa31-48f1-9861-96400ec2dcba docker-lbAssociated health monitor c6b2ebbf-fa31-48f1-9861-96400ec2dcba## Curl away!Now we can curl the docker-lb VIP and hit each container...curtis@laptop ~  $ while true; do curl http:///; sleep 1; doneversion 2 51166cb9c64cversion 2 adc5bdca0cbaversion 2 3ee186d80986^C&lt;/code&gt;&lt;/pre&gt;Phewph. That was a lot of typing.## docker-machineI quite like docker-machine. It seems like a rather simple way to get multiple docker hosts up and running and working together. I could see this being very useful for organizations that want to keep things simple. That said, I haven't looked \"under the hood\" so to speak, with regards as to how it's working. I would imaging running it in production would be considerably more difficult. I haven't even touched on use of volumes, etc.## Future workOne thing to note is that I'm just running one container per host and using the host's external IP and port 80 as the endpoint for the loadbalancer. But what we'd really want to do is be able to have a container running on any port and have the lb round robin to that port, regardless of whether or not it's port 80 or some random port. That's a common thing to do. I did test it out quickly but using another port such as 8080 wasn't working with the lb. At some point a normal docker user would want to run many, many containers and have the lb use them. Something to figure out in the near future.Having said that, a strategy I might pursue regardless of how the lb works is to use the Neutron LBaaS as a layer 4 lb (which is what it is in this specific case) that brings the traffic onto layer 7 haproxy servers running in containers in the swarm which would further distribute traffic. That way the public VIP is highly available, and I can use the more powerful haproxy to do sophisticated loadbalancing at the application layer.Also--I'm reminded of how working with images with Docker is difficult. If I ever decide to use Docker in production I'd have to really work on getting the right image workflow. It also makes me wonder if anyone is working on a private Docker repository system as a service in OpenStack. That'd be interesting."
    } ,
  
    {
      "title"    : "DevOps is a Useful Label",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/12/05/devops-is-a-useful-label.html",
      "date"     : "2015-12-05 00:00:00 -0500",
      "content"  : "tl;drDefine DevOps on your own. It’s mostly about teamwork, not just automation. Ignore the haters–it’s a useful label.DevOps and IaaS make their way up northI live in Edmonton, Alberta, Canada. Because I’ve lived here so long I sometimes forget how isolated we can be. However, I don’t mean isolated in terms of where Edmonton physically is. Surprising to many, we have a great university, a large international airport (I can fly direct to Amsterdam, and used to be able to fly direct to New York City), lots of good restaurants, a great theatre festival, and all kinds of wonderful things–especially in the summer when we celebrate being warm. :)Instead what I mean is that it takes a long time for new technologies and related concepts to make their way up north. I’m talking about things like DevOps and using infrastructure as a service–be it AWS, other public clouds, and/or OpenStack. I might be in a bit of a filter bubble because based on my daily readings the technologies I just mentioned are so commonplace as to be standards. But not in Edmonton (yet). However, that is changing, and it seems to be changing right now.In the last few weeks I’ve been talking to many people and organizations in Edmonton that are interested in modernizing, so to speak. Even though few companies in Edmonton are using public/private clouds or implementing concepts like DevOps, there are more every day.DevOps - What is it?Many people I talk to don’t like the term. They think it’s a dumb fad. In fact I had to unfollow some people on twitter to get away from the constant negativity. I definitely think it’s a real, and useful, thing.When people talk to me about DevOps I usually try to qualify what I think it is with the following statements:1) It’s a useful label2) It has no real official definition, and in fact my be defined by the fact that it tries not to be defined3) In my opinion, it’s about creating high performing teams (which means empathy and communication, ie. teamwork)I should note that my perspective is one of deploying infrastructure, because that’s what I do for a living. Most organizations use DevOps to deploy their applications(s) to a public cloud like AWS. A SaaS company is a canonical example: no servers of their own, many daily deploys, devs and ops working together, etc.In my day to day work the application being deployed is OpenStack, and the code is Ansible playbooks. The infrastructure I maintain is defined in code or yaml as much as possible, and also goes through a continuous integration workflow. But I still have to deal with IPMI. :)If you want to see a good presentation on the most recent thinking on DevOps, checkout this talk by J. Paul Reed. I wasn’t sure that I was going to like the talk at the start, but in the end I agreed with much of what Reed said. It made me start to think about artifacts and aesthetics.It’s not just automationMost IT organizations want to increase their performance, but don’t know how. To management, automation sounds great because then they might be able to reduce staff (which is, of course, not true). In fact I find that automation requires more staff, not less. Many companies try to get by with the minimal people required to complete a project and they hope to bring more on once the project is profitable. Unfortunately that is backwards. Automation projects should have a large front-end investment in people. But I can understand how finances can get in the way of doing that.Automation projects also have high expectations. However, automation, like DevOps, is not a panacea. There’s a great set of slides titled _Infrastructure as Code Might Literally be Impossible that has many good points about how difficult things like programming (even bash) and operating system packages are. Basically a form of “how do computers even?” Once you abandon some abstractions it becomes clear that it is hard to determine what a complex system is doing. Repeatedly operating on that complex system in a predictable way is difficult.At the very least automation systems like Puppet, Chef, and Ansible manage configuration files, services, and packages. If, at a low level, we can’t trust how packages and package managers work, then how can we trust automation? Perhaps this is where abstraction fails us in computer systems engineering. I have been doing automation for quite some time and still find that it’s easy to do an OK job, but difficult to do a great job. Perhaps I’m not at the level I need to be yet, or I’m not choosing the right tools and workflows, or looking at it scientifically enough…I’m not sure. Regardless, writing a bunch of Puppet manifests isn’t going to create magic.The switch to DevOps - change is hardNot being able to define something important drives people crazy. Agile is nice because they have a convenient manifesto made up of a set of statements, or rules (which are often ignored). I’m sure that wasn’t the entire point of Agile, but regardless the document exists and people link to it.With DevOps organizations need to define it on their own. I have seen more organizations fail at implementing DevOps related practices than succeed. The transition is extremely difficult because it requires a different mindset, and in the end relies on teamwork, which most organizations don’t do at all, let alone well.A convenient labelDevOps is an extremely useful label. Just because it’s difficult to define doesn’t mean an organization can’t work on defining it internally (in fact that is what I think they should do). Just because it seems like a fad doesn’t mean it can’t help people to solve difficult problems and to create amazing teams that develop and operate complex systems and applications that are extremely profitable.On one hand, there is no such thing as a “DevOps” but on the other hand, there are many people and organizations who do implement multiple facets of the DevOps paradigm, enough to be considered as “doing DevOps,” certainly on their own terms. What’s more, I often find that people who are already doing DevOps-like things are the most irritated by the label, perhaps because it’s often misused and co-opted. I’m not clear on why people want to adopt DevOps strategies but reject the term.Finally, if trying to work with the tools, processes and people involved in DevOps makes my life easier, and putting it on a resume helps me to find a great job, then I’m going to do that. People are both literally and figuratively searching for DevOps. Whether or not some people dislike the term, it’s a useful label, and, I believe, a process for creating high performing teams."
    } ,
  
    {
      "title"    : "OpenStack Tokyo Summit",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/11/07/openstack-tokyo-summit.html",
      "date"     : "2015-11-07 00:00:00 -0500",
      "content"  : "I was lucky enough to be able to attend the OpenStack Tokyo Summit, which of course, is in Tokyo. Weirdly enough it is quite easy to get to Tokyo from Edmonton–all I had to do was the short flight from Edmonton to Vancouver, up and over the Rockies, and then from there take All Nippon Air (ANA) to Tokyo. I flew on on a nice, new 787 complete with USB connections. (Aside: ANA is the best airline I’ve flown on.)I think it was cheaper, just flight and hotel, to send me to Tokyo for a week than it was for the Palo Alto operators meetup for three days. Tokyo is not that expensive for a major city, and Palo Alto bloody well is (for a minor one).The SummitThe summit took place at the Grand Prince International Convention center. This was a collection of buildings, more like a campus, that the summit spread out over. I’ve never looked at the same map so many times. I missed the start of a couple of sessions because I got a bit lost. The Vancouver convention center was much easier to navigate, but it’s also much newer, so not a fair comparison.For the last few summits I’ve been to I spend most, if not all of my time in the design summit going to operator sessions. Which makes sense because I am a OpenStack operator. Surprisingly I am a real person, not some mythical beast. :) I have to wrangle this massive OpenStack conglomerate into a working, highly available system. It’s not easy, but it’s not impossible either.Here are some of the sessions and talks I went to:  Ops - Quotas and Billing (which I moderated as best I could)  Ops - How to work with the community on a small budget  Cross project workshop - Performance  Talk - Kuryr: Docker networking in an OpenStack World  Ops - Upgrades  Ops working session - Large Deployments Team  Talk - How to reboot the cloud (in the face of a Xen vulnerability)  Magnum - Networking past, present, and future (related to Kuryr)  Ansible collaboration day - Most sessions  Talk - Real world devops experience securing a cloud (this was not a great presentation unfortunately)  Security - How should security serve the community? (very few in attendance, mostly for security team members, which I am not)  Talk - CloudKittyAll of the sessions were great, if short and either really busy or not busy at all. There were some sessions where it was so busy I couldn’t even get into the room and had to go watch something else. Basically I learned my lesson on getting there early. A couple of the Ansible collaboration day sessions and the informal operators meetup on the Friday were both so busy I couldn’t get in or get a seat because I showed up a bit late.Primarily it seemed that at this summit I was interested in billing and networking. I am not a networking expert, but I certainly have to know a lot about it, especially with Neutron. I almost want to run a few clouds to try out different networking models, like Midokura, DragonFlow, and Calico. One thing I noticed is how there are fewer security sessions now. OpenStack security has moved to primarily supplying tools like Bandit and security notes, and not much more than that.I can definitely say that Ansible’s popularity has expanded considerably compared to the last summit, which was only six months ago. Here’s hoping the Redhat purchase goes well. OpenStack infra uses Ansible as well and the new Shade library is an important piece of the puzzle. The OpenStack Ansible project (OSAD) also continues to move along, almost too fast for me to keep up. Well, not almost–I can’t keep up with the improvements just due to lack of time. If you want to learn about how to use Ansible that is the best project to look at in terms of actually writing playbooks and modules and callbacks.Billing is also interesting because it’s heavily tied into metrics, ie. Ceilometer. CloudKitty is fascinating and I’m sure I will be implementing it soon. Perhaps that is something we can contribute back to.Regional CloudsProbably the best session I had was at the LDT meeting where we talked a bit about public clouds, specifically what I call “regional” public clouds, which are typically clouds meant for use in a particular country or geographical area, eg. Canada or New Zealand or Europe. Many people don’t know this, but, for example, in Canada there are many companies that want to use IaaS but don’t want their data in the US. It’s like some kind of data-protectionism and it happens in many countries, which leads to regional clouds.These clouds are usually smaller than other clouds, but have to be more featureful–or “complete” was another word that was used. I think it’s harder to run a smaller cloud with more features, as opposed to a larger cloud that has less features, especially around the networking component.Weirdly it’s somewhat lonely working in a regional cloud. Typically we (as in regional clouds) don’t have a large staff, nor a large budget to send people to meetups and summits. Further we don’t have the resources to contribute back much because we’re so busy (like all operators generally speaking), and, what’s more, because of the smaller scale we don’t typically run into common larger issues like running cells or applying custom patches to run a simpler network setup. Instead we run a lot of features such as LBaaS and have to do real billing. Customers want features like backups and DNS and containers and VPNaaS, etc, etc. Larger OpenStack public clouds typically don’t provide those services, but we have to. Oh, and AWS compatibility, that’s a big one too. Big clouds do less but have more vms running, regional clouds do more but have less vms.I’m hoping that various regional clouds can start working together in some fashion, even if it only means sharing operational aspects, or perhaps even doing a exchange. I’m not sure if the OpenStack foundation is interested in helping regional clouds, but we’ll do our best anyways.Japan/TokyoI stayed a few days longer in Tokyo. I didn’t leave the city except for a short trip out to Hakone (which kind of felt like Japan’s Jasper). I visited most of the museums, including the incredible 500 Arhats exhibit at the Mori. I also visited all the areas such as Akihabara and “Kitchen Town.” Mostly I rode the subway a lot and then caught an awful cold. The public transport system is massive, but by the end I had it kinda figured out. I think I visited every vegetarian restaurant in Tokyo. For a city of 13 million plus, there is not much for vegetarian options. Edmonton probably has twice the vegetarian restaurants that Tokyo does. Strange.My trip was a bit up and down–at first I was like “Wow! Look at all these people!” then after a while it became overwhelming. I spent a lot of mental energy just walking around trying not to bump into people and remembering to walk on the left side. But then after a couple more days I came back around to enjoying it. I had one poor experience, which I won’t mention other than to say I had one poor experience.My favorite morning activity was to grab a coffee at the Starbucks in the train station (yeah, Starbucks, but the location was perfect for people watching) and sit and watch all the commuters on their way to work. The number of men wearing a black suit and a white shirt with no tie was staggering. I tried counting for a while but after ten or twenty seconds I couldn’t keep up. Literally thousands would pass in an hour. Sometimes the cadence of their footsteps would all fall in line and echo throughout the station. Very surreal.Finally it was time to come home. In the end I packed four bottles of (what I hope is) premium sake–which the nice Canadian border guard let me in without having to pay duty on! She sure asked me a lot of questions but then didn’t care about the sake.Overall I had a great first trip to Japan, and now that I’ve been there once I think I have a good idea of what I’d like to do the next time I go. I found the flight to Tokyo to be fairly easy, but the jet lag was a killer. I’m still not sleeping 100% correctly. But only having one stop in Vancouver is convenient.I’m looking forward to the next operators meetup which should be relatively soon, and also, I’m guessing, may not be in North America."
    } ,
  
    {
      "title"    : "Thoughts on Redhat buying Ansible",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/10/18/redhat-buys-ansible.html",
      "date"     : "2015-10-18 00:00:00 -0400",
      "content"  : "Recently Redhat bought Ansible. Most people went “yay!”, but I, as the curmundgeonly sysadmin, said “that sucks…for me.”The “exit”People seem happy that Redhat bought Ansible–there were many congratulatory tweets. It appeared that, at least on the surface, most thought it was a good exit for Ansible. $150 million. Pay back those VCs, and slip back into the comforting arms of old softy Redhat.However, I don’t think it’s good for me, and I think that’s an Ok position to take–as a long time user I think I should get some kind of pass. The company I work for also purchased Ansible Tower so we weren’t just using it for free either. Also I think it’s the only open source project that I can actually say I contributed some code to (even if that code was only used by myself).There are different views on how companies exit. I think most people would consider me contrarian. To that point, I don’t know if companies should exit. What does that even mean? In startup terms, I think exit usually means either go IPO, or get bought out–both so that you can repay the venture capitalists. However, those are not the only options for a company. In fact, despite not being an economist by any stretch of the imagination, I feel like there is a lot of value in our economies that is made up of small or medium sized businesses–companies that aren’t part of some conglomerate “ecosystem,” ie. sales machine.Puppet and Chef are still individual organizations. However, it seems like at some point they too will be bought up by a larger conglomerate like, I guess, EMC/Dell, uh, IBM?, Cisco–companies that many people think are doomed. Or I suppose their real goal is to IPO. Ugh.  HP. CISCO. DELL. EMC. IBM. Oracle. Think of them as the walking dead. Oh, sure, they’ll shuffle along for some time. They’ll sell some stuff. They’ll make some money. They’ll command some headlines. They may even do some new things. But as tech giants, they’re dead. (wired)Does Redhat fit into that list as well? If not, why not? Is Redhat a niche company or is it the open-source EMC? Or is it something completely different? (Unlikely.)As of this moment, Redhat has a market cap of about 14 billion. Their PE is about 74. I think they are over-valued, otherwise I’d actually own their stock. This article talks a bit about the problems with open source companies. Redhat is a one-off situation. I don’t know if we’ll see something like Redhat again for some time. For whatever reason, the business/finance world is much different than the innovation world. So we end up with companies like EMC and Redhat kind of fuddling along, extracting boring cash for newish systems from enterprise companies, who for the most part are moving to AWS, or other public clouds. (Great, now I’ve confused myself.)Redhat acquisitionsThere’s a nice list of acquisitions made by Redhat here. Their model seems to be to pick up small companies that produce valuable open source systems, and to do so for about 100 million. In that respect I don’t see them picking up anything else I use, other than perhaps haproxy. MariaDB would probably be too expensive for them. OpenStack is run by a foundation so you can’t really buy it, though you can buy expertise, just like Redhat did with eNovance. Codership, who I believe makes the galera component of MySQL/MariaDB (I may be wrong) might be something good to look into.Examples:  JBoss - Which they bought for $420 million in 2006 (I’ve never used it)  KVM  Gluster  Ceph/Inktank  eNovance (OpenStack consultants)  AnsibleKVM is probably the most important piece of software they’ve “picked up” and I have to admit it seems to be doing fine. It’s what I use with Ubuntu. Gluster and Ceph also seem to be doing fine too. eNovance I only added because it is OpenStack related. So just based on that list perhaps I shouldn’t be too concerned.Configuration management toolsI’ve used Chef and Puppet enough to know that I don’t enjoy using them. I have preferred Ansible and run a large OpenStack system exclusively with it. However, it certainly has limitations, as does every other configuration management (or automation? or orchestration?) system. I’m not super happy with any of them, though I definitely feel like Day 1 (ie. the installation) is easy, but what about day 40? Or day 200? Ansible seems a better fit for actually operating a system over time. For example, I’m not clear on how one would migrate a bunch of databases from one cluster to another using Puppet, but then again I haven’t looked too hard. Puppet seems like the Java of configuration management. But I digress…Inability to competeAs a company, Ansible is quite small. I believe about 50 people. I’m sure Puppet has more than 300. (Aside: I was in the Puppet office once. It’s the nicest office I have ever been in. Full kitchen. Beer. Presentation space. It was way too nice. It was spooky.) Presumably Chef is similar in size and funding to Puppet. They are bigger than Ansible and better funded (to what end, I don’t know). They have sales teams with white boards and quotas; perhaps dreams of an IPO.Funding (pulled from Crunchbase:)  Chef - $137 million in 6 rounds  Puppet - $85 million in 5 rounds  Ansible - $6 million in 1 round, then bought for about $150 million  SaltStack - $685K in a seed round (probably a good deal–someone buy them)So Puppet and Chef are big. Puppet is probably working on another round of funding. They are both putting out features like nobody’s business, with Puppet releasing some kind of orchestration engine and Chef with a continuous delivery system.Basically, I’m sure Ansible felt much too small and underfunded to “take on” Puppet and Chef. Or perhaps they didn’t feel like they had the staff to keep up with all the changes coming into Ansible from the public, ie. they couldn’t merge all the pull requests in a timely fashion. Either way, too small. Not enough cash to burn on hiring for the short term as Ansible matures as a CM. Which is too bad, because I think they should have found a way to stay small, but that may not be possible with an open source license.The future of CMMy feeling is that CM products aren’t doing a great job. That we need something different, and that it might be at the OS layer. But we’re so entrenched with the OS that it will take a long, long time before anything changes there. Perhaps AWS’ Lambda is closest to something really new, something really different. Even though I’ve been managing servers for over a decade, I don’t think all these servers is a good thing–it seems like a waste of resources, not only to run them but also to manage them with a CM. Sure, we have all these new container scheduler systems, but I don’t think that’s the endgame. Containers are great but virtual machines are only 2% overhead anyways, so where’s the gain? Boot up time? Who knows.In the endI do think Redhat managed to obtain a great company for a reasonable amount of money (considerably less than what VCs will be trying to extract from Puppet/Chef). So it will be good for Redhat. However, I don’t think it will be good for me, as a long time Ansible user. I’m sure Redhat will put more resources (especially sales, lol) towards Ansible, but I imagine a lot of those resources will be going towards integrating Ansible into the Redhat stable of products, and not necessarily towards making Ansible better for people like myself, who, right now anyways, don’t use any Redhat products.Further more, if Redhat is such an unusual company, and with AWS absorbing enterprise IT like a black hole, and huge sales organizations like EMC failing, soon Redhat itself will crumble. Which is scary.Finally, one has to wonder how open source makes money. I need to do some research into smaller, private companies that provide open source software. I have a feeling there are quite a few software-as-a-service companies that are doing Ok, but otherwise…it’s complicated. Just based on my current investigation, I don’t think I would start a company that completely open sources its product, nor would I aim for exiting to pay back venture capitalists. Frankly I don’t see the issue with small, non-public companies, and perhaps I’m projecting that onto Ansible. Hashicorp will be one to watch, though they may be an aberration.Anyways, looking forward to Cloudforms (whatever that is) integrating with Ansible, and also looking forward to exploring other methods of system automation."
    } ,
  
    {
      "title"    : "Mariadb Galera 5.5.42 Crash",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/10/17/galera-crash.html",
      "date"     : "2015-10-17 00:00:00 -0400",
      "content"  : "Recently I have been working on setting up a “global” MariaDB 5.5.42 Galera cluster, which means a cluster that has to sync up across a wide area network (WAN), one that is several thousand kilometers long. Normally I’d try to avoid doing this kind of thing, but in this particular situation, running a multi-region OpenStack deployment, I don’t have much choice.Unfortunately during my initial test, yes the very first test, the cluster crashed. It didn’t partition due to a network issue or anything obvious like that, it just simply hard crashed.In this situation I had 3 regions:Region 1) 3 initial nodes in a clusterRegion 2) 1 galera arbitratorRegion 3) 1 new nodeI added the new node and after a couple of hours, the entire cluster had crashed. The arbitrator and the new node were still running, but the databases were inaccessible. The 3 nodes in region #1 had all crashed with the below message. I had planned on adding the other 2 nodes in region #3 later, but didn’t have the chance before it crashed.ubuntu@node2:/var/log/mysql$ tail -100 galera_server_error.log     eb91a487,1} joined {} left {} partitioned {})151014 22:50:09 [Note] WSREP: save pc into disk151014 22:50:09 [Note] WSREP: New COMPONENT: primary = yes, bootstrap = no, my_idx = 2, memb_num = 5151014 22:50:09 [Note] WSREP: STATE EXCHANGE: Waiting for state UUID.151014 22:50:09 [Note] WSREP: STATE EXCHANGE: sent state msg: 1afbdabb-728b-11e5-a0f7-7b10174ed580151014 22:50:09 [Note] WSREP: STATE EXCHANGE: got state msg: 1afbdabb-728b-11e5-a0f7-7b10174ed580 from 0 (garb)151014 22:50:09 [Note] WSREP: STATE EXCHANGE: got state msg: 1afbdabb-728b-11e5-a0f7-7b10174ed580 from 1 (node3)151014 22:50:09 [Note] WSREP: STATE EXCHANGE: got state msg: 1afbdabb-728b-11e5-a0f7-7b10174ed580 from 2 (node2)151014 22:50:09 [Note] WSREP: STATE EXCHANGE: got state msg: 1afbdabb-728b-11e5-a0f7-7b10174ed580 from 3 (node5)151014 22:50:10 [Note] WSREP: STATE EXCHANGE: got state msg: 1afbdabb-728b-11e5-a0f7-7b10174ed580 from 4 (node1-zone-1)151014 22:50:10 [Note] WSREP: Quorum results:    version    = 3,    component  = PRIMARY,    conf_id    = 20,    members    = 4/5 (joined/total),    act_id     = 134798064,    last_appl. = 134797950,    protocols  = 0/7/3 (gcs/repl/appl),    group UUID = 16e6742c-d7b9-11e4-8f72-af6e67877366151014 22:50:10 [Note] WSREP: Flow-control interval: [36, 36]151014 22:50:10 [Note] WSREP: New cluster view: global state: 16e6742c-d7b9-11e4-8f72-af6e67877366:134798064, view# 21: Primary, number of nodes: 5, my index: 2, protocol version 3151014 22:50:10 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.151014 22:50:10 [Note] WSREP: REPL Protocols: 7 (3, 2)151014 22:50:10 [Note] WSREP: Service thread queue flushed.151014 22:50:10 [Note] WSREP: Assign initial position for certification: 134798064, protocol version: 3151014 22:50:10 [Note] WSREP: Service thread queue flushed.151014 22:50:10 [Note] WSREP: Member 4.1 (node1-zone-1) requested state transfer from '*any*'. Selected 3.0 (node5)(SYNCED) as donor.151014 22:50:12 [Note] WSREP: (acc7d568, 'tcp://0.0.0.0:4567') turning message relay requesting off151014 22:50:15 [Note] WSREP: 3.0 (node5): State transfer to 4.1 (node1-zone-1) complete.151014 22:50:15 [Note] WSREP: Member 3.0 (node5) synced with group.151014 22:50:29 [Note] WSREP: 4.1 (node1-zone-1): State transfer from 3.0 (node5) complete.151014 22:50:29 [Note] WSREP: Member 4.1 (node1-zone-1) synced with group.pure virtual method calledterminate called without an active exception151015  0:48:27 [ERROR] mysqld got signal 6 ;This could be because you hit a bug. It is also possible that this binaryor one of the libraries it was linked against is corrupt, improperly built,or misconfigured. This error can also be caused by malfunctioning hardware.To report this bug, see http://kb.askmonty.org/en/reporting-bugsWe will try our best to scrape up some info that will hopefully helpdiagnose the problem, but since we have already crashed, something is definitely wrong and this may fail.Server version: 5.5.42-MariaDB-1~trusty-wsrep-logkey_buffer_size=134217728read_buffer_size=131072max_used_connections=551max_threads=2002thread_count=15It is possible that mysqld could use up to key_buffer_size + (read_buffer_size + sort_buffer_size)*max_threads = 4523910 K  bytes of memoryHope that's ok; if not, decrease some variables in the equation.Thread pointer: 0x0x7f1406812000Attempting backtrace. You can use the following information to find outwhere mysqld died. If you see no messages after this, something wentterribly wrong...stack_bottom = 0x7f144f95ea00 thread_stack 0x48000/usr/sbin/mysqld(my_print_stacktrace+0x2e)[0x7f14502bfa0e]/usr/sbin/mysqld(handle_fatal_signal+0x457)[0x7f144fea6f57]/lib/x86_64-linux-gnu/libpthread.so.0(+0x10340)[0x7f144e8fa340]/lib/x86_64-linux-gnu/libc.so.6(gsignal+0x39)[0x7f144df51cc9]/lib/x86_64-linux-gnu/libc.so.6(abort+0x148)[0x7f144df550d8]/usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x155)[0x7f144e6466b5]/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x5e836)[0x7f144e644836]/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x5e863)[0x7f144e644863]/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x5f33f)[0x7f144e64533f]/usr/sbin/mysqld(_Z14wsrep_apply_cbPvPKvmjPK14wsrep_trx_meta+0x7c7)[0x7f144fe57c37]/usr/lib/galera/libgalera_smm.so(_ZNK6galera9TrxHandle5applyEPvPF15wsrep_cb_statusS1_PKvmjPK14wsrep_trx_metaERS6_+0xd8)[0x7f144bb188f8]/usr/lib/galera/libgalera_smm.so(+0x1df27d)[0x7f144bb4f27d]/usr/lib/galera/libgalera_smm.so(_ZN6galera13ReplicatorSMM9apply_trxEPvPNS_9TrxHandleE+0xd2)[0x7f144bb51b32]/usr/lib/galera/libgalera_smm.so(_ZN6galera13ReplicatorSMM11process_trxEPvPNS_9TrxHandleE+0x10e)[0x7f144bb5498e]/usr/lib/galera/libgalera_smm.so(_ZN6galera15GcsActionSource8dispatchEPvRK10gcs_actionRb+0x1b8)[0x7f144bb33668]/usr/lib/galera/libgalera_smm.so(_ZN6galera15GcsActionSource7processEPvRb+0x58)[0x7f144bb33ef8]/usr/lib/galera/libgalera_smm.so(_ZN6galera13ReplicatorSMM10async_recvEPv+0x73)[0x7f144bb54ef3]/usr/lib/galera/libgalera_smm.so(galera_recv+0x18)[0x7f144bb634e8]/usr/sbin/mysqld(+0x49c5a1)[0x7f144fe585a1]/usr/sbin/mysqld(start_wsrep_THD+0x514)[0x7f144fcb3cc4]/lib/x86_64-linux-gnu/libpthread.so.0(+0x8182)[0x7f144e8f2182]/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7f144e01547d]Trying to get some variables.Some pointers may be invalid and cause the dump to abort.Query (0x0): is an invalid pointerConnection ID (thread ID): 2Status: NOT_KILLEDOptimizer switch: index_merge=on,index_merge_union=on,index_merge_sort_union=on,index_merge_intersection=on,index_merge_sort_intersection=off,engine_condition_pushdown=off,index_condition_pushdown=on,derived_merge=on,derived_with_keys=on,firstmatch=on,loosescan=on,materialization=on,in_to_exists=on,semijoin=on,partial_match_rowid_merge=on,partial_match_table_scan=on,subquery_cache=on,mrr=off,mrr_cost_based=off,mrr_sort_keys=off,outer_join_with_cache=on,semijoin_with_cache=on,join_cache_incremental=on,join_cache_hashed=on,join_cache_bka=on,optimize_join_buffer_size=off,table_elimination=on,extended_keys=offThe manual page at http://dev.mysql.com/doc/mysql/en/crashing.html containsinformation that should help you find out what is causing the crash.151015 00:48:27 mysqld_safe Number of processes running now: 0151015 00:48:27 mysqld_safe WSREP: not restarting wsrep node automatically151015 00:48:27 mysqld_safe mysqld from pid file /var/lib/mysql/node2.pid endedSimilar errorsI found this error in the Galera bug tracker. The submitter notes:  One node in my 5-node cluster suddenly failed today with the following backtrace. I haven’t seen a node go down abruptly like this before, and don’t know how to reproduce.Interesting that they also experienced the bug with a 5 node cluster. That bug leads to this Codership github issue.Minor versionsOne thing I noticed is that I had 5.5.45 in the new node, not 5.5.42 like all the other nodes. Presumably that is not a good thing to do. So I setup my automation to ensure I’m pulling from the right archive because the standard MariaDB mirrors only include the latest version.You can find archived versions of MariaDB software here.While I can’t be sure the version difference was the actual issue, as I haven’t had time to try to replicate this bug, I’m sure it didn’t help.Even small version increments can be problematic. It’s not good enough to install mariadb-galera-server-5.5, I have to ensure the minor version number is the same as well. This has me looking at all the other major software packages I’m running and I know I’ll have to setup my automation to include minor versions.Current statusNow that all the nodes have the same version of MariaDB Galera Server and I have expanded the nodes from 5 to the full 7 (including the arbitrator) things seem to be running ok.MariaDB [(none)]&gt; show status like 'wsrep_cluster%';+--------------------------+--------------------------------------+| Variable_name            | Value                                |+--------------------------+--------------------------------------+| wsrep_cluster_conf_id    | 17                                   || wsrep_cluster_size       | 7                                    || wsrep_cluster_state_uuid | 16e6742c-d7b9-11e4-8f72-af6e67877366 || wsrep_cluster_status     | Primary                              |+--------------------------+--------------------------------------+4 rows in set (0.01 sec)&lt;/pre&gt;## 5.5.46 is outI would like to eventually get to a newer version of MariaDB. I know that 10.0 is out, it'd be great to get there soon. I am considering going to 5.5.46 if I have any other issues with the cluster in testing."
    } ,
  
    {
      "title"    : "First look at ZeroTier",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/10/03/zerotier.html",
      "date"     : "2015-10-03 00:00:00 -0400",
      "content"  : "ZeroTier is:  …an imaginary Ethernet switch or a WiFi network with unlimited range. Actual traffic is carried by your local LAN or the Internet, but the network virtualization engine takes care of encrypting, authenticating, and routing it.It’s (I think anyways) an ad-hoc virtual private network, and apparently it uses its own VXLAN-like encapsulation and includes encryption. The two use cases they espouse on their website are 1) hybrid cloud connections and 2) peer to peer VPNs.I decided to take a look into what this is, or at least try it out, because I have recently been doing some work with OpenVPN and other VPN solutions, of which there are few, and none of them make me very happy. This zerotier essentially sounds like a simple to use VPN with a “joiner” concept.InstallI tested this out on two Ubuntu Trusty boxes. I downloaded the deb from zerotier, installed it and started the service.zclient1:~$ wget https://download.zerotier.com/dist/zerotier-one_1.0.5_amd64.deb--2015-10-03 14:47:15--  https://download.zerotier.com/dist/zerotier-one_1.0.5_amd64.debResolving download.zerotier.com (download.zerotier.com)... 104.207.155.202, 2001:19f0:6000:91c7:5400:ff:fe10:2421Connecting to download.zerotier.com (download.zerotier.com)|104.207.155.202|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 326330 (319K) [application/octet-stream]Saving to: ‘zerotier-one_1.0.5_amd64.deb’100%[=====================================================================================================================&gt;] 326,330     1.96MB/s   in 0.2s   2015-10-03 14:47:15 (1.96 MB/s) - ‘zerotier-one_1.0.5_amd64.deb’ saved [326330/326330]zclient1:~$ sudo dpkg -i zerotier-one_1.0.5_amd64.deb sudo: unable to resolve host openvpn1Selecting previously unselected package zerotier-one.(Reading database ... 51354 files and directories currently installed.)Preparing to unpack zerotier-one_1.0.5_amd64.deb ...Unpacking zerotier-one (1.0.5) ...Setting up zerotier-one (1.0.5) ...*** ZeroTier One install/update ***Getting version of existing install... NONEExtracting files...tmp/tmp/systemd_zerotier-one.servicetmp/init.d_zerotier-onevar/var/lib/var/lib/zerotier-one/var/lib/zerotier-one/zerotier-onevar/lib/zerotier-one/ui/var/lib/zerotier-one/ui/main.jsvar/lib/zerotier-one/ui/ZeroTierNetwork.jsxvar/lib/zerotier-one/ui/ztui.min.jsvar/lib/zerotier-one/ui/index.htmlvar/lib/zerotier-one/ui/ZeroTierNode.jsxvar/lib/zerotier-one/ui/react.min.jsvar/lib/zerotier-one/ui/simpleajax.min.jsvar/lib/zerotier-one/ui/zerotier.cssvar/lib/zerotier-one/uninstall.shGetting version of new install... 1.0.5Creating symlinks...Installing zerotier-one service...Done! Installed and service configured to start at system boot.To start now or restart the service if it's already running:  sudo service zerotier-one restartNow to start it up…zclient1# service zerotier-one startStarting ZeroTier One...&lt;/code&gt;Nice, that was super easy.Then I joined the “earth” network, a test network provided by zerotier.zclient2# zerotier-cli join e5cd7a9e1ce71cff 200 join OKzclient2# ip ad shSNIP!6: zt0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 2800 qdisc pfifo_fast state UNKNOWN group default qlen 500    link/ether 02:63:6c:ab:02:48 brd ff:ff:ff:ff:ff:ff    inet 28.183.225.45/7 brd 29.255.255.255 scope global zt0       valid_lft forever preferred_lft forever    inet6 fd80:56c2:e21c:0:199:9363:6cb7:e08a/88 scope global        valid_lft forever preferred_lft forever    inet6 fe80::63:6cff:feab:248/64 scope link        valid_lft forever preferred_lft foreverSNIP!After doing so I have a zt0 network interface with an IP from the 28.0.0.0/7 network.zclient2# zerotier-cli listnetworks200 listnetworks       200 listnetworks 8056c2e21c000001 earth.zerotier.net 02:63:6c:ab:02:48 OK PUBLIC zt0 28.183.225.45/7,fd80:56c2:e21c:0000:0199:9363:6cb7:e08a/88&lt;/code&gt;&lt;/pre&gt;## Creating your own private networkUnfortunately it seems that you have to create an account, login to the zerotier web interface (ie. it's a saas service), and create private networks from there. There are a few options, and the interface is a litle unpolished but it's straight forward enough. Once you've created a private network (which is default, and can only have a max of 10 devices or you are required to pay $4 a month which is fine) you can have systems/devices join that private network. One of the choices you'll have to make is what network IP space to give it, or to manage it yourself. By default zerotier will manage who gets what IP.zclient1# zerotier-cli join 200 join OK&lt;/code&gt;&lt;/pre&gt;And then in the interface set the device to be allowed and it'll get an IP.zclient1# zerotier-cli listnetworks200 listnetworks       200 listnetworks  testnet fe:42:1f:5d:f2:7d OK PRIVATE zt0 10.147.17.23/24&lt;/code&gt;&lt;/pre&gt;## Quick performance testI ran a quick iperf test against the two nodes. So first I should note that I'm running these tests on virtual machines inside an OpenStack system and the network between them is also \"virtualized\", ie. is based on a VXLAN overlay, which is then running on VLANS underneath. So there is a lot of encapsulation going on here. :) Performance over the zerotier VPN:zclient2# iperf -c 10.147.17.23------------------------------------------------------------Client connecting to 10.147.17.23, TCP port 5001TCP window size: 85.0 KByte (default)------------------------------------------------------------[  3] local 10.147.17.61 port 59802 connected with 10.147.17.23 port 5001[ ID] Interval       Transfer     Bandwidth[  3]  0.0-10.0 sec   509 MBytes   426 Mbits/secPerformance over the regular network:zclient2# iperf -c 192.168.44.35------------------------------------------------------------Client connecting to 192.168.44.35, TCP port 5001TCP window size: 45.0 KByte (default)------------------------------------------------------------[  3] local 192.168.44.2 port 39575 connected with 192.168.44.35 port 5001[ ID] Interval       Transfer     Bandwidth[  3]  0.0-10.0 sec  1.06 GBytes   911 Mbits/secSo you can see we get almost wirespeed without zerotier and then about half that with zerotier. So there is a performance hit. And, as I mentioned, the \"normal network\" is based on VXLAN, so it's already encapsulated. Perhaps the performance hit comes from the encryption. That is most likely. I haven't looked into the encryption provided by zerotier at all yet.## ConclusionI'm a big fan of software defined networking, whatever it actually means. I am not a big fan of the current state of virtual private networking. It seems OpenVPN is the best we have right now, and it's quite complicated and unwieldy. I think it's fairly obvious that one of the best things about SDN is going to be easier creation of secure virtual private networks, and I think they will either look something like zerotier, or even easier.While I like how easy zerotier is, I'm not sure I like that it is essentially software as a service (saas), which means centralized. They say their system hasn't gone down, but it will. Then again they mention that existing devices will be able to communicate if their central system is down, but new devices can't be added and new networks can't be created. However, I'm a proponent of the public cloud (at least IaaS) so it'd be hypocritical of me to say that a centralized \"cloud\" network controller is not a good idea when I think using compute and storage in a public cloud is. That said, I'm not sure what performance issues there might be in having a cloud based controller. I would imagine that zerotier will eventually provide a controller that can be installed on premise. That'd be interesting, and if they made encryption optional then maybe performance would come up. Then again, it would be interesting to encrypt all network communication on premise too.I quite like the idea of servers joining networks. Recently I have been installing a lot of Linux boxes, sure, using an automated process based on Cobbler, but I find the networking component to be extremely time consuming and frustrating, even just in terms of setting up templates. I'd really like to have the networks on servers setup by pulling from some kind of data center level source of truth (CMDB?) and just automatically being allowed to join specific networks based on their function. Can I hack that kind of process up, yes, but I don't want to do that work myself all the time. I want to just \"happen\" based on some tags pulled from the SST."
    } ,
  
    {
      "title"    : "MariaDB MaxScale Read and Write Splitting",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/09/29/mariadb-maxscale.html",
      "date"     : "2015-09-29 00:00:00 -0400",
      "content"  : "(the image is supposed to be like that ;P )Maxscale is a new open source product from MariaDB. It’s a MySQL/MariaDB proxy and loadbalancer, and, what’s most interesting to me, is that it can send write queries to one particular node of a cluster and reads to other nodes, thereby avoiding some nastiness in terms of writing to multiple nodes. There are some caveats to writing to multiple masters and not every application can.  MariaDB MaxScale is an open-source, database-centric proxy that works with MariaDB Enterprise, MariaDB Enterprise Cluster, MariaDB 5.5, MariaDB 10 and Oracle MySQL®. It’s pluggable architecture is designed to increase flexibility and aid customization.Read Write SplitIt’s fairly common to want to try to split reads and writes up using some kind of proxy system. I’m not up to date on the history of the requirement, but it has been around for a long time. There are, I believe, a few systems that try to do the RW split, but I’m not sure anything has really succeeded.As an OpenStack operator I’m keenly aware that I need a highly available MySQL/MariaDB Galera cluster, but also that I can only, as far as I know at this time, write to one of the cluster nodes at a time. If I write to all the nodes using a master/master strategy I’ll run into issues. This is better laid out by this Percona post. Some parts of OpenStack use a “SELECT … FOR  UPDATE”:  The SELECT … FOR UPDATE construct reads the given records in InnoDB, and locks the rows that are read from the index the query used, not only the rows that it returns. Given how write set replication works, the row locks of SELECT … FOR UPDATE are not replicated. – From the above Percona postBut, using some kind of load balancing system, eg. haproxy, so that I only write to one cluster node also means that I only read from one cluster node as well. That’s where MaxScale’s ability to split read and writes comes into play.I should note there is a good post on SeveralNine’s site that has some good information in it on MaxScale.Install MaxScaleI created an account on MariaDB’s site. MaxScale is, I believe, GPL2. But in order to grab the deb file from MariaDB you need to login and get your own unique repository. Once I did that I configured the repository and now MaxScale is available to install via apt. You could always download the source and compile on your own if you would like.ubuntu@maxscale-1:~$ apt-cache policy maxscalemaxscale:  Installed: 1.2.0  Candidate: 1.2.0  Version table: *** 1.2.0 0       1000 http://downloads.mariadb.com/enterprise//mariadb-maxscale/latest/ubuntu/ trusty/main amd64 Packages        100 /var/lib/dpkg/status&lt;/code&gt;&lt;/pre&gt;## ConfigurationSurprisingly MaxScale is pretty straight forward to configure for read/write splitting. This is what my config file looks like (just in my testing phase, so this is not in production at all):[maxscale]threads=4[Splitter Service]type=servicerouter=readwritesplitservers=dbserv1,dbserv2,dbserv3user=maxscalepasswd=7313125C85ABDFB93A4CE397FC2B198Dmax_slave_connections=100%router_options=slave_selection_criteria=LEAST_CURRENT_OPERATIONS[Splitter Listener]type=listenerservice=Splitter Serviceprotocol=MySQLClientport=3306socket=/tmp/ClusterMaster[dbserv1]type=serveraddress=192.168.44.32port=3306protocol=MySQLBackend[dbserv2]type=serveraddress=192.168.44.33port=3306protocol=MySQLBackend[dbserv3]type=serveraddress=192.168.44.34port=3306protocol=MySQLBackend[Galera Monitor]type=monitormodule=galeramondiable_master_failback=1servers=dbserv1, dbserv2, dbserv3user=maxscalepasswd=7313125C85ABDFB93A4CE397FC2B198D[CLI]type=servicerouter=cli[CLI Listener]type=listenerservice=CLIprotocol=maxscaledaddress=localhostport=6603As you can see each of the three nodes in my cluster is listed in the configuration file: dbserv1, dbserv2, and dbserv3.I like the config file. While I hand-rolled this one, the format certainly lends itself to automation.Now maxscale can be started.## maxpasswd and maxkeysI should mention that the \"password\" in the configuration file above was creating using a combination of maxpasswd and maxkeys. I believe you can just enter the plaintext password into the config file if you want to avoid that extra step.## Start maxscaleIf you used the deb to install, then it comes with the init scripts. Just use \"service maxscale start\" and it should start up.ubuntu@maxscale-1:/etc$ sudo service maxscale status * Checking MaxScale * maxscale is running## maxadminMaxScale has a command line interface to MaxScale.ubuntu@maxscale-1:/etc$ maxadmin -pmariadb list serversServers.-------------------+-----------------+-------+-------------+--------------------Server             | Address         | Port  | Connections | Status              -------------------+-----------------+-------+-------------+--------------------dbserv1            | 192.168.44.32   |  3306 |           5 | Slave, Synced, Runningdbserv2            | 192.168.44.33   |  3306 |          83 | Master, Synced, Runningdbserv3            | 192.168.44.34   |  3306 |          89 | Slave, Synced, Running-------------------+-----------------+-------+-------------+--------------------As we can see above, MaxScale has made dbserv2 \"the master\" which is a MaxScale only construction (ie. has nothing really to do with the cluster itself, rather it'll send writes to that node and reads to the other, assuming it's setup in a RW split configuration).We can list the services configured.ubuntu@maxscale-1:/etc$ maxadmin -pmariadb list servicesServices.--------------------------+----------------------+--------+---------------Service Name              | Router Module        | #Users | Total Sessions--------------------------+----------------------+--------+---------------Splitter Service          | readwritesplit       |      2 | 54357CLI                       | cli                  |      2 |    68--------------------------+----------------------+--------+---------------We can also check stats on the splitter service.ubuntu@maxscale-1:/etc$ maxadmin -pmariadb show service \"Splitter Service\"Service 0x35bc5e0    Service:                Splitter Service    Router:                 readwritesplit (0x7f0ad8ace4a0)    State:                  Started    Number of router sessions:              54339    Current no. of router sessions:         0    Number of queries forwarded:            4052477    Number of queries forwarded to master:  2704498    Number of queries forwarded to slave:   1347979    Number of queries forwarded to all:     55274    Started:                Sun Sep 27 21:54:32 2015    Root user access:           Disabled    Backend databases        192.168.44.34:3306  Protocol: MySQLBackend        192.168.44.33:3306  Protocol: MySQLBackend        192.168.44.32:3306  Protocol: MySQLBackend    Users data:                 0x35168d0    Total connections:          54357    Currently connected:            2    SSL:    DisabledWhile running a test we can check sessions too.ubuntu@maxscale-1:/etc$ maxadmin -pmariadb list sessionsSessions.-----------------+-----------------+----------------+--------------------------Session          | Client          | Service        | State-----------------+-----------------+----------------+--------------------------0x38819f0        | 127.0.0.1       | CLI            | Session ready for routing0x7f0ab4044620   | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0ab4063280   | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0ab4001800   | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0ab403a630   | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0ab4029110   | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0ab00c60d0   | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0ab00c73a0   | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0ab4017710   | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0ab4090310   | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0aa80113e0   | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0aa8034430   | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0aa8032e30   | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0aa8032180   | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0ab405e270   | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0aa8022f20   | 192.168.44.35   | Splitter Service | Session ready for routing0x35dfa90        | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0aa8022790   | 192.168.44.35   | Splitter Service | Session ready for routing0x35c9070        | 192.168.44.35   | Splitter Service | Session ready for routing0x7f0ab4000de0   | 192.168.44.35   | Splitter Service | Session ready for routing0x35c95b0        | 192.168.44.35   | Splitter Service | Session ready for routing0x35b8b90        | 192.168.44.35   | Splitter Service | Session ready for routing0x35c7b30        |                 | CLI            | Listener Session0x35b8960        |                 | Splitter Service | Listener Session0x35b98f0        |                 | Splitter Service | Listener Session-----------------+-----------------+----------------+--------------------------## Stop a cluster nodeIf I stop MariaDB on one of the cluster nodes, MaxScale knows:ubuntu@maxscale-1:/etc$ maxadmin -pmariadb list serversServers.-------------------+-----------------+-------+-------------+--------------------Server             | Address         | Port  | Connections | Status              -------------------+-----------------+-------+-------------+--------------------dbserv1            | 192.168.44.32   |  3306 |           5 | Slave, Synced, Runningdbserv2            | 192.168.44.33   |  3306 |          83 | Master, Synced, Runningdbserv3            | 192.168.44.34   |  3306 |         108 | Down-------------------+-----------------+-------+-------------+--------------------MaxScale will only bring the node back into rotation once it's synced up.## Test it outFirst, I tried using sysbench. However I found that sysbench seems to work in such a way that it will only, even though maxscale, connect to one node of the cluster at time. I'll have to look into this. I was even using the \"--oltp-skip-trx=on\" option.In the end I just used a mysqlslap test that is running a simple select from the fake employees database I installed. The 192.168.77.6 IP address is that of the MaxScale node.ubuntu@mysql-client-1:~$ cat mysqlslap.sh #!/bin/bashmysqlslap \\--user=sysbench \\--password=syb3nch \\--host=192.168.77.6 \\--concurrency=20 \\--number-of-queries=1000 \\--create-schema=employees \\--query=\"/home/ubuntu/select.sql\" \\--delimiter=\";\" \\--verbose \\--iterations=2 \\--debug-infoubuntu@mysql-client-1:~$ cat select.sqlSELECT * FROM employees;Here's innotop output on one of the nodes:root@dbsrv1:/home/ubuntu# innotop --count 1 --nonintcmd mysql_thread_id state   user    hostname    db  time    infoQuery   132 Sending data    sysbench    192.168.77.6    employees   00:03   SELECT * FROM employeesQuery   135 Sending data    sysbench    192.168.77.6    employees   00:03   SELECT * FROM employeesQuery   139 Sending data    sysbench    192.168.77.6    employees   00:03   SELECT * FROM employeesQuery   141 Sending data    sysbench    192.168.77.6    employees   00:03   SELECT * FROM employeesQuery   130 Writing to net  sysbench    192.168.77.6    employees   00:02   SELECT * FROM employeesQuery   136 Sending data    sysbench    192.168.77.6    employees   00:02   SELECT * FROM employeesQuery   129 Sending data    sysbench    192.168.77.6    employees   00:01   SELECT * FROM employeesQuery   131 Sending data    sysbench    192.168.77.6    employees   00:01   SELECT * FROM employeesQuery   133 Sending data    sysbench    192.168.77.6    employees   00:01   SELECT * FROM employeesQuery   137 Sending data    sysbench    192.168.77.6    employees   00:01   SELECT * FROM employeesQuery   138 Sending data    sysbench    192.168.77.6    employees   00:01   SELECT * FROM employeesQuery   134 Sending data    sysbench    192.168.77.6    employees   00:00   SELECT * FROM employeesQuery   140 Sending data    sysbench    192.168.77.6    employees   00:00   SELECT * FROM employeesAnd on the \"master\" node, which shouldn't be getting any reads:root@dbsrv2:/home/ubuntu# innotop --nonint --count 1cmd mysql_thread_id state   user    hostname    db  time    infoThese are the results of my overly simplistic test using the MaxScale proxy:ubuntu@mysql-client-1:~$ ./mysqlslap.sh Benchmark    Average number of seconds to run all queries: 112.675 seconds    Minimum number of seconds to run all queries: 111.592 seconds    Maximum number of seconds to run all queries: 113.759 seconds    Number of clients running queries: 20    Average number of queries per client: 50User time 137.57, System time 69.91Maximum resident set size 717064, Integral resident set size 0Non-physical pagefaults 2160414, Physical pagefaults 0, Swaps 0Blocks in 0 out 0, Messages in 0 out 0, Signals 0Voluntary context switches 610086, Involuntary context switches 277132And then without it...going directly to a single node:ubuntu@mysql-client-1:~$ ./mysqlslap.sh Benchmark    Average number of seconds to run all queries: 194.296 seconds    Minimum number of seconds to run all queries: 193.582 seconds    Maximum number of seconds to run all queries: 195.011 seconds    Number of clients running queries: 20    Average number of queries per client: 50User time 144.00, System time 71.97Maximum resident set size 746008, Integral resident set size 0Non-physical pagefaults 1948959, Physical pagefaults 0, Swaps 0Blocks in 0 out 0, Messages in 0 out 0, Signals 0Voluntary context switches 1781510, Involuntary context switches 163916## Basic introductionSo that was my initial exploration of MaxScale. Seems promising. The biggest problem is that I don't know a whole heck of a lot about databases. Despite being a sysadmin for quite a while, I've not had to do much with databases, be it performance or otherwise, so the limitations here seem to be on my end.The other issue is that MaxScale is relatively new. I'd be willing to try it out in a staging environment for sure. Using it might require a support contract for me to feel entirely comfortable, but then again if I had issues with it I could just pull it from use and go straight to on node. I don't say that lightly either because I don't have much in the way of support contracts right now. ;)Hopefully in the future I can do more testing with MaxScale as it seems extremely useful, especially in terms of being able to stop servers and split read/write.I also have some more work to do with sysbench. It seems like it's the best tool, that I know of, to test database performance with, but it is not working well with MaxScale."
    } ,
  
    {
      "title"    : "Notes on MySQL, MariaDB, and Galera",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/09/25/notes-on-mysql-galera-mariadb.html",
      "date"     : "2015-09-25 00:00:00 -0400",
      "content"  : "This is just a set of random notes on MySQL, MariaDB, Galera, other database related thoughts, and googling results for test databases and performance testing MySQL/MariaDB as I work towards getting a better understanding of my MariaDB galera cluster.MariaDB ClusterI’ve got a MariaDB cluster made up (well, as of right this moment) 3 nodes on one network and a garbd on another. This is not the right way to do this, but again, this is just a set of notes. Eventually I’ll have 3 networks (in test) that are meant to represent 3 data centers. Two networks will have 3 nodes on each (for a total of 6) and another network a single garbd server. What I’m aiming for is for any of the DCs to go down and to still have a working cluster in the remaining DC. The nodes are all virtual machines.I’m a bit behind on my MariaDB version, so I’m still back on 5.5, but that is what is in production right now so that is what I need to test with.root@mariadb-1:/home/ubuntu# dpkg --list | grep mariadbii  libmariadbclient18                5.5.45+maria-1~trusty            amd64        MariaDB database client libraryii  mariadb-client                    5.5.45+maria-1~trusty            all          MariaDB database client (metapackage depending on the latest version)ii  mariadb-client-5.5                5.5.45+maria-1~trusty            amd64        MariaDB database client binariesii  mariadb-client-core-5.5           5.5.45+maria-1~trusty            amd64        MariaDB database core client binariesii  mariadb-common                    5.5.45+maria-1~trusty            all          MariaDB database common files (e.g. /etc/mysql/conf.d/mariadb.cnf)ii  mariadb-galera-server-5.5         5.5.45+maria-1~trusty            amd64        MariaDB database server with Galera cluster binariesOpenStack Ansible Galera playbooksI (internally) forked the OpenStack Ansible Galera playbooks some time ago. They are a good way to get a MariaDB Galera cluster up and running quickly. The roles can easily be found on the github site.Galera-arbitrator (arbiter?)Galera-arbitrator (garb or garbd) is a useful service that can help a Galera cluster with maintaining quorum, but doesn’t take up as many resources to run it as it would a full-fledged database server. Usually people who only have two good database servers use garbd on a lower-end server to help with quorum because you shouldn’t have a cluster of two nodes or you’ll end up in split-brain, and split-brain is as bad as it sounds. So if you have MariaDB + Galera on two good servers and garbd on a third (less good) server, then you should be able to avoid split-brain.In my case I have two datacenters with multiple galera nodes in a large cluster, and I want a garbd running in a third datacenter so that if I lose an entire DC, or the interconnect between them, I don’t end up in split-brain at the datacenter level.ubuntu@garb-1:~$ dpkg --list | grep galeraii  galera-arbitrator-3               25.3.9-trusty                    amd64        Galera arbitrator daemonThis is what my /etc/default/garb looks like. Again there are four nodes, which isn’t quite correct, but I’m in testing mode. :)ubuntu@garb-1:/etc/default$ cat garb # Copyright (C) 2012 Codership Oy# This config file is to be sourced by garb service script.# A space-separated list of node addresses (address[:port]) in the clusterGALERA_NODES=\"192.168.77.6:4567 192.168.44.34:4567 192.168.44.33:4567 192.168.44.32:4567\"# Galera cluster name, should be the same as on the rest of the nodes.GALERA_GROUP=\"rpc_galera_cluster\"# Optional Galera internal options string (e.g. SSL settings)# see http://www.codership.com/wiki/doku.php?id=galera_parameters# GALERA_OPTIONS=\"\"# Log file for garbd. Optional, by default logs to syslog# LOG_FILE=\"\"This is what the wsrep_incoming_addresses looks like on one of the mariadb nodes.MariaDB [(none)]&gt; show status like 'wsrep_incoming_addresses';+--------------------------+-----------------------------------------------------------+| Variable_name            | Value                                                     |+--------------------------+-----------------------------------------------------------+| wsrep_incoming_addresses | 192.168.44.33:3306,,192.168.44.34:3306,192.168.44.32:3306 |+--------------------------+-----------------------------------------------------------+1 row in set (0.00 sec)Lots of interesting information. :)Test (fake) databasesOne that I found is the employees database. I believe I stumbled upon the existence of the employees test database through this post.Once you download that and unbzip it, you’ll have these files.ubuntu@mariadb-1:~/employees_db$ lsChangelog                   employees_partitioned.sql  load_dept_emp.dump      load_salaries.dump  READMEemployees_partitioned2.sql  employees.sql              load_dept_manager.dump  load_titles.dump    test_employees_md5.sqlemployees_partitioned3.sql  load_departments.dump      load_employees.dump     objects.sql         test_employees_sha.sqlThen you can simply import the database using:$ mysql &lt; employees.sqlFor example, the salaries table has quite a few entries.MariaDB [employees]&gt; select count(*) from salaries;+----------+| count(*) |+----------+|  2844047 |+----------+1 row in set (0.70 sec)MySQL ProceduresWhile I was looking for test databases, I stumbled on this stackoverflow post that had an example prepared statement in it. I figured why not give it a try, I’d never used a prepared statement in MySQL before. Another technology to look into…root@mariadb-1:/home/ubuntu# cat fake_data.sql CREATE TABLE your_table (id int NOT NULL PRIMARY KEY AUTO_INCREMENT, val int);DELIMITER $$CREATE PROCEDURE prepare_data()BEGIN  DECLARE i INT DEFAULT 100;  WHILE i &lt; 100000 DO    INSERT INTO your_table (val) VALUES (i);    SET i = i + 1;  END WHILE;END$$DELIMITER ;-- CALL prepare_data()All it’s going to do is create a table called “your_table” and load ~100000 entries into it.I ran it a few times to try it out.MariaDB [fake_data]&gt; select count(*) from your_table;+----------+| count(*) |+----------+|   299700 |+----------+1 row in set (0.16 sec)Here’s how to list the procedures.MariaDB [fake_data]&gt; SHOW PROCEDURE STATUS;+-----------+--------------+-----------+---------+---------------------+---------------------+---------------+---------+----------------------+----------------------+--------------------+| Db        | Name         | Type      | Definer | Modified            | Created             | Security_type | Comment | character_set_client | collation_connection | Database Collation |+-----------+--------------+-----------+---------+---------------------+---------------------+---------------+---------+----------------------+----------------------+--------------------+| fake_data | prepare_data | PROCEDURE | root@%  | 2015-09-25 22:27:16 | 2015-09-25 22:27:16 | DEFINER       |         | utf8                 | utf8_general_ci      | utf8_unicode_ci    |+-----------+--------------+-----------+---------+---------------------+---------------------+---------------+---------+----------------------+----------------------+--------------------+1 row in set (0.02 sec)Size of databasesHere’s one way to get the size of the databases in your MySQL/MariaDB cluster. This was borrowed from this stackoverflow post. (I guess I use stackoverflow questions/answers more than I thought.)MariaDB [(none)]&gt; SELECT table_schema \"table name\", sum( data_length + index_length ) / 1024 / 1024 \"Data Base Size in MB\"     -&gt; FROM information_schema.TABLES GROUP BY table_schema ; +--------------------+----------------------+| table name         | Data Base Size in MB |+--------------------+----------------------+| employees          |         197.43750000 || fake_data          |           8.51562500 || information_schema |           0.15625000 || mysql              |           0.62678719 || performance_schema |           0.00000000 || sysbench           |        4752.00000000 |+--------------------+----------------------+6 rows in set (0.14 sec)sysbenchI put up a quick Ansible playbook that installs the lastest sysbench here. Currently that is version 0.5. Apparently 0.5 adds the ability to use lua scripts, and in fact comes with some example scripts which I use below.Following this post I setup and ran tests using the below commands (where all the right databases and users and permissions and such were put into place).ubuntu@mysql-client-1:/usr/local/bin$ cat sysbench-prepare-test.sh #!/bin/bashsysbench \\--db-driver=mysql \\--mysql-table-engine=innodb \\--oltp-table-size=20000000 \\--mysql-host=192.168.44.34 \\--mysql-db=sysbench \\--mysql-port=3306 \\--mysql-user=sysbench \\--mysql-password=syb3nch \\--test=/usr/local/src/sysbench/sysbench/tests/db/oltp.lua \\prepare20000000 records is probably way to many for the size of servers I’m using now (which is about 4 gigs of memory per MariaDB node).This is the test run script:ubuntu@mysql-client-1:/usr/local/bin$ cat sysbench-run-test.sh #!/bin/bashsysbench \\--db-driver=mysql \\--num-threads=8 \\--max-requests=50000 \\--oltp-table-size=20000000 \\--oltp-test-mode=complex \\--test=/usr/local/src/sysbench/sysbench/tests/db/oltp.lua \\--mysql-host=192.168.44.34 \\--mysql-db=sysbench \\--mysql-port=3306 \\--mysql-user=sysbench \\--mysql-password=syb3nch \\runAnd the results of running that test:ubuntu@mysql-client-1:/usr/local/bin$ ./sysbench-run-test.sh sysbench 0.5:  multi-threaded system evaluation benchmarkRunning the test with following options:Number of threads: 8Random number generator seed is 0 and will be ignoredThreads started!OLTP test statistics:    queries performed:        read:                            700000        write:                           200000        other:                           100000        total:                           1000000    transactions:                        50000  (309.68 per sec.)    read/write requests:                 900000 (5574.19 per sec.)    other operations:                    100000 (619.35 per sec.)    ignored errors:                      0      (0.00 per sec.)    reconnects:                          0      (0.00 per sec.)General statistics:    total time:                          161.4584s    total number of events:              50000    total time taken by event execution: 1291.3610s    response time:         min:                                  7.33ms         avg:                                 25.83ms         max:                                172.26ms         approx.  95 percentile:              43.51msThreads fairness:    events (avg/stddev):           6250.0000/118.29    execution time (avg/stddev):   161.4201/0.01When I dropped the number of entries to 50000, these are my results.ubuntu@mysql-client-1:/usr/local/bin$ ./sysbench-run-test.sh sysbench 0.5:  multi-threaded system evaluation benchmarkRunning the test with following options:Number of threads: 8Random number generator seed is 0 and will be ignoredThreads started!^[9OLTP test statistics:    queries performed:        read:                            700000        write:                           200000        other:                           100000        total:                           1000000    transactions:                        50000  (270.09 per sec.)    read/write requests:                 900000 (4861.63 per sec.)    other operations:                    100000 (540.18 per sec.)    ignored errors:                      0      (0.00 per sec.)    reconnects:                          0      (0.00 per sec.)General statistics:    total time:                          185.1233s    total number of events:              50000    total time taken by event execution: 1480.6339s    response time:         min:                                  7.53ms         avg:                                 29.61ms         max:                                269.02ms         approx.  95 percentile:              50.34msThreads fairness:    events (avg/stddev):           6250.0000/158.26    execution time (avg/stddev):   185.0792/0.01For some kind of comparison, good or bad, here’s the same test run on a single instance of the default mysql server you get when you install it on Ubuntu trusty. Same instance type as the above tests were run on.ubuntu@mysql-client-1:/usr/local/bin$ ./sysbench-run-test.sh sysbench 0.5:  multi-threaded system evaluation benchmarkRunning the test with following options:Number of threads: 8Random number generator seed is 0 and will be ignoredThreads started!OLTP test statistics:    queries performed:        read:                            700000        write:                           200000        other:                           100000        total:                           1000000    transactions:                        50000  (511.80 per sec.)    read/write requests:                 900000 (9212.48 per sec.)    other operations:                    100000 (1023.61 per sec.)    ignored errors:                      0      (0.00 per sec.)    reconnects:                          0      (0.00 per sec.)General statistics:    total time:                          97.6936s    total number of events:              50000    total time taken by event execution: 781.2354s    response time:         min:                                  5.55ms         avg:                                 15.62ms         max:                                234.14ms         approx.  95 percentile:              24.31msThreads fairness:    events (avg/stddev):           6250.0000/163.33    execution time (avg/stddev):   97.6544/0.01More work to doSo, like I said, these are just a bunch of notes I took when messing around with a virtual Galera cluster and doing some basic research into performance testing. I’ll update this post as I continue on. Now that I have a virtualized test cluster that I can destroy and rebuild at will I can really get into understanding how it works and what the failure domains are, as well as how it performs. Eventually I would like to get MariaDB MaxScale into the loop as well, and send writes to one host and reads to all."
    } ,
  
    {
      "title"    : "Why I like SDN",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/08/25/why-i-like-sdn.html",
      "date"     : "2015-08-25 00:00:00 -0400",
      "content"  : "Recently, I helped give a presentation on SDN to the Edmonton Go meetup. Basically my part of the presentation was to talk about what SDN is and how it relates to containers and Go. That is actually easy because SDN, containers, and Go are the parts of a “so hot right now” trifecta. There’s a lot of money in those three things, perhaps more when together. I did joke that I hoped we’d come out of the presentation with a fully funded startup. Alas that did not happpen, as we are in Edmonton, not Palo Alto.The only startup I’m aware of that did all three, Socketplane, was bought by Docker before they even put out a real product. I’m surprised there aren’t more Go-based SDN startups. Maybe there are and I’m just not aware of them. Maybe they all get bought before they get a twitter account. [1]I use SDN every dayAs a systems administrator, I tend to concern myself with applying technologies in production, which is to say that theory is not enough. I use Midokura’s Midonet (which is open source[2]) to provide multi-tenant virtual private networks for openstack users. It’s in production and has been for months. Midonet acts as a plugin to Neutron and centralizes state in a Zookeeper cluster. Everything’s been working great. My point is that I am using a software defined network every day to meet customers needs, and I think it would be difficult to do it without some kind of SDN system.There are quite a few “plugins” (or perhaps “network models” would be a better term) for OpenStack that could be considered SDN. If you don’t like overlay or encapsulation models and prefer a large layer 3 design, perhaps Project Calico would be of interest. Also the neutron team and some large OpenStack deployers are working on a large layer 3 design.Some thoughts on SDNFirst off, SDN is “an abstract concept which can mean many things.” The term of overloaded.For a while OpenFlow defined SDN, but that’s no longer the case as it seems the market has outrun OpenFlow. I would think that at the very least an SDN system would have to provide an API. After that it gets harder to define.What’s great about SDN?  Making the network programmable  Reducing capex and opex  Enabling multi-tenancy and massive networks  Avoid some limitations (eg. &gt; 4096 VLANs, which, honestly, is still a lot of VLANs)  Providing a bit of a shakeup to networking (yes, the Internet works amazingly well in terms of layer 3 routing, but is it the ultimate networking model?[3])What’s great about containers and SDN?Docker has been around for a couple years now, but the container “space” is still on fire. Every day there are new announcements. Even if you don’t like containers, you have to admit there is a lot of work being done, a lot of innovation happening and that innovation is “trickling down” to other areas, such as networking. Maybe all the “new” ideas existed before. At any rate I think Linux namespaces and cgroups have been a catalyst for change, perhaps under the guise of Docker (and now the Open Container Initiative). These systems need network information, and screen-scraping or grepping dhcp.leases isn’t going to do it.The canonical example of containers and SDN is how easy it is to setup one Docker host, and how much tricker it is to setup two docker hosts with networking between containers.Another example I like is if you have many container hosts with a large proxy layer on top, and you are recreating containers like crazy, how does that proxy layer know the network address of all the containers and what they do? Through an API provided by SDN.I believe the model used by most container schedulers is one IP per container. Given that containers don’t contain, multi-tenant container systems aren’t used in production (yet) which means a flat layer 3 model works well. But at some point I think multi-tenant containers will be commonplace and then it will make more sense to apply other networking models, just like we do with vms.Network ops pushbackI get a lot of pushback on SDN. Other than being interested in SDN, enough knowledge to get servers up and running, and messing around with BGP and OSPF for high availability, I’m not a networking expert. No one has ever let me login to a switch at work (they defend those things with their lives). And yet here I am, managing a rather large (virtual, software defined) network inside an OpenStack cloud. I’m looking at change in a few areas, 1) SDN on the WAN 2) Using whitebox switches and Cumulus Linux and 3) getting rid of virtual IPs by using BPG and OSPF. The first two are tough battles, only #3 seems doable in my current environment.Centralized control plane?  We’ve had centralized architectures for decades, from SNA to various WAN technologies (SDH/SONET, Frame Relay and ATM). They all share a common problem: when the network partitions, the nodes cut off from the central intelligence stop functioning (in SNA case) or remain in a frozen state (WAN technologies). [4]I’m still slightly confused on the centralized control plane. The above quote makes it fairly clear that a centralized control plane is counter-productive when thinking about scale-out systems.ConclusionSo, while there are many popular new technologies (some would say “over-hyped,” though I would not) I think over time SDN will prove invaluable, and the ability to program a network will slowly seep into almost every vendors system, and will dominate open source networking solutions. Eventually SDN will just be the way networks are done. Certainly that could take years, even decades (as IPv6 hasn’t even been accepted yet, and we really need it), but I would feel safe making a long-term bet on SDN and related technologies.1: It’s not like it really matters whether it’s Java, C, Go, or some other language. I just happen to like Go.2: Midokura has open sourced their application. But it is definitely more like open core. There aren’t a lot of non-Midokura contributors. But again, networking gear isn’t free either.3: I’m really asking. Is it?4: Does Centralized Control Plane Make Sense?"
    } ,
  
    {
      "title"    : "Bin Packing with Python",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/08/23/bin-packing-python.html",
      "date"     : "2015-08-23 00:00:00 -0400",
      "content"  : "I’ve been working with OpenStack for a while now, since back at the Essex release, and every once and a while I hear the phrase “bin packing” with regards to scheduling vms on physical hosts. I didn’t study computer science in university, and am not particularly interested in that kind of thing, but at work we have also been discussing how to deploy racks of servers, or what size of physical hosts to buy (ie. how much memory, disk, cpu to put in them).As far as server racks go, we are constrained on power, space, and possibly network ports. Given those constraints, how can we best deploy servers and network gear? Then, with regards to vms, if we have several types of openstack flavors, how does that work into what we can support and what the distribution on the physical hosts will look like?For the most part I believe people just do whatever in terms of hypervisor host sizing. If deploying openstack they just buy the same hosts as they have always bought for virtualization. But I wanted to 1) take planning a step further and at least be able to calculate requirements in some fashion and 2) finally figure out what bin packing is.What is bin-packing?  In the bin packing problem, objects of different volumes must be packed into a finite number of bins or containers each of volume V in a way that minimizes the number of bins used. In computational complexity theory, it is a combinatorial NP-hard problem. [1]Basically, if you have a bunch of different sized objects and bins to put them in, how do they best fit? I would imagine most compsci grads saw this problem and related problem in early courses.The other interesting thing is that it is NP-hard. This is another term that I’ve heard quite often but haven’t researched what it is. Until now. :)  What does it mean to be NP-hard? It means that if you can solve an NP-hard problem in polynomial time, then you can solve all the NP problems in polynomial time. [2]From my layman point of view, it means that bin packing is a difficult problem to solve in that it can take a long time to get an answer, and what’s more, that if we could solve it quickly then we could solve a lot of other things quickly too. Because it can take a long time to find the answer, often bin packing algorithms take shortcuts to provide an answer quickly.Bin packing with pythonI did a lot of googling to find examples of bin packing with python. The first good one I came across was pyShipping which has a couple of examples of bin packing and 3d bin packing. But after looking through that a bit I realized that I was looking for bin packing with multiple constraints.Finally I ended up on one of the oldest looking websites I’ve seen in a long time, OpenOpt. Yes, that “coming soon” gif is blinking.Anyways, OpenOpt has a couple nice examples and there is a short page on the site for bin packing. I based my work on is the advanced, multiple constaints example.Laymans bin packingI’ve just been doing a bit of testing with the OpenOpt example. Below my example has three flavors (vm sizes): small, medium, and large, each with different cpu, memory, and disk requirements.Then I have server “bins”, ie. hypervisor hosts, with 2TB of disk, ~240GB of memory, and 48 cpus with a 4x overcommit (so 192 virtual cpus). When I run the bpp algo which uses the glpk solver, it distributes the vms over the hypervisors in what we hope is the best use of the resources, with the lowest number of hypervisors being used.The code:#!/usr/bin/pythonfrom openopt import *N = 60 items = []for i in range(N):    small_vm = {        'name': 'small%d' % i,        'cpu': 2,        'mem': 2048,        'disk': 20,        'n': 1        }    med_vm = {        'name': 'medium%d' % i,        'cpu': 4,        'mem': 4096,        'disk': 40,        'n': 1        }    large_vm = {        'name': 'large%d' % i,        'cpu': 8,        'mem': 8192,        'disk': 80,        'n': 1        }    items.append(small_vm)    items.append(med_vm)    items.append(large_vm)bins = {'cpu': 48*4, # 4.0 overcommit with cpu'mem': 240000, 'disk': 2000,}p = BPP(items, bins, goal = 'min') r = p.solve('glpk', iprint = 0) # requires cvxopt and glpk installed, see http://openopt.org/BPP for other solversprint(r.xf) print(r.values) # per each binprint \"total vms is \" + str(len(items))print \"servers used is \" + str(len(r.xf))for i,s in enumerate(r.xf):    print \"server \" + str(i) + \" has \" + str(len(s)) + \" vms\"The results:$ time python vms.py Initialization: Time = 6.7 CPUTime = 6.7------------------------- OpenOpt 0.5625 -------------------------problem: unnamed   type: MILP    goal: minsolver: glpk  iter  objFunVal  log10(maxResidual)      0  0.000e+00               0.00 GLPK Integer Optimizer, v4.5433480 rows, 32580 columns, 162900 non-zeros32580 integer variables, none of which are binaryPreprocessing...720 rows, 32580 columns, 130140 non-zeros32580 integer variables, all of which are binaryScaling... A: min|aij| =  1.000e+00  max|aij| =  2.400e+05  ratio =  2.400e+05GM: min|aij| =  9.204e-01  max|aij| =  1.087e+00  ratio =  1.181e+00EQ: min|aij| =  8.812e-01  max|aij| =  1.000e+00  ratio =  1.135e+002N: min|aij| =  5.000e-01  max|aij| =  1.000e+00  ratio =  2.000e+00Constructing initial basis...Size of triangular part is 720Solving LP relaxation...GLPK Simplex Optimizer, v4.54720 rows, 32580 columns, 130140 non-zeros      0: obj =   0.000000000e+00  infeas =  1.706e+02 (0)    500: obj =   3.010416667e+00  infeas =  4.995e+01 (0)-   628: obj =   4.375000000e+00  infeas =  1.599e-14 (0)OPTIMAL LP SOLUTION FOUNDInteger optimization begins...+   628: mip =     not found yet &gt;=              -inf        (1; 0)+  6048: &gt;&gt;&gt;&gt;&gt;   5.000000000e+00 &gt;=   5.000000000e+00   0.0% (46; 0)+  6048: mip =   5.000000000e+00 &gt;=     tree is empty   0.0% (0; 91)INTEGER OPTIMAL SOLUTION FOUND    1  0.000e+00            -100.00 istop: 1000 (optimal)Solver:   Time Elapsed = 6.86   CPU Time Elapsed = 6.85objFuncValue: 5 (feasible, MaxResidual = 0)[{'medium12': 1, 'medium13': 1, 'medium10': 1, 'medium11': 1, 'small9': 1, 'medium18': 1, 'medium57': 1, 'medium55': 1, 'large8': 1, 'small11': 1, 'small10': 1, 'small12': 1, 'large1': 1, 'large3': 1, 'large2': 1, 'large6': 1, 'large33': 1, 'medium9': 1, 'small55': 1, 'medium0': 1, 'large14': 1, 'medium2': 1, 'large16': 1, 'large11': 1, 'large10': 1, 'large12': 1, 'medium26': 1, 'medium22': 1, 'large57': 1, 'large54': 1, 'small58': 1, 'medium1': 1, 'large56': 1, 'large22': 1, 'large26': 1}, {'medium39': 1, 'medium35': 1, 'medium36': 1, 'medium54': 1, 'medium52': 1, 'medium50': 1, 'medium51': 1, 'medium59': 1, 'small13': 1, 'small51': 1, 'small50': 1, 'small52': 1, 'large37': 1, 'large36': 1, 'large35': 1, 'large34': 1, 'large15': 1, 'large17': 1, 'small35': 1, 'large18': 1, 'medium49': 1, 'medium48': 1, 'large51': 1, 'large50': 1, 'large52': 1, 'medium47': 1, 'medium46': 1, 'large49': 1, 'large46': 1, 'large47': 1, 'large23': 1, 'large48': 1, 'small48': 1, 'small49': 1, 'small47': 1, 'medium7': 1, 'small40': 1, 'small41': 1}, {'medium38': 1, 'medium34': 1, 'small8': 1, 'medium37': 1, 'medium32': 1, 'medium33': 1, 'medium53': 1, 'large9': 1, 'small53': 1, 'large32': 1, 'small39': 1, 'small38': 1, 'small54': 1, 'small33': 1, 'large39': 1, 'large38': 1, 'small37': 1, 'small36': 1, 'medium6': 1, 'small34': 1, 'medium42': 1, 'medium41': 1, 'medium40': 1, 'large53': 1, 'large41': 1, 'medium45': 1, 'medium44': 1, 'medium20': 1, 'medium3': 1, 'small28': 1, 'large21': 1, 'large44': 1, 'large45': 1, 'large42': 1, 'large43': 1, 'large40': 1, 'large27': 1, 'small46': 1, 'small44': 1, 'small45': 1, 'small42': 1, 'small43': 1, 'small27': 1}, {'small1': 1, 'small0': 1, 'small3': 1, 'small2': 1, 'small5': 1, 'small4': 1, 'small7': 1, 'small6': 1, 'large0': 1, 'large5': 1, 'large4': 1, 'large7': 1, 'medium8': 1, 'medium56': 1, 'small59': 1, 'large58': 1, 'medium4': 1, 'medium5': 1}, {'medium16': 1, 'medium17': 1, 'medium14': 1, 'medium15': 1, 'medium19': 1, 'medium30': 1, 'medium31': 1, 'medium58': 1, 'small15': 1, 'small14': 1, 'small17': 1, 'small16': 1, 'small19': 1, 'small18': 1, 'large31': 1, 'large30': 1, 'large19': 1, 'small57': 1, 'small56': 1, 'small32': 1, 'small31': 1, 'small30': 1, 'medium43': 1, 'large13': 1, 'large59': 1, 'medium29': 1, 'medium28': 1, 'medium27': 1, 'large55': 1, 'medium25': 1, 'medium24': 1, 'medium23': 1, 'medium21': 1, 'large28': 1, 'large29': 1, 'large20': 1, 'small29': 1, 'large24': 1, 'large25': 1, 'small20': 1, 'small21': 1, 'small22': 1, 'small23': 1, 'small24': 1, 'small25': 1, 'small26': 1}]{'mem': (196608.0, 196608.0, 196608.0, 75776.0, 194560.0), 'disk': (1920.0, 1920.0, 1920.0, 740.0, 1900.0), 'cpu': (192.0, 192.0, 192.0, 74.0, 190.0)}total vms is 180servers used is 5server 0 has 35 vmsserver 1 has 38 vmsserver 2 has 43 vmsserver 3 has 18 vmsserver 4 has 46 vmsreal    0m24.788suser    0m23.894ssys 0m0.944sSo from the above, we know we would need five hypervisor hosts to run this set of 180 vms, 60 small, 60 medium, and 60 large.From the output it also seems like we max out on cpus and are actually getting pretty close on disk too.{ 'mem': (196608.0, 196608.0, 196608.0, 75776.0, 194560.0),  'disk': (1920.0, 1920.0, 1920.0, 740.0, 1900.0),  'cpu': (192.0, 192.0, 192.0, 74.0, 190.0)}Now we can play around with numbers and see if we can reduce the number of hypervisors. In the next attempt I changed the cpu overcommit to 5 and the disk on the host to 3TB.{ 'mem': (184320.0, 198656.0, 239616.0, 237568.0),  'disk': (1800.0, 1940.0, 2340.0, 2320.0),  'cpu': (180.0, 194.0, 234.0, 232.0)}total vms is 180servers used is 4server 0 has 37 vmsserver 1 has 39 vmsserver 2 has 55 vmsserver 3 has 49 vmsNow we are down to only four hypervisors fitting the 180 vms.Certainly this is the most simplistic example. But I feel like it’s still pretty powerful, and it will make much more sense when I can input real-world data for flavor usage in a cloud. If I can input real usage data in terms of flavors, instead of just 60 of each, then I can really start to understand what the minimum hardware investment is, or at least what the servers could look like for memory, disk, and cpu.Live migrationI’ve never been a big fan of live migration. Usually it means a distributed file system, or a shared file system, and those are both hard to run and scale. Also, there is the whole pets vs cattle thing. Using that metaphor, there’s no reason to live migrate cattle.However, I did read through a paper, “Adaptive Resource Provisioning for the CloudUsing Online Bin Packing”:http://www.cs.princeton.edu/~haipengl/papers/binpacking.pdf, that discussed using bin-packing to move vms around in a cloud so that some hypervisors could be turned off and thus save power. To me that sounds like a good use of live migration.We have talked about making booting from volumes a default as potential methodology in an OpenStack cloud, which would enable live migration without a large shared file system or a distributed file system. So that could be a direction to go in. I’ve always thought of cinder with plain old lvm hosts to be pretty powerful and having a small failure domain. In theory we could let OpenStack schedule however it wants, and then run a bin packing algorithm every once and a while and live migrate instances to make better use of hypervisors. Easier said than done of course, but would be interesting.ConclusionIn the end, despite being a compsci layman, I think I’ve got what I wanted: a calculator that can help me to look at our rack resources and our hypervisor servers and do some sizing. As I continue down this path I’m sure I’ll learn more about bin packing. It’s a start. :)The next area I need to understand is how OpenStack schedules vms…1: Bin Packing Problem2: Explanation of P versus NP"
    } ,
  
    {
      "title"    : "Palo Alto OpenStack Operators Meetup",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/08/19/pao-openstack-operators-meetup.html",
      "date"     : "2015-08-19 00:00:00 -0400",
      "content"  : "I am really early for my flight home to Edmonton from the San Franciso airport. Maybe I didn’t need to be as concerned about traffic as I was. I bought The Martian book so I should be reading it, but I’m not, instead I’m writing this blog post about the Palo Alto OpenStack Operators mid-cycle meetup that I attended.If you want to see the full, official notes from each session, you can find links to them from this etherpad page.My notes from the meetupHere are some general notes I made from the sessions. Sorry the below is rambling, but that’s how I think. Also these are my interpretations of what was said, and I’ve been wrong before. If you notice something incorrect, just let me know in the comments and I’ll change it.  The vast, vast majority of operators have moved off of nova-network and are on Neutron now. Only one person had their hand up for running nova-network, though I’m sure there are more, they just didn’t respond to the informal poll. I think it’s good that Neutron is now the standard. Neutron is more complicated than nova-network and doesn’t necessarily meet everyones needs, but I think having a common base will help get things moving forward. The large deployments team is working on helping the Neutron team create a model that works for them, and I think once that happens everyone will be happy. For the record my neutron “model” is to use Midonet with VXLAN. I really like Midonet and the overlay model fits our customer base well. Several large deployments prefer straight up L3 based networks, and I’m sure Neutron will provide that model soon. That said, it will be hard for Neutron to meet the requirements of every single network model.  RabbitMQ isn’t as much of a burning issue as it used to be. Seems like people still have no idea whether to run it behind a load balancer or not, or to list all the rabbit’s in the config, or what. Still quite confusing. However, a definite consensus is that Kilo is much better with rabbit. I’d like to backport some of the things that make it better as we are still on Juno, but I’m not sure we’ll have time for that. Soon we will upgrade from Juno to Kilo anyways. Another point made was that you probably have issues whether you use a loadbalancer or not, so either way…  Also–someone from RabbitMQ was at the meeting and gave an update. They are working on making RabbitMQ easier to operate. They are also keenly aware of how much it is used in OpenStack, and may provide OpenStack-specific documentation on using RabbitMQ. That would be great.  CMDB was a big topic at this meetup. Lots of people want a good CMDB so that they can start to do capacity planning. Obviously this means there are many large OpenStack installations out there.  HP talked about their billing solution. It’s fairly complex…as far as I can remember it pulls information off of various sources, such as rabbit, sticks them into Hadoop, there are jobs that crunch that data and store it into an SQL database which is fronted by an API that internal systems can query for billing. They intend to opensource this system in some fashion, or maybe just parts of it. Not sure.  Burning issues: The etherpad can be found here.  Many, many hands were put up for Neutron as a “burning issue,” but when the time came around to actually complain about it, no one really said anything, or perhaps I just missed it. My impression is that Neutron is working pretty well for most.  Ceilometer is still an issue, but people are also still using it, and many are investing heavily in getting it working. I think Ceilometer is an important project and I feel like eventually it’ll be production worthy. I keep forgetting Heat can use it for autoscaling. We stopped using it due to performance issues on the backend (like many others) but we also didn’t put that much effort into it. Many other metrics gathering systems are resource intensive, and also use backends like Mongo. If you throw too much data at a backend it stops working, no matter what technology you’re using. One provider mentioned using Influxdb and ceilometer, I believe with statsd as an intermediary.  Keystone was also listed fairly high in terms of burning issues. Many clouds would like to flexible roles, as basically there are only three right now: admin, member, and no role. As usual the policy json files were mentioned as being complex and somewhat undocumented. I’m personally not sure how much effort to put into the policy files if there are only three roles. Something to look into. The important thing to know is that the Keystone project is well aware of these requirements and they are working on them. Also, many clouds have added an extra layer on top of Keystone to allow some tenants to do admin-like functions, such as add users to their project and things like that. But that functionality is “hacked” in, so to speak by the providers.  I didn’t get to ask my question about putting Keystone tokens in memcached. I read a bug a while ago where it was mentioned that the memcached driver being used is not that great, and that using memcached isn’t recommended, even though it’s part of the official install docs (or was the last time I looked). Darned if I can find that bug listing again. Several providers have moved to Fernet tokens which don’t need to be stored, though are slower. I would imagine that once we get to Kilo that we will use Fernet tokens as well and just avoid the whole caching/storage issue.  Containers: There was some discussion about containers. It seemed forced. Some people use containers in their OpenStack infrastructure…LXC, Docker. I will probably use so-called “fat” containers, ie. LXC, for some infrastructure. Yeah, containers are still hot. Docker is cool. Multi-tenancy is hard, etc, etc.  Install guide: There was at least one mention of how a particular technology was used in someones OpenStack cloud because it was a step in the install guide. I think the install guide is a lot more important than people know in terms of how people deploy OpenStack. If memcached is listed as an install step then people will use it. If OVS is mentioned in the install guide then people will use it (as opposed to Linux bridge, which large operators seem to prefer, at least at this time).  Many operators use MySQL/MariaDB and Galera. Most using this combination, if not all, only write to one node. Usually a virtual IP points to one node and clients read and write to it. I think one operator had managed to get read-only APIs up and running and those work against the read nodes of a Galera cluster. That would be something I would like to do, at least send all reads out to any node, but writes to one node. It was noted that CERN does this for Ceilometer to make sure it doesn’t overload the write APIs. I’ve been looking at some MySQL proxies that can do this, but maybe it’s just extra complexity that I don’t need.  Everyone loves haproxy. I quite like haproxy myself. Some ops prefer it over the $$$ commercial solution they are also using. It’s very powerful and is working well for me, and I, like others, terminate SSL with it.  Most operators have to do some database cleanup. I’m not clear on all the things that need to be cleaned up. There is at least one project, ospurge, that might help get some of the way towards all the cleanup jobs that need to happen. 80% of the time it’ll get you 60% of the way.  Every summit and operator meetup has a call for a place to share tools. Every summit and meetup osops is mentioned and that is where the conversation ends. osops has been around for quite some time but there aren’t many contributions to it. One of the great things about openstack is how you can run it in almost any way you want, but that also means that every openstack install is different, and it makes it somewhat hard to share tools…which is why I think this repo doesn’t get much love.  There was talk about sharing Grafana configs. Then there was the mention that the new Kibana (not Grafana, Kibana) makes it difficult to import/export configs. I’m doing some testing with Grafana and Influxdb for metrics, so that would be great to at least see what others are graphing, if not actually use those graphs.  Dragonflow was mentioned as a very new project. I don’t know anything about it other than what is says on at the github repo: “Dragonflow - SDN based Distributed Virtual Router for OpenStack Neutron.” Even though I’m not an expert at networking, I am fascinated by SDN.Public cloudThere were also some concrete steps towards getting a public cloud group going in/around OpenStack. I think this is great and hope to participate as much as possible. There may not be many public clouds based on OpenStack, and some are much larger than others, but I think this is an important sub-group of operators, and is not necessarily the same as the large deployments team. Public clouds definitely have some specific requirements.Short tripFor me it was a short trip. We were in late Monday night and had to leave about 3PM on the Wednesday, so we missed some of the tail end of the meetup, mostly around Tokyo planning, which was unfortunate but unavoidable due to California traffic and flight times.Thanks to HP and Godaddy as sponsors and for all the work put into the meetup by the foundation and volunteers. While I’m not a big fan of the Palo Alto [1] location, the meetup certainly met its goal of enabling the exchange of ideas and practical OpenStack experiences.If you’re wondering, my flight back was great because there was an extra seat beside me, and it’s only a 2.5 hour trip back to sunny Edmonton from overcast SF0. :) Also I read almost all of “The Martian.” If you like NASA then you will read this book in one sitting, it’s a fascinating and thrilling read with tons of science.1: Palo Alto hotels are expensive and you pretty much have to rent a car. It’s great that companies are willing to sponsor the meetup with a location and food and such, but it seemed to me like the costs were just transfered to hotels and rental cars. Then again, a large number of attendees were from California, so maybe it all evens out."
    } ,
  
    {
      "title"    : "The Layers of OpenStack",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/07/25/openstack-layers.html",
      "date"     : "2015-07-25 00:00:00 -0400",
      "content"  : "I think anyone who has looked into OpenStack has read that it is complicated and difficult to install let alone operate over time. However, that’s not the point of this post–rather the point of this short post is just to discuss what pieces are involved in OpenStack and how I tend to approach deploying it, an approach that I feel simplifies the system somewhat.Computering is complicated. Sometimes it helps to simplify it from a high level, and I use “layers” to do that. What’s more this model can help people who don’t have to understand every single component of the entire system to get in idea of what it looks like. An OpenStack production system can be very large, especially when considering all the ancilliary infrastructure required.I do want to note that once it’s deployed the order of the layers is not all that relevant, but I think putting the layers in order prior to deployment can help people understand the components of OpenStack.Prior to installationBefore installing all kinds of decisions have to be made, things like what server vendor will be used, same for network, datacenter requirements, what OpenStack will look like (eg. are you using Ceph for block and object, booting from volumes, nova network or neutron, neutron with a plugin, etc, etc) but I’m not going to cover any of that, rather I will just simply list what I usually install and in what order.Before starting these layers I have applied the base OS to every server, and there is basic network connectivity on the managment network that Ansible operates over.The LayersI use Ansible to deploy OpenStack. Perhaps that’s why I’ve settled on the layer strategy. Ansible pretty much works in a serial fashion, one task after another, one playbook after another. Ansible is applied “top to bottom” versus something like Puppet where modules are compiled and then applied. So with Puppet order isn’t defined unless it’s specifically defined.The basic layers are below. Some of the layers are more complicated than others, ie. have more tasks to complete.  ssh-keys - Setup operator ssh keys  node-network - Setup more comlicated /etc/network/interfaces  sshd - Setup sshd securely, eg. only listen on specific interfaces, no password logins, etc  baremetal - Some basic common requirements and security settings  rsyslog - Setup rsyslog server and clients  etc-hosts - Configure /etc/hosts  lxc-hosts - Configure servers that will be lxc hosts  lxc-containers - Start various lxc-containers  mariadb-galera - MySQL Galera cluster  haproxy - haproxy does all the API load balancing  rabbitmq-cluster - Rabbit cluster  memcached - Setup memcached servers  openstack-repo - If installing OpenStack from packages, setup the repo (eg. Juno, Kilo, etc)  keystone - Keystone authentication service  swift-common - Next up, Swift because it’s backing Glance  swift-object - Setup Swift storage nodes  swift-ring - Create Swift ring  swift-fetch-ring-files - Obtain the created Swift ring files  swift-distribute-ring-files - Distribute the ring files across all Swift nodes  swift-proxy - Configure Swift-proxy servers  glance - Setup Glance  nova-common - Next up, Nova for compute  nova-controller  nova-compute  neutron-common - Networking!  neutron-controller  neutron-network  neutron-compute  cinder-common - Block storage!  cinder-controller  cinder-storage  heat - And Heat (heat is nice and easy to install)  horizon - Finally the GUIObviously there are more pieces (metrics, monitoring, cron jobs, backups, alerting, more logging, DNS, etc) but I won’t cover those in this post. And of course there are many other OpenStack projects that could be used (and more every day).OpenStack deployed!Now with all those layers laid down, you have an OpenStack system. At this point the layers don’t really exist in a defined order and you are going to be operating this system over a long period of time. And that, my Internet friend, is a whole other story. :)"
    } ,
  
    {
      "title"    : "Dealing with Zombie Cinder Volumes",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/07/15/cinder-zombie-volumes.html",
      "date"     : "2015-07-15 00:00:00 -0400",
      "content"  : "For some reason I ended up with a few volumes that were attached to non-existent virtual machines–ie. the vm had been deleted, but cinder thinks it is still attached to the vm. But the vm doesn’t exist so you can’t unattach or delete the volume. Maybe it’s a chicken-egg volume not a zombie volume.Here’s a query to find these “zombie” volumes. I’m building somewhat on this blog post. (Don’t tell anyone but this was my first ever join SQL command–I haven’t had to work much with databases.) This join was kind of fun because I had to pull information from both the nova and cinder databases, so note that it is using both the cinder and nova databases and expecting that their names will be “cinder” and “nova” respectively.MariaDB [cinder]&gt; SELECT tn.display_name as 'VM Name', tn.uuid as 'VM UUID', tn.vm_state as 'VM State', tc.status as 'Vol Status', tc.attach_status as 'Vol Attach Status', tc.id as \"Vol UUID\" FROM cinder.volumes tc JOIN nova.instances tn ON tc.instance_uuid = tn.uuid WHERE tn.vm_state = 'deleted' AND tc.attach_status = 'attached';+-----------------------------------------+--------------------------------------+----------+------------+-------------------+--------------------------------------+| VM Name                                 | VM UUID                              | VM State | Vol Status | Vol Attach Status | Vol UUID                             |+-----------------------------------------+--------------------------------------+----------+------------+-------------------+--------------------------------------+| coolvm1                                 | 2cf50467-044e-4f6c-8aad-283fb1c18f49 | deleted  | in-use     | attached          | 59a92e86-df60-4046-9cbe-221f9501cc5d || coolvm2                                 | 80f837ec-a076-4116-8443-34c17ff8b363 | deleted  | in-use     | attached          | 92926e7e-8015-449e-893e-e84b4a7a9fdc || coolvm3                                 | 7b6a15b2-b9ad-439b-a05f-acf6af4e7a86 | deleted  | in-use     | attached          | ffc905ad-6655-4e8b-8e21-6322e93773c4 |+-----------------------------------------+--------------------------------------+----------+------------+-------------------+--------------------------------------+3 rows in set (0.00 sec)Once I found all the volumes I considered “zombie” I updated their status manually in the cinder database. This was not much fun from an operator perspective. I think there is a “force_detach” command coming in future cinders.MariaDB [cinder]&gt; update volumes set attach_status='detached',status='available',instance_uuid=NULL where id='6fbd830c-3976-4427-be4b-c4daa11f9f49';Query OK, 1 row affected (0.00 sec)Rows matched: 1  Changed: 1  Warnings: 0I haven’t been able to replicate this issue, so not sure why it happened in the first place."
    } ,
  
    {
      "title"    : "Rabbitmq Server with SSL/TLS",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/06/23/rabbitmq-server-ssl.html",
      "date"     : "2015-06-23 00:00:00 -0400",
      "content"  : "For some reason I am a glutton for punishment as I try to “TLS enable all the things” which doesn’t always work out. Note that the rabbitmq documentation for SSL/TLS is pretty good; I’m not showing here much more than you can get from that, but I thought I’d post it anyway. :)Anyways, one of the more interesting things I’ve enabled TLS on lately is Rabbitmq. What’s more this is in production right now and is working fine. There is some debate as to whether or not it’s a good idea to do TLS with Rabbitmq, especially if it’s an internal only queue, but I think it’s always best to encrypt when we can. I suppose there is the possibility of performance issues, but I don’t mind throwing hardware at it. I should also note that I’m just doing “over the wire” encryption. The certificates aren’t being used for authentication. (Future work.)This is an example configuration for a three node Rabbitmq cluster:[ {ssl, [{versions, ['tlsv1.2', 'tlsv1.1']}]}, {rabbit, [           {cluster_nodes, ['rabbit`node2','rabbit`node5']},           {tcp_listeners, []},           {ssl_listeners, [{\"192.168.0.12\",5671}]},           {ssl_options, [{cacertfile,\"/etc/ssl/certs/example.com-intermediate.crt\"},                          {certfile,  \"/etc/ssl/private/example.com.crt\"},                          {keyfile,   \"/etc/ssl/private/example.com.key\"},                          {versions, ['tlsv1.2', 'tlsv1.1']}                         ]}          ]}].This is the contents of the rabbitmq-env.conf file, not that it should be necessary in most cases. Sometimes I name the node something different than the hostname, or perhaps internal communication only happens on a specific VLAN so it has to listen on a specific interface.export RABBITMQ_NODENAME=rabbit@node2export RABBITMQ_NODE_IP_ADDRESS=192.168.0.12Then rabbitmq will be listening on port 5671.ubuntu@node2:/etc/rabbitmq$ sudo lsof -i -nP | grep LISTEN | grep \":5671\"beam.smp  18935  rabbitmq   20u  IPv4  165304426      0t0  TCP 192.169.0.12:5671 (LISTEN)Now any of your applications that are using the rabbitmq queue can connect via TLS on port 5671.There’s a lot more work to be done here, but it’s a start!"
    } ,
  
    {
      "title"    : "Monitorama 2015",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/06/21/monitorama-2015.html",
      "date"     : "2015-06-21 00:00:00 -0400",
      "content"  : "Last week I was privileged to attend the 2015 Monitorama conference. It’s a great small conference that deals with the area of monitoring, from logging to metrics to stream processing, alerting, people…everything. It’s actually a giant topic.It’s amazing how many different kinds of conferences there are. Recently I was at the OpenStack Vancouver summit which has about 6000 people and takes over a huge conference center. Compared to Monitorama the summit is a massive, “enterprisey-feeling” event where they have to feed 6000 people lunch. Conversely, at Monitorama in “The Pearl” section of Portland you can just walk out the door and there are tons of restaurants and food trucks.The PresentationsHere I’ll just go through some of the presentations I recall. I missed a few presentations because of work, so this isn’t necessarily a list of “the best” presentations or anything like that, just ones that were memorable for me.      Zero to Capacity Planning (video) - If I recall correctly this was the journey the presenter Inés Sombra went on to find out about capacity planning and do something about it. The slides are online. In order to plan to grow you need to monitor your infrastructure and then decide when to add more capacity. But what does that actually mean? How does it happen?        Stephen Boak - I’m a designer and I’m here to help (video) - I think there was a lot to learn from this. The first thing anyone does when adding a monitoring solution is make sure there are a bunch of graphs to show people. Who knows what those graphs actually mean or if they are of any use is another matter. (Can’t find the slides online.)        Observability, Interactivity and Mental Models in Web Operations (video) - Benjamin Anderson gave a very interesting talk. Unfortunately I can’t find the slides online. Will update when I can find them. Let’s just say “anthropomorphizing systems.” Update: Video is up now for this talk.        Laura Thomson - Engineering Happiness (video) - Process, software…and people. This presentation went over some ideas around what metrics one could monitor to determine how people are feeling, but not from a creepy perspective, rather looking at things like hours worked or technical debt to try to determine employee happiness (ie. working too much makes people unhappy). We certainly all want to be happy at work and this presentation gave some ideas on how to make that happen.        Loris Degioanni - The Dark Art of Container Monitoring (video-) Basically, sysdig is amazing and the sysdig monitoring cloud can map your infrastructure and make it zoomable, which drew some gasps from the audience.        Measuring Hard to Measure Things (video-) Chrissie Brodigan gave a great talk on the user research work she does at Github. The talk was really about dealing with people at a macro level to make things better for individuals. She was really focussed on making things better, and in one particular case, making Github Enterprise easier to administrate. One thing I realized is that often oganizations only have surveys to determine how people feel about products and they need demographic information in those surveys to do the math right.        Stream Processing Inside Librato (video) - Dave Josephsen gave one of the first talks at Monitorama 2015 on stream processing, essentially comparing stream processing with signal processing using the sound mixer as an example.        Dave Josephsen - Lightning talk (video) - Dave gave an emotional talk on imposter syndrome, through relating stories of his difficult time in the Marines. It was an amazing presentation full of honesty and vulnerability–two very difficult things to do in the modern world.  TechnologiesHere are some technologies I wrote down in my notes as the conference went on:  StandUps  sysdig  zktraffic  Tinypulse  Bosun  HekaConclusionI’m constantly amazed at how organizers can pull off conferences like this; how good they are at dealing with people and trying to make things better in the IT industry. I was also impressed at how honest some of the presenters were in terms of being open about their fears and themselves in general. At the end I kept thinking of what Big Chris says in Lock Stock and Two Smoking Barrels: “It’s been emotional.”"
    } ,
  
    {
      "title"    : "ucarp Virtual IP Addresses",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/04/05/ucarp.html",
      "date"     : "2015-04-05 00:00:00 -0400",
      "content"  : "I haven’t written a blog post for a while; like everyone else I’m working on other things and haven’t been able to dedicate any time towards blogging. However, on a slow Sunday morning I thought I’d take a quick look at ucarp which is a way to provide virtual IPs, ie. IP addresses that can failover to another server, should the one it’s running on stop working.Highly available IP addressesFrankly I don’t have a lot of experience providing highly available IP addresses (from now on I’ll call the virtual IPs or vips, though that’s probably not the right term). There are quite a few ways to achieve vips, such as using technologies like VRRP, Pacemaker, Corosync, Keepalived, ECMP/BGP, etc. All have their pros and cons, some are simpler than others…make your own well-informed decision. :)For my particular purposes, at this time at least, I chose CARP.CARPThere was a big kerfuffle back in the late 90’s about Cisco creating, and patenting, VRRP. OpenBSD was not happy with the situation, and created CARP. I’ve used CARP a lot over the years, especially with OpenBSD firewalls. So when it came time to do highly available virtual IPs with Linux, CARP seemed a good choice. I didn’t want to over-engineer too early, so using something simple like CARP seemed a good strategy.PS. Use more OpenBSD!ucarpUCARP allows a couple of hosts to share common virtual IP addresses in orderto provide automatic failover. It is a portable userland implementation of thesecure and patent-free Common Address Redundancy Protocol (CARP, OpenBSD’salternative to the patents-bloated VRRP).In Ubuntu the ucarp system/package provides CARP virtual IP capability.$ dpkg --list ucarpDesired=Unknown/Install/Remove/Purge/Hold| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)||/ Name                                            Version                      Architecture                 Description+++-===============================================-============================-============================-===================================================================================================ii  ucarp                                           1.5.2-1+nmu1ubuntu1          amd64                        user-space replacement to VRRP -- automatic IP fail-overConfigurationOnce the ucarp package is installed, the /etc/network/interfaces file can be configured to use ucarp.Below are a couple example sections from a ucarp configured network interface file. Please note that I am using bonding and VLANs as well as ucarp. Bonding and vlans are not required to use ucarp, but it’s fun so why not.The primary node (node1):# internal networkauto bond0.777iface bond0.777 inet static  vlan-raw-device bond0  address 192.168.1.12  netmask 255.255.252.0  ucarp-vid      1  ucarp-vip      192.168.1.9  ucarp-password SHAREDSECRET  ucarp-advskew  10  ucarp-advbase  1  ucarp-master   yesAnd here is a secondary (node2):# internal networkauto bond0.777iface bond0.777 inet static  vlan-raw-device bond0  address 192.168.1.13  netmask 255.255.252.0  ucarp-vid      1  ucarp-vip      192.168.1.9  ucarp-password SHAREDSECRET  ucarp-advskew  20  ucarp-advbase  1  ucarp-master   noOnce I configured the interfaces, I rebooted all the nodes.Who has the vip?Here’s the output from running ansible across all the nodes looking for the ucarp vip.$ ansible -m shell -a \"ip ad sh | grep bond0.777:ucarp\" ucarp-nodenode1 | success | rc=0 &gt;&gt;    inet 192.168.1.9/32 brd 192.168.1.9 scope global bond0.777:ucarpnode2 | FAILED | rc=1 &gt;&gt;node3 | FAILED | rc=1 &gt;&gt;As can be seen above, only one of the nodes has the ucarp vip. If I were to power off node2 one of the other nodes would obtain the vip. If you see more than one node with the same IP then likely ucarp is configured but the ucarp package is not installed.Thoughts and future workRight now, to me ucarp’s only limitation is that it’s active/passive. However, ucarp is quite easy to setup and works well. Simplicity helps uptime too.There are a few things I still have to investigate  Dialing in the failover time  Determining if the master should take back the vip when it’s restored  Configuring more than on vip per interface is not straight forwardWhile we are investigating other more complicated active/active HA processes for IP addresses, thanks to its simplicity I’m quite sure parts of my infrastructure will continue to use ucarp."
    } ,
  
    {
      "title"    : "Trying OSPF with Quagga and OpenBGP",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/02/02/quagga-openospfd.html",
      "date"     : "2015-02-02 00:00:00 -0500",
      "content"  : "I’ve recently being doing some research into Equal Cost Multipath Routing (ECMP) as well as Open Shortest Path First (OSPF).The first thing that I should note is that I have no idea really what I’m doing…I’m just messing around with OSPF really.I put together a quick Vagrant + Ansible test setup that uses a single OpenBSD router, two Ubuntu Trusty servers and a Trusty client instance. That means the Vagrantfile I created will setup four virtual machines. I used OpenBSD’s ospfd (part of OpenBGP) and Quagga on the Ubuntu hosts.Get startedyou@workstation$ git clone https://github.com/ccollicutt/ansible-ospf-testyou@workstation$ cd ansible-ospf-testyou@workstation$ vagrant upSNIP!you@workstation$ $ ansible -m ping allrouter | success &gt;&gt; {    \"changed\": false,    \"ping\": \"pong\"}client | success &gt;&gt; {    \"changed\": false,    \"ping\": \"pong\"}zone0 | success &gt;&gt; {    \"changed\": false,    \"ping\": \"pong\"}zone1 | success &gt;&gt; {    \"changed\": false,    \"ping\": \"pong\"}you@workstation$ ansible-playbook site.ymlSNIP!PLAY RECAP ********************************************************************client                     : ok=3    changed=1    unreachable=0    failed=0router                     : ok=11   changed=8    unreachable=0    failed=0zone0                      : ok=9    changed=7    unreachable=0    failed=0zone1                      : ok=9    changed=7    unreachable=0    failed=0RoutesOnce that is done we should be able to login to the client and see the default route has changed to 10.0.10.2.curtis# vagrant ssh clientSNIP!vagrant@client:~$ ip ro shdefault via 10.0.10.2 dev eth110.0.2.0/24 dev eth0  proto kernel  scope link  src 10.0.2.1510.0.10.0/24 dev eth1  proto kernel  scope link  src 10.0.10.10We should be able to ping 172.0.3.10 from the client, and it’ll go through the OpenBSD router, which is at 10.0.10.2.vagrant@client:~$ ping -c 1 -w 1 172.0.3.10PING 172.0.3.10 (172.0.3.10) 56(84) bytes of data.64 bytes from 172.0.3.10: icmp_seq=1 ttl=63 time=0.696 ms--- 172.0.3.10 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.696/0.696/0.696/0.000 msThe zone servers should have their default gateway set to the internal IP of the OpenBSD router.vagrant@zone0:~$ ip ro shdefault via 172.0.1.2 dev eth110.0.2.0/24 dev eth0  proto kernel  scope link  src 10.0.2.15172.0.1.0/24 dev eth1  proto kernel  scope link  src 172.0.1.10The router’s table ends up looking like this:$ route -nv show -inetRouting tablesInternet:Destination        Gateway            Flags   Refs      Use   Mtu  Prio Iface Labeldefault            10.0.2.2           UGS        0     6499     -     8 em0   DHCLIENT 1689910.0.2/24          link#1             UC         1        0     -     4 em010.0.2.2           52:54:00:12:35:02  UHLc       2     1060     -     4 em010.0.2.15          08:00:27:53:fe:30  UHLl       0        0     -     1 lo010.0.10/24         link#2             UC         0        0     -     4 em110.0.10.2          08:00:27:b5:8a:87  UHLl       0        0     -     1 lo0127/8              127.0.0.1          UGRS       0        4 32768     8 lo0127.0.0.1          127.0.0.1          UH         1        0 32768     4 lo0172.0.1/24         link#3             UC         2        0     -     4 em2172.0.1/24         172.0.1.2          UG         0        0     -    32 em2172.0.1.2          08:00:27:99:55:cd  UHLl       1        0     -     1 lo0172.0.1.10         08:00:27:e0:98:1b  UHLc       1       12     -     4 em2172.0.1.11         08:00:27:b6:ef:74  UHLc       1        9     -     4 em2172.0.3.10/32      172.0.1.10         UGP        0       27     -    32 em2172.0.3.10/32      172.0.1.11         UGP        0        0     -    32 em2224/4              127.0.0.1          URS        0        0 32768     8 lo0ConfigurationOnce Ansible has finished, the quagga ospfd.conf file looks like this on both zone servers:$ vagrant ssh zone1SNIP!vagrant@zone1:~$ sudo cat /etc/quagga/ospfd.conf! -*- ospf v10 -*-![]()hostname zone1password zebraenable password zebra![]()interface eth1![]()router ospf  router-id 172.0.1.11  network 172.0.1.11/24 area 0.0.0.0  network 172.0.3.0/24 area 0.0.0.0log file /var/log/quagga/ospfd.logThe ospfd.conf file on the OpenBSD box looks like this:$ sudo cat /etc/ospfd.confrouter-id 5.5.5.5area 0.0.0.0 {  interface em2 {  }}ospf neighbors$ sudo ospfctl show neighborID              Pri State        DeadTime Address         Iface     Uptime172.0.1.11      1   FULL/BCKUP   00:00:33 172.0.1.11      em2       02:19:25172.0.1.10      1   FULL/OTHER   00:00:33 172.0.1.10      em2       02:19:25The fun partNow with all that setup I can start pinging 172.0.3.11, an IP that is on both of the zone servers, then delete it from one and pings will switchover to the other zone server.vagrant@client:~$ cat ping.sh#!/bin/bashCOUNTER=0while true; do  if ping -c 1 -w 1 172.0.3.10 &gt; /dev/null; then  echo \"$COUNTER - 172.0.3.10 is alive\"  else  echo \"$COUNTER - 172.0.3.10 is dead\"  fi  let COUNTER=COUNTER+1  sleep 1donevagrant@client:~$ ./ping.sh0 - 172.0.3.10 is alive1 - 172.0.3.10 is alive2 - 172.0.3.10 is alive3 - 172.0.3.10 is alive4 - 172.0.3.10 is alive5 - 172.0.3.10 is alive6 - 172.0.3.10 is alive7 - 172.0.3.10 is alive8 - 172.0.3.10 is dead9 - 172.0.3.10 is alive10 - 172.0.3.10 is alive11 - 172.0.3.10 is alive12 - 172.0.3.10 is alive13 - 172.0.3.10 is alive^CThe part where we cat 172.0.3.10 being dead is when I drop it on the zone server currently being used by the OpenBSD router table.vagrant@zone1:~$ sudo ip add del 172.0.3.10/32 dev eth1Routing table looks like this now:# route -nv show -inet | grep \"172.0.3.10\"172.0.3.10/32      172.0.1.10         UGP        0        9     -    32 em2tcpdump and ospfDumping OSPF is pretty easy…# tcpdump -n -e -ttt -i em2 proto ospftcpdump: listening on em2, link-type EN10MBtcpdump: WARNING: compensating for unaligned libpcap packetsFeb 02 23:22:47.527622 08:00:27:b6:ef:74 01:00:5e:00:00:05 0800 78: 172.0.1.11 &gt; 224.0.0.5: OSPFv2-hello  44: rtrid 172.0.1.11 backbone [tos 0xc0] [ttl 1]Feb 02 23:22:47.527988 08:00:27:99:55:cd 01:00:5e:00:00:05 0800 86: 172.0.1.2 &gt; 224.0.0.5: OSPFv2-hello  52: rtrid 5.5.5.5 backbone dr 172.0.1.2 bdr 172.0.1.10 [tos 0xc0] [ttl 1]Feb 02 23:22:47.528136 08:00:27:99:55:cd 01:00:5e:00:00:05 0800 94: 172.0.1.2 &gt; 224.0.0.5: OSPFv2-ls_upd  60: rtrid 5.5.5.5 backbone [tos 0xc0] [ttl 1]SNIP!Issues  I could not get md5/crypt authentication working between the OpenBSD server running ospfd (part of OpenBGP) and Quagga. Not sure what the problem was, there are no examples I could find of getting this working. Ran out of time to test, so for now there is no authentication.Ping funSo that was a lot of work to do a simple ping. Kinda fun though. I haven’t done a lot with networking, but as I work more with systems such as OpenStack and containers and such I realize how important it is and how much I’m missing out by not knowing more. This was a nice, quick little foray into OSPF.I don’t know how much anyone will get from reading this post, but I did learn a few things myself, and perhaps at least it’ll show that getting a quick OSPF environment up for playing around with isn’t that difficult. (Though perhaps interacting between Linux and Quagga and OpenBSD and OpenBGP isn’t that easy, maybe I should have just gone 100% Ubuntu for this example.)I had hoped to learn a bit about ECMP as well, but that didn’t quite materialize. OpenBSD does support ECMP, but I’m not sure how it would work in this example. Certainly with OpenBSD it’s possible to route over multiple equal interfaces; most of the examples are for when you have multiple uplinks, eg. more than one Internet provider.Hopefully I don’t lead anyone down the wrong path with this blog post, but I like to write up little projects like this to help me determine what I learned and what I did not. Sometimes you get into something to find out how much you don’t know. :)"
    } ,
  
    {
      "title"    : "Tinc VPN (and Ansible)",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/01/29/tinc-vpn-ansible.html",
      "date"     : "2015-01-29 00:00:00 -0500",
      "content"  : "VPNs are a pain. The more I work with technology the more I see how some things like VPNs are just poor constructs and are, in a way, workarounds. Maybe that’s all IT is…workarounds. I don’t know. This is philosophical. At any rate, VPNs are difficult to setup, secure, and use, so sometimes it’s fun to play with technologies that try to make things easier, and I think tinc fits into that category. Would I use it in production? Probably not, but it’s still good to take a look at other implementations and see what can be done.TincI think the most interesting thing about tinc is that it sets up a mesh.  Regardless of how you set up the tinc daemons to connect to each other, VPN traffic is always (if possible) sent directly to the destination, without going through intermediate hops.Also, it’s easy to extend.  When you want to add nodes to your VPN, all you have to do is add an extra configuration file, there is no need to start new daemons or create and configure new devices or network interfacesSo those are a couple of good reasons to try out tinc.Tinc with AnsibleI was playing around with tinc a few days ago and came up with an Ansible playbook to setup a tinc VPN between several Ubuntu Trusty servers.Right now I have four virtual machines running in the same OpenStack infrastructure, which is a FlatDHCP system, so the tenants don’t share a private network…unless we set one up for them with Tinc. (This is not something you’d normally want for production systems…I don’t think, I’m just messin’ around.)curtis@parker:~/tinc-testing$ cat hoststrusty1 ansible_ssh_host=x.y.z.21trusty2 ansible_ssh_host=x.y.z.22trusty3 ansible_ssh_host=x.y.z.23trusty4 ansible_ssh_host=x.y.z.24I can ping all those with Ansible.curtis@parker:~/tinc-testing$ ansible -m ping alltrusty3 | success &gt;&gt; {    \"changed\": false,    \"ping\": \"pong\"}trusty4 | success &gt;&gt; {    \"changed\": false,    \"ping\": \"pong\"}trusty1 | success &gt;&gt; {    \"changed\": false,    \"ping\": \"pong\"}trusty2 | success &gt;&gt; {    \"changed\": false,    \"ping\": \"pong\"}And I can run the Ansible playbook that sets up one VPN, called “vpnone.”curtis@parker:~/tinc-testing$ ansible-playbook site.ymlSNIP!PLAY RECAP ********************************************************************trusty1                    : ok=17   changed=1    unreachable=0    failed=0trusty2                    : ok=17   changed=1    unreachable=0    failed=0trusty3                    : ok=17   changed=1    unreachable=0    failed=0trusty4                    : ok=17   changed=1    unreachable=0    failed=0Now Tinc has setup a tun0 on each node.curtis@parker:~/tinc-testing$ ansible -a \"ip ad sh tun0\" alltrusty3 | success | rc=0 &gt;&gt;4: tun0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 500    link/none    inet 10.0.0.23/24 scope global tun0       valid_lft forever preferred_lft forevertrusty2 | success | rc=0 &gt;&gt;4: tun0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 500    link/none    inet 10.0.0.22/24 scope global tun0       valid_lft forever preferred_lft forevertrusty1 | success | rc=0 &gt;&gt;4: tun0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 500    link/none    inet 10.0.0.21/24 scope global tun0       valid_lft forever preferred_lft forevertrusty4 | success | rc=0 &gt;&gt;4: tun0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 500    link/none    inet 10.0.0.24/24 scope global tun0       valid_lft forever preferred_lft foreverAnd I can ping nodes from one another. In this example I just ping 10.0.0.24 from all four nodes.curtis@parker:~/tinc-testing$ ansible -m shell -a \"ping -c 1 -w 1 10.0.0.24\" alltrusty2 | success | rc=0 &gt;&gt;PING 10.0.0.24 (10.0.0.24) 56(84) bytes of data.64 bytes from 10.0.0.24: icmp_seq=1 ttl=64 time=0.685 ms--- 10.0.0.24 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.685/0.685/0.685/0.000 mstrusty4 | success | rc=0 &gt;&gt;PING 10.0.0.24 (10.0.0.24) 56(84) bytes of data.64 bytes from 10.0.0.24: icmp_seq=1 ttl=64 time=0.029 ms--- 10.0.0.24 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.029/0.029/0.029/0.000 mstrusty3 | success | rc=0 &gt;&gt;PING 10.0.0.24 (10.0.0.24) 56(84) bytes of data.64 bytes from 10.0.0.24: icmp_seq=1 ttl=64 time=0.501 ms--- 10.0.0.24 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.501/0.501/0.501/0.000 mstrusty1 | success | rc=0 &gt;&gt;PING 10.0.0.24 (10.0.0.24) 56(84) bytes of data.64 bytes from 10.0.0.24: icmp_seq=1 ttl=64 time=0.702 ms--- 10.0.0.24 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.702/0.702/0.702/0.000 msBy default the playbook sets up “vpnone” but the name is configurable. But it only does one VPN right now, though tinc can support many.ubuntu@trusty1:~$ ls /etc/tinc/vpnone/hosts  rsa_key.priv  tinc.conf  tinc-down  tinc-upubuntu@trusty1:~$ ls /etc/tinc/vpnone/hoststrusty1  trusty2  trusty3  trusty4ubuntu@trusty1:~$ ls /etc/tinc/nets.boot  vpnoneFirewall rulesI should note that tcp/udp on port 655 needs to be open between the nodes.So there you goIf you want a VPN setup between several hosts, then tinc is a good way to do that, and if you want to use my playbook to automatically setup a single vpn between several hosts, then that’d be great. The playbook is not perfect, so if you see something you’d like changed, just let me know or send a pull request. :)Happy tincing!"
    } ,
  
    {
      "title"    : "Ansible Custom Facts",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/01/27/ansible-custom-facts.html",
      "date"     : "2015-01-27 00:00:00 -0500",
      "content"  : "(nasa image)I like Ansible. It’s not perfect but nothing is. Recently I was playing around with the tinc vpn system and wanted a way to set a custom fact per virtual machine based on the vms public ip address. I figured the best way to do that would be to setup a custom fact. It turns out there isn’t that much documentation on just how to do that, or I simply can’t find it. So I’m going to describe what I did in order to setup and use a custom fact.A bash scriptI just wanted a simple script to parse the ip address of the server and return a private ip based on the last octet of the address.This is the script I ended up using.#!/bin/bashIP_INDEX=`hostname -i | cut -f 4 -d \".\"`cat &lt;&lt;EOF{    \"vpn_ip\" : \"10.0.0.$IP_INDEX\"}EOFUsing ansible I put that file into the /etc/ansible/facts.d directory. Note it has to be executable, return json, and end with the .fact extension.ubuntu@trusty2:~$ cd /etc/ansible/facts.d/ubuntu@trusty2:/etc/ansible/facts.d$ ./tinc_facts.fact{    \"vpn_ip\" : \"10.0.0.22\"}Pretty simple so far.Loading the custom factSo, the first time that file is loaded onto the server Ansible setup won’t have it yet (unless it was loaded up in a previous role)  so if you do load the facts file in the same role you’ll have to use setup in the task list to load the custom fact.Here’s a snippet of my playbook. It’s creating the facts.d directory, copying over the script, and finally re-running setup with the ansible_local filter.I should be registering a variable and only reload the ansible_local if the facts.d scripts have changed on this particular run.- name: ensure custom facts directory exists  file: &gt;    path=/etc/ansible/facts.d    recurse=yes    state=directory- name: install custom fact module for IP address  template: &gt;    src=tinc_facts.sh.j2    dest=/etc/ansible/facts.d/tinc_facts.fact    mode=0755- name: reload ansible_local  setup: filter=ansible_localOtherwise, on the next Ansible run it’ll be available.Using the factIt seems as though the fact gets loaded into the ansible_local namespace under the name of the name of the fact script.$ ansible -m setup trusty2 | grep -A 4 ansible_local        \"ansible_local\": {            \"tinc_facts\": {                \"vpn_ip\": \"10.0.0.22\"            }        },I use the fact like this in my playbook.- name: ensure subnet ip address is properly set in tinc host file  lineinfile: &gt;    dest=/etc/tinc/{{ vpn_name }}/hosts/{{ ansible_hostname }}    line=\"Subnet = {{ ansible_local.tinc_facts.vpn_ip }}/32\"    create=yes  notify:    - restart tincAnd there we go, relatively simple custom facts for Ansible. Now we can do all sorts of silly things like make new ips based on old ips. haha."
    } ,
  
    {
      "title"    : "Non-Cartesian OpenStack",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/01/25/non-cartesian-openstack.html",
      "date"     : "2015-01-25 00:00:00 -0500",
      "content"  : "(Gaston Bachelard)Recently I gave the same talk twice, once at the Calgary OpenStack meetup and the other at a lunch and learn at a Vancouver based IT company.In the talk I discuss quite a few things, from trying to answer the question as to what OpenStack is, to my SimpleStack automated deployment, to armchair philosophy. These topics are related to a central theme of simplicty, complication, and complexity, especially around the idea that OpenStack is difficult to define and deploy.Some examples of thoughts on complexity:  At the recent Edmonton Ruby meetup I was talking to someone about what I do for a living (which is essentially work with OpenStack) and they said, “oh yeah, isn’t that that thing that’s really complicated to install?”  As I write this there is a small conversation on the OpenStack-operators list about what a small OpenStack installed (three to five compute nodes) would look like. (Nobody seems to really know.)  People who I follow on twitter who often say things like “computers are hard” or “how is this [computer] even working?” These are sentiments I can certainly relate to.OpenStack Structure ReformOpenStack has recently decided, or at least I think the technical committee has decided, that their current model of defining components of OpenStack as either integrated or incubating is not working.  First, the integrated release as it stands today is not a useful product for our users…Skilled operators aren’t deploying “the integrated release”: they are picking and choosing between components they feel are useful. New users, however, are presented with a complex and scary “integrated release” as the thing they have to deploy and manage: this inhibits adoption, and this inhibits the adoption of a slice of OpenStack that could serve their need.Non-CartesianWhile staying in Vancouver I went into a local used book store. I needed something to read. I’ve always wanted to know more about philosophy and who some great thinkers are and their main thesis. While wandering around the store and I found the book Fifty Key Contemporary Thinkers and decided that would be a good place to start.The first person discussed in that book is Bachelard. On page five, author Lechte provides this passage:  Whereas Descartes had argued that to progress, thought had to start from the point of clear and simple ideas, Bachelard charges that there are no simple ideas, only complexities.Lechte further describes this as Bachelard’s anti-Cartesian stance, and goes on to say that Bachelard suggests:  …that reality is never simple, and that in the history of science attempts to achieve simplicity…have invariable turned out to be over-simplification.Rejection of over-simplificationI think what has happened in OpenStack recently is the realization that trying to simplify the definition of what it is (ie. all of the “integrated components”) isn’t working and doesn’t work. And, I think, despite not knowing much about philosophy and epistemology, that they are moving in the same general philosophical direction as some of Bachelard’s work.While I’m not even worthy of the title of armchair philosopher, I do think it’s interesting to think of how we could consider technology in the same way that epistemologists think about science, and further, that trying to distill a (over?) complicated system like OpenStack down to a simple definition will likely fail.I end this blog post with a picture of the coordinates of an OpenStack system, so that you can easily find it. :)"
    } ,
  
    {
      "title"    : "OpenStack - Bridge not replying to ARP Requests",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/01/21/bridge-not-replying-arp.html",
      "date"     : "2015-01-21 00:00:00 -0500",
      "content"  : "(a super simple diagram to give you an idea of the network layout)I’ve been working on my SimpleStack OpenStack install lately. This design currently uses a FlatDHCP nova-network.I have a couple of hardware servers that I’m deploying it on at work, and I was having a ton of trouble with the public and flat interface being able to receive arp requests, but not sending them back.It turns out the answer was simple: the vlan coming down the port to the interface was tagged.This is part of the nova.conf file:flat_network_bridge = br100flat_interface = em2public_interface = em2Here’s some ARP traffic off the em2 interface.curtis@compute1:~$ sudo tcpdump -n -e -ttt -i br100 arp00:00:00.888602 5c:5e:ab:d9:e0:f0 &gt; ff:ff:ff:ff:ff:ff, ethertype 802.1Q (0x8100), length 60: vlan 601, p 0, ethertype ARP, Request who-has x.y.z.22 tell x.y.z.18, length 4200:00:00.900232 5c:5e:ab:d9:e0:f0 &gt; ff:ff:ff:ff:ff:ff, ethertype 802.1Q (0x8100), length 60: vlan 601, p 0, ethertype ARP, Request who-has x.y.z.22 tell x.y.z.18, length 4200:00:00.600620 5c:5e:ab:d9:e0:f0 &gt; ff:ff:ff:ff:ff:ff, ethertype 802.1Q (0x8100), length 60: vlan 601, p 0, ethertype ARP, Request who-has x.y.z.22 tell x.y.z.18, length 4200:00:00.703113 5c:5e:ab:d9:e0:f0 &gt; ff:ff:ff:ff:ff:ff, ethertype 802.1Q (0x8100), length 60: vlan 601, p 0, ethertype ARP, Request who-has x.y.z.22 tell x.y.z.18, length 42As you can see, we have “ethertype 802.1Q” and vlan 601. It took me quite a while to notice that, but once I did everything made sense.The arp requests were getting to em2, but because they were tagged with vlan 601 the server wouldn’t reply because it wasn’t setup to use that vlan.As soon as I added the 601 vlan and configured nova to use it, everything started working.curtis@compute1:~$ sudo modprobe 8021qcurtis@compute1:~$ sudo vconfig add em2 601Added VLAN with VID == 601 to IF -:em2:-curtis@compute1:~$ sudo ifconfig em2.601 upThe I changed the nova.conf file to look like this:flat_network_bridge = br100flat_interface = em2.601public_interface = em2.601Then I restarted nova-network.Finally I ended up with this bridge config:curtis@compute1:/etc/nova# sudo brctl showbridge name\tbridge id\t\tSTP enabled\tinterfacesbr100\t\t8000.782bcb617008\tno\t\tem2.601              vnet0virbr0\t\t8000.000000000000\tyesAnd I was able to have my flat network, which is all public IPs, working.Tagged and untagged, getting lost in detailsNow, there’s no reason (in this situation) for this port to have the vlan tagged on it, we just need that one network on the physical interface, but it was tagged and in this case it was just easier to go with that then to try to have any changes made on the network operator side.For a while we were chasing red herrings on this problem, and finally I just took a hard look at the arp packets and realized it was a tagged vlan. Obvious now, but it took a while to figure out just how obvious.It was a good feeling to realize the problem, fix it, and to get SimpleStack up and running on physical computers with public IPv4 addresses so that I can continue to experiment with small OpenStack installations. I’ve already realized this network design probably wouldn’t work for anyone who doesn’t have a lot of IPv4 addresses or can’t support IPv6. IPv6 would be so great for this. When will we really start using it…ISPs I’m looking at you…"
    } ,
  
    {
      "title"    : "Providing gems as debs using fpm and Package Cloud",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/12/13/building-debs-from-gems.html",
      "date"     : "2014-12-13 00:00:00 -0500",
      "content"  : "There are a lot of package managers.Some of them provide binary packages, which are pre-compiled. But others don’t, especially ones around programming language packages such as pip (python) and rubygems (ruby). Basically if you want to install a pip or gem package you have to have a build environment (ie. compilers, make, etc) installed on each instance or container in order to install the packages. That increases the size of images, and magnifies the attack surface of the instance/container, among other things.Before I get too far…Many of the pip and gem packages are available already as OS packages. Usually pip packages start with “python-“ and ruby gems with “ruby-“. But not all of them are packaged. Most of the common packages can be installed with apt/yum, but not all. I did a brief search and this page listed a few good differences between getting python libraries via pip and apt-get.SensuRecently I went to install sensu. Sensu provides a repository that provides the base sensu install. (It gets installed in /opt, as an “omnibus” package.)However, if you want to use plugins, many of them use the “sensu-plugins” gem, so that needs to be installed. The instructions say that you don’t need to install the sensu-plugins gem if you’re using the omnibus package.  If you have installed Sensu from the omnibus packages you can continue to installing the check-procs.rb plugin. Otherwise we need to install the sensu-plugin gem which has various helper classes used by many of the community plugins.Unfortunately, I believe that would mean ensuring plugins are called using the “embedded” ruby.Example:ubuntu@curtis-sensu:/etc/sensu/plugins$ /opt/sensu/embedded/bin/ruby check-procs.rbCheckProcs OK: Found 83 matching processesBut without the sensu-plugin gem…ubuntu@curtis-build:/etc/sensu/plugins$ ./check-procs.rb/usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require': cannot load such file -- sensu-plugin/check/cli (LoadError)\tfrom /usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require'\tfrom ./check-procs.rb:29:in `'&lt;/code&gt;&lt;/pre&gt;Most plugins will use the ruby pointed to by env.ubuntu@curtis-sensu:/etc/sensu/plugins$ head -1 check-procs.rb#!/usr/bin/env rubyI guess one question would be as to what environment plugin developers are writing plugins for--a base OS ruby or the embedded ruby in Sensu's omnibus. Probably the OS ruby would be specified, but that would depend on your developers and organization. The plugin files could be altered to use the embedded ruby.Thus, I decided to package up that gem.## Packaging gems with fpm[fpm](https://github.com/jordansissel/fpm) to the rescue!One of the first things I do at any job is setup a packaging server. Typically I'm the only one that uses it, but I still do it. :)For my build/packaging server I installed: * ruby-dev * gcc * makefrom the OS packages, and: * fpm * package_cloudfrom rubygems.I've been using [packagecloud](http://packagecloud.io) for a while now, nd it's a very handy and easy way to setup your own custom repository. Once you build a pacakge you can just use the package cloud CLI to push that package up to your own \"cloudy\" repo. But I'm getting ahead of myself...[fpm](https://github.com/jordansissel/fpm) is a great tool to easily build packages. You can tell the developer of fpm takes this very seriously by his all caps usage. :)&gt; It helps you build packages quickly and easily (Packages like RPM and DEB formats). FUNDAMENTAL PRINCIPLE: IF FPM IS NOT HELPING YOU MAKE PACKAGES EASILY, THEN THERE IS A BUG IN FPM.Using fpm we can build debian packages from gems.First, install the gems.ubuntu@curtis-build:~$ gem install --no-ri --no-rdoc --install-dir /tmp/gems sensu-pluginNext, build the debs using fpm.ubuntu@curtis-build:~$ mkdir debs; cd debsubuntu@curtis-build:~/debs$ find /tmp/gems/cache -name '*.gem' | xargs -rn1 fpm -d ruby -s gem -t debThat will build all the dependencies of sensu-plugin as well.ubuntu@curtis-build:~/debs$ lsrubygem-json_1.8.1_amd64.deb      rubygem-sensu-plugin_1.1.0_all.debrubygem-mixlib-cli_1.5.0_all.deb## Package cloudNext we can push those debs to package cloud.ubuntu@curtis-build:~/debs$ package_cloud push serverascode/custom/ubuntu/trusty rubygem-mixlib-cli_1.5.0_all.debI do that will all the gems in that dir. Now they show up in my [package cloud repo](https://packagecloud.io/serverascode/custom).Package cloud has some examples as to how configure your custom repos on your server, but for me it basically looks like this:ubuntu@curtis-sensu:/etc/apt/sources.list.d$ cat packagecloud_io_serverascode_custom_ubuntu.listdeb https://packagecloud.io/serverascode/custom/ubuntu/ trusty main(Don't forget the package cloud GPG key too.)I usually use Ansible to setup my servers, including configuring this repository. Below is a snippet from a playbook.- name: install the serverascode custom repository  apt_repository: &gt;    repo='deb https://packagecloud.io/serverascode/custom/ubuntu/ trusty main'    state=present- name: ensure the packagecloud reposity gpg key is installed  apt_key: &gt;    url=https://packagecloud.io/gpg.key    state=presentNow that the repo is setup, I can install my custom built packages.ubuntu@curtis-build:~/debs$ apt-cache policy rubygem-sensu-pluginrubygem-sensu-plugin:  Installed: 1.1.0  Candidate: 1.1.0  Version table: *** 1.1.0 0        500 https://packagecloud.io/serverascode/custom/ubuntu/ trusty/main amd64 Packages        100 /var/lib/dpkg/statusAnd, the ultimate goal, run the sensu plugin.ubuntu@curtis-build:/etc/sensu/plugins$ ./check-procs.rbCheckProcs OK: Found 68 matching processes## ConclusionSo, certainly there are still some issues here. I haven't used this process a lot yet, but I plan on doing a lot more with it because I don't want to be installing compilers and other build tools in every virtual machine or (especially) containers. Containers, as an example, should be as small as is reasonably possible, which I think is going to preclude any build tools. Also there are some security concerns with having build tools in the OS, and a few other infosec issues that I should refamiliarize myself with.At any rate, at some point most systems administrators will need to package their own debs or rpms, and using fpm and [Package Cloud](http://packagecloud.io) to package and provide repositories is quick and painless."
    } ,
  
    {
      "title"    : "Inaugural Vancouver OpenStack and Ansible Meetups",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/12/06/vancouver-openstack-ansible-meetups.html",
      "date"     : "2014-12-06 00:00:00 -0500",
      "content"  : "(openstack meetup at the plenty of fish “aquarium”)Last week I was lucky enough to be in Vancouver for the inaugural Vancouver OpenStack meetup, sponsored by Auro and Plenty of Fish, as well as a pre-planning session for the Vancouver Ansible meetup.Vancouver OpenStack MeetupThe first Vancouver OpenStack meetup was fun and had a good turnout.Being from Edmonton, where the tech community is a bit smaller, I would have been happy with a few people. In fact that last AWS meetup I went to in Edmonton had about four people, and I don’t really use AWS at all. I think there was about 30 people in attendance at the Vancouver meetup, so that is a great start.Unfortunately one of the speakers had to cancel on account of illness, but we still managed to fill up the time with a presentation from Sean Winn of EMC/Cloudscaling as well as from Auro’s Matt Mckinney.Sean gave an introduction to OpenStack, discussed how to contribute code, and also how to find a career related to OpenStack. (I’m hoping contributing has become easier because about 18 months ago someone spent six hours helping me through the process of getting setup in the workflow and we still didn’t complete it.)Matt discussed the results of the recent OpenStack User survey and what the results might mean for OpenStack users. His slides are here. What I got from the survey results is that OpenStack is being used in production more and there are still several pain points in doing that, especially around networking.Given the next OpenStack summit is in Vancouver, it’s good timing to get the OpenStack meetups going.Vancouver Ansible Meetup Pre-planningAgain, being from Edmonton, I’m not sure we could even support an Ansible meetup right now. But there were seven or eight people just at the Vancouver pre-planning session, so that was great.Part of the planning was deciding where to host the event, and luckly Vancouver has several companies and organizations that have good social spaces for this kind of meetup. In fact it’s almost becoming common for some of the larger startups to create a social space for events like this, eg. Hootsuite, OpenDNS, Plenty of Fish, etc.Once the planning session was over some of us hung out for a bit and just talked about technology, which is something I don’t actually do very much. We talked about all kinds of things, such as how FreeBSD might be due for a renaissance, about how SmartOS is actually pretty cool and probably doesn’t get used enough (Joyent open sourced their data center management tool as well), and a several other technologies. Also, pretty much every operator is in awe of Brendan Gregg.I also tried to get the group to call the meetup the “Vansible meetup” but I don’t think that was generally accepted. :)ConclusionWhile I will be in Edmonton most of the time I hope to make it out to a few of the Vancouver OpenStack and Ansible meetups over the next while.PS. Calgary OpenStack MeetupI think Calgary will be having an OpenStack meetup sometime in January, and I will likely make the three hour trek down to attend that too."
    } ,
  
    {
      "title"    : "SimpleStack",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/11/14/simple-openstack-simplestack.html",
      "date"     : "2014-11-14 00:00:00 -0500",
      "content"  : "(simple diagram of…simplestack network setup)For some reason I’m fascinated by the thought of simplifying an OpenStack installation. As a Linux/Unix systems administrator, I actually think OpenStack is great–it’s like this huge ball of every Linuxy system and service packed together to provide useful infrastructure via an API. It’s got everything: storage backends, database, queues, web servers, hypervisors, firewalls, image files, file systems, load balancers, plus all the networking components . You name it you can probably use it in OpenStack. Thus, as a systems administrator, you have to be able to use pretty much everything, and then automate it and operate it. From PXE boot to haproxy. It’s a challenge that’s for sure.Because OpenStack can be so varied and pluggable it means a large number of choices. However, generally speaking, people don’t want choices.ChoicesI read about this paper once a few years ago: Decisions and Revisions: The Affective Forecasting of Changeable Outcomes. In the study, students were asked to pick a photograph. One group had to pick it right away, and the other group could wait for up to four days to decide which print they wanted....students believed that having the opportunity to change their minds about which prints to keep would not influence their liking of the prints. However, those who had the opportunity to change their minds liked their prints less than those who did not.I’m not a scientist of any sort, but that paper has always stuck in my mind as being an example of how having too many choices makes things hard for people, and in fact can lead to regret. This is part of why IT workers are always searching for the mythical “best practice,” as in:How do I reduce the possibility of regretting what I did by narrowing my choices, and reducing the time it takes to make those choices?Linux distributionsI’m typing this post on Ubuntu 14.10. I didn’t build my own Linux Kernel or the entire system using Linux From Scratch (though maybe everyone should try that at least once). Most systems administrators and developers I’ve worked with haven’t even heard of the Linux File System Hierarchy.Linux distos are a way of reducing complexity; of making things simple. Linux distributions make all kinds of choices for us, especially with regards to packaging. When we use the OpenStack packages from Ubuntu 14.04 Canonical has made many decisions regarding OpenStack already. For most of us that is a good thing. (Though certainly many OpenStack operators build their own packages of OpenStack.) Choices are reduced, and for most, that’s good.Too many features?As I write this Amazon re:invent is going on. The size of AWS is difficult to grasp. It’s so huge. It has hundreds of features and services. Even billing is extremely complex. Some people and organizations need these features. Some are intimidated by them.With AWS complexity in mind, I can see how an infrastructure as a service (IaaS) provider such as Digital Ocean can be successful in a niche. They provide a vastly simplified system with few extra features:  Simple virtual machine sizes and pricing  Multiple regions across the world  One shared internal network per region  Snapshotting, copying snapshots to all regions  DNS  No volumesThat’s it. I think over time they will add additional features in order to try to grow, but I hope they don’t. I hope they stay in their niche and keep it as simple as possible. Nothing wrong with a successful company that doesn’t grow by 20% year after year (except in the stock market).SimpleStackI put together SimpleStack to use as an example of a “simple” OpenStack installation. I’ve got more work to do on it, so it’s just an example, but a fun place to start, if you’re into that kind of thing. :)SimpleStack will setup and install one controller and two compute nodes using Vagrant and Ansible.I did recently use it to provide a company with a test installation of OpenStack on two hardware servers, a sort of Proof-of-Concept, so I know it can work in a Virtualbox/Vagrant as well as a hardware environment, though the playbooks would need a couple of changes to work in a non-vagrant environment (something I’ll fix soon).Notable features, or lack thereof:  Automated, idempotent install using Ansible  One controller, multiple compute nodes  Front facing APIs using self-signed SSL  Multihost FlatDHCP networking  No private network (or private networks)  No Horizon (so no web gui)  No Cinder (so no volumes)  Not using Neutron, instead nova-networkAnd that’s it.Try it outIf you want to give it a test drive, just checkout the latest README file in the Github repo. As I mentioned, there is more work to do so it’s not perfect.I like that I can see exactly how many actions it takes to setup a simple OpenStack system, 55 for the controller and 18 for the compute nodes. Though I’m using templates so that reduces a lot of the tasks.curtis$ ansible-playbook site.ymlSNIP!PLAY RECAP ********************************************************************compute01                  : ok=18   changed=0    unreachable=0    failed=0compute02                  : ok=18   changed=0    unreachable=0    failed=0controller01               : ok=55   changed=0    unreachable=0    failed=0ConclusionThis is a fun little experiment to see how much OpenStack can be simplified, and, in effect try to make it look something like Digital Ocean, though even SimpleStack is missing some features comparitively.A few things I want to try out in the next “version”:  Separate the networks in the Vagrant config (all vms are on all networks at this point)  Add a shared private network ala DO, automatic floating IPs on the public interface for all instances  Find a way to figure out the interface’s purpose without using names like em2, eth0, ect, so that SimpleStack can work in any environment, not just Vagrant (that is, without changing the playbooks–there are a couple spots where “ansible_eth2” and the like is used, this is an Ansible variable issue I need to figure out)  IPv6?  A second region? Share glance images?  A simplified API with a command line tool?  Firewalling and routing with OpenBSD?Let me know if you have any ideas or see a mistake. :)"
    } ,
  
    {
      "title"    : "8 Months with So You Start (OVH) - A review",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/11/04/review-of-soyoustart-ovh.html",
      "date"     : "2014-11-04 00:00:00 -0500",
      "content"  : "For the last eight months I’ve had a server at So You Start (SYS) which is a division of OVH, and is meant to be an entry level service. This harware server (not virtual, hardware) costs me about $60 a month, has a eight cores (hyperthreading), 32GB of ECC ram, a 250MB Internet connection, and 2x2TB disks. Also it’s in Canada, which, as a Canadian, suits me great.But had to give it upI have recently given up this server, but only because I wasn’t using it, not because it didn’t work for me. In fact quite the opposite–it was working great. It was a great deal, real hardware (albeit a bit old), and it was up pretty much the whole darn time. I wish I could have kept it because it was a good deal, especially considering it’s probably 2/3 of the price most people (at least in Canada) pay for their cell phone.Lack of complaintsI really don’t have any complaints. Most of the below notes are minor issues or positives.      Always seem to be sold out of all the different models of hardware they have (I think this is because they are very popular). You’d almost have to write a script to check the site. This is the opposite of the OVH “business” site on which most of the servers are available within 120 seconds.        There is no KVM attached. This is pretty hair-raising if when updating the server. It seems they have a KVM service, which is $30 for 24 hours, and they say they will attach it within two hours. This would make it pretty hard to run a “real” production service off So You Start, but I don’t think that’s the point.        If you need any help from So You Start, you probably won’t get it in any timely fashion. But, I’m just guessing based on forum posts and reputation–I never asked for help from them so can’t say from experience.        This is my own fault, but I should have got an SSD based server. Spinning disks are so slow.        Billing: They could not do recurring credit card payments. You could pay for one or several months in a row, but not recurring. I always felt like if I forgot to pay, just once, my server would be deleted and given to someone else. I’d have preferred to have recurring CC payments, but you could do multi-months.        FTP backup: They do provide 100GB of FTP backup which I used. I gpg-zipped my backup files and uploaded them to the FTP server. If I was running a production service I’d find a way to go off-site with those backups, but it’s great that So You Start provides 100Gb of backup.        No 2-factor authentication to the admin panel, so if your login was compromised then your server is owned. I’m not even sure if the higher end OVH service provides this, not that they should differentiate on that feature. Every service should 2-factor their logins.        ECC memory: my server was ECC, but many of the So You Start models are not. On the range page they do incidate which servers are ECC and the rest are not. This is something to pay attention to if you want to avoid memory errors cropping up. As of this writing it seems like none of the models with SSDs are ECC.  root# dmidecode --type 16# dmidecode 2.12SMBIOS 2.7 present.Handle 0x0028, DMI type 16, 23 bytesPhysical Memory Array\tLocation: System Board Or Motherboard\tUse: System Memory\tError Correction Type: Single-bit ECC\tMaximum Capacity: 32 GB\tError Information Handle: Not Provided\tNumber Of Devices: 4root# dmidecode --type 17# dmidecode 2.12SMBIOS 2.7 present.Handle 0x002B, DMI type 17, 34 bytesMemory Device\tArray Handle: 0x0028\tError Information Handle: Not Provided\tTotal Width: 72 bits\tData Width: 64 bits\tSize: 8192 MB\tForm Factor: DIMM\tSet: None\tLocator: DIMM_A2\tBank Locator: BANK 0\tType: DDR3\tType Detail: Synchronous\tSpeed: 1600 MHz\tManufacturer: Samsung\tSerial Number: 378CA173\tAsset Tag: 9876543210\tPart Number: M391B1G73QH0-YK0\tRank: 2\tConfigured Clock Speed: 1600 MHzSNIP!UptimeI had this server for 8 months. It had ~18 minutes of downtime, about 15 of which were my own fault. Otherwise, I had pingdom monitoring it for the last four months, and it had 7 small outages, most less than a few seconds, and none that I recieved an email notice from Pingdom about.Hardware infomationI believe this system is the E3-SAT-3 model, though it wasn’t called that when I first purchased the service. Also I believe mine is actually ECC memory, though the E3-SAT-3 model isn’t supposed to be.For CPUs it has eight cores: four physical, four hyperthreading. I’m just showing one core below.processor\t: 7vendor_id\t: GenuineIntelcpu family\t: 6model\t\t: 58model name\t: Intel(R) Xeon(R) CPU E3-1245 V2 @ 3.40GHzstepping\t: 9microcode\t: 0x15cpu MHz\t\t: 3401.000cache size\t: 8192 KBphysical id\t: 0siblings\t: 8core id\t\t: 3cpu cores\t: 4apicid\t\t: 7initial apicid\t: 7fpu\t\t: yesfpu_exception\t: yescpuid level\t: 13wp\t\t: yesflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm ida arat epb xsaveopt pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase smep ermsbogomips\t: 6784.57clflush size\t: 64cache_alignment\t: 64address sizes\t: 36 bits physical, 48 bits virtualpower management:For memory:MemTotal:       32902800 kBMemFree:          474852 kBBuffers:          179312 kBCached:         29534464 kBSwapCached:        37392 kBActive:         18247988 kBInactive:       12369164 kBActive(anon):     551720 kBInactive(anon):   499556 kBActive(file):   17696268 kBInactive(file): 11869608 kBUnevictable:       65104 kBMlocked:           65104 kBSwapTotal:       5118972 kBSwapFree:        4963700 kBDirty:               200 kBWriteback:             0 kBAnonPages:        931436 kBMapped:           125728 kBShmem:            141504 kBSlab:            1149056 kBSReclaimable:     985140 kBSUnreclaim:       163916 kBKernelStack:        4608 kBPageTables:        32776 kBNFS_Unstable:          0 kBBounce:                0 kBWritebackTmp:          0 kBCommitLimit:    21570372 kBCommitted_AS:    7632684 kBVmallocTotal:   34359738367 kBVmallocUsed:      522704 kBVmallocChunk:   34358922952 kBHardwareCorrupted:     0 kBAnonHugePages:    397312 kBHugePages_Total:       0HugePages_Free:        0HugePages_Rsvd:        0HugePages_Surp:        0Hugepagesize:       2048 kBDirectMap4k:      166884 kBDirectMap2M:    33341440 kB&lt;/pre&gt;## Disk IOPretty much looks like a 2x2TB SATA mirror. Bleh. But again, expected. I usually take disk IO testing pretty seriously, but I just chucked this test together in a couple minutes, so take it as you will.# cat *.fio[random-read]rw=randreadsize=128mdirectory=/tmp/fio-testing/data[random-write1mb]rw=randwritesize=128mdirectory=/tmp/fio-testing/datadirect=1bs=1m[random-write]rw=randwritesize=128mdirectory=/tmp/fio-testing/datadirect=1Random read:# fio randread.fiorandom-read: (g=0): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=sync, iodepth=1fio-2.1.3Starting 1 processrandom-read: Laying out IO file(s) (1 file(s) / 128MB)SNIP!random-read: (groupid=0, jobs=1): err= 0: pid=10023: Mon Nov  3 21:22:14 2014  read : io=131072KB, bw=1009.3KB/s, iops=252, runt=129874msec    clat (usec): min=52, max=236152, avg=3955.39, stdev=4034.99     lat (usec): min=52, max=236152, avg=3955.79, stdev=4035.00    clat percentiles (usec):     |  1.00th=[   55],  5.00th=[   77], 10.00th=[  114], 20.00th=[  119],     | 30.00th=[  126], 40.00th=[ 2416], 50.00th=[ 3696], 60.00th=[ 4960],     | 70.00th=[ 6240], 80.00th=[ 7520], 90.00th=[ 8768], 95.00th=[ 9408],     | 99.00th=[10176], 99.50th=[10304], 99.90th=[18560], 99.95th=[31616],     | 99.99th=[96768]    bw (KB  /s): min=  506, max= 4221, per=98.48%, avg=993.71, stdev=325.57    lat (usec) : 100=6.09%, 250=27.71%, 500=0.05%, 750=0.04%, 1000=0.11%    lat (msec) : 2=2.93%, 4=15.37%, 10=45.83%, 20=1.77%, 50=0.06%    lat (msec) : 100=0.02%, 250=0.01%  cpu          : usr=0.28%, sys=0.98%, ctx=33070, majf=0, minf=28  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     issued    : total=r=32768/w=0/d=0, short=r=0/w=0/d=0Run status group 0 (all jobs):   READ: io=131072KB, aggrb=1009KB/s, minb=1009KB/s, maxb=1009KB/s, mint=129874msec, maxt=129874msecDisk stats (read/write):  sda: ios=32491/327, merge=0/601, ticks=128180/16464, in_queue=144612, util=98.77%Random writes 4k blocksize.# fio randwrite.fiorandom-write: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=sync, iodepth=1fio-2.1.3Starting 1 processrandom-write: Laying out IO file(s) (1 file(s) / 128MB)Jobs: 1 (f=1): [w] [100.0% done] [0KB/472KB/0KB /s] [0/118/0 iops] [eta 00m:00s]random-write: (groupid=0, jobs=1): err= 0: pid=11028: Mon Nov  3 21:45:39 2014  write: io=131072KB, bw=484491B/s, iops=118, runt=277028msec    clat (msec): min=1, max=227, avg= 8.45, stdev= 6.98     lat (msec): min=1, max=227, avg= 8.45, stdev= 6.98    clat percentiles (msec):     |  1.00th=[    3],  5.00th=[    4], 10.00th=[    5], 20.00th=[    6],     | 30.00th=[    7], 40.00th=[    8], 50.00th=[    8], 60.00th=[    9],     | 70.00th=[    9], 80.00th=[   10], 90.00th=[   10], 95.00th=[   11],     | 99.00th=[   40], 99.50th=[   43], 99.90th=[   74], 99.95th=[   95],     | 99.99th=[  147]    bw (KB  /s): min=  184, max=  591, per=100.00%, avg=473.57, stdev=48.38    lat (msec) : 2=0.28%, 4=7.89%, 10=85.24%, 20=2.71%, 50=3.70%    lat (msec) : 100=0.15%, 250=0.03%  cpu          : usr=0.15%, sys=0.73%, ctx=33273, majf=0, minf=27  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     issued    : total=r=0/w=32768/d=0, short=r=0/w=0/d=0Run status group 0 (all jobs):  WRITE: io=131072KB, aggrb=473KB/s, minb=473KB/s, maxb=473KB/s, mint=277028msec, maxt=277028msecDisk stats (read/write):  sda: ios=0/33913, merge=0/5241, ticks=0/329844, in_queue=329792, util=99.17%Rand writes 1mb blocksize:# fio randwrite1mb.fiorandom-write1mb: (g=0): rw=randwrite, bs=1M-1M/1M-1M/1M-1M, ioengine=sync, iodepth=1fio-2.1.3Starting 1 processrandom-write1mb: Laying out IO file(s) (1 file(s) / 128MB)Jobs: 1 (f=1)random-write1mb: (groupid=0, jobs=1): err= 0: pid=11513: Mon Nov  3 21:50:09 2014  write: io=131072KB, bw=75199KB/s, iops=73, runt=  1743msec    clat (msec): min=8, max=26, avg=13.56, stdev= 2.34     lat (msec): min=8, max=26, avg=13.61, stdev= 2.33    clat percentiles (usec):     |  1.00th=[ 8256],  5.00th=[ 9792], 10.00th=[10688], 20.00th=[11712],     | 30.00th=[12352], 40.00th=[13120], 50.00th=[13760], 60.00th=[14400],     | 70.00th=[14784], 80.00th=[15040], 90.00th=[15808], 95.00th=[16192],     | 99.00th=[19840], 99.50th=[26240], 99.90th=[26240], 99.95th=[26240],     | 99.99th=[26240]    bw (KB  /s): min=73142, max=76749, per=99.63%, avg=74922.67, stdev=1803.93    lat (msec) : 10=6.25%, 20=92.97%, 50=0.78%  cpu          : usr=0.40%, sys=0.69%, ctx=135, majf=0, minf=27  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     issued    : total=r=0/w=128/d=0, short=r=0/w=0/d=0Run status group 0 (all jobs):  WRITE: io=131072KB, aggrb=75199KB/s, minb=75199KB/s, maxb=75199KB/s, mint=1743msec, maxt=1743msecDisk stats (read/write):  sda: ios=0/241, merge=0/0, ticks=0/2876, in_queue=2900, util=92.62%## BandwidthUsing the speedtest-cli, which is very handy. Only found out about it today.Closest speedtest server:# speedtestRetrieving speedtest.net configuration...Retrieving speedtest.net server list...Testing from OVH Hosting ()...Selecting best server based on latency...Hosted by 3Men@Work (Montreal, QC) [2.38 km]: 39.06 msTesting download speed........................................Download: 358.12 Mbits/sTesting upload speed..................................................Upload: 58.36 Mbits/s&lt;/code&gt;&lt;/pre&gt;New York City!# speedtest --server 5029Retrieving speedtest.net configuration...Retrieving speedtest.net server list...Testing from OVH Hosting ()...Hosted by AT&amp;T (New York City, NY) [533.43 km]: 45.257 msTesting download speed........................................Download: 384.99 Mbits/sTesting upload speed..................................................Upload: 18.05 Mbits/s&lt;/code&gt;&lt;/pre&gt;My hometown ISP, Shaw. Good old monopolies.# speedtest --list | grep -i \"edmonton\"4242) Shaw Communications (Edmonton, AB, Canada) [2972.64 km]3050) Telus (Edmonton, AB, Canada) [2972.64 km]1051) Tera-byte Dot Com Inc (Edmonton, AB, Canada) [2972.64 km]root@vurt01:~# speedtest --server 4242Retrieving speedtest.net configuration...Retrieving speedtest.net server list...Testing from OVH Hosting ()...Hosted by Shaw Communications (Edmonton, AB) [2972.64 km]: 126.014 msTesting download speed........................................Download: 118.54 Mbits/sTesting upload speed..................................................Upload: 38.73 Mbits/s&lt;/code&gt;&lt;/pre&gt;Toronto:# speedtest --server 3575Retrieving speedtest.net configuration...Retrieving speedtest.net server list...Testing from OVH Hosting ()...Hosted by TELUS (Toronto, ON) [504.80 km]: 65.678 msTesting download speed........................................Download: 126.21 Mbits/sTesting upload speed..................................................Upload: 45.24 Mbits/s&lt;/code&gt;&lt;/pre&gt;## ConclusionThis server worked great for me. I had no problems. However, if I was thinking of hosting a production server, I would probably go with the OVH \"business\" servers which are more money, but have additional features like full-time KVM and a virtual network that can be setup between servers. Also I would like SSDs and ECC memory, a combination So You Start doesn't seem to provide. But, given my positive experience at So You Start, I would certainly give it a try at OVH, especially because they are in Quebec!"
    } ,
  
    {
      "title"    : "Also Blogging at flatlinesecurity.com",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/10/15/flatline-security.html",
      "date"     : "2014-10-15 00:00:00 -0400",
      "content"  : "I’ve decided to get back into information security. I used to do a lot more infosec related work, including being the security administrator for a large university. To that effect, I’m doing some security blogging over at Flatline Security and I’ll write posts about the things I learn about infosec as I refresh my knowledge. There are a couple of certifications I’m going to work on as well, such as the Certified Ethical Hacker qualification.I don’t know whether it’s a good idea to have two different but slightly related blogs, but I’m going to give it a try. serverascode.com will be for sysadmin related posts, and Flatline Security for information security associated writing. I think Flatline might have less technical posts occasionally, and more philosophical or high level concepts.I have a few ideas for blog posts here, and will soon have more time to write articles. If you have any ideas for what you’d like to see, let me know in the comments."
    } ,
  
    {
      "title"    : "Why OpenStack Swift is Great for Platform as a Service",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/08/23/swift-paas.html",
      "date"     : "2014-08-23 00:00:00 -0400",
      "content"  : "I’m a big fan of object storage. What is object storage? To me:  Object storage is a system that allows storing and retrieving files via a HTTP restful interface.Object storage is not a file system and doesn’t look anything like one.In terms of this blog post, I think object storage system should also be highly available, scalable, redundant, and durable. (Maybe some of those terms mean the same thing.)So we have these minimal requirements:  Store and retrieve files via restful HTTP interface  Highly available / Redundant  Scalable  DurableGuess what open source solution meets those requirements? OpenStack Swift.PaaS: Platform as a serviceI really like the concept of platform as a service (PaaS). However, like almost every difficult to define term in information technology, it’s become overloaded. PaaS could mean anything from Heroku to a git hook.Recently Ander Shafer wrote a blog post for Pivotal that suggests the “as a service” should be silent.  [Is a] platform is just how one deploys code?I would tend to agree with that sentiment, in that PaaS is really just a platform to which code is deployed and turned into a managed application (of some kind). Why make it more complicated than that? There are a lot of different PaaS implementations and I doubt there will ever be a canonical definition. What is PaaS for me could just be Mesos and Marathon, and what is PaaS for you could be an exact Heroku clone, or Cloud Foundry, or OpenShift, or a bash script kicked off by a git hook.Typically, however, there are certain things that people want out of a PaaS system:  Don’t want to run the servers  Want it to be scalable in some fashion  Highly available  Just push code (binary, git, zip, war…whatever)  Some state: database, file, nosql…thus backups!I don’t want to go much farther into what PaaS is…or isn’t (partly because I don’t know). I think it suffices enough to say that most users want the PaaS system they use to at least be scalable and highly available, and that is where OpenStack Swift can help.1. Replace shared and distributed file systems with SwiftMany applications require the ability to upload or generally create files and then be able to access them later. However, if you run multiple application servers each application server probably needs to access those files. Now you need some kind of shared or distributed file system, eg. NFS or Gluster (among others) respectfully. While those systems can scale up or out fairly far, at some point they might not be enough for a large system, or to be a data store for a PaaS.Enter Swift. If your application can be (re?)written to support object storage such as OpenStack Swift, and the PaaS being used supports it as a backend, then you don’t need a complicated distributed or shared filesystem–you can just store and retrieve files from object storage.2. Store Docker images in SwiftMany PaaS systems use, or will use, Docker as an important component for isolation. Docker is heavily dependendant on its registry server to manage images. Thus, any HA PaaS will also needs its Docker registry to be up as much as possible. Certainly Docker images could be cached on the servers that run Docker, but at some point a new image might need to be downloaded, and that has to come from a Docker registry.Not surprisingly, the Docker Registry server already supports using OpenStack Swift as a backend.3. Store code in SwiftThe code to be deployed has to come from somewhere. One example I have is with Apache Mesos and the Marathon framework. When you create an application in Marathon you can specify a uniform resource identifier (URI) where the application/code can be downloaded from. When a new instance of that application is created, for example when an application is scaled up, Marathon downloads the code from the URI(s).(screenshot from creating an application in the marathon webgui)Thus, in order to scale an application, at least in this example, the URI, typically a web server URL, needs to be up and running so that the code can be downloaded.If the application files are stored in an HA OpenStack Swift system, then that code should be availble to each application node to install. Certainly there are a lot of other ways this could be done, but I like the idea of using OpenStack Swift for this. Many of the examples given for deploying an application with Marathon show using Amazon’s S3 object storage system.I even see at least one try to back git with Swift.4. Store backups in SwiftDepending on your definition, perhaps magic backups are part of your PaaS requirements. I say magic only slightly facetiously. I do believe that if a PaaS supplies data stores that they are backed up properly.If you’re building your own PaaS, having backups automatically replicated and replicated off-site would be a nice thing to have. Thus, if you have an OpenStack Swift cluster that spans multiple data centers, then all you have to do is stick whatever backup files you have into Swift and they will be replicated across zones. Swift even has the ability to create a replication only network that can run over your data center interconnect so you can do quality of service (QoS) if desired. Because of Swift’s eventual consistency model it doesn’t even break the CAP theorem. Upload a file once and have it replicated across timezones? Yes please!ConclusionI am just learning about PaaS, but already I can see how valuable object storage can be to platform as a service, and not just in terms of replacing file systems. OpenStack Swift is a mature and well thought out system that can have storage servers and proxy servers down for maintenance–unplanned or otherwise–and objects will still be available. Further, it’s possible to upgrade Swift in-place without downtime. Also, as mentioned, it can scale across data centers.If you are deploying a PaaS system then I heavily suggest taking some time to consider deploying OpenStack Swift along side it as a datastore.I’m sure there are more good uses of object storage in a PaaS system, so if you think of anything let me know and I’ll add it.Notes      Like OpenStack in general, Swift can be a bit difficult to deploy. It depends on your skillset (ie. Linux). Start with Swift All-in-one to get an idea of the complexity. I don’t think deploying Swift, or OpenStack completely, is that bad, but maybe some do. Also I have a project called Swiftacular that can setup a multi-host test OpenStack Swift system using Vagrant and Ansible.        PaaS is not a silver bullet, but I do think it helps to consider a platform as just a platform, with all its parts, good, bad, and missing. This is a great blog post (and the previous one, Paas for Realists) which discuss what may or may not be missing from popular PaaS systems, and what people building PaaS systems should consider as requirements.        There are many people who believe that scaling applications is not as easy as some PaaS sytems make it out to be. Certainly it would be fairly straight forward to scale a stateless app up and down, but almost all apps have state of some kind, be it files or database or nosql entries, and at some point those data stores might fall over if there are too many application servers making requests. Also, same for the network.  "
    } ,
  
    {
      "title"    : "Provision and Configure OpenStack Instances in One Ansible Run",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/08/19/provision-openstack-instances-with-ansible.html",
      "date"     : "2014-08-19 00:00:00 -0400",
      "content"  : "In order to configure servers with tools like Ansible, they need to be up and running. These servers could be hardware systems in your data center, or, more likely, virtual machines running in any number of infrastructure-as-a-service (IaaS) providers, including a private OpenStack cloud (which is what I will be using here).The goal in this blog post is not just to provision the instances, but to provision and configure them in one Ansible playbook run. This process is somewhat complicated by the fact that you don’t know the ip address of the instance until it’s created, which typically means provisioning and configuration happens in at least two runs, perhaps even with different tools.Ansible modulesTwo Ansible modules make what I’m doing possible: nova_compute and add_host.nova_compute can create instances in most OpenStack clouds, and add_host can update information about hosts, such as the instances ansible_ssh_host ip address, and do so while the playbook is running.In combination with a custom inventory script these two modules can be used to provision and configure/converge instances in one playbook run.The playbookIn order to use nova_compute, at least in the way that I am using it, we need a minimum of five files, which are shown below. I’m using the Ansible roles model to configure the playbook.1) site.yml2) hosts3) group_vars/openstack_instances4) roles/openstack_instances/tasks/main.yml5) nova.py - a custom inventory script (ugly as it may be)Note that there is a github repository that has all the files used in this example.For simplicities sake, all this playbook is going to do is create all of the virtual machines in OpenStack, and then ping them via a role called common.nova.py and the hosts fileBecause of the custom inventory script I’m using, called nova.py (but not the same as what comes with Ansible by default), I have a somewhat unusual ansible hosts file, though it does follow the typical Ansible hosts file format.Below we can see there is a group called openstack_instances and there are four instances listed there, each with a flavor_id and group variable associated.curtis$ cat hosts[openstack_instances]lb flavor_id=1 group=load_balancersapp flavor_id=2 group=application_serversdb flavor_id=2 group=database_serversapp2 flavor_id=2 group=application_servers&lt;/pre&gt;The _nova.py_ script reads each line of the hosts file and sets up the flavor_id and group for each instance. If I run _nova.py_ I get json output that looks like this:curtis$ ./nova.py{    \"_meta\": {        \"hostvars\": {            \"app\": {                \"flavor_id\": \"2\"            },            \"app2\": {                \"flavor_id\": \"2\"            },            \"db\": {                \"flavor_id\": \"2\"            },            \"lb\": {                \"flavor_id\": \"1\"            }        }    },    \"application_servers\": {        \"hosts\": [            \"app\",            \"app2\"        ]    },    \"database_servers\": {        \"hosts\": [            \"db\"        ]    },    \"load_balancers\": {        \"hosts\": [            \"lb\"        ]    },    \"openstack_instances\": {        \"hosts\": [            \"lb\",            \"app\",            \"db\",            \"app2\"        ]    },    \"undefined\": {        \"hosts\": [            \"lb\",            \"app\",            \"db\",            \"app2\"        ]    }}As can be seen above, each inventory entry has it's _flavor_id_ meta variable set, as well as being put into a specific group.## The openstack_instances fileI've included an example openstack_instances file. Copy that to _group_vars/openstack_instances_ and fill it out with your OpenStack credentials.curtis$ ls group_vars/openstack_instances.examplegroup_vars/openstack_instances.examplecurtis$ cp group_vars/openstack_instances.examplegroup_vars/openstack_instancescurtis$ vi group_vars/openstack_instances # and enter your credentialsJust as an example, this is what the _nova_compute_ task looks like.curtis$ cat roles/openstack_instances/tasks/main.yml---- name: ensure instance exists in openstack  nova_compute:    state: present    login_username: \"{{ os_username }}\"    login_password: \"{{ os_password }}\"    login_tenant_name: \"{{ os_tenant_name }}\"    auth_url: \"{{ os_auth_url }}\"    region_name: \"{{ os_region_name }}\"    name: \"{{ inventory_hostname }}\"    image_id: \"{{ os_image_id }}\"    key_name: \"{{ os_key_name }}\"    flavor_id: \"{{ flavor_id }}\"    security_groups: default## ssh_configI should also mention that I have a gateway server setup in my OpenStack tenant and that is configured to be used with the private OpenStack network for that tenant. So while my OpenStack instances have a private ip address, they can still be accessed remotely via the ssh gateway server. Another option would be to run Ansible from inside the tenant.host openstack-gw   Hostname some.floating.ip.address   User ubuntuhost 10.2.*.*   ProxyCommand ssh -q openstack-gw netcat %h 22   User ubuntu## Run the playbookFinally, with all those files created and OpenStack credentials entered, we can run the playbook and create the instances if necessary.curtis$ ansible-playbook -i nova.py site.ymlPLAY [openstack_instances] ****************************************************GATHERING FACTS ***************************************************************ok: [db]ok: [lb]ok: [app]ok: [app2]SNIP!TASK: [common | debug msg=\"\"] ****************************ok: [app] =&gt; {    \"msg\": \"app\"}ok: [app2] =&gt; {    \"msg\": \"app2\"}ok: [db] =&gt; {    \"msg\": \"db\"}ok: [lb] =&gt; {    \"msg\": \"lb\"}PLAY RECAP ********************************************************************app                        : ok=12   changed=2    unreachable=0    failed=0app2                       : ok=12   changed=2    unreachable=0    failed=0db                         : ok=12   changed=2    unreachable=0    failed=0lb                         : ok=12   changed=2    unreachable=0    failed=0&lt;/pre&gt;If we run it again, it won't recreate the instances, because they already exist.curtis$ ansible-playbook -i nova.py site.ymlPLAY [openstack_instances] ****************************************************GATHERING FACTS ***************************************************************ok: [app]ok: [app2]ok: [db]ok: [lb]SNIP!PLAY RECAP ********************************************************************app                        : ok=11   changed=1    unreachable=0    failed=0app2                       : ok=11   changed=1    unreachable=0    failed=0db                         : ok=11   changed=1    unreachable=0    failed=0lb                         : ok=11   changed=1    unreachable=0    failed=0If I run nova list I can see the instances. (Note that in the private cloud I am using IPv6 addresses are also provided.)curtis$ nova list | grep \"app\\|db\\|lb\"| 11b0fcae-b296-44fd-9105-ea0edc8e796b | app   | ACTIVE | -          | Running     | private=10.2.0.160, 2605:fd00:4:1001:f816:3eff:fec3:b1b1                 || 79fa841a-6cc9-4542-b523-e7f55e13663d | app2  | ACTIVE | -          | Running     | private=10.2.0.127, 2605:fd00:4:1001:f816:3eff:fe9d:3287                 || 89085c7b-eee6-4cb2-b2ee-c2bf0cf7931a | db    | ACTIVE | -          | Running     | private=10.2.0.159, 2605:fd00:4:1001:f816:3eff:feda:41f                  || 6330e609-463a-47e7-93c3-864d04e5a840 | lb    | ACTIVE | -          | Running     | private=10.2.1.18, 2605:fd00:4:1001:f816:3eff:fe67:aa12                  |## ConclusionAt this point we have an ansible playook that can provision OpenStack instances, find out their ip address, and then configure them, all in one playbook run. Of course to do this we have to use a custom inventory script, but I don't mind that. Python is a great language to do things like this in, and since Ansible is written in Python it's much easier.## Issues- My custom nova.py script isn't very smart.- Depending on your OpenStack provider you may need to change the name of the network in the _set_fact_ task, which below is set to the name \"private.\" Sometimes different clouds have different default network names.- set_fact: ansible_ssh_host={{ nova.info.addresses.private[0].addr }}- Also, sometimes a particular instance won't come up. Just run the playbook again and as long as your OpenStack provider is working, everything should complete at some point. I found that sometimes it would take about a minute for some instances sshd to become available; that Ansible would have connection problems on the first run. Right now there is a 30 second pause in the playbook to try to account for that in a non-intelligent way.- Another problem could be that OpenStack can have many instances with the same name, so there could be 10 \"app2\" servers. _nova_compute_ is using the name rather than the OpenStack uuid to see if instances are instantiated. Something to look into because that could go sideways quickly.- Maybe there is a better way to do this? If so, let me know. :)"
    } ,
  
    {
      "title"    : "Basic Configuration of a Cisco 1000V CSR",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/07/17/basic-configuration-cisco-1000v-csr.html",
      "date"     : "2014-07-17 00:00:00 -0400",
      "content"  : "In a “previous post”:/2014/07/14/cisco-1000v-csr-libvirt-kvm.html I looked at installing the Cisco 1000V Cloud Services Router into a KVM environment. Now let’s do some basic configuration. Again I have to note that I’m not a professional Cisco network administrator by any stretch, and in fact it would be great if readers noticed mistakes or better ways of doing things and let me know in the comments. :)%{color:red}Issue%At this time TCP seems busted with CSR + KVM. I’m not sure why at this time. ICMP seems to work but not TCP.Troubleshooting steps so far:  Insert a simple Linux router: This worked fine, ~20 gbps with iperf  Try out this process in virtual box on a Windows computer: Also worked as expected  Try out on a Ubuntu 12.04.4 system–still no dice. I would have expected this to work as it is a supported environment for the CSR. Perhaps some configuration issue with KVM…  Also tried the qcow2 image instead of installing from the ISO, no joySo I’m not sure at this time what the issue is. Something to do with the CSR in this particular environment, be it misconfiguration on my part, or something else. I’ll update when I figure out what is going on. If you do try it and get the expected performance (such as 50000 kbps when the premium license is enabled) please let me know.Even though the KVM environments aren’t yet working, all the configuration information below should be Ok (I think). I did run this exact same process in Windows with Virtualbox and that worked fine. Also, like I said above, a simple Linux router works as well. However, when using the CSR and KVM, everything runs Ok, it’s just that TCP isn’t working. Very weird.My plan is to pass this post around a bit and see if anyone has any ideas.The environmentI’m working in an Ubuntu Trusty 14.04 KVM single host environment. I’m also using he default Openvswitch that comes with Trusty, and of course I’m using libvirt.For now I’ll connect the CSR to three virtual networks within the KVM host, two of which are managed by openvswitch and one by libvirt.# ovs-vsctl list-brcorebr0distbr0# brctl show  |grep virbr0virbr0\t\t8000.fe540021831d\tyes\t\tvnet0So the router will have three interfaces:  “virbr0” - Default libvirt network, Linux bridge  “corebr0” - Openvswitch  “distbr0” - OpenvswitchI altered the libvirt default network to start providing DHCP addresses at 192.168.122.3 instead of the default 192.168.122.2 so that the router can have 192.168.122.2 as it’s IP on the default virbr0 bridge.Basic configurationFirst, turn off domain lookups, otherwise the route will think mistyped commands are potential domains and spend time looking them up.Router&gt;enableRouter#conf tEnter configuration commands, one per line.  End with CNTL/Z.Router(config)#no ip domain-lookupHere we can see all the interfaces and their mapping. (Note that the MAC addresses might be different from other examples in this post–that’s Ok, I’ve done this same process several times.)Router#show platform software vnic-if interface-mapping------------------------------------------------------------- Interface Name        Driver Name         Mac Addr------------------------------------------------------------- GigabitEthernet3       virtio             5254.0064.67e2  GigabitEthernet2       virtio             5254.0039.2c47  GigabitEthernet1       virtio             5254.0006.685d -------------------------------------------------------------Configure the first interface.Router(config)#int gigabit 1Router(config-if)#ip address 192.168.122.2 255.255.255.0Router(config-if)#no shutdownRouter(config-if)#exitTry pinging 192.168.122.2.Router#ping 192.168.122.2Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 192.168.122.2, timeout is 2 seconds:![](!!!)Success rate is 100 percent (5/5), round-trip min/avg/max = 1/1/3 msConfigure the second interface.Router(config)#int gigabit 2Router(config-if)#ip address 10.100.0.1 255.255.255.0Router(config-if)#no shutdownRouter(config-if)#exitConfigure the third interface.Router(config)#int gigabit 3Router(config-if)#ip address 10.100.1.1 255.255.255.0Router(config-if)#no shutdownRouter(config-if)#exitShow the routes.Router#sh ip route   Codes: L - local, C - connected, S - static, R - RIP, M - mobile, B - BGP       D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area        N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2       E1 - OSPF external type 1, E2 - OSPF external type 2       i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2       ia - IS-IS inter area, * - candidate default, U - per-user static route       o - ODR, P - periodic downloaded static route, H - NHRP, l - LISP       a - application route       + - replicated route, % - next hop overrideGateway of last resort is not set      10.0.0.0/8 is variably subnetted, 4 subnets, 2 masksC        10.100.0.0/24 is directly connected, GigabitEthernet2L        10.100.0.1/32 is directly connected, GigabitEthernet2C        10.100.1.0/24 is directly connected, GigabitEthernet3L        10.100.1.1/32 is directly connected, GigabitEthernet3      192.168.122.0/24 is variably subnetted, 2 subnets, 2 masksC        192.168.122.0/24 is directly connected, GigabitEthernet1L        192.168.122.2/32 is directly connected, GigabitEthernet1Configure DHCP serversI’m going to put DHCP servers on gigabit 2 and gigabit 3, core and dist networks respectfully.Router(config)#ip dhcp pool coreRouter(dhcp-config)#network 10.100.0.0 /24Router(dhcp-config)#default-router 10.100.0.1Router(dhcp-config)#exitRouter(config)#ip dhcp excluded-address 10.100.0.1 10.100.0.100Check the pool.Router#sh ip dhcp poolPool core : Utilization mark (high/low)    : 100 / 0 Subnet size (first/next)       : 0 / 0  Total addresses                : 254 Leased addresses               : 0 Excluded addresses             : 100 Pending event                  : none 1 subnet is currently in the pool : Current index        IP address range                    Leased/Excluded/Total 10.100.0.1           10.100.0.1       - 10.100.0.254      0     / 100   / 254  Do the same on gigabit 3 with network 10.100.1.0/24.Add virtual machines to the networksI want to add a server onto the core network, expecting that the CSR will hand it a DHCP address.# virsh list  |grep core 4     core-srv1                      runningNow that that’s up, let’s see if it gets an IP from the router.Router#sh ip dhcp pool   Pool core : Utilization mark (high/low)    : 100 / 0 Subnet size (first/next)       : 0 / 0  Total addresses                : 254 Leased addresses               : 1 Excluded addresses             : 100 Pending event                  : none 1 subnet is currently in the pool : Current index        IP address range                    Leased/Excluded/Total 10.100.0.102         10.100.0.1       - 10.100.0.254      1     / 100   / 254 Router#sh arpProtocol  Address          Age (min)  Hardware Addr   Type   InterfaceInternet  10.100.0.1              -   5254.0013.bd53  ARPA   GigabitEthernet2Internet  10.100.0.2              1   b60d.1368.3044  ARPA   GigabitEthernet2Internet  10.100.0.101            1   5254.00db.4454  ARPA   GigabitEthernet2Internet  10.100.1.1              -   5254.001e.fceb  ARPA   GigabitEthernet3Internet  192.168.122.2           -   5254.0021.831d  ARPA   GigabitEthernet1As can be seen the KVM virtual machine received IP 10.100.0.101, and we can ping that from the router.Router#ping 10.100.0.101Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 10.100.0.101, timeout is 2 seconds:![](!!!)Success rate is 100 percent (5/5), round-trip min/avg/max = 1/1/2 msI can access the console of the vm using “virsh console”.# virsh console core-srv1I’ve also started a second virtual machine on the dist network.# virsh list | grep dist 13    dist-srv1                      runningFrom core-srv1 I can ping dist-srv1, and we are using the router to manage the traffic. Notice I am using -I due to the TCP issue I mention at the start of the post.ubuntu@core-srv1:~$ traceroute -I 10.100.1.101traceroute to 10.100.1.101 (10.100.1.101), 64 hops max  1   10.100.0.1  0.736ms  0.601ms  0.494ms   2   10.100.1.101  0.896ms  0.663ms  0.646ms ubuntu@core-srv1:~$ ip route showdefault via 10.100.0.1 dev eth0 10.100.0.0/24 dev eth0  proto kernel  scope link  src 10.100.0.101 We can see the mac addresses of the virtual machines in the routers arp table, including dist-srv1 and core-srv1.Router#sh arpProtocol  Address          Age (min)  Hardware Addr   Type   InterfaceInternet  10.100.0.1              -   5254.0013.bd53  ARPA   GigabitEthernet2Internet  10.100.0.101          172   5254.00db.4454  ARPA   GigabitEthernet2Internet  10.100.1.1              -   5254.001e.fceb  ARPA   GigabitEthernet3Internet  10.100.1.2             52   7e65.7502.9f41  ARPA   GigabitEthernet3Internet  10.100.1.101            4   5254.00ec.b81e  ARPA   GigabitEthernet3Internet  192.168.122.1          91   5254.0073.6d2e  ARPA   GigabitEthernet1Internet  192.168.122.2           -   5254.0021.831d  ARPA   GigabitEthernet1LicenseBy default the unlicensed CSR is pretty limited bandwidth-wise.Router#show platform hardware throughput levelThe current throughput level is 2500 kb/sUse the standard level evaluation license. (I think.)Router#conf tEnter configuration commands, one per line.  End with CNTL/Z.Router(config)#license boot level standard\t Feature Name:prem_evalSNIP!ACCEPT? (yes/[no]): yes*Jul 18 19:08:12.085: %LICENSE-6-EULA_ACCEPTED: EULA for feature prem_eval 1.0 has been accepted. UDI=CSR1000V:9L7UG7XECKE; StoreIndex=0:Built-In License Storage% use 'write' command to make license boot config take effect on next bootReload.Router#reloadAs can be seen, the license level has been changed, throughput is now 50000 kbps.SNIP!*Jul 18 19:10:51.476: %SMART_LIC-6-AGENT_READY: Smart Agent for Licensing is initialized*Jul 18 19:10:51.481: %VUDI-6-EVENT: [serial number: 9L7UG7XECKE], [vUDI: ], vUDI is successfully retrieved from license file*Jul 18 19:10:51.684: %IOS_LICENSE_IMAGE_APPLICATION-6-LICENSE_LEVEL: Module name = csr1000v Next reboot level = standard and License = prem_eval*Jul 18 19:10:52.557: %VXE_THROUGHPUT-6-LEVEL: Throughput level has been set to 50000 kbpsSNIP!Router#sh platform hardware throughput levelThe current throughput level is 50000 kb/sShutdownI just wanted to know that “virsh shutdown” will halt this router just fine, and this message will appear on the router console.*Jul 19 16:36:44.836: %IOSXE-5-PLATFORM: F0: shutdown: shutting down for system haltIperfI would like to run iperf between the hosts, but given the weird issue I’m seeing on KVM hosts, I can’t.ConclusionSo here we have a super basic virtual network setup that is routed by a Cisco 1000V CSR router. What I’ve done here could easily be accomplished with a simple firewall or other basic router, but I think it’s a good step towards getting a test environment setup with a Cisco router.Again, it must be noted that this is not working yet in a KVM environment (Ubuntu 12.04 or 14.04) but does work with Windows + Virtualbox."
    } ,
  
    {
      "title"    : "Using the Cisco 1000v CSR with Libvirt and KVM",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/07/14/cisco-1000v-csr-libvirt-kvm.html",
      "date"     : "2014-07-14 00:00:00 -0400",
      "content"  : "This blog post is going to cover installing and booting the Cisco 1000v Cloud Services Router with KVM on Ubuntu Trusty 14.04.It’s important to note that I haven’t touched a Cisco device in over a decade. At least until the blog post that is. I’ve been an open source based systems administrator for all of my career, and now I would like to learn a bit more about networking, which is considerably more closed source than I am used to. Thankfully, however, Cisco offers a free download of their cloud router.Getting the Cisco 1000v CSR images and ISOsFirst, if you don’t have an account with Cisco, then create one and login.To find the download page, this link might work. If if doesn’t work:  CSR Index page  Right menu, “Download Software for this Product”  Right panel, “Cisco Cloud Services outer 1000V”  Select “IOS XE Software”  Pick your download, ISO, OVA, qcow2, etc.In this post I’m going to work from the ISO.Install from ISO into qcow2 imageUsing the ISO process, we boot the ISO using KVM, which will automatically install the router software onto the disk image specified. Once that’s done you can boot the qcow2 image like any regular virtual machine.I’ve downloaded the ISO file.# ls *.isocsr1000v-universalk9.03.12.00.S.154-2.S-std.isoNext, create a backing image to install the software onto. It has to be at least 8 gigs.# qemu-img create -f qcow2 csr.img 8GFormatting 'csr.img', fmt=qcow2 size=8589934592 encryption=off cluster_size=65536 lazy_refcounts=off Now we can boot the ISO with KVM and set the backing image on which the ISO will install the 1000v router. Note that you need to hit a key pretty quickly to get to the GRUB boot menu.# kvm -boot d csr.img -enable-kvm -m 4096M -cpu Nehalem -smp 4,sockets=4,cores=1,threads=1 -cdrom csr1000v-universalk9.03.12.00.S.154-2.S-std.iso -nographicAfter hitting the “any” key, you should see the below. Select “Serial Console” and hit enter.   GNU GRUB  version 0.97  (639K lower / 3144696K upper memory) +-------------------------------------------------------------------------+ | CSR 1000V Virtual Console -- Wed-26-Mar-14-15:35                        |   | CSR 1000V Serial Console -- Wed-26-Mar-14-15:35                         | |                                                                         | |                                                                         | |                                                                         | |                                                                         | |                                                                         | |                                                                         | |                                                                         | |                                                                         | |                                                                         | |                                                                         |   +-------------------------------------------------------------------------+      Use the ^ and v keys to select which entry is highlighted.      Press enter to boot the selected OS, or 'c' for a command-line.At this point the ISO should install the 1000v CSR router into the csr.img qcow2 file, and some text should fly by, such as the below. It should only take a minute or two to install the CSR onto the hd image.Booting 'CSR 1000V Serial Console -- Wed-26-Mar-14-15:35'root (cd) Filesystem type is iso9660, using whole diskkernel /boot/csr1000v-universalk9.03.12.00.S.154-2.S-std.SPA.bin rw root=/dev/ram quiet console= max_loop=64 HARDWARE=virtual SR_BOOT=cdrom:csr1000v-universalk9.03.12.00.S.154-2.S-std.isopackage header rev 1 structure detectedCalculating SHA-1 hash...doneSHA-1 hash:        calculated   f51efee9:bfc569d7:9a732dee:4af42ccc:7003719d        expected     f51efee9:bfc569d7:9a732dee:4af42ccc:7003719dPackage type:0x7530, flags:0x0   [Linux-bzImage, setup=0x2e00, size=0x11706720]   [isord @ 0x6fe6c000, 0x10183000 bytes]SNIP!%IOSXEBOOT-4-BOOT_CDROM: (rp/0): Installing GRUB%IOSXEBOOT-4-BOOT_CDROM: (rp/0): Copying super package csr1000v-universalk9.03.12.00.S.154-2.S-std.SPA.bin%IOSXEBOOT-4-BOOT_CDROM: (rp/0): Expanding super package on /bootflash%IOSXEBOOT-4-BOOT_CDROM: (rp/0): Creating /boot/grub/menu.lst%IOSXEBOOT-4-BOOT_CDROM: (rp/0): CD-ROM Installation finished%IOSXEBOOT-4-BOOT_CDROM: (rp/0): Ejecting CD-ROM tray%IOSXEBOOT-4-BOOT_CDROM: (rp/0): Rebooting from HDSNIP!Press any key to continue.Press any key to continue.Press any key to continue.Again you have to be quick on hitting a key when the “Press any key to continue” message comes up. (Must be a better way to do this.) The vm is now booting off of the hd image instead of the ISO image.Select virtual console once more and the router should boot up and ask if we want to configure from a dialog.SNIP!cisco CSR1000V (VXE) processor with 2170596K/6147K bytes of memory.Processor board ID 9W17YZL34P21 Gigabit Ethernet interface32768K bytes of non-volatile configuration memory.4194304K bytes of physical memory.7774207K bytes of virtual hard disk at bootflash:.         --- System Configuration Dialog ---Would you like to enter the initial configuration dialog? [yes/no]:I enter no, though you may want to enter yes. The router continues booting, eventually stops and we can just hit enter to get the “Router&gt;” prompt.SNIP!*Jul 15 01:40:39.304: %VMAN-5-PACKAGE_SIGNING_LEVEL_ON_INSTALL: F0: vman:  Package 'csrmgmt.1_3_1.20140213_121708.ovaBuilding configuration...' for service container 'csr_mgmt' is 'Cisco signed', signing level cached on original install is 'Cisco signed'*Jul 15 01:40:39.745: Not MO, application name is csr_mgmt*Jul 15 01:40:39.745: %VIRT_SERVICE-5-INSTALL_STATE: Successfully installed virtual service csr_mgmt*Jul 15 01:40:39.748: IOS-FIREWALL-POLICY-SHIM-REGISTER[OK]*Jul 15 01:40:42.273: %CONFIG_CSRLXC-5-CONFIG_DONE: Configuration was applied and saved to NVRAM. See bootflash:/csrlxc-cfg.log for more details.# hit enter!Router&gt; #here we are at the router prompt!Now we have a full fledged Cisco router!At this point I usually kill the KVM process from another terminal, and the installation is complete. (Killing from another terminal is a little awkward, so let me know if you find a better way, which probably involves not using “-serial stdio”, but I like that it just streams in the terminal. Lots of ways to do this.)Using the image fileNow that the ISO has finished installing the software, we have an image file to work with.# du -hs csr.img1.6G\tcsr.img# file csr.imgcsr.img: QEMU QCOW Image (unknown version)Copy that image to /var/lib/libvirt/images.# cp csr.img /var/lib/libvirt/images/In /var/lib/libvirt/images, create a qcow2 snapshot.# qemu-img create -f qcow2 -b csr.img csr-01.imgFormatting 'csr-01.img', fmt=qcow2 size=8589934592 backing_file='csr.img' encryption=off cluster_size=65536 lazy_refcounts=off Now we can use that image with libvirt.Libvirt XML fileCreate an XML file like the below, ensuring to replace the image file location if necessary. I believe the CSR requires 4GB of memory and 4 VCPUS.Note that if you want the CSR to have more than one interface, you’ll have to add it to the XML file, and perhaps add networks to libvirt.# cat csr-01.xml   csr-01/name&gt;  4194304  4194304  4      hvm            Nehalem                      destroy  restart  restart      /usr/bin/kvm                                                                                                                               &lt;/domain&gt;&lt;/code&gt;&lt;/pre&gt;Define the vm and start it.# virsh define csr-01.xmlDomain csr-01 defined from csr-01.xml# virsh start csr-01Domain csr-01 startedYou can use \"virsh console csr-01\" to access the console. To exit (at least when using OSX's terminal) hit \"CTRL-5.\"# virsh console csr-01  Booting 'CSR1000v - packages.conf'root (hd0,0) Filesystem type is ext2fs, partition type 0x83kernel /packages.conf rw quiet root=/dev/ram console= max_loop=64 HARDWARE=virtual SR_BOOT=bootflash:packages.confCalculating SHA-1 hash...doneSHA-1 hash:        calculated   514e2831:94ee1441:2404193c:f37dac1e:4c196e19        expected     514e2831:94ee1441:2404193c:f37dac1e:4c196e19package header rev 1 structure detectedCalculating SHA-1 hash...doneSHA-1 hash:        calculated   134e1e2e:319d85c6:34a4d2b3:965dcb75:dc20afef        expected     134e1e2e:319d85c6:34a4d2b3:965dcb75:dc20afefPackage type:0x7531, flags:0x0   [Linux-bzImage, setup=0x2e00, size=0xd1c0720]   [isord @ 0x743b2000, 0xbc3d000 bytes]   SNIP!Check the interfaces and show version.SNIP!*Jul 15 02:46:04.712: %CONFIG_CSRLXC-5-CONFIG_DONE: Configuration was applied and saved to NVRAM. See bootflash:/csrlxc-cfg.log for more details.Router&gt;enableRouter#show int descInterface                      Status         Protocol DescriptionGi1                            admin down     down     Gi2                            admin down     down     Gi3                            admin down     down     Router#show versionCisco IOS XE Software, Version 03.12.00.S - Standard Support ReleaseCisco IOS Software, CSR1000V Software (X86_64_LINUX_IOSD-UNIVERSALK9-M), Version 15.4(2)S, RELEASE SOFTWARE (fc2)Technical Support: http://www.cisco.com/techsupportCopyright (c) 1986-2014 by Cisco Systems, Inc.Compiled Wed 26-Mar-14 21:09 by mcpreSNIP!Again, you can hit \"CTRL-5\" to exit \"virsh console.\"## ConclusionNow that we have a base image and a working libvirt XML file, we can create all kinds of interesting network configurations and learn how to use a Cisco router without actually having any Cisco hardware. Nothing is stopping you from booting several CSR virtual machines and configuring them to work together."
    } ,
  
    {
      "title"    : "Nested Virtualization and KVM",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/07/09/nested-virtualization-kvm.html",
      "date"     : "2014-07-09 00:00:00 -0400",
      "content"  : "Recently I was working on getting virtual machines to boot inside a virtual machine running on Ubuntu Trusty 14.04 (insert inception joke here). However I was getting an error:libvirtError: internal error no supported architecture for os type 'hvm'In my case that mean that nested virtualization was not turned on in the operating system that is running on the baremetal. (I should note there’s another good blog post here that describes turning on nested virtualization in KVM as well.)To enable it, I removed the kvm_intel module and re-added it with nested=1.# modprobe -r kvm_intel# modprobe kvm_intel nested=1Now I should see a “Y” after the below command. (Why it reports a “Y” I don’t understand.)# cat /sys/module/kvm_intel/parameters/nestedYAnd we are good to go with nested virtualization.I also setup this file:# cat /etc/modprobe.d/kvm_intel.conf options kvm_intel nested=1So that it will be added on a reboot.Happy nesting!"
    } ,
  
    {
      "title"    : "Fake OpenStack with Dwarf",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/07/07/dwarf-openstack.html",
      "date"     : "2014-07-07 00:00:00 -0400",
      "content"  : "Dwarf is this really cool little project by Juerg Haefliger that provides a subset of the OpenStack APIs to use libvirt on a single host. For some context, here’s the original email that was sent to the OpenStack list. What it does is allow you to use manage a single libvirt host as though it were OpenStack, ie. use nova, glance, and keystone commands to manage libvirt virtual machines.Why?For some reason I find faking APIs really interesting. I guess a better word than faking would be “compatability” but really what is going on is APIs are being faked. For example, OpenStack has always, as far as I know, provided some Amazon Web Services (AWS) compatibility. OpenStack Swift also can provide Amazon S3 API compatibility. Another example is Cloudscaling providing a GCE compatabile API for OpenStack.I think fake APIs also suggests that a certain application or service is becoming popular, and so having a little fake OpenStack subset API using Dwarf is a compliment to OpenStack. Also it can help in terms of understanding how OpenStack works. Do you wonder what the keystone catalog does? Well, now you can mess around with it in Dwarf and find out.CaveatsAs Haefliger says in the README for Dwarf:  No authentication!  Just for one host  A subset of OpenStack commands  Serialized and blockingInstall DwarfFirst install the Dwarf PPA. I’m running this on Ubuntu Precise 12.04, which is itself a virtual machine, and thus we’ll be doing nested virtualization, which may need to be turned on in some hosts.ubuntu@dwarf:~$ sudo apt-add-repository ppa:juergh/dwarf Then install Dwarf.ubuntu@dwarf:~$ sudo apt-get install dwarfNow we have a dwarf command.ubuntu@dwarf:~$ which dwarf/usr/bin/dwarfWe can start dwarf with service.root@dwarf:~# service dwarf startThat starts a few processes listening, openstack identity, openstack compute, and openstack images, ie. keystone, nova, and glance respectively.root@dwarf:/etc# netstat -antActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address           Foreign Address         State      tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN     tcp        0      0 127.0.0.1:35357         0.0.0.0:*               LISTEN     tcp        0      0 127.0.0.1:8774          0.0.0.0:*               LISTEN     tcp        0      0 127.0.0.1:9292          0.0.0.0:*               LISTEN     tcp        0      0 192.168.122.77:22       192.168.122.1:38835     ESTABLISHEDtcp6       0      0 :::22                   :::*                    LISTEN Need python-novaclient, python-keystoneclient too. I’m going to use pip to get more recent versions of these commands. I guess glance comes from glance-client? Weird.root@dwarf:~# apt-get install python-piproot@dwarf:~# pip install python-novaclientroot@dwarf:~# nova --version2.17.0root@dwarf:~# glance --version0.12.0root@dwarf:~# keystone --version0.9.0Use DwarfCreate a default openstack rc file and source it. Again note there is no real authentication.root@dwarf:~# cat dwarfrc export OS_AUTH_URL=http://127.0.0.1:35357/v2.0/export OS_COMPUTE_API_VERSION=1.1export OS_REGION_NAME=dwarf-regionexport OS_TENANT_NAME=dwarf-tenantexport OS_USERNAME=dwarf-userexport OS_PASSWORD=dwarf-passwordroot@dwarf:~# . dwarfrcAnd finally we can run some nova commands.root@dwarf:~# nova list+----+------+--------+----------+| ID | Name | Status | Networks |+----+------+--------+----------++----+------+--------+----------+root@dwarf:~# nova flavor-list+-----+-----------------+-----------+------+-----------+------+-------+-------------+|  ID |       Name      | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor |+-----+-----------------+-----------+------+-----------+------+-------+-------------+| 100 | standard.xsmall | 512       | 10   | N/A       |      | 1     |             || 101 | standard.small  | 768       | 30   | N/A       |      | 1     |             || 102 | standard.medium | 1024      | 30   | N/A       |      | 1     |             |+-----+-----------------+-----------+------+-----------+------+-------+-------------+root@dwarf:~# keystone catalogWARNING:keystoneclient.httpclient:Failed to retrieve management_url from tokenService: image+-------------+----------------------------+|   Property  |           Value            |+-------------+----------------------------+|  publicURL  | http://127.0.0.1:9292/v1.0 ||    region   |        dwarf-region        ||   tenantId  |            1000            ||  versionId  |            1.0             || versionInfo | http://127.0.0.1:9292/v1.0 || versionList |   http://127.0.0.1:9292    |+-------------+----------------------------+Service: compute+-------------+---------------------------------+|   Property  |              Value              |+-------------+---------------------------------+|  publicURL  | http://127.0.0.1:8774/v1.1/1000 ||    region   |           dwarf-region          ||   tenantId  |               1000              ||  versionId  |               1.1               || versionInfo |    http://127.0.0.1:8774/v1.1   || versionList |      http://127.0.0.1:8774      |+-------------+---------------------------------+For some reason the latest python-glanceclient was broken in my install. Not sure if it’s something I did wrong or what, but I ended up using 0.12.0. Note the glance that comes with 12.04 does not have the image-create command.root@dwarf:/tmp# pip install python-glanceclient==0.12.0root@dwarf:/tmp# which glance/usr/local/bin/glanceroot@dwarf:/tmp# glance --version0.12.0root@dwarf:~# glance indexNo handlers could be found for logger \"keystoneclient.httpclient\"ID                                   Name                           Disk Format          Container Format     Size          ------------------------------------ ------------------------------ -------------------- -------------------- --------------96b8b4cc-bf45-4dc2-add0-c6d0fc96aec4                                                                                        d0a3d8f7-d336-40d1-b548-fbb5e5e01d8f                                                                                        ced791fc-bd11-4d91-9eb6-3fe892dd2a6d                                                                                        40bee026-03a4-4020-88bc-bc0acf9465a6                                                                                        0a29e5fc-fb0b-487d-b957-f4fd296d71b1                                                                                        f51efef5-fe73-4957-94c0-bf94038a2685 Interestingly running glance image-create with no options with dwarf creates empty images. I deleted all those and also added a cirros image.Download the Cirros image. I’m using Cirros because I’m running Dwarf inside a virtual machine, so have limited resources.root@dwarf:~# wget http://download.cirros-cloud.net/0.3.2/cirros-0.3.2-x86_64-disk.imgAdd that image to Dwarf using glance.root@dwarf:~# glance image-create --name \"Cirros\" --file cirros-0.3.2-x86_64-disk.img Now it’s in glance.root@dwarf:~# glance image-listNo handlers could be found for logger \"keystoneclient.httpclient\"+--------------------------------------+--------+-------------+------------------+----------+--------+| ID                                   | Name   | Disk Format | Container Format | Size     | Status |+--------------------------------------+--------+-------------+------------------+----------+--------+| 56105cdc-00d8-4e69-beae-fbe20abcbe36 | Cirros |             |                  | 13167616 | ACTIVE |+--------------------------------------+--------+-------------+------------------+----------+--------+Add a keypair. (Though cirros won’t use it.)root@dwarf:~# nova keypair-add --pub-key ~/.ssh/id_rsa.pub rootroot@dwarf:~# nova keypair-list+------+-------------------------------------------------+| Name | Fingerprint                                     |+------+-------------------------------------------------+| root | 7f:21:e1:9b:ee:3d:84:89:a5:bc:c1:3e:79:20:e5:c0 |+------+-------------------------------------------------+If you are nesting, ie. a vm inside a vm, before going further edit the default libvirt network. Change 192.168.122.0/24 to some other network, such as 10.0.0.0/24. 192.168.122.0/24 will likely already be in use and the default network won’t start, and neither will libvirt based vms.root@dwarf:~# virsh net-edit default# Edit to look like this:  default  7fd26ceb-ed87-7887-198e-d9cbc4759b70                      &lt;/pre&gt;Start that network.root@dwarf:~# virsh net-start defaultsetlocale: No such file or directoryNetwork default startedroot@dwarf:~# brctl showbridge name bridge id   STP enabled interfacesvirbr0    8000.525400a6a92a yes   virbr0-nicLooks good.Next: boot a vm. root@dwarf:~# nova boot --flavor 100 --image 56105cdc-00d8-4e69-beae-fbe20abcbe36 --key_name root test1ERROR: 'NoneType' object has no attribute 'get'I see there is an error reported, but the vm does indeed get started up. Something to look into, might have to do with the version of nova client being used. I certainly had some trouble with the glance client.That vm is now running:root@dwarf:~# virsh listsetlocale: No such file or directory Id Name                 State----------------------------------  2 dwarf-00000003       runningroot@dwarf:~# nova list+--------------------------------------+-------+--------+------------+-------------+-------------------+| ID                                   | Name  | Status | Task State | Power State | Networks          |+--------------------------------------+-------+--------+------------+-------------+-------------------+| 0e8a17bf-4c99-455d-87d6-4eb8d35af1d7 | test1 | ACTIVE | N/A        | N/A         | private=10.0.0.28 |+--------------------------------------+-------+--------+------------+-------------+-------------------+root@dwarf:~# cat /var/lib/libvirt/dnsmasq/default.leases 1404773235 52:54:00:2c:94:14 10.0.0.28 * 01:52:54:00:2c:94:14root@dwarf:~# ping -c 1 -w 1 10.0.0.28PING 10.0.0.28 (10.0.0.28) 56(84) bytes of data.64 bytes from 10.0.0.28: icmp_req=1 ttl=64 time=0.694 ms--- 10.0.0.28 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.694/0.694/0.694/0.000 msssh into the vm...login \"cirros\", password \"cubswin:)\"root@dwarf:~# ssh cirros@10.0.0.28The authenticity of host '10.0.0.28 (10.0.0.28)' can't be established.RSA key fingerprint is 44:8a:7a:ce:25:d6:f6:aa:2f:98:bb:c3:ec:a2:e8:2a.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added '10.0.0.28' (RSA) to the list of known hosts.cirros@10.0.0.28's password: $ ifconfig eth0eth0      Link encap:Ethernet  HWaddr 52:54:00:2C:94:14            inet addr:10.0.0.28  Bcast:10.0.0.255  Mask:255.255.255.0          inet6 addr: fe80::5054:ff:fe2c:9414/64 Scope:Link          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1          RX packets:92 errors:0 dropped:0 overruns:0 frame:0          TX packets:70 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:1000           RX bytes:9758 (9.5 KiB)  TX bytes:7502 (7.3 KiB)$ cat /proc/meminfo | head -1MemTotal:         503476 kBIn fact Dwarf does at least one nice thing for us in that it'll determine the virtual machines IP address automatically, which libvirt doesn't do.## ConclusionUsing Dwarf we can boot instances using libvirt and qemu. The [code is out there on github](https://github.com/juergh/dwarf) ready to be hacked on and improved or forked for your own purposes. Once you get your host configured properly and the right nova and glance clients installed it seems to work well.Thanks [Juerg Haefliger](https://github.com/juergh) for Dwarf. :)"
    } ,
  
    {
      "title"    : "Add SSD as cache to ZFS on Linux",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/07/03/add-ssd-cache-zfs.html",
      "date"     : "2014-07-03 00:00:00 -0400",
      "content"  : "I just physically added a SSD into my home backup server and I would like to configure it as a ZFS l2arc cache device. Doing so will mean new files will be written to the SSD first, then the spinning disk later, and that recently used files will be accessed via the SSD drive instead of the slower spinning disks. Depending on your workload, this should make most disk operations faster.In my case, the SSD is /dev/sdc.root@storage:/home/curtis# lsblk /dev/sdcNAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTsdc      8:32   0 232.9G  0 disk ├─sdc1   8:33   0 232.9G  0 part └─sdc9   8:41   0     8M  0 part It’s a 240GB Samsung 840.root@storage:/home/curtis# smartctl -a /dev/sdcsmartctl 6.2 2013-07-26 r3841 [x86_64-linux-3.13.0-24-generic] (local build)Copyright (C) 2002-13, Bruce Allen, Christian Franke, www.smartmontools.org=== START OF INFORMATION SECTION ===Model Family:     Samsung based SSDsDevice Model:     Samsung SSD 840 SeriesSerial Number:    S14GNEACC11801KLU WWN Device Id: 5 002538 5500d4588Firmware Version: DXT06B0QUser Capacity:    250,059,350,016 bytes [250 GB]Sector Size:      512 bytes logical/physicalRotation Rate:    Solid State DeviceDevice is:        In smartctl database [for details use: -P show]ATA Version is:   ACS-2, ATA8-ACS T13/1699-D revision 4cSATA Version is:  SATA 3.1, 6.0 Gb/s (current: 3.0 Gb/s)Local Time is:    Wed Jul  2 18:57:51 2014 MDTSMART support is: Available - device has SMART capability.SMART support is: DisabledSMART Disabled. Use option -s with argument 'on' to enable it.(override with '-T permissive' option)Now let’s simply add it to the tank zpool. Note that “-f” means force, as this SSD was previously used with other file systems.root@storage:/home/curtis# zpool add -f tank cache sdcHasn’t even been used!root@storage:/home/curtis# zpool iostat -v tank               capacity     operations    bandwidthpool        alloc   free   read  write   read  write----------  -----  -----  -----  -----  -----  -----tank         757G   635G      0      0    262  1.25K  mirror     757G   635G      0      0    262  1.25K    sdb         -      -      0      0  1.35K  1.77K    sdd         -      -      0      0  1.82K  1.77Kcache           -      -      -      -      -      -  sdc        400K   233G      0      0    595  1.76K----------  -----  -----  -----  -----Fio is my favorite disk performance tool, so lets use that to test the new cache device. Note that this is just a basic, example fio test. Interestingly ZFS doesn’t support “direct=1”.root@storage:/home/curtis# cat random-rw.fio [random_rw]rw=randrwsize=1024mdirectory=/tank/buproot@storage:/home/curtis# fio random-rw.fio random_rw: (g=0): rw=randrw, bs=4K-4K/4K-4K/4K-4K, ioengine=sync, iodepth=1fio-2.1.3Starting 1 processrandom_rw: Laying out IO file(s) (1 file(s) / 1024MB)Jobs: 1 (f=1): [m] [99.1% done] [10468KB/10444KB/0KB /s] [2617/2611/0 iops] [eta 00m:01s]random_rw: (groupid=0, jobs=1): err= 0: pid=1932: Wed Jul  2 18:43:00 2014  read : io=524704KB, bw=5017.9KB/s, iops=1254, runt=104567msec    clat (usec): min=3, max=3444, avg=16.18, stdev=34.60     lat (usec): min=3, max=3444, avg=16.42, stdev=34.61    clat percentiles (usec):     |  1.00th=[    5],  5.00th=[    7], 10.00th=[    8], 20.00th=[    9],     | 30.00th=[   10], 40.00th=[   11], 50.00th=[   12], 60.00th=[   13],     | 70.00th=[   14], 80.00th=[   15], 90.00th=[   36], 95.00th=[   41],     | 99.00th=[   53], 99.50th=[   63], 99.90th=[  189], 99.95th=[  462],     | 99.99th=[ 1848]    bw (KB  /s): min= 3200, max=17824, per=99.26%, avg=4979.99, stdev=1839.92  write: io=523872KB, bw=5009.1KB/s, iops=1252, runt=104567msec    clat (usec): min=9, max=5490, avg=772.32, stdev=244.37     lat (usec): min=9, max=5490, avg=772.62, stdev=244.38    clat percentiles (usec):     |  1.00th=[  213],  5.00th=[  318], 10.00th=[  414], 20.00th=[  620],     | 30.00th=[  692], 40.00th=[  748], 50.00th=[  804], 60.00th=[  852],     | 70.00th=[  916], 80.00th=[  972], 90.00th=[ 1020], 95.00th=[ 1048],     | 99.00th=[ 1112], 99.50th=[ 1144], 99.90th=[ 2928], 99.95th=[ 3152],     | 99.99th=[ 3440]    bw (KB  /s): min= 3408, max=17048, per=99.27%, avg=4972.41, stdev=1761.31    lat (usec) : 4=0.01%, 10=13.65%, 20=29.22%, 50=6.90%, 100=0.48%    lat (usec) : 250=0.93%, 500=5.35%, 750=14.01%, 1000=22.28%    lat (msec) : 2=7.04%, 4=0.12%, 10=0.01%  cpu          : usr=1.59%, sys=8.40%, ctx=132043, majf=0, minf=27  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     issued    : total=r=131176/w=130968/d=0, short=r=0/w=0/d=0Run status group 0 (all jobs):   READ: io=524704KB, aggrb=5017KB/s, minb=5017KB/s, maxb=5017KB/s, mint=104567msec, maxt=104567msec  WRITE: io=523872KB, aggrb=5009KB/s, minb=5009KB/s, maxb=5009KB/s, mint=104567msec, maxt=104567msecNow the stats show some usage. The stats below are after a few test runs.root@storage:/home/curtis# zpool iostat -v tank               capacity     operations    bandwidthpool        alloc   free   read  write   read  write----------  -----  -----  -----  -----  -----  -----tank         757G   635G      0     57  1.15K  6.68M  mirror     757G   635G      0     57  1.15K  6.68M    sdb         -      -      0     55  1.24K  6.68M    sdd         -      -      0     55  1.14K  6.68Mcache           -      -      -      -      -      -  sdc       11.7G   221G      0     54     79  6.81M----------  -----  -----  -----  -----  -----  -----That’s all it takes! Please let me know if you see anything incorrect.Happy caching!"
    } ,
  
    {
      "title"    : "Install ZFS on Ubuntu Trusty 14.04",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/07/01/zfs-ubuntu-trusty.html",
      "date"     : "2014-07-01 00:00:00 -0400",
      "content"  : "In this blog post I’ll install ZFS-on-Linux (ZoL) on trusty old Ubuntu Trusty 14.04.ZFS is an amazing file system that is now also usable on Linux. One of ZFS’ best features is that it can “self heal” as it is a checksumming file system. Also it can use SSDs in a couple of different ways, such as the ZIL drive and the L2ARC cache.There are other interesting file systems and ways to cache with solid state drives. btrfs is continually getting better (I use it with Docker) and recently the Linux kernel gained a few ways to do SSD caching: dmcache, flashcache, and bcache.In my situation I have various media files from short films I’ve made that I need to backup and protect from bitrot. To do that I’ve decided to use ZFS on Linux. I worked with ZFS + FreeBSD a bit, but I also want the ability to mount many different types of file systems, and surprisingly FreeBSD doesn’t support that many of them. I’m also a big fan of XFS, which I believe FreeBSD only supports in read-only mode. So Linux it is.ZoL PPAThe easiest way to get ZoL is to use the ZFS-native PPA. The software-properties-common package is required for the add-apt-repository command.curtis@storage:~$ sudo apt-get install software-properties-commoncurtis@storage:~$ sudo add-apt-repository ppa:zfs-native/stableNow we can install ZoL. Installing will also compile a kernel module.NOTE: I’m removing a lot of the output below for brevity; I usually mark that with SNIP!.curtis@storage:~$ sudo apt-get updatecurtis@storage:~$ sudo apt-get install -y ubuntu-zfsSNIP!zfs.ko:Running module version sanity check. - Original module   - No original module exists within this kernel - Installation   - Installing to /lib/modules/3.13.0-24-generic/updates/dkms/depmod....SNIP!Setting up ubuntu-zfs (8~trusty) ...Processing triggers for libc-bin (2.19-0ubuntu6) ...Now load the module.curtis@storage:~$ modprobe zfscurtis@storage:~$ lsmod | grep zfszfs                  1185541  0zunicode              331251  1 zfszavl                   15010  1 zfszcommon                51321  1 zfsznvpair                89166  2 zfs,zcommonspl                   175436  5 zfs,zavl,zunicode,zcommon,znvpairConfigure zpoolI have an older computer that I am using as the zfs backup server. In this example it has two 1.5TB drives that I want to use in a zfs mirror (ie. RAID1). I’ll add more storage later but for this example just the two 1.5TB drives, sdb and sdd. They were previously used elsewhere and need to be reformatted for zfs.curtis@storage:~$ lsblkNAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTsda      8:0    0 119.2G  0 disk├─sda1   8:1    0 113.3G  0 part /├─sda2   8:2    0     1K  0 part└─sda5   8:5    0     6G  0 part [SWAP]sdb      8:16   0   1.4T  0 disk├─sdb1   8:17   0   200M  0 part├─sdb2   8:18   0   1.4T  0 part└─sdb3   8:19   0   128M  0 partsdc      8:32   0 465.8G  0 disk├─sdc1   8:33   0    64K  0 part├─sdc2   8:34   0   462G  0 part└─sdc3   8:35   0   3.8G  0 partsdd      8:48   0   1.4T  0 disk├─sdd1   8:49   0   200M  0 part├─sdd2   8:50   0   1.4T  0 part└─sdd3   8:51   0   128M  0 partsr0     11:0    1  1024M  0 romWe’ll create a zpool mirror callled tank.curtis@storage:~$ sudo zpool create tank mirror sdb sddcurtis@storage:~$ zfs listNAME   USED  AVAIL  REFER  MOUNTPOINTtank  91.5K  1.34T    30K  /tankInterestingly zfs didn’t warn me about reformatting.There is now a /tank directory of about ~1.4TB.curtis@storage:~$ df -h | grep tanktank            1.4T     0  1.4T   0% /tankNow create another file system on tank. Note the casesensitivity=mixed for use with Windows.curtis@storage:/tank$ zfs create -o casesensitivity=mixed tank/bupSamba and ZFSAs stated previously, I want to use this as a backup server. I do a lot of work with video and audio files and that is all, unfortunately, done from a windows workstation. So I want to be able to backup from Windows to the ZoL backup server. I’ll use samba (SMB) to do that.Please note that I haven’t used samba in years, so I’m not quite sure this is the right way to go about it. But it is working for me. :)First, install samba.curtis@storage:~$ sudo apt-get install sambaNow we can create a file system in /tank and share that via SMB.curtis@storage:~$ sudo zfs set sharesmb=on tank/bupcurtis@storage:~$ sudo chown curtis:curtis /tank/bupCheck what zfs thinks about the share status with regards to samba and nfs.root@storage:/var/log/samba# sudo zfs get sharesmb,sharenfsNAME      PROPERTY  VALUE     SOURCEtank      sharesmb  on        localtank      sharenfs  off       defaulttank/bup  sharesmb  on        localtank/bup  sharenfs  off       defaultBased on this blog post I added the below to /etc/samba/smb.conf and restarted smbd and nmbd. These settings may or may not be appropriate for your use case.usershare path = /var/lib/samba/usersharesusershare max shares = 100usershare allow guests = yesusershare owner only = nNext, add a samba user.root@storage:/var/log/samba# sudo smbpasswd -a curtisNew SMB password:Retype new SMB password:Added user curtis.Finally I can connect to that server with \\storage\\tank_bup or the server and share should be browsable from the Windows workstation, assuming they are on the same network, and in this case they are.ConclusionIn this post I’ve done a couple things:  Install ZFS on Linux  Create a pool with mirrored drives  Configure a samba share to access from WindowsSo far the performance has been fine. I get about 111MB/s write which is basically as fast as a 1GB network can go.Soon I’ll add an SSD caching device which will get me more IOPS but I’ve hit the limit on the network.Updates  Commenter Ofer B says apt-get update is necessary, so I added that."
    } ,
  
    {
      "title"    : "Deploy OpenStack Swift OnlyOne to Digital Ocean",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/06/26/openstack-swift-onlyone-digitalocean.html",
      "date"     : "2014-06-26 00:00:00 -0400",
      "content"  : "In this blog post I want to show how to get your very own internet available object storage system using OpenStack Swift and Docker. Also it will be terminated by SSL (though with a self-signed certificate).It’s important to note that this is a special case OpenStack Swift setup–it only has one storage device and will only make one replica, which I call OpenStack Swift OnlyOne. Normally Swift installations are huge! But his one is small, which I think is cool. Or fun. But not fun and cool. That’s too much.This is what we are going to do:  Boot a virtual machine on Digital Ocean  Configure it to be able to use xattr  Pull a few docker images  Start three docker containers  Create swift containers and upload files into them  Set a container to serve an index.html pageGet a docker virtual machineHandily Digital Ocean has an image that comes with Docker 1.0 already. I’m going to use the tugboat CLI.curtis$ tugboat images --global | grep DockerDocker 1.0 on Ubuntu 14.04 (id: 4296335, distro: Ubuntu)Dokku v0.2.3 on Ubuntu 14.04 (w/ Docker 1.0) (id: 4381169, distro: Ubuntu)Let’s boot it. 66 is the 512MB image.NOTE: If you really plan on using this for work instead of just testing Swift, a larger droplet size will likely be necessary. I did get some out of memory errors with the 512MB size.curtis$ tugboat create swifty-onlyone -i 4296335 -s 66 -k 118429Queueing creation of droplet 'swifty-onlyone'...donecurtis$ tugboat dropletsswifty-onlyone (ip: , status: new, region: 4, id: 1945827)&lt;/code&gt;&lt;/pre&gt;Wait until it's active, then ssh in.curtis$ tugboat dropletsswifty-onlyone (ip: , status: active, region: 4, id: 1945827)curtis$ ssh root@SNIP!root@swifty-onlyone:~# &lt;/code&gt;&lt;/pre&gt;Add xattr attribute to fstab for root and remount. _NOTE: Swift requires the file system support xattr. I'm not sure if it's enabled by default or not._root@swift-onlyone:~# # vi /etc/fstab and add user_xattrroot@swift-onlyone:~# grep xattr /etc/fstabUUID=050e1e34-39e6-4072-a03e-ae0bf90ba13a /               ext4    errors=remount-ro,user_xattr 0       1root@swift-onlyone:~# mount -o remount /## Get docker imagesPull some docker images:- busybox- serverascode/swift-onlyone- serverascode/poundroot@swift-onlyone:~# docker pull busybox; docker pull serverascode/swift-onlyone; docker pull serverascode/poundNow we have all those images locally.root@swifty-onlyone:~# docker imagesREPOSITORY                   TAG                   IMAGE ID            CREATED             VIRTUAL SIZEserverascode/swift-onlyone   latest                1b562d4e3975        3 hours ago         349.2 MBserverascode/pound           latest                2bfef1fdc39d        3 hours ago         285.2 MBbusybox                      buildroot-2013.08.1   d200959a3e91        3 weeks ago         2.489 MBbusybox                      ubuntu-14.04          37fca75d01ff        3 weeks ago         5.609 MBbusybox                      ubuntu-12.04          fd5373b3d938        3 weeks ago         5.455 MBbusybox                      buildroot-2014.02     a9eb17255234        3 weeks ago         2.433 MBbusybox                      latest                a9eb17255234        3 weeks ago         2.433 MB## Create the containersWe're going to create three containers:1. SWIFT_DATA: A volume only container2. SWIFT: Has OnlyOne installed, volume from SWIFT_DATA3. A pound ssl termination container, linked to SWIFTFirst, create a volume only container.root@swift-onlyone:~# docker run -v /srv --name SWIFT_DATA busyboxroot@swift-onlyone:~# docker ps -a | grep DATA838c68ce031b        busybox:buildroot-2014.02   /bin/sh             15 seconds ago      Exited (0) 14 seconds ago                       SWIFT_DATA    Should see a volume in /var/lib/docker/volumes now.root@swift-onlyone:~# ls /var/lib/docker/volumes/1b6e87f07e2e5c0e49362bfa51f22fb8a32bca691a12d5c5872db0b90baf5241  _tmpNow create the OnlyOne container using a volume from SWIFT_DATA. Make sure to call it SWIFT.Please note a couple of environment variables being set:- *SWIFT_STORAGE_URL_SCHEME=https* Tells Swift Proxy to use https for the storage url- *SWIFT_SET_PASSWORDS=yes* The startmain.sh script for Swift OnlyOne will change the default password for each use to one password.root@swift-onlyone:~# docker run -d -e SWIFT_SET_PASSWORDS=yes -e SWIFT_STORAGE_URL_SCHEME=https --volumes-from SWIFT_DATA --name SWIFT -t serverascode/swift-onlyoneIf SWIFT_SET_PASSWORDS=yes was set, then the password will be echoed to the container log.As an example, below it's been set to: laibiibooghu.root@swift-onlyone:~# docker logs 6807caaaaf3b | headRing files already exist in /srv, copying them to /etc/swift...Setting default_storage_scheme to https in proxy-server.conf...storage_url_scheme = httpsSetting passwords in /etc/swift/proxy-server.confuser_test_tester = laibiibooghu .adminuser_test2_tester2 = laibiibooghu .adminuser_test_tester3 = laibiibooghuStarting supervisord...Starting to tail /var/log/syslog...(hit ctrl-c if you are starting the container in a bash shell)Jun 27 16:46:24 6807caaaaf3b object-replicator: Starting object replicator in daemon mode.Finally create a pound container. This will be the ssl termination point and will be available from the Internet.This container will be linked to the SWIFT container.root@swift-onlyone:~# docker run -d --link SWIFT:SWIFT -p 443:443 -t serverascode/poundNow we have three containers, two of them running, and the other being the volume only container.root@swift-onlyone:~# docker ps -aCONTAINER ID        IMAGE                               COMMAND                CREATED              STATUS                         PORTS                  NAMES2f6dcdae1db2        serverascode/pound:latest           /bin/sh -c /usr/loca   15 seconds ago       Up 14 seconds                  0.0.0.0:443-&gt;443/tcp   naughty_turing               76d27dafa403        serverascode/swift-onlyone:latest   /bin/sh -c /usr/loca   About a minute ago   Up About a minute              8080/tcp               SWIFT,naughty_turing/SWIFT   838c68ce031b        busybox:buildroot-2014.02           /bin/sh                About an hour ago    Exited (0) About an hour ago                          SWIFT_DATANow from my laptop I can run the swift command line.curtis$ alias sw='swift --insecure -A https:///auth/v1.0 -U test:tester -K 'curtis$ sw stat       Account: AUTH_test    Containers: 0       Objects: 0         Bytes: 0  Content-Type: text/plain; charset=utf-8   X-Timestamp: 1403882745.61961    X-Trans-Id: tx28102150d50b484a92f3a-0053ad8cf9X-Put-Timestamp: 1403882745.61961&lt;/code&gt;&lt;/pre&gt;And upload a directory with a file in it.curtis$ echo \"hi\" &gt; index.htmlcurtis$ sw upload www index.htmlindex.htmlSet permissions so that anyone can read the files in the www container, ie. they are public.curtis$ sw post --read-acl='.r:*,.rlistings' wwwcurtis$ sw stat www       Account: AUTH_test     Container: www       Objects: 1         Bytes: 3      Read ACL: .r:*,.rlistings     Write ACL:       Sync To:      Sync Key: Accept-Ranges: bytes   X-Timestamp: 1403883848.54012    X-Trans-Id: txd858295e7d294d39bdf3e-0053ad921d  Content-Type: text/plain; charset=utf-8Make index.html the default web index.curtis$ sw post -m 'web-index:index.html' wwwNow we can access that page in a web browser, and get the index.html. curtis$ wget --no-check-certificate https:///v1/AUTH_test/www/--2014-06-27 11:48:20--  https:///v1/AUTH_test/www/Connecting to :443... connected.WARNING: cannot verify 's certificate, issued by '/C=US/ST=Oregon/L=Portland/O=IT/CN=172.17.0.13':  Self-signed certificate encountered.    WARNING: certificate common name '172.17.0.13' doesn't match requested host name ''.HTTP request sent, awaiting response... 200 OKLength: 3 [text/html]Saving to: 'index.html'100%[====================================================================================================================================================&gt;] 3           --.-K/s   in 0s      2014-06-27 11:48:20 (109 KB/s) - 'index.html' saved [3/3]curtis$ cat index.html hi&lt;/code&gt;&lt;/pre&gt;Note that I just wanted to use that as a demonstration, not the actual use case for Swift. Swift stores unstructured data, which we, as a planet, have a lot of. It doesn't have to serve web pages.## ConclusionNow for $5 a month you have a little swift install. The storage on that instance is pretty limited, at 20GB, but at any rate you can put all kinds of [DevOps reactions](http://devopsreactions.tumblr.com/) gifs there if you want. Or, perhaps use it to create interesting, proof-of-concept scalable web systems.I should note as well that you could deploy OnlyOne in the same fashion on any Docker host, which is one of Docker's most interesting features."
    } ,
  
    {
      "title"    : "Automated deployment of the Wordpress database",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/06/22/automated-wordpress-database-deployment.html",
      "date"     : "2014-06-22 00:00:00 -0400",
      "content"  : "I’ve been working on a proof-of-concept (PoC) Wordpress-as-a-service (WPaas?) that uses docker (among other technologies). However, one thing I can’t seem to find is anyone showing how to easily install the Wordpress database after the application files have been installed and the Wordpress configuration file…er…configured.Most automated installs seem to stop after creating the database, which means you access the site via http and then continue the install manually. But obviously I don’t want to stop at that point, I want the site name, admin password, admin user, etc, all entered and the database installed so the user can simply access the the site they asked for, you know, as though the install was fully automated.First try: http postMy first try was just doing a post with the right variables. Here’s a simplified (no error checking) snippet of the python code I was using to do this.payload = {'weblog_title': sitename, 'user_name': 'adminName', 'admin_password': dbPass, 'admin_password2': dbPass, 'admin_email': adminEmail }r = requests.post(\"http://\" + fullSiteName + \"/wp-admin/install.php?step=2\", data=payload)You could also use a post request with something like curl:$ curl -d \"weblog_title=&amp;user_name=&amp;admin_password=&amp;admin_password2=2&amp;admin_email=\" \\http:///wp-admin/install.php?step=2&lt;/code&gt;&lt;/pre&gt;This worked and seemed like an Ok option, but the only thing I didn't like was that I would have to wait until the container was up to send the http request. Not only was it annoying to have to wait (it's containers after all) but it's not very secure to have the site up but not completely installed, ie. anyone who happened onto it could set the admin user name and password. Even if I didn't add the site to the web router to be available externally, it still doesn't feel right. So while this was Ok for some basic testing, it's not good enough.## Second try: wp_install() functionAfter a bit more sleuthing I found that wordpress has a handy install function. There's a good example of using it on [this blog](http://www.openlogic.com/wazi/bid/324425/How-to-install-WordPress-from-the-command-line). However, I found that I need the WP_SITEURL set as well.Below is a snippet of a bash script that calls out to php to use wp_install().#!/bin/bash# ...# Variable settings not shown/usr/bin/php -r \"define('WP_SITEURL', '\"${WP_SITEURL}\"');include '/app/wp-admin/install.php';wp_install('\"${WP_SITE}\"', 'admin', '\"${WP_EMAIL}\"', 1, '', '\"${DB_PASSWORD}\"');\"This works better. Note the above is the bare minimum, and there should be some error checking as well, in case the wp_install call fails. It should really be a separate php script that can take arguments and return errors to the bash code calling it.Running this from the container still concerns me a little.## Database snapshots?As I mentioned previously, this is a PoC. What would likely happen in a production environment is that certain versions of Wordpress would be supported, and the SQL for that would be configured automatically, ie. generate an SQL file for each site, and simply install that into the database instead of using wp_install. Or perhaps do something more advanced with database or file system snapshots. Once the db was snapshotted/cloned you could just change variables like siteurl, the admin user, and their password. Would likely be faster too. I'm sure there are a lot of interesting ways to perform database snapshots or clones.Obviously newer versions of Wordpress could come with different database schemas, which would mean I'd have to \"certify\" various versions and properly create snapshots for those versions, should their be any schema differences. Totally doable though. Something to look into in the future, but a bit much for my little PoC.## WP-CLI[WP-CLI](http://wp-cli.org/), or \"wordpress command line\", is a pretty interesting project. I'm not sure how valuable it is in my particular use case, but it seems like a good piece of software, especially if you admin a multisite wordpress installation. Worth taking a look at."
    } ,
  
    {
      "title"    : "Build your own platform as a service with Docker",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/06/16/build-your-own-paas-docker.html",
      "date"     : "2014-06-16 00:00:00 -0400",
      "content"  : "First off, let me be clear–Docker is not a “platform as a service” (PaaS) by itself. However, I do think it’s an important component, one that makes deploying a PaaS much simpler.Second, for the most part I’m discussing the concept of building “your own PaaS” from my personal perspective, which is that I have a few blogs and Wordpress sites that I run for friends (ie. not a business venture or anything) and I have a single hardware server out there on the Internet that I use to do this. I thought it would be fun to use that server to think about how I one could provide a Wordpress PaaS that could potentially scale out to multiple hosts, even though I could never afford to or need to do that.In a way, what I’m doing is creating a proof of concept (PoC) wordpress as a service, mostly for a single host which once in place could provide almost any application, and could potentially scale out to multiple container hosts.Why not just use Dokku?Dokku is a piece of software created by Jeff Lindsay that essentially creates a single-host PaaS.  Docker powered mini-Heroku. The smallest PaaS implementation you’ve ever seen.But I’m not going to use Dokku, even though that is probably the best way to go about this. Instead I’m going to mostly layout the minimum systems and components needed to create a low-fi PaaS. Also Deis, Flynn and other container management systems, in various states with differing features, exist.ComponentsThese are the minimum components I think you would need to create your own PaaS.  Wildcard DNS entry  “Web router” (such as Hipache)  Docker - Application server/container provisioning and image creation  Application source code  Environment variables  Datastores, such as MySQL, NoSQL, object storage, etc  Something to tie them all togetherWildcard DNS entryThe first thing you need is a wildcard DNS entry for a domain name. This gist describes how to configure one using Namecheap, which happens to also be my registrar of choice (like a few other registrars, but not all, they support two factor authentication). So I bought a domain name like “somedomainapp.com” to run my “apps” and then configured a wildcard entry using Namecheap’s DNS service.Obviously in a larger production environment you’d either manage your own nameservers or perhaps use something like Google’s DNS as a service, which I would love to try out, and some loadbalancers or similar.At this point, at minimum, you have a wildcard DNS entry pointing to an IP on your server which apps will be able to use, such as *.yourdomainapp.com.Web router(Above: hipache non-existent domain page)I’m not sure what to call this layer. Heroku calls this HTTP routing. Web routing works for me.Essentially what his does is route incoming requests for apps to the right webserver, which in our case will be a docker container, or several docker containers. A request for someapp.somedomainapp.com would go to 127.0.0.1:49899 or 172.17.0.3:80 and such, which are docker containers.In my situation I am using hipache which is backed by redis. This means you can add routes to hipache by entering them into redis, and hipache isn’t required to restart because it will query redis for domain configuration. By default hipache allows the use of wildcard domains, so it’ll route any configured entry, or send a default page if it doesn’t exist.My PoC python script, which is called “wpd” (more on that later), can dump the keys setup in redis for hipache. The below output means hipache will randomly balance requests for someapp.yourdomainapp.com to the two containers listed in redis.$ wpd listkeyssomeapp.yourdomainapp.com===&gt; http://127.0.0.1:49156===&gt; http://127.0.0.1:49157$ redis-cli lrange someapp.yourdomainapp.com 0 -11) \"someapp.yourdomainapp.com\"2) \"http://127.0.0.1:49156\"3) \"http://127.0.0.1:49157\"There are many other ways to do “web routing.” Dokku uses nginx. There is also vulcand which is backed by etcd and though it’s new it sounds exciting. Hipache does support SSL, but as of a few weeks ago Vulcand did not, though I think it’s on the roadmap (but I am a golang fanboy, so am biased).Docker!Again comparing Heroku to what we are doing here, I think that Docker would fill the roles of buildpack and dyno though perhaps in terms of the buildpack part not containing a “slug” of the application code, rather only the environment the application would run in. Perhaps it’s better to consider a Dockerfile a type of buildpack.Using my wordpress example, the Dockerfile would create the image which docker then runs to create a wordpress application container, for example using apache2 + php.Docker manages the container and provides networking and network address translation to expose the apache2’s port to the web router.So Docker is doing quite a bit of the work for us. Without Docker, we would probably need a way to create virtual machines images programmatically, and a way to start up and instance and get it networked, which could be as simple as something like packer and libvirtd (perhaps with kvm or lxc) or other combinations such as packer and openstack. Certainly that would require a few more resources. (Interestingly packer can build docker images as well.)Application source codeIn Dokku code is pushed to a git repository which kicks off other processes. This is also how Heroku works. Those processes somehow get the code into the application container.However, in my wordpress example, the wordpress code is downloaded in the startup script. Once the container is started from the wordpress image, the startup script runs something like:if [ ! -e /app/wp-settings.php ]; then        cd /app        curl -O http://wordpress.org/latest.tar.gz        tar zxvf latest.tar.gz        mv wordpress/* /app        rm -f /app/wordpress        chown -R www-data:www-data /app        rm -f latest.tar.gzfito grab the code. The url used to download could be taken from an environment variable instead of just being static as in the example.The git push/recieve style probably makes more sense in a PaaS, but I haven’t looked into what it takes to do that. Again Jeff Lindsay has gitrecieve and Flynn (a Jeff Lindsay project) has gitrecieved. Also he has execd and more. Busy guy!Obviously there are a lot of ways to get code into the container to run it. If there is any one important thing a PaaS does, it’s run your code.Environment variablesI think Docker images should be fairly generic. Also you don’t want to commit configuration information to the image, such as passwords, so they should come from environment variables, and those variables need to get injected into the containers environment somehow.For my wordpress example, I set environment variables with Docker. The Docker run command has the “-e” option which allows setting environment variables which will be exposed in the container. Below is an example.$ docker run -e FOO=bar -e BAR=foo busybox envHOME=/PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=6cf2d6e8acb3FOO=barBAR=fooMy wordpress startup script checks for a few environment variablesDB_NAME=${DB_NAME:-\"wordpress\"}DB_USER=${DB_USER:-\"wordpress\"}DB_PASSWORD=${DB_PASSWORD:-\"wordpress\"}DB_HOST=${DB_HOST:-$1}and uses them to create the wordpress configuration file with the right database settings.Later on I’ll talk about “tying the whole room together” which I do with a python script, and in it I set the environment variables for the container.Below is a snippet of python code used to start a container with environment variables.env = {  'DB_HOST': MYSQL_HOST,  'DB_NAME': dbname,  'DB_USER': dbname,  'DB_PASSWORD': dbpass,}container = dockerConn.create_container(image, detach=True, environment=env)Docker and python go well together using docker-py.Another method is using a shared configuration system. Previously I mentioned etcd.  [etcd is] a highly-available key value store for shared configuration and service discovery.etcd can store configuration information and confd is a configuration management agent which can query etcd for information and generate configuration files that applications can use, and also restart the services that use those configuration files.Given that I’m suggesting environment/configuration variables are a key part to PaaS, things like etcd, confd, and consul are going to be important projects. But again, with my limited wordpress PaaS I’m just working on a very simplified PoC, where environment variables come at container runtime. However, I would imagine most larger PaaS or PaaS-like systems will use something like consul or etcd.DatastoresIf your application needs to persist data, then it’s got to put that data somewhere and the application container itself is not a good place. In general, I think there are two approaches:  Another container  A separate serviceIn terms of “a separate service” I’m talking about something like Amazon RDS or OpenStack Trove (both of those being Database as a service) or object storage like OpenStack Swift. In short I mean a service that is managed by someone else, perhaps the same provider that either runs Docker or the server that Docker is running on.The other option is another container. Again using the wordpress example, instead of running one application container, perhaps I would run a second container that has a MySQL server running (or both services could even be in a single container). Maybe that MySQL service is a container, maybe it’s a hardware server configured with Ansible. Who knows. Docker also recommends the volumes from approach, which works great when the data doesn’t have to be distributed across multiple containers, which would be the case in a MySQL or OpenStack Swift container.My feeling is that either is Ok, but I prefer to run a separate service. So in my wordpress example there is a single MySQL server that all wordpress apps will connect to, each having their own database. Perhaps that separate service is using Docker too.Something to tie them all togetherI’m working on a python script called “wpd” that ties all of this together, and what it does is:  Creates a site entry in the wpd MySQL database  Creates a database datastore that the site can use for wordpress  Creates couple of wordpress containers  Provides those containers with environment variables regarding how to connect to a worpress database  Adds that site to redis so that hipache can route/loadbalance requests to those containers$ wpd -husage: wpd [-h] {listkeys,addsite,listsites,addimage,deploysite,dumpsite} ...positional arguments:  {listkeys,addsite,listsites,addimage,deploysite,dumpsite}    listkeys            list all the keys in redis    addsite             add a site to the database    listsites           list all the sites in the database    addimage            add a docker image to the database    deploysite          startup a sites containers    dumpsite            show all information about a siteoptional arguments:  -h, --help            show this help message and exitAs you can see it has a few options, such as “addsite” and “deploysite”. It’s not complete yet. Adding a site just enters it into the wpd database, and deploying it means starting up containers and adding the information to redis so that hipache can route http requests to them.What this would look like in a larger system…I’m not sure. It seems like a user management system more than anything else–users have sites, sites have names, containers, images and datastores.IssuesThere are a few issues that I’d like to mention (though I’m probably not covering them all).Logging - Getting logs out of docker is still a bit of an problem. At this point likely you’ll need to configure syslog in the container to ship logs to a centralized system. I expect that eventually docker will have more advanced ways of dealing with logs, if they don’t already.File systems - Wordpress is a good example of a web application that is difficult to scale because it relies a lot on the file system for data persistence, such as media uploaded by users. In order to scale out a file system across multiple docker hosts you’ll need a distributed file system, which is a huge pain and can also increase the size of your failure domain dramatically. I suggest not using file systems to store files and instead use object storage such as OpenStack Swift, which isn’t as hard to deploy as some might think. What’s more Swift doesn’t think it can do consistency and availability at the same time.Datastore credentials - I’m not sure what the best way to securely store these is. Credentials and other important configuration information will need to be injected into the container somehow, and thus will need to be stored in a database or etcd or similar. Something to look into.ConclusionIn the end, I think that docker is a great system to use as a component in PaaS and that, to me, it simplifies rolling your own small platform as a service host. All you need is a web router, a Dockerfile and a docker host, a way to get the app into the container, and pow now you’re cookin’ with PaaS. Just remember to make your containers fairly generic, pipe in environment variables for configuration (or get them from somewhere else), and avoid file systems if possible.The Future!Obviously I’m leaving a lot out. There’s a huge difference between toying around with a PoC like this and running a production PaaS. I haven’t even mentioned the terms “service discovery” or “container scheduling”, which theoretically things like etcd and libswarm could take care of respectively, though I’m not sure libswarm will turn into a container scheduler. Recently Google released Kubernetes which is as docker cluster manager of some kind, though it currently only runs on GCE. Mesosphere is also working on its Deimos project. Further, CoreOS has fleet and Spotify helios. Shipyard can also control multiple docker hosts. Also there is Havok which is a way to monitor docker containers and add them to vulcand via etcd. Neither have I discussed limiting resources, such as container memory and CPU and a about a billion other things, but I look forward to learning more.Updates  Changed Apache Mesos to Mesosphere with regards to Deimos  Added Shipyard and Havok"
    } ,
  
    {
      "title"    : "Swift OnlyOne - Run OpenStack Swift in Docker",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/06/12/run-swift-in-docker.html",
      "date"     : "2014-06-12 00:00:00 -0400",
      "content"  : "First, let met say that this is not about how to run a cluster of OpenStack Swift servers in Docker, rather it’s about running a single container that has a version of OpenStack Swift all-in-one deployed, and specifically that version only has one storage device (a docker volume) and is configured to store one replica on that device.Why Swift OnlyOne?I’m calling it Swift OnlyOne because it just has one server, one device, and is configured to do one replica. Swift All-in-one, on the other hand, sets up four servers and devices, though within one virtual machine.Most people will use Swift in a much larger context as typically Swift is used to store huge amounts (petabytes!) of files. But in this case I just want a small deployment that can provide object storage to several other containers, or perhaps to do development against.As an example–I think that OpenStack Swift is a perfect partner for Docker, and I don’t just mean deploying Swift with Docker, I mean having Docker containers use Swift as a datastore.If you have multiple containers on multiple hosts, and some of the containers run an application which needs access to the same files, then you need a way to share those files. Usually this is done with a distributed file system (DFS), but that has all kinds of complexity associated, and, I think, makes Docker hard to use in an idiomatic way. What if, instead of using a DFS you used OpenStack Swift, and thus, instead of relying on a file system your application used object storage? I think you would be better off in the long run.Use Swift OnlyOneFirst, because Swift requires the filesystem have xattr, Docker must be setup with either btrfs or XFS (or some other xattr supporting file system). I have only used it with btrfs. So /var/lib/docker is a btrfs volume and the docker daemon is run with “-s btrfs”.NOTE: I have Vagrant file and Ansible playbook on github that will setup a Vagrant-based virtual machine with docker and btrfs configured. So all you would have to do is clone that repo and run “vagrant up” to get docker + btrfs.vagrant@host1:~$ sudo btrfs fi show /var/lib/dockerLabel: none  uuid: 732ee044-4b3a-4391-8b53-fd7da224c008\tTotal devices 1 FS bytes used 1.99GiB\tdevid    1 size 20.00GiB used 4.04GiB path /dev/sdbBtrfs v3.12vagrant@host1:~$ ps ax | grep [d]ocker  997 ?        Sl     6:40 /usr/bin/docker.io -d -s btrfsThere is a github repository that has the Dockefile and Swift configuration files used for this example, or you can pull it from the Docker repository.vagrant@host1:~$ docker pull serverascode/swift-onlyonePulling repository serverascode/swift-onlyone7e8283467cba: Download complete SNIP!d7279e38d8cd: Download complete Now we can run an interactive Swift OnlyOne container.vagrant@host1:/vagrant/dockerfiles/swift-onlyone$ docker run -i -t serverascode/swift-onlyone /bin/bashroot@f2f8ccb82c0e:/# /usr/local/bin/startmain.sh Device d0r1z1-127.0.0.1:6010R127.0.0.1:6010/sdb1_\"\" with 1.0 weight got id 0Reassigned 128 (100.00%) partitions. Balance is now 0.00.Device d0r1z1-127.0.0.1:6011R127.0.0.1:6011/sdb1_\"\" with 1.0 weight got id 0Reassigned 128 (100.00%) partitions. Balance is now 0.00.Device d0r1z1-127.0.0.1:6012R127.0.0.1:6012/sdb1_\"\" with 1.0 weight got id 0Reassigned 128 (100.00%) partitions. Balance is now 0.00.Starting to tail /var/log/syslog...(hit ctrl-c if you are starting the container in a bash shell)^CNote that I hit “ctrl-c” when it says “Starting to tail…” because when running this container in non-interactive mode, I tail /var/log/syslog to be able to do “docker logs $ID” and get the Swift logs.We can see what processes are running, and there are quite a few, including rsyslog and memcached. Usually Docker models processes, but in this case I am taking a role-based approach to using Docker.root@f2f8ccb82c0e:/# ps ax  PID TTY      STAT   TIME COMMAND    1 ?        Ss     0:00 /bin/bash   43 ?        Ss     0:00 /usr/bin/python /usr/bin/supervisord -c /etc/supervisor/conf.d/supervisord.conf   45 ?        S      0:00 /usr/bin/python /usr/bin/swift-container-server /etc/swift/container-server.conf   46 ?        S      0:00 /usr/bin/python /usr/bin/swift-account-reaper /etc/swift/account-server.conf   47 ?        S      0:00 /usr/bin/python /usr/bin/swift-object-replicator /etc/swift/object-server.conf   48 ?        S      0:00 /usr/bin/python /usr/bin/swift-account-auditor /etc/swift/account-server.conf   49 ?        S      0:00 /usr/bin/python /usr/bin/swift-object-server /etc/swift/object-server.conf   50 ?        S      0:00 /usr/bin/python /usr/bin/swift-container-sync /etc/swift/container-server.conf   51 ?        S      0:00 /usr/bin/python /usr/bin/swift-account-replicator /etc/swift/account-server.conf   52 ?        S      0:00 /usr/bin/python /usr/bin/swift-account-server /etc/swift/account-server.conf   53 ?        S      0:00 /bin/bash -c source /etc/default/rsyslog &amp;&amp; /usr/sbin/rsyslogd -n -c3   54 ?        S      0:00 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf   55 ?        S      0:00 /usr/bin/python /usr/bin/swift-object-updater /etc/swift/object-server.conf   56 ?        Sl     0:00 /usr/bin/memcached -u memcache   57 ?        Sl     0:00 /usr/sbin/rsyslogd -n -c3   86 ?        S      0:00 /usr/bin/python /usr/bin/swift-object-server /etc/swift/object-server.conf   87 ?        S      0:00 /usr/bin/python /usr/bin/swift-container-server /etc/swift/container-server.conf   88 ?        S      0:00 /usr/bin/python /usr/bin/swift-account-server /etc/swift/account-server.conf   89 ?        S      0:00 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf   91 ?        R+     0:00 ps axNow we can use Swift.root@f2f8ccb82c0e:/# swift -A http://127.0.0.1:8080/auth/v1.0 -U test:tester -K testing stat       Account: AUTH_test    Containers: 0       Objects: 0         Bytes: 0  Content-Type: text/plain; charset=utf-8   X-Timestamp: 1402590362.23352    X-Trans-Id: tx1c32b455aa7c4178a4add-005399d49aX-Put-Timestamp: 1402590362.23352root@f2f8ccb82c0e:/# swift -A http://127.0.0.1:8080/auth/v1.0 -U test:tester -K testing upload etc_swift /etc/swiftetc/swift/dispersion.confetc/swift/account-server.confetc/swift/backups/1402588704.container.ring.gzetc/swift/backups/1402588704.object.ring.gzetc/swift/backups/1402588704.container.builderetc/swift/proxy-server.confetc/swift/object-server.confetc/swift/swift.confetc/swift/backups/1402588704.object.builderetc/swift/container-server.confetc/swift/backups/1402588704.account.builderetc/swift/backups/1402588705.account.ring.gzetc/swift/object.builderetc/swift/backups/1402588705.account.builderetc/swift/object.ring.gzetc/swift/container.builderetc/swift/account.ring.gzetc/swift/supervisord.logetc/swift/container.ring.gzetc/swift/account.builderetc/swift/supervisord.pidroot@f2f8ccb82c0e:/# We can also start the container in non-interactive mode, and add a port mapping.vagrant@host1:~$ ID=$(docker run -d -p 8080 -t serverascode/swift-onlyone)Now that container should be running with a port mapped to 8080.vagrant@host1:~$ docker psCONTAINER ID        IMAGE                               COMMAND                CREATED             STATUS              PORTS                     NAMES0c57f60e1de6        serverascode/swift-onlyone:latest   /bin/sh -c /usr/loca   3 seconds ago       Up 3 seconds        0.0.0.0:49162-&gt;8080/tcp   loving_hawking      Above we can see that port 49162 on the container host is mapped to 8080 on the container.We can also check the logs.     vagrant@host1:~$ docker logs $IDDevice d0r1z1-127.0.0.1:6010R127.0.0.1:6010/sdb1_\"\" with 1.0 weight got id 0Reassigned 128 (100.00%) partitions. Balance is now 0.00.Device d0r1z1-127.0.0.1:6011R127.0.0.1:6011/sdb1_\"\" with 1.0 weight got id 0Reassigned 128 (100.00%) partitions. Balance is now 0.00.Device d0r1z1-127.0.0.1:6012R127.0.0.1:6012/sdb1_\"\" with 1.0 weight got id 0Reassigned 128 (100.00%) partitions. Balance is now 0.00.Starting to tail /var/log/syslog...(hit ctrl-c if you are starting the container in a bash shell)Let’s run stat again, but this time from the container host, not the container.Note that it’s not port 8080 any more for the auth url, it’s 49162.vagrant@host1:~$ swift -A http://127.0.0.1:49162/auth/v1.0 -U test:tester -K testing stat       Account: AUTH_test    Containers: 0       Objects: 0         Bytes: 0  Content-Type: text/plain; charset=utf-8   X-Timestamp: 1402590701.15270    X-Trans-Id: txfebf58919cbf4c61ac73c-005399d5edX-Put-Timestamp: 1402590701.15270        vagrant@host1:~$ swift -A http://127.0.0.1:49162/auth/v1.0 -U test:tester -K testing upload test swift.txt swift.txtvagrant@host1:~$ swift -A http://127.0.0.1:49162/auth/v1.0 -U test:tester -K testing list testswift.txtCheck the logs again:vagrant@host1:~$ docker logs $ID | tailJun 12 16:33:16 0c57f60e1de6 account-replicator: Replication run OVERJun 12 16:33:16 0c57f60e1de6 account-replicator: Attempted to replicate 1 dbs in 0.00341 seconds (292.99204/s)Jun 12 16:33:16 0c57f60e1de6 account-replicator: Removed 0 dbsJun 12 16:33:16 0c57f60e1de6 account-replicator: 0 successes, 0 failuresJun 12 16:33:16 0c57f60e1de6 account-replicator: no_change:0 ts_repl:0 diff:0 rsync:0 diff_capped:0 hashmatch:0 empty:0Jun 12 16:33:16 0c57f60e1de6 object-replicator: Starting object replication pass.Jun 12 16:33:16 0c57f60e1de6 object-replicator: 1/1 (100.00%) partitions replicated in 0.00s (767.48/sec, 0s remaining)Jun 12 16:33:16 0c57f60e1de6 object-replicator: 1 suffixes checked - 0.00% hashed, 0.00% syncedJun 12 16:33:16 0c57f60e1de6 object-replicator: Partition times: max 0.0003s, min 0.0003s, med 0.0003sJun 12 16:33:16 0c57f60e1de6 object-replicator: Object replication complete. (0.00 minutes)At this point we have a nice little OpenStack Swift install that we could use by linking with other containers.Using a data only containerIt would be best to run with a data only container as well, and use that volume on /srv.So first create a data only container.vagrant@host1:~$ docker run -v /srv --name SWIFT_DATA busyboxvagrant@host1:~$ docker ps --all  |grep SWIFT_DATA6c13b4e27320        busybox:buildroot-2014.02           /bin/sh                7 seconds ago       Exit 0                          SWIFT_DATA   Now we can create a docker container with “–volumes-from” the SWIFT_DATA container.vagrant@host1:~$ ID=$(docker run -d -p 8080 --volumes-from SWIFT_DATA -t serverascode/swift-onlyone)And if we inspect that container we can see where the container’s volume is. Sorry that the lines will probably not wrap very nicely.vagrant@host1:~$ docker inspect $ID | grep VolumesFrom        \"VolumesFrom\": \"SWIFT_DATA\",vagrant@host1:~$ docker inspect $ID | grep \"\\/srv\"        \"/srv\": \"/var/lib/docker/vfs/dir/8d437b57f36a2d849cece0752c8316c6916c31ec12fd9049d4203662806d3fe2\"        \"/srv\": truevagrant@host1:~$ sudo ls /var/lib/docker/vfs/dir/8d437b57f36a2d849cece0752c8316c6916c31ec12fd9049d4203662806d3fe2sdb1I think the data only container is an ideomatic way to use Docker.ConclusionI’m sure there is a lot that can be done to improve this Dockerfile, so please let me know if you see any issues or have any ideas. I have not done a lot of testing with it yet. But if it does work, then I think this is a nice way to quickly get access to a Swift instance, even if it is a purposely limited one, and hopefully help people move away from the complexity of a DFS when using containers.Issues  At this time the proxy is not using SSL, nor is there an SSL terminator in front of the proxy, so it’s all plain text. You wouldn’t want to do this in production."
    } ,
  
    {
      "title"    : "Docker and btrfs",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/06/09/docker-btrfs.html",
      "date"     : "2014-06-09 00:00:00 -0400",
      "content"  : "As I write this the first Docker convention is going on in San Francisco (sounds funny to write it was “docker convention” instead of “dockercon”…a convention sounds like a scene from Fear and Loathing in Las Vegas), and Docker has hit 1.0 and been declared production worthy.Docker has had, for a few versions, the ability to use btrfs as it’s backing driver, instead of aufs.Install btrfs on Ubuntu 14.04/TrustyBefore installing docker I’m going to setup btfrs.In this example I have a disk device called /dev/vdb to use with btrfs. I’ve removed some of the output from the commands.$ sudo su$ apt-get install btrfs-tools$ mkfs.btrfs /dev/sdb$ mkdir /var/lib/docker$ mount /dev/sdb /var/lib/docker$ mount | grep btrfs/dev/sdb on /var/lib/docker type btrfs (rw)Now that btrfs is installed and a device is formatted and mounted under /var/lib/docker we can install docker.Install dockerNext we need to install docker. It’s important that we also configure docker to startup with the “-s btrfs” option as well.$ apt-get install docker.io$ vi /etc/default/docker.ioWe need to change the DOCKER_OPTS entry to look like this:$ grep DOCKER_OPTS docker.io # Use DOCKER_OPTS to modify the daemon startup options.DOCKER_OPTS=\"-s btrfs\"Now restart docker.$ service docker.io restartWe should see that docker is running with “-s btrfs”:$ ps ax  |grep [d]ocker10088 ?        Sl     0:02 /usr/bin/docker.io -d -s btrfsIf it’s not running with “-s btrfs” then ensure it’s set to do so in /etc/default/docker.io and has been restarted.I’ve already created several containers and pulled images, actually just the base busybox image which is quite small, so if I run “btrfs subvolume list /var/lib/docker” I should see some subvolumes have been created.vagrant@host1:~$ sudo btrfs subvolume list /var/lib/docker | head -5ID 258 gen 11 top level 5 path btrfs/subvolumes/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158ID 259 gen 16 top level 5 path btrfs/subvolumes/42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229ID 260 gen 13 top level 5 path btrfs/subvolumes/c120b7cab0b0509fd4de20a57d0f5c17106f3451200dfbfd8c6ab1ccb9391938ID 261 gen 13 top level 5 path btrfs/subvolumes/d200959a3e91d88e6da9a0ce458e3cdefd3a8a19f8f5e6a1e7f10f268aea5594ID 262 gen 15 top level 5 path btrfs/subvolumes/120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16And we can see the size of the btrfs file system.$ btrfs filesystem df /var/lib/dockerData, single: total=1.01GiB, used=337.56MiBSystem, DUP: total=8.00MiB, used=16.00KiBSystem, single: total=4.00MiB, used=0.00Metadata, DUP: total=1.00GiB, used=2.72MiBMetadata, single: total=8.00MiB, used=0.00$$ df -hFilesystem      Size  Used Avail Use% Mounted on/dev/sda1        40G  1.2G   37G   4% /none            4.0K     0  4.0K   0% /sys/fs/cgroupudev            745M   12K  745M   1% /devtmpfs           150M  368K  150M   1% /runnone            5.0M     0  5.0M   0% /run/locknone            750M     0  750M   0% /run/shmnone            100M     0  100M   0% /run/uservagrant         233G  185G   49G  80% /vagrant/dev/sdb         20G  344M   18G   2% /var/lib/docker$$ sudo btrfs filesystem show /var/lib/dockerLabel: none  uuid: c8f11393-9268-475a-82de-cbd697ab3847  Total devices 1 FS bytes used 340.30MiB  devid    1 size 20.00GiB used 3.04GiB path /dev/sdbBtrfs v3.12Each image and container has a subvolume.And, as far as I know at this point, that’s it–we’re now running docker with btrfs.Ansible playbookI’ve setup an Ansible playbook with a Vagrant file that will setup a virtual machine with docker and btrfs configured in the same way that I describe in this blog post. Vagrant will automatically provision the virtual machine using Ansible.$ git clone https://github.com/ccollicutt/vagrant-docker-btrfs$ cd vagrant-docker-btrfs$ vagrant up --provider virtualboxBringing machine 'host1' up with 'virtualbox' provider...==&gt; host1: Importing base box 'trusty64'...SNIP!TASK: [add alias for vagrant user docker = docker.io] ************************* changed: [host1]NOTIFIED: [restart docker] **************************************************** changed: [host1]PLAY RECAP ******************************************************************** host1                      : ok=17   changed=16   unreachable=0    failed=0 $  Now that that is booted up and configured, we can login and check a couple things.$ vagrant sshSNIP!vagrant@host1:~$ sudo btrfs subvolume list /var/lib/dockervagrant@host1:~$# none because we have no images or containers vagrant@host1:~$ mount  |grep btrfs/dev/sdb on /var/lib/docker type btrfs (rw)vagrant@host1:~$ ps ax  |grep [d]ocker10041 ?        Sl     0:00 /usr/bin/docker.io -d -s btrfsvagrant@host1:~$ docker pull busyboxPulling repository busyboxa9eb17255234: Download complete d200959a3e91: Download complete fd5373b3d938: Download complete 37fca75d01ff: Download complete 511136ea3c5a: Download complete 42eed7f1bf2a: Download complete f06b02872d52: Download complete 120e218dd395: Download complete c120b7cab0b0: Download complete 1f5049b3536e: Download complete vagrant@host1:~$ docker imagesREPOSITORY          TAG                   IMAGE ID            CREATED             VIRTUAL SIZEbusybox             buildroot-2013.08.1   d200959a3e91        4 days ago          2.489 MBbusybox             ubuntu-14.04          37fca75d01ff        4 days ago          5.609 MBbusybox             ubuntu-12.04          fd5373b3d938        4 days ago          5.455 MBbusybox             buildroot-2014.02     a9eb17255234        4 days ago          2.433 MBbusybox             latest                a9eb17255234        4 days ago          2.433 MBvagrant@host1:~$# now we should have some subvolumesvagrant@host1:~$ sudo btrfs subvolume list /var/lib/dockerID 258 gen 12 top level 5 path btrfs/subvolumes/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158ID 259 gen 17 top level 5 path btrfs/subvolumes/42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229ID 260 gen 14 top level 5 path btrfs/subvolumes/f06b02872d5253f5123284edcf49749b352400a1c5880b5ebf2864f5afddeb22ID 261 gen 14 top level 5 path btrfs/subvolumes/37fca75d01ffc49df7b99aacdbcd4a0ebae39de299787b8f77bb5b6698414308ID 262 gen 16 top level 5 path btrfs/subvolumes/120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16ID 263 gen 16 top level 5 path btrfs/subvolumes/a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721ID 264 gen 19 top level 5 path btrfs/subvolumes/1f5049b3536eb73e7a660a672976ae9c19e8460bf57c2528f9c1e4b2c4bf309fID 265 gen 19 top level 5 path btrfs/subvolumes/c120b7cab0b0509fd4de20a57d0f5c17106f3451200dfbfd8c6ab1ccb9391938ID 266 gen 19 top level 5 path btrfs/subvolumes/d200959a3e91d88e6da9a0ce458e3cdefd3a8a19f8f5e6a1e7f10f268aea5594ID 267 gen 19 top level 5 path btrfs/subvolumes/fd5373b3d93820744a327e609ee86166e5984d7377987f0fde78daeaa345705dConclusionI haven’t explored much more in terms of using btrfs with docker yet, but this is a good start."
    } ,
  
    {
      "title"    : "Using Docker with Python and iPython",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/06/05/docker-python.html",
      "date"     : "2014-06-05 00:00:00 -0400",
      "content"  : "Right now Docker is one of the hottest projects on the planet, so that means some people aren’t going to like it simply based on that fact alone.Having said that, I really enjoy the paradigm shift in terms of working with containers, service discovery, and all the interesting new ideas and areas being created. (If you feel like having your mind warped, just read Jeff Linday’s twitter feed.)In this post I thought I would take a quick look at using the docker-py module to use Docker containers via Python and, one of my favorite programming applications, iPython.Install docker-pyFirst, you need docker-py. Note that in the examples show here I am using Ubuntu Trusty/14.04.$ pip install docker-pyipythonI really like iPython for exploring Python. It’s kind of an advanced Python shell, but also does much more.$ sudo apt-get install ipythonSNIP!$ ipythonPython 2.7.6 (default, Mar 22 2014, 22:59:56) Type \"copyright\", \"credits\" or \"license\" for more information.IPython 1.2.1 -- An enhanced Interactive Python.?         -&gt; Introduction and overview of IPython's features.%quickref -&gt; Quick reference.help      -&gt; Python's own help system.object?   -&gt; Details about 'object', use 'object??' for extra details.In [1]:Install dockerIf docker isn’t already installed, then go ahead and install it.$ sudo apt-get install docker.ioI also alias docker.io to docker.$ alias docker='docker.io'$ docker versionClient version: 0.9.1Go version (client): go1.2.1Git commit (client): 3600720Server version: 0.9.1Git commit (server): 3600720Go version (server): go1.2.1Last stable version: 0.11.1, please update dockerDocker should now have a socket open that we can connect to.$ ls /var/run/docker.sock /var/run/docker.sockPull an imageLet’s download the busybox image.$ docker pull busyboxPulling repository busybox71e18d715071: Download complete 98b9fdab1cb6: Download complete 1277aa3f93b3: Download complete 6e0a2595b580: Download complete 511136ea3c5a: Download complete b6c0d171b362: Download complete 8464f9ac64e8: Download complete 9798716626f6: Download complete fc1343e2fca0: Download complete f3c823ac7aa6: Download complete Now we are ready to use docker-py.Working with docker-pyNow that we have docker-py, iPython, Docker, and the busybox image, we can start some containers!If you’re not familiar with iPython, have a look at the tutorial. iPython is quite powerful.First, fire up ipython and import docker.$ ipythonPython 2.7.6 (default, Mar 22 2014, 22:59:56) Type \"copyright\", \"credits\" or \"license\" for more information.IPython 1.2.1 -- An enhanced Interactive Python.?         -&gt; Introduction and overview of IPython's features.%quickref -&gt; Quick reference.help      -&gt; Python's own help system.object?   -&gt; Details about 'object', use 'object??' for extra details.In [1]: import dockerNext we make a connection to Docker.In [2]: c = docker.Client(base_url='unix://var/run/docker.sock',   ...:                   version='1.9',   ...:                   timeout=10)Now we have a connection to Docker.iPython offers tab completion. If I type “c.” and then hit the TAB key, ipython will show me what the Docker connection object has to offer.In [3]: c.c.adapters                      c.headers                       c.pullc.attach                        c.history                       c.pushc.attach_socket                 c.hooks                         c.putc.auth                          c.images                        c.remove_containerc.base_url                      c.import_image                  c.remove_imagec.build                         c.info                          c.requestc.cert                          c.insert                        c.resolve_redirectsc.close                         c.inspect_container             c.restartc.commit                        c.inspect_image                 c.searchc.containers                    c.kill                          c.sendc.cookies                       c.login                         c.startc.copy                          c.logs                          c.stopc.create_container              c.max_redirects                 c.streamc.create_container_from_config  c.mount                         c.tagc.delete                        c.options                       c.topc.diff                          c.params                        c.trust_envc.events                        c.patch                         c.verifyc.export                        c.port                          c.versionc.get                           c.post                          c.waitc.get_adapter                   c.prepare_request               c.head                          c.proxies   Let’s look at c.images. I I put a “?” after the object, ipython will provide details about the object.In [5]: c.images?Type:       instancemethodString Form:&lt;bound method Client.images of &lt;docker.client.Client object at 0x7f3acc731790&gt;&gt;File:       /usr/local/lib/python2.7/dist-packages/docker/client.pyDefinition: c.images(self, name=None, quiet=False, all=False, viz=False)Docstring:  &lt;/code&gt;&lt;/pre&gt;And grab the busybox image.In [6]: c.images(name=\"busybox\")Out[6]: [{u'Created': 1401402591,  u'Id': u'71e18d715071d6ba89a041d1e696b3d201e82a7525fbd35e2763b8e066a3e4de',  u'ParentId': u'8464f9ac64e87252a91be3fbb99cee20cda3188de5365bec7975881f389be343',  u'RepoTags': [u'busybox:buildroot-2013.08.1'],  u'Size': 0,  u'VirtualSize': 2489301}, {u'Created': 1401402590,  u'Id': u'1277aa3f93b3da774690bc4f0d8bf257ff372e23310b4a5d3803c180c0d64cd5',  u'ParentId': u'f3c823ac7aa6ef78d83f19167d5e2592d2c7f208058bc70bf5629d4bb4ab996c',  u'RepoTags': [u'busybox:ubuntu-14.04'],  u'Size': 0,  u'VirtualSize': 5609404}, {u'Created': 1401402589,  u'Id': u'6e0a2595b5807b4f8c109f3c6c5c3d59c9873a5650b51a4480b61428427ab5d8',  u'ParentId': u'fc1343e2fca04a455f803ba66d1865739e0243aca6c9d5fd55f4f73f1e28456e',  u'RepoTags': [u'busybox:ubuntu-12.04'],  u'Size': 0,  u'VirtualSize': 5454693}, {u'Created': 1401402587,  u'Id': u'98b9fdab1cb6e25411eea5c44241561326c336d3e0efae86e0239a1fe56fbfd4',  u'ParentId': u'9798716626f6ae4e6b7f28451c0a1a603dc534fe5d9dd3900150114f89386216',  u'RepoTags': [u'busybox:buildroot-2014.02', u'busybox:latest'],  u'Size': 0,  u'VirtualSize': 2433303}]Create a container. Note that I'm adding a command to be run, in this example the \"env\" command.In [8]: c.create_container(image=\"busybox\", command=\"env\")Out[8]: {u'Id': u'584459a09e6d4180757cb5c10ac354ca46a32bf8e122fa3fb71566108f330c87', u'Warnings': None}Start the container using the Id.In [9]: c.start(container=\"584459a09e6d4180757cb5c10ac354ca46a32bf8e122fa3fb71566108f330c87\")And we can check the logs, should see the output of the command \"env\" that we configured when the container was created.In [11]: c.logs(container=\"584459a09e6d4180757cb5c10ac354ca46a32bf8e122fa3fb71566108f330c87\")Out[11]: 'HOME=/\\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\nHOSTNAME=584459a09e6d\\n'If I run a container with the same options using the dokcer command line, I should see something similar.$ docker run busybox envHOME=/PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=ce3ad38a52bfAs far as I can tell, docker-py does not have a run option, instead we have to create a container and then start it.Here's another example:In [17]: busybox = c.create_container(image=\"busybox\", command=\"echo hi\")In [18]: busybox?Type:       dictString Form:{u'Id': u'34ede853ee0e95887ea333523d559efae7dcbe6ae7147aa971c544133a72e254', u'Warnings': None}Length:     2Docstring:dict() -&gt; new empty dictionarydict(mapping) -&gt; new dictionary initialized from a mapping object's    (key, value) pairsdict(iterable) -&gt; new dictionary initialized as if via:    d = {}    for k, v in iterable:        d[k] = vdict(**kwargs) -&gt; new dictionary initialized with the name=value pairs    in the keyword argument list.  For example:  dict(one=1, two=2)In [19]: c.start(busybox.get(\"Id\"))In [20]: c.logs(busybox.get(\"Id\"))Out[20]: 'hi\\n'If you haven't used busybox images with docker yet, I definitely suggest it. I also suggest the debian:jessie image which is only 120MB, quite a bit smaller than, say, the Ubuntu images.## ConclusionDocker is a fascinating new system and it's going to be used to build interesting new technologies, especially around cloud services. Using iPython we've explored how to programmatically create docker containers using the docker-py module. Now using python we can create those next generation ideas using docker and containers. "
    } ,
  
    {
      "title"    : "Wordpress with FORCE_SSL_ADMIN behind a reverse proxy",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/05/31/wordpress-ssl-reverse-proxy.html",
      "date"     : "2014-05-31 00:00:00 -0400",
      "content"  : "This is a pretty specific problem that I have run into a couple of times, both times forgetting the solution and spending an hour or two googling, so I decided to blog it so I can easily find it next time. :)I am running a wordpress site behind hipache, which is also doing ssl termination. So the actual wordpress site is served via plaintext http, and if ssl is required then hipache will provide it. Thus the connection from the client to hipache is ssl, but the connection from hipache to the apache server running wordpress is http.Further, I want to force logins and access to /wp-admin to be ssl enabled using the wordpress option FORCE_SSL_ADMIN.The problem is that with FORCE_SSL_ADMIN configured, but no https for apache serving wordpress, connections to wp-login.php or /wp-admin enter into a redirect loop.The solution is adding an extra bit of configuration code to wp-config.php when using a reverse proxy to terminate ssl. Here is the page that describes what addtional configuration changes to make, which I also show below.define('FORCE_SSL_ADMIN', true);if ($_SERVER['HTTP_X_FORWARDED_PROTO'] == 'https')       $_SERVER['HTTPS']='on';With that added to the top of wp-config.php I can now have hipache serve up plain http for all of the wordpress site except wp-login.php and /wp-admin. Hopefully I remember to look here next time… :)"
    } ,
  
    {
      "title"    : "Environment variables with Docker",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/05/29/environment-variables-with-docker.html",
      "date"     : "2014-05-29 00:00:00 -0400",
      "content"  : "I’ve been working with Docker for a little while, and was wondering how environment variables work, so I took a few minutes to look into it.Docker supports setting environment variables with the -e switch.Below is the simplest example. As can be seen, the FOO variable is indeed set to bar within the container.# docker run -e FOO=bar busybox envHOME=/PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=9c6f6cd077b2FOO=barMultiple variables can be added with multiple -e switches.# docker run -e FOO=bar -e BAR=foo busybox envHOME=/PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=6cf2d6e8acb3FOO=barBAR=fooUsing environment variables in a script in a docker imageBelow is a simple docker file that adds a script called run.sh to the image.# file DockerfileFROM busyboxADD run.sh run.shRUN chmod +x run.shCMD ./run.shThis is what is in the run.sh script. If FOO is not set, the container should report “FOO is empty” and otherwise it should print what the variable has been set to.# file run.sh#!/bin/shif [ -z \"$FOO\" ]; then\techo \"FOO is empty\"else\techo \"FOO is $FOO\"fiI can now build that Dockerfile.# lsDockerfile  run.sh# docker build -t testenv .Uploading context 3.584 kBUploading context Step 0 : FROM busybox ---&gt; 4c0f792ebd1eStep 1 : ADD run.sh run.sh ---&gt; 6154a355bdd6Step 2 : RUN chmod +x run.sh ---&gt; Running in 20784036cde1 ---&gt; 784a1682769bStep 3 : CMD ./run.sh ---&gt; Running in d86187fc3a6f ---&gt; 1c952ed5cc6dSuccessfully built 1c952ed5cc6dRemoving intermediate container c598898e1706Removing intermediate container 20784036cde1Removing intermediate container d86187fc3a6fNow that the image has been built, I can run a container based off it, and the run.sh script should execute.# docker run testenvFOO is emptyAnd now adding the FOO environment variable to the docker run command:# docker run -e FOO=bar testenvFOO is bar# docker run -e FOO=$RANDOM testenvFOO is 10567# docker run -e FOO=$RANDOM testenvFOO is 8898Awesome. Now with this ability, we could, for example, create one Wordpress container and have it configure wp-config.php with the correct MySQL connection variables, and have that container connect out to the MySQL server with the right user and password, rather than creating an image for each Wordpress site."
    } ,
  
    {
      "title"    : "Manage docker hosts with shipyard",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/05/25/docker-shipyard-multihost.html",
      "date"     : "2014-05-25 00:00:00 -0400",
      "content"  : "Docker is a container management system. Containers are a form of virtualization, but they are not like typical virtualization that one would achieve with KVM or VMWare. I think it’s best to consider them as modeling processes instead of servers. If you haven’t experimented with Docker yet, here is a good blog post on the terminology used which should help to alleviate some of the confusion around what Docker actually does and how.It’s pretty easy to install docker. Here I install it on Ubuntu Trusty 14.04.vagrant@host1:~$ sudo apt-get install -y docker.ioAnd that’s it. Once that command completes the server is a docker host.But what if we want to manage multiple docker hosts using one interface and/or API? What if we want ten docker hosts running and to be able to manage them all as though it’s one container-as-a-service system?That’s where shipyard comes in.ShipyardShipyard is a project that can manage multiple docker hosts. Quoting from the website:  Shipyard gives you the ability to manage Docker resources including containers, images, hosts, and more all from a single management interface. Shipyard can manage multiple Docker hosts giving the flexibility to build redundant, highly available applications.There are actually several similar systems being worked on right now that can tie together multiple docker hosts. One example is Flynn which has flynn-host, another is OpenStack which can manage docker containers, yet another is CoreOS, Project Atomic is new…and there are more out there. Typically these systems will do much more than just manage containers (or jobs) but they will all have that functionality. (Feel free to correct me if I’m wrong.)Setup the shipyard hostThis is as easy as starting a container. The shipyard project has a pre-created and distributed shipyard docker container. All we have to do is install that image and run it. Note that this will download a few hundred megabytes of images if they do not already exist on the docker host.vagrant@host1:~$ sudo docker.io run -i -t -v /var/run/docker.sock:/docker.sock shipyard/deploy setupSNIP!eba9b5f1d1d1: Download complete 08852c160ec2: Download complete 2cbf6e5024d8: Download complete Shipyard Stack DeployedYou should be able to login with admin:shipyard at http://:8000You will also need to setup and register the Shipyard Agent. See http://github.com/shipyard/shipyard-agent for details.&lt;/code&gt;&lt;/pre&gt;As the output says, the web gui can be accessed on the docker host and port 8000 with the default user/pass of admin/shipyard.## Shipyard clientNow we need to install the shipyard agent on the docker hosts.First, make sure Docker is listening on a localhost via tcp. By default--for security reasons--docker will only listen on a local socket.Change the DOCKER_OPTS in /etc/default/docker to:DOCKER_OPTS=\"-H tcp://127.0.0.1:4243 -H unix:///var/run/docker.sock\"Then restart docker. Docker should be listening on 4243.vagrant@host1:/etc/default$ netstat -ant  |grep 4243tcp        0      0 127.0.0.1:4243          0.0.0.0:*               LISTEN  Now that the shipyard host is up and Docker is listening on a tcp port, we can register an agent. I installed shipyard-agent in /usr/local/bin by downloading the [latest release](https://github.com/shipyard/shipyard-agent/releases).vagrant@host1$ sudo ./shipyard-agent -url http://192.168.5.89:8000 -register2014/05/25 14:36:37 Using 10.0.2.15 for the Docker Host IP for Shipyard2014/05/25 14:36:37 If this is not correct or you want to use a different IP, \\please update the host in Shipyard2014/05/25 14:36:37 Registering at http://192.168.5.89:80002014/05/25 14:36:37 Agent Key:  b3d356b1294d4a729cd43beac8d7c01cvagrant@host1$Once the shipyard-agent with -register is run, it will appear in the shipyard web gui to be activated, but first lets run it with that key.vagrant@host1:/usr/local/bin$ sudo ./shipyard-agent -address=\"192.168.5.89\" \\-url http://192.168.5.89:8000 -key b3d356b1294d4a729cd43beac8d7c01c2014/05/25 14:50:39 Shipyard Agent (http://192.168.5.89:8000)2014/05/25 14:50:39 Listening on 192.168.5.89:4500# it stays in the foreground(Note that you would normally want to run shipyard-agent out of some kind of process supervisory system.)host1 now appears in the web gui.![](https://raw.githubusercontent.com/ccollicutt/ccollicutt.github.com/master/img/shipyard_new_host.png)In the web gui we can manually activate the host. Also note that it picked the eth0 IP address which when using Vagrant is not the one we want to use, so I manually set it to the eth1 IP address which in my case is 192.168.5.89. There doesn't seem to be an option at this time to specify the IP.Once the IP is changed and the host is activated, we can click on containers and get a list of what is running.![](https://raw.githubusercontent.com/ccollicutt/ccollicutt.github.com/master/img/shipyard_hosts.png)Above we can see that the shipyard containers are displayed as running on host1, which is where the shipyard host was installed.I then went through the same process to add the second host. Now shipyard is managing two docker hosts: host1 and host2.![](https://raw.githubusercontent.com/ccollicutt/ccollicutt.github.com/master/img/shipyard_both_hosts.png)With shipyard managing the docker hosts, we can do things like pull the busybox image from the docker registry. By clicking on images-&gt;import and entering the tag of the docker image and clicking import again, the busybox image will be downloaded to both hosts.![](https://raw.githubusercontent.com/ccollicutt/ccollicutt.github.com/master/img/shipyard_busybox.png)vagrant@host1:~$ sudo docker.io images | grep busyboxbusybox     buildroot-2013.08.1   123fb16d32f8        26 hours ago        2.489 MBbusybox     ubuntu-14.04          b9ca777960b9        26 hours ago        5.609 MBbusybox     ubuntu-12.04          8ba0d1860bb6        26 hours ago        2.433 MBbusybox     buildroot-2014.02     4c0f792ebd1e        38 hours ago        2.433 MBbusybox     latest                4c0f792ebd1e        38 hours ago        2.433 MBAnd on host2:vagrant@host2:~$ sudo docker.io images | grep busyboxbusybox     buildroot-2013.08.1   123fb16d32f8        26 hours ago        2.489 MBbusybox     ubuntu-14.04          b9ca777960b9        26 hours ago        5.609 MBbusybox     ubuntu-12.04          8ba0d1860bb6        26 hours ago        2.433 MBbusybox     buildroot-2014.02     4c0f792ebd1e        38 hours ago        2.433 MBbusybox     latest                4c0f792ebd1e        38 hours ago        2.433 MB## Shipyard cliMost of what I have been showing with regards to shipyard is the web gui. But I'm not a big fan of web guis. I want to use virtual machines and containers programatically, or at the very least from the command line.Shipyard has a golang [cli](https://github.com/shipyard/shipyard-cli) that is in \"active development and has limited functionality,\" but let's try it out.curtis$ git clone https://github.com/shipyard/shipyard-clicurtis$ cd shipyard-clicurtis$ makegithub.com/gcmurphy/getpass (download)github.com/shipyard/shipyard-go (download)github.com/wsxiaoys/terminal (download)curtis$ lsMakefile\tcli\t\treadme.md\tshipyardNow that the shipyard binary has been compiled we can use it. curtis$ ./shipyard NAME:   Shipyard CLI - Command line interface for ShipyardUSAGE:   Shipyard CLI [global options] command [command options] [arguments...]VERSION:   0.1.1COMMANDS:   login\tLogin   apps\t\tApplication Management   containers\tContainer Management   images\tImage Management   hosts\tHost Management   config, cfg\tShow current Shipyard config   info, info\tShow Shipyard Info   help, h\tShows a list of commands or help for one command   GLOBAL OPTIONS:   --username \t\t\t\t\tShipyard API Username   --key \t\t\t\t\tShipyard API Key   --url \t\t\t\t\tShipyard URL   --api-version '1'\t\t\t\tShipyard API Version   --config, -c '/Users/curtis/.shipyard.cfg'\tConfig File   --version, -v\t\t\t\tprint the version   --help, -h\t\t\t\t\tshow help  First we login.curtis$ ./shipyard loginURL: http://192.168.5.89:8000Username: adminPassword: Version (default: 1):  Login successfulThe login command creates a .shipyard.cfg file for us so that we don't have to \"login\" again.curtis$ cat ~/.shipyard.cfg {\"Username\":\"admin\",\"ApiKey\":\"cc7d9720798af55c05684d240a7b5186405d0e80\",\\\"Url\":\"http://192.168.5.89:8000\",\"Version\":\"1\"}Now we can run commands.curtis$ ./shipyard hosts host2 (192.168.5.90) host1 (192.168.5.89)curtis$ ./shipyard images 8ba0d1860bb6 busybox:ubuntu-12.04 6379130228c2 shipyard/lb:latest 180e6bd6c10d debian:jessie b48b681ac984 shipyard/redis:latest 123fb16d32f8 busybox:buildroot-2013.08.1 b9ca777960b9 busybox:ubuntu-14.04 4c0f792ebd1e busybox:buildroot-2014.02 590fa59c6dc3 shipyard/router:latest 123fb16d32f8 busybox:buildroot-2013.08.1 8ba0d1860bb6 busybox:ubuntu-12.04 4c0f792ebd1e busybox:buildroot-2014.02 b9ca777960b9 busybox:ubuntu-14.04 626eb587cec1 shipyard/db:latest bc62aa0fb727 shipyard/deploy:latest 30e0b59613ff shipyard/shipyard:latest## ConclusionIn the end what we have done here is fairly basic--just install a shipyard host and a couple of clients. Certainly there are other systems that do the same thing and much more, but I think shipyard is a good way to get introduced to the concepts of a multihost docker system. I'd also like to automate the deployment of shipyard, but have a couple things to figure out, such as how to register the agent automatically, activating the hosts without the web gui, as well as setup some sort of supervisory system for the agent. Also there are a few things that shipyard can do that I haven't touched on, such as the concept of applications.Hopefully in the next couple of weeks I'll explore more with regards as to how shipyard works and what can be done with it, as well as consider how it compares and contrasts to other systems. I believe that containers are an important technology, and that there is room for simpler tools that can provide containers-as-a-service, perhaps as part of a PaaS system, or just on their own. There are many different and interesting ways to virtualize, compartmentalize, and control mulitihost systems."
    } ,
  
    {
      "title"    : "Swiftacular - Install OpenStack Swift on Ubuntu Trusty 14.04",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/05/24/swiftacultar-ubuntu-trusty-1404.html",
      "date"     : "2014-05-24 00:00:00 -0400",
      "content"  : "Recently I updated my Swiftacular project to support Ubuntu Trusty 14.04 and OpenStack Icehouse (which Trusty comes with by default).Swiftacular installs OpenStack Swift using Ansible on CentOS 6.5, Ubuntu 12.04, and Ubuntu 14.04. In CentOS and Ubuntu 12.04 it installs the OpenStack Havana release of Swift, and in Ubuntu 14.04 it uses Icehouse.This post shows how to install OpenStack Swift Icehouse on Ubuntu 14.04 using Swiftacular.RequirementsBelow are the tools required:  Git  Ansible  Virtualbox  Vagrant  Internet connection  Enough resources for seven virtual machinesNote that I’ve only tested this on OSX Mavericks. But it should be quite easy to adapt to any situation if you don’t mind changing some IP addresses in the group_vars/all file. Another goal I have is to have examples of getting Swiftacular up and running using libvirt + kvm, AWS, Digital Ocean, and other IaaS providers.Setup OpenStack Swift using SwiftacularFirst, clone the Swiftacular repository and add some libraries. (I should put this all into a make file–it’s on the todo list!)curtis$ git clone https://github.com/ccollicutt/swiftacular.gitcurtis$ cd swiftacularcurtis$ git clone https://github.com/openstack-ansible/openstack-ansible-modules library/openstackEdit the Vagrantfile so that the Ubuntu Trusty box will be used instead of the default Precise box.curtis$ grep config.vm.box Vagrantfile     config.vm.box = \"trusty64\"    config.vm.box_url = \"https://cloud-images.ubuntu.com/vagrant/trusty/current/trusty-server-cloudimg-amd64-vagrant-disk1.box\"    #config.vm.box = \"centos65\"    #config.vm.box_url = \"http://puppet-vagrant-boxes.puppetlabs.com/centos-65-x64-virtualbox-nocm.box\"    #config.vm.box = \"precise64\"    #config.vm.box_url = \"http://files.vagrantup.com/precise64.box\"Now we can use vagrant to start the virtual machines.curtis$ vagrant upSNIP!   swift-storage-03: Warning: Remote connection disconnect. Retrying...==&gt; swift-storage-03: Machine booted and ready!==&gt; swift-storage-03: Checking for guest additions in VM...==&gt; swift-storage-03: Setting hostname...==&gt; swift-storage-03: Configuring and enabling network interfaces...==&gt; swift-storage-03: Mounting shared folders...    swift-storage-03: /vagrant =&gt; /Users/curtis/working/swiftacularcurtis$ # done booting all the vmsTest connectivity using ansible ping.curtis$ ansible -m ping all192.168.100.100 | success &gt;&gt; {    \"changed\": false,     \"ping\": \"pong\"}192.168.100.50 | success &gt;&gt; {    \"changed\": false,     \"ping\": \"pong\"}192.168.100.30 | success &gt;&gt; {    \"changed\": false,     \"ping\": \"pong\"}192.168.100.200 | success &gt;&gt; {    \"changed\": false,     \"ping\": \"pong\"}192.168.100.20 | success &gt;&gt; {    \"changed\": false,     \"ping\": \"pong\"}192.168.100.201 | success &gt;&gt; {    \"changed\": false,     \"ping\": \"pong\"}192.168.100.202 | success &gt;&gt; {    \"changed\": false,     \"ping\": \"pong\"}Good.We can also use vagrant status to list all the vms. Their names should describe what they do fairly well. The lbssl server is the ssl termination point for the swift proxy. It’s not doing any load balancing in this situation, but could if there were multiple proxy servers.curtis$ vagrant statusSNIP!swift-package-cache-01    running (virtualbox)swift-keystone-01         running (virtualbox)swift-lbssl-01            running (virtualbox)swift-proxy-01            running (virtualbox)swift-storage-01          running (virtualbox)swift-storage-02          running (virtualbox)swift-storage-03          running (virtualbox)Now lets set the ansible variables. The default variables should work, but the passwords are set to CHANGEME.curtis$ cp group_vars/all.example group_vars/allcurtis$ # edit the group_vars/all file Finally we can run ansible-playbook and install OpenStack Swift Icehouse across all the nodes. Depending on your internet connection and disk performance this should only take five or six minutes.curtis$ ansible-playbook site.yml PLAY [package_cache] ********************************************************** GATHERING FACTS *************************************************************** ok: [192.168.100.20]SNIP!PLAY RECAP ******************************************************************** 192.168.100.100            : ok=28   changed=22   unreachable=0    failed=0   192.168.100.20             : ok=22   changed=16   unreachable=0    failed=0   192.168.100.200            : ok=46   changed=37   unreachable=0    failed=0   192.168.100.201            : ok=46   changed=37   unreachable=0    failed=0   192.168.100.202            : ok=46   changed=37   unreachable=0    failed=0   192.168.100.30             : ok=20   changed=17   unreachable=0    failed=0   192.168.100.50             : ok=43   changed=36   unreachable=0    failed=0  If I ssh into the proxy server I can see what OpenStack packages have been installed.curtis$ vagrant ssh swift-proxy-01SNIP!vagrant@swift-proxy-01:~$ dpkg --list | grep swift | tr -s \" \" | cut -f 2,3 -d \" \"python-swift 1.13.1-0ubuntu1python-swiftclient 1:2.0.3-0ubuntu1swift 1.13.1-0ubuntu1swift-object 1.13.1-0ubuntu1swift-plugin-s3 1.7-3swift-proxy 1.13.1-0ubuntu1Same with the storage servers.vagrant@swift-storage-01:~$ dpkg --list | grep swift | tr -s \" \" | cut -f 2,3 -d \" \"python-swift 1.13.1-0ubuntu1python-swiftclient 1:2.0.3-0ubuntu1swift 1.13.1-0ubuntu1swift-account 1.13.1-0ubuntu1swift-container 1.13.1-0ubuntu1swift-object 1.13.1-0ubuntu1Now we can run a small test by uploading a text file into a container. I usually run this from the package cache server that is created as part of Swiftacular. But you can run the swift command line client from anywhere that the proxy server can be accessed from.vagrant@swift-package-cache-01:~$ . testrc vagrant@swift-package-cache-01:~$ echo \"swift is cool\" &gt; swift.txtvagrant@swift-package-cache-01:~$ swift upload test_container swift.txt swift.txtvagrant@swift-package-cache-01:~$ swift listtest_containervagrant@swift-package-cache-01:~$ swift list test_containerswift.txtAt this point we have a nice little working test cluster of OpenStack Swift Icehouse release running on Ubuntu Trusty 14.04.There is still a lot of work I would like to do with Swiftacular. If you have any suggestions, comments, or criticisms please do let me know in the comments or enter an issue into the github repository."
    } ,
  
    {
      "title"    : "gpg-zip",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/05/04/gpg-zip.html",
      "date"     : "2014-05-04 00:00:00 -0400",
      "content"  : "I have a dedicated server I use to host a few things, like friends wordpress sites, some personal websites, etc. One of the services the hosting company provides is access to a backup server via ftp, ie. they give me some free space to backup files to.But I don’t want to just drop backup files there in plaintext–I want them to be encrypted with gpg. Enter gpg-zip!gpg-zip is handy utility that can tar a directory and then encrypt the resulting file. That file could then be placed anywhere, and, if gpg is working properly, unless someone has access to the private key of the public key that it was encrypted with (or they were set as the recipient), they should not be able to decrypt it. It’s important to note that the encrypted file is only as secure as the private key. Obviously is someone has access to the private key then they can decrypt the file.First, I have a key that I’m going to use for this example. Creating keys and subkeys securely is a bit beyond the scope of this article. Here’s a good blog post on creating new gpg keys, and another here.Below is the key I will use.# gpg --list-keys/root/.gnupg/pubring.gpg------------------------pub   2048R/4FCDA707 2014-05-04uid                  curtis &lt;curtis-backups@serverascode.com&gt;sub   2048R/25AEB942 2014-05-04It’s important to note that that key is not itself encrypted–it does not have a passphrase set, so it can be used by anyone who has access to a copy of the key. This is not a good thing to do with an important key, but in this case I am going to want to automate this process, and there is no good way, that I’m aware of, to have an automated process decrypt the key without the passphrase also being stored in cleartext. That being said, encrypting the file does not require the password to unlock the private key, but decrypting in an automated fashion would.Let’s create a test directory with some files in it to gpg-zip.# cd /tmp; mkdir test# for i in $(seq 1 100); do echo \"hi $i\" &gt; test/$i.txt; done# ls test | wc -l100Now that we have a directory filled with files to backup it can be encrypted with gpg. I’m essentially encrypting the file with myself as the recipient, ie. a message to myself.# gpg-zip --encrypt --recipient curtis-backup@serverascode.com test &gt; test.tar.gz.gpg# file test.tar.gz.gpg test.tar.gz.gpg: dataI can also use gpg-zip to list the files (but obviously I can only do that if I can decrypt it):# gpg-zip --list-archive test.tar.gz.gpg | tailgpg: encrypted with 2048-bit RSA key, ID 25AEB942, created 2014-05-04      \"curtis &lt;curtis-backup@serverascode.com&gt;\"test/57.txttest/26.txttest/84.txttest/6.txttest/16.txttest/4.txttest/18.txttest/20.txttest/45.txttest/76.txtAnd I can restore the files as well. I’ll do that in a restore directory.# mkdir restore; cd restore# gpg-zip --decrypt /tmp/test.tar.gz.gpg gpg: encrypted with 2048-bit RSA key, ID 25AEB942, created 2014-05-04      \"curtis &lt;curtis-backup@serverascode.com&gt;\"test/test/22.txttest/74.txttest/47.txttest/5.txtSNIP!Looks good. Now that I have a feeling for how gpg-zip works I can start automating my backups, encrypting them, and shipping them off to the remote ftp server.Conclusiongpg-zip is an easy way to encrypt directories. The resulting file can then be shipped across the (unencrypted) wire and stored on a remote system, and I should feel reasonably confident that unless someone surreptitiously accesses my server and steals my gpg keys, that the file will remain secure. There are a few caveats of course, but overall I think this workflow is reasonable.If you see any mistakes or other issues with this post, please let me know in the comments. :)"
    } ,
  
    {
      "title"    : "First look at Digital Ocean",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/04/26/first-look-digital-ocean.html",
      "date"     : "2014-04-26 00:00:00 -0400",
      "content"  : "Even though I have worked a great deal with OpenStack and various virtualization technologies, I haven’t really used infrastructure as a service all that much. Being Canadian has made using Amazon Web Services unlikely as I normally work at organizations that have concerns (justified or not) about their data staying in Canada. Usually that means AWS is off the table. I’ve used Rackspace a bit to test out their Rackspace files product which is based on OpenStack Swift and I’ve also used Heroku to deploy a hubot, but that’s about it.Digital OceanDigital Ocean (DO) keeps popping up in my RSS feeds, so I thought I’d give it a shot. From what I’ve read DO is a simplified IaaS provider that also separates itself from the rest of the pack by its use of only solid state drives–no spinning rust here. While I believe Rackspace now provides all SSDs, DO was kind of the first to really market themselves using SSDs (at least the first I was aware of at any rate).I mention the word “simplified” above. This is another way that I believe DO is distinguishing itself from other IaaS providers–a simplified web interface, possibly a simplified product offering, and easy to understand pricing.Sign-upThe sign-up is very straight forward. Nothing but an email and a password. I don’t think a second field to verify a password for a new sign-up is necessary, especially given how passwords can usually be reset with a link in an email, so the fact that DO only has one password field seems smart to me. Also the fact that the sign up is on the front page is telling.Credit card infoThe first thing I wanted to do is find out what Operating Systems they support for virtual machines. I’m sure they support all the standard OSes, but the sysadmin in me wants to go to that page first. However, as far as I can tell I can’t see that information without putting in a credit card first. So I enter my credit card info.Simple web interfaceI’m definitely enjoying the simple web interface. I think there is a lot of room in IaaS to provide a very simple web gui. I think DO is doing a smart thing here, courting developers, simple web interface, simple usage, relatively low cost, and SSDs. AWS has tons of features and products that are difficult to understand, and what’s more, it’s hard to figure out what your bill is going to be (though again, I haven’t used AWS much).There’s not much going on in the sidebar. Just:  Create  Droplets  Images  SSH keys  Billing  Support  DNS  APIMost of these concepts are going to be in every IaaS provider. Create a virtual machine/instance/droplet. What instances do I already have running? What images can these droplets be based off of? What SSH keys are automatically added to the image when it’s booted? (Most IaaS providers inject an SSH key into the virtual machine and also turn off SSH password access–only access using SSH keys is allowed at first to remove the ability of bots to break the password and login.)Add an SSH keyA big button says “Add SSH key.” It’s calling to me. :)Surprisingly I can’t just upload the file I have to paste it into the text box. Sure. Once I do that it shows my key in the SSH keys list.Now I’m going to guess that having entered my CC information, and having uploaded at least one SSH key, I can now boot a droplet/vm/instance.But I don’t want to do that with the web gui, because I don’t really like to use gui interfaces. Looks like I need to find a command line application that can use the API that DO seems to have…TugboatSo I google search for “digital ocean command line” and below are some of the results. Looks like there is an command line app called “tugboat.”Let’s get that. The github README for tugboat says “gem install tugboat.”curtis$ sudo gem install tugboatSNIP!10 gems installedI noticed that during the install a digital_ocean gem was brought down too:curtis$ gem list | grep digitaldigital_ocean (1.0.1)I’m willing to bet that DO provides that gem, and Ruby code can take advantage of it, just like tugboat.Now I’ve got a tugboat command.curtis$ which tugboat/usr/bin/tugboatHere are all the commands tugboat provides:curtis$ tugboatCommands:  tugboat add-key NAME                                 # Upload an ssh public key.  tugboat authorize                                    # Authorize a DigitalOcean account with tugboat  tugboat create NAME                                  # Create a droplet.  tugboat destroy FUZZY_NAME                           # Destroy a droplet  tugboat destroy_image FUZZY_NAME                     # Destroy an image  tugboat droplets                                     # Retrieve a list of your droplets  tugboat halt FUZZY_NAME                              # Shutdown a droplet  tugboat help [COMMAND]                               # Describe commands or a specific command  tugboat images                                       # Retrieve a list of your images  tugboat info FUZZY_NAME [OPTIONS]                    # Show a droplet's information  tugboat info_image FUZZY_NAME [OPTIONS]              # Show an image's information  tugboat keys                                         # Show available SSH keys  tugboat password-reset FUZZY_NAME                    # Reset root password  tugboat rebuild FUZZY_NAME IMAGE_NAME                # Rebuild a droplet.  tugboat regions                                      # Show regions  tugboat resize FUZZY_NAME -s, --size=N               # Resize a droplet  tugboat restart FUZZY_NAME                           # Restart a droplet  tugboat sizes                                        # Show available droplet sizes  tugboat snapshot SNAPSHOT_NAME FUZZY_NAME [OPTIONS]  # Queue a snapshot of the droplet.  tugboat ssh FUZZY_NAME                               # SSH into a droplet  tugboat start FUZZY_NAME                             # Start a droplet  tugboat verify                                       # Check your DigitalOcean credentials  tugboat version                                      # Show version  tugboat wait FUZZY_NAME                              # Wait for a droplet to reach a stateOops, looks like I could have added my key with tugboat. Oh well.The tugboat README says use “tugboat authorized” to setup the API information to allow tugboat to access the DO API using my credentials.Note that I found my API credentials and keys in the web gui under “API.”curtis$ tugboat authorizeNote: You can get this information from digitalocean.com/api_accessEnter your client key: Enter your API key: Enter your SSH key path (optional, defaults to ~/.ssh/id_rsa): ~/.ssh/id_dsaEnter your SSH user (optional, defaults to curtis): Enter your SSH port number (optional, defaults to 22): To retrieve region, image, size and key ID's, you can use the corresponding tugboat command, such as `tugboat images`.Defaults can be changed at any time in your ~/.tugboat configuration file.Enter your default region ID (optional, defaults to 1 (New York)): Enter your default image ID (optional, defaults to 350076 (Ubuntu 13.04 x64)): Enter your default size ID (optional, defaults to 66 (512MB)): Enter your default ssh key ID (optional, defaults to none): curtisEnter your default for private networking (optional, defaults to false): trueEnter your default for enabling backups (optional, defaults to false): Authentication with DigitalOcean was successful.&lt;/code&gt;&lt;/pre&gt;I set everything as default except put the right location for my local key, and pointed it to the SSH key I already uploaded as default, and also enabled private networking. I know from having read a blog post on DO that they support a form of private networking in certain regions.Lets see what I get from tugboat.curtis$ tugboat dropletsYou don't appear to have any droplets.Try creating one with `tugboat create`# so no droplets yetcurtis$ tugboat keysSSH Keys:curtis (id: 118429)What images are available? \"--global\" means show all images, not just my own images.curtis$ tugboat images --globalMy Images:No images foundGlobal Images:CentOS 5.8 x64 (id: 1601, distro: CentOS)CentOS 5.8 x32 (id: 1602, distro: CentOS)Debian 6.0 x64 (id: 12573, distro: Debian)Debian 6.0 x32 (id: 12575, distro: Debian)Ubuntu 10.04 x64 (id: 14097, distro: Ubuntu)Ubuntu 10.04 x32 (id: 14098, distro: Ubuntu)Arch Linux 2013.05 x64 (id: 350424, distro: Arch Linux)Arch Linux 2013.05 x32 (id: 361740, distro: Arch Linux)CentOS 6.4 x32 (id: 376568, distro: CentOS)CentOS 6.4 x64 (id: 562354, distro: CentOS)Ubuntu 12.04.4 x32 (id: 3100616, distro: Ubuntu)Ubuntu 12.04.4 x64 (id: 3101045, distro: Ubuntu)Ubuntu 13.10 x32 (id: 3101580, distro: Ubuntu)Ubuntu 12.10 x32 (id: 3101888, distro: Ubuntu)Ubuntu 12.10 x64 (id: 3101891, distro: Ubuntu)Ubuntu 13.10 x64 (id: 3101918, distro: Ubuntu)Debian 7.0 x32 (id: 3102384, distro: Debian)Debian 7.0 x64 (id: 3102387, distro: Debian)Fedora 19 x32 (id: 3102721, distro: Fedora)Fedora 19 x64 (id: 3102879, distro: Fedora)Ubuntu 12.10 x64 Desktop (id: 3104282, distro: Ubuntu)Docker 0.10 on Ubuntu 13.10 x64 (id: 3104894, distro: Ubuntu)MEAN on Ubuntu 12.04.4 (id: 3118235, distro: Ubuntu)GitLab 6.6.5 CE (id: 3118238, distro: Ubuntu)LAMP on Ubuntu 12.04 (id: 3120115, distro: Ubuntu)Ghost 0.4.2 on Ubuntu 12.04 (id: 3121555, distro: Ubuntu)Wordpress on Ubuntu 13.10 (id: 3135725, distro: Ubuntu)Ruby on Rails on Ubuntu 12.10 (Nginx + Unicorn) (id: 3137635, distro: Ubuntu)Redmine on Ubuntu 12.04 (id: 3137903, distro: Ubuntu)Ubuntu 14.04 x32 (id: 3240033, distro: Ubuntu)Ubuntu 14.04 x64 (id: 3240036, distro: Ubuntu)CentOS 6.5 x32 (id: 3240847, distro: CentOS)CentOS 6.5 x64 (id: 3240850, distro: CentOS)Fedora 20 x32 (id: 3243143, distro: Fedora)Fedora 20 x64 (id: 3243145, distro: Fedora)Dokku v0.2.3 on Ubuntu 14.04 (id: 3288841, distro: Ubuntu)Lots of interesting images there. I can see ghost, wordpress, a basic LAMP stack, things I've never heard of, and more! Docker 0.10 too, that might be interesting to try out. Notice they each have an ID. This is all very similar to OpenStack.## Spend some moneySo now I assume I can create a droplet from the command line. But first lets look at how much this is going to cost me.![](https://raw.githubusercontent.com/ccollicutt/ccollicutt.github.com/master/img/digital_ocean/pricing.png)I should have clicked on hourly.Hourly costs are:- 512MB - $0.007 per hour- 1GB - $0.015 per hour- 2GB - $0.03 per hourI'm not going to leave these running all the time. I just want to experiment. One note: Rackspace offers billing per minute, whereas DO just has monthly or per hour.I'm going to boot a Ubuntu 14.04 32 bit image. First I need the image ID.curtis$ tugboat images --global | grep 14.04Ubuntu 14.04 x32 (id: 3240033, distro: Ubuntu)Ubuntu 14.04 x64 (id: 3240036, distro: Ubuntu)Dokku v0.2.3 on Ubuntu 14.04 (id: 3288841, distro: Ubuntu)Now I can see that the ID is 3240033. I'm going to use that and my default tugboat settings to boot an instance.curtis$ tugboat create tester -i 3240033Queueing creation of droplet 'tester'...the server responded with status 404!You specified an invalid region for Droplet creation.Double-check your parameters and configuration (in your ~/.tugboat file)Uh oh. Error message.curtis$ tugboat regionsRegions:San Francisco 1 (id: 3)New York 2 (id: 4)Amsterdam 2 (id: 5)Singapore 1 (id: 6)Looks like I have New York 1 set, when I need New York 2, which has ID 4.curtis$ tugboat create tester -i 3240033 -r 4Queueing creation of droplet 'tester'...donetugboat exits. Guess I'll have to check the status with tugboat droplets.tester (ip: 162.243.253.240, status: new, region: 4, id: 1539322)# ... few seconds latercurtis$ tugboat dropletstester (ip: 162.243.253.240, status: active, region: 4, id: 1539322)Ok, the droplet has status active. curtis$ ping -c 1 162.243.253.240PING 162.243.253.240 (162.243.253.240): 56 data bytes64 bytes from 162.243.253.240: icmp_seq=0 ttl=54 time=81.364 ms--- 162.243.253.240 ping statistics ---1 packets transmitted, 1 packets received, 0.0% packet lossround-trip min/avg/max/stddev = 81.364/81.364/81.364/0.000 msI can ping it too.I'm going to try to ssh into the droplet.curtis$ ssh root@162.243.253.240# hangs, and eventually asks for my password...Hanging. Wonder why. Check my email while I'm waiting, turns out DO sends and email when a new vm is created. There is a password in the email. Oh no. It says that if I'd prefer not to receive emails with passwords in them then to add an ssh key to DO and also create droplets with that key. I was assuming tugboat was using the default key I set up. Maybe I need to specifically put in the key.curtis$ tugboat help createUsage:  tugboat create NAMEOptions:  -s, [--size=N]              # The size_id of the droplet  -i, [--image=N]             # The image_id of the droplet  -r, [--region=N]            # The region_id of the droplet  -k, [--keys=KEYS]           # A comma separated list of SSH key ids to add to the droplet  -p, [--private-networking]  # Enable private networking on the droplet  -b, [--backups-enabled]     # Enable backups on the droplet  -q, [--quiet]               Create a droplet.I'll destroy and recreate my droplet.curtis$ tugboat destroy testerDroplet fuzzy name provided. Finding droplet ID...done, 1539346 (tester)Warning! Potentially destructive action. Please confirm [y/n]: yQueuing destroy for 1539346 (tester)...doneNow lets create tester2.But...first, I realized the API wants an SSH key ID, not key name. I edit ~/.tugboat and set my key to use the ID not the name \"curtis.\"curtis$ tugboat keysSSH Keys:curtis (id: 118429)curtis$ vi ~/.tugboat# make id changecurtis$ grep ssh_key ~/.tugboat   ssh_key_path: /Users/curtis/.ssh/id_dsa  ssh_key: 118429# looks goodOk, let's try that again.curtis$ tugboat create tester2 -i 3240033 -r 4Queueing creation of droplet 'tester2'...donecurtis$ tugboat dropletstester2 (ip: 162.243.253.240, status: new, region: 4, id: 1539401)curtis$ tugboat dropletstester2 (ip: 162.243.253.240, status: active, region: 4, id: 1539401)Now it's active. Try ssh one more time...curtis$ ssh root@162.243.253.240SNIP!root@tester2:~# ifconfig eth0eth0      Link encap:Ethernet  HWaddr 04:01:17:7d:ad:01            inet addr:162.243.253.240  Bcast:162.243.253.255  Mask:255.255.255.0          inet6 addr: fe80::601:17ff:fe7d:ad01/64 Scope:Link          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1          RX packets:1645 errors:0 dropped:0 overruns:0 frame:0          TX packets:185 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:1000           RX bytes:104117 (104.1 KB)  TX bytes:23629 (23.6 KB)I can login. Nice!NOTE: It seemed like the IP took awhile to be available, longer than for the vm to become active. Not too sure what to make of that.Further--DO did *not* send me an email with passwords in it. That suggests I have ssh keys setup properly in tugboat.(Once the droplet is active and you've sshed in, I would suggest turning off password authentication in /etc/ssh/sshd_config and only use ssh keys to access servers.)## Private networkDO recently announced that they support [private networks](https://www.digitalocean.com/company/blog/introducing-private-networking/). But they aren't private in the way most would consider...instead it's one big open internal non-routable network. Every droplet that requests a private network can talk to any other droplet. Seems kind of wild-west to me. But that is how it works. root@tester2:~# ifconfig eth1eth1      Link encap:Ethernet  HWaddr 04:01:17:7d:ad:02            inet addr:10.128.183.80  Bcast:10.128.255.255  Mask:255.255.0.0          inet6 addr: fe80::601:17ff:fe7d:ad02/64 Scope:Link          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1          RX packets:133 errors:0 dropped:0 overruns:0 frame:0          TX packets:7 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:1000           RX bytes:5816 (5.8 KB)  TX bytes:578 (578.0 B)If I booted another droplet and it had a private IP I could access it that way. This would be good for things like small clusters, or high availability maybe, and the traffic doesn't count towards your bandwidth limit. It's important to note that if I boot a vm with a private network, and another user does to, technically I can connect to their server on the private network if I know its IP. So it's not a compartmentalized private network for each user or tenant--just one big network! Kind of like the public Internet...## Destroy!I'm going to destroy that droplet so I don't leave it up and running and have it cost me. (Though that said, a month of a 512MB droplet isn't much more than a couple coffees at $5.)curtis$ tugboat destroy tester2Droplet fuzzy name provided. Finding droplet ID...done, 1539401 (tester2)Warning! Potentially destructive action. Please confirm [y/n]: yQueuing destroy for 1539401 (tester2)...donecurtis$ tugboat dropletsYou don't appear to have any droplets.Try creating one with `tugboat create`Create...destroy...it's the DevOps way. :)## ConclusionI like the simplicity and ease of getting an account on Digital Ocean; that that aren't a ton of features to get in the way. I like that there is a command line application I can use to create droplets. As well, I think DO is doing smart to target developers. Developers are more important that ever, and part of cloud or utility computing is about making things easy to get going--sign up, input CC information, and boot servers to run code. Also simplifying charges is good too. AWS, Rackspace, etc, often have additional networking charges that are hard to calculate. With DO you get X terabytes of transfer, and moving data over the private network costs nothing. Not too much more to think about unless you hit the bandwidth limit.One thing I see is that it's not possible to upload an image. I believe I can create a new image based off an existing droplet, but not upload a new image. So if I wanted an OpenBSD droplet I'm out of luck until they support a base OpenBSD image.As far as disk space--I wonder if at some point they will have to offer volume storage, like what AWS does with EBS and what OpenStack does with Cinder volumes. I do think that EBS is a bit of a problem in that it's hard to offer a service like that without the possibility of it crashing big time--see the CAP theorem for more on that. But at some point users might run out of space, but then they can just move to a larger droplet size, though that will cost more obviously.  It will be interesting to see how DO copes with trying to keep it simple while also being profitable, ie. how to make more money without adding more features. I quite like DO, and I think it's interesting to compare their offering with what OpenStack or AWS provides."
    } ,
  
    {
      "title"    : "Adding networks to libvirt",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/04/26/adding-networks-libvirt.html",
      "date"     : "2014-04-26 00:00:00 -0400",
      "content"  : "For my Swiftacular project I’d like to figure out if I can run some automated tests and create a new Swiftacular cluster using libvirt. My Swiftacular configuration currently has five networks used to create an OpenStack Swift cluster (complete with a separate replication network). My Vagrantfile for Swiftacular sets up those networks, so I need to replicate that configuration using libvirt.By default with a libvirt host on Ubuntu 12.04 you get one network: the default network.# virsh net-listName                 State      Autostart-----------------------------------------default              active     yes       That default network comes with a bridge and a dnsmasq server to provide dhcp addresses.# ifconfig | grep virbrvirbr0    Link encap:Ethernet  HWaddr 52:54:00:a3:05:65  # ps ax  |grep dns 4943 ?        S      0:02 /usr/sbin/dnsmasq -u libvirt-dnsmasqSNIP!Each virtual machine booted using libvirt will have a default network configured. Below is a snippet of the xml defining a virtual machine. Note how the network to use is set to “default.”SNIP!                                SNIP!So first we need to add more networks, and then we need to configure virtual machines xml definition file with the networks, and then ensure that the vms has more interfaces set up.More libvirt networksIn Swiftacular I have five networks:  eth0 - default - Used by Vagrant  eth1 - public - 192.168.100.0/24 - The “public” network that users would connect to  eth2 - lbssl - 10.0.10.0/24 - This is the network between the SSL terminator and the Swift Proxy  eth3 - internal - 10.0.20.0/24 - The local Swift internal network  eth4 - replication - 10.0.30.0/24 - The replication network which is a feature of OpenStack Swift starting with the Havana releaseI’m going to replicate that with libvirt networks, and the eth0 network will be the default network.Here is an xml definition file for the internal network I would like to add:  internal                      Now that the xml file has been created, we can use virsh net-define to define it.# virsh net-define internal_network.xml Network internal defined from internal_network.xmlAnd we can see the network show up in virsh net-list.# virsh net-list --allName                 State      Autostart-----------------------------------------default              active     yes             internal             inactive   no        But as can bee seen above, the network is not active, nor set to autostart. So let’s do that.# virsh net-start internalNetwork internal started# virsh net-autostart internalNetwork internal marked as autostarted# virsh net-listName                 State      Autostart-----------------------------------------default              active     yes       internal             active     yes            Also we have an internal bridge and a dnsmasq process for that network as well which sets up a leases file.# brctl show  |grep internalinternalbr0\t\t8000.525400185a02\tyes\t\tinternalbr0-nic# ls /var/lib/libvirt/dnsmasq/internal.leases /var/lib/libvirt/dnsmasq/internal.leasesI’m going to setup the rest of my networks in the same fashion.# virsh net-list Name                 State      Autostart-----------------------------------------default              active     yes       internal             active     yes       lbssl                active     yes       private              active     yes       replication          active     yes  Ok, now we can move onto the other steps.Add network interfaces to libvirt vm definition fileIn order for those networks to be available to a virtual machine they need to be configured in the xml file that defines the vm.SNIP!                                                                                                                                                                                                                            SNIP!That snippet would have to be in every vm definition file.Cloud-init and user-dataI use cloud-init to help configure vms in libvirt.I have a user-data and meta-data file for each vm which is converted into an image file and connected to the virtual machine.In my user-data file I configure the files that will setup eth2 to eth4 as dhcp. eth1 is already setup in the image, so I don’t have to configure it with cloud-init.SNIP!write_files:  - content: |      auto eth1      iface eth1 inet dhcp    path: /etc/network/interfaces.d/eth1.cfg  - content: |      auto eth2      iface eth2 inet dhcp    path: /etc/network/interfaces.d/eth2.cfg  - content: |      auto eth3      iface eth3 inet dhcp    path: /etc/network/interfaces.d/eth3.cfg  - content: |      auto eth4      iface eth4 inet dhcp    path: /etc/network/interfaces.d/eth4.cfgSNIP!When the vm boots with a cloud-init image attached, the cloud-init client on the vm will setup the files as configured so that each new interface on the new networks will get an IP from dnsmasq on the corresponding bridge.Completely going over how cloud-init works is beyond the scope of this blog post, but it is a very important part of any cloud or virtualization platform. Ok, well not every virtualization system. OpenStack too though. :)Boot a vmLet’s boot a test vm.I have a script called generic.sh that creates a custom cloud-init image and configures the image in the libvirt xml file for the vm.I am using Ubuntu 14.04 as the OS for the virtual machine. This has a different cloud-init version that Ubuntu 12.04 so there may be differences in terms of cloud-init if trying to boot a Precise vm vs a Trusty vm.# ./generic.sh trusty testnetworksDomain testnetworks defined from /tmp/tmp.yJ6w0CO3A2Domain testnetworks startedNow I have a vm called “testnetworks” running in libvirt. I can check the various dnsmasq lease files in /var/lib/libvirt/dnsmasq/*.leases to see if it got an IP address.# virsh list  |grep test 40 testnetworks         running# cat /var/lib/libvirt/dnsmasq/internal.leases 1398535697 52:54:00:72:db:0e 10.0.20.68 testnetworks *And we can see that it did.Also we can find out the macs that were randomly given to the vm via libvirt using virsh dumpxml.# virsh dumpxml testnetworks | grep mac    hvm                              If I ssh into the virtual machine I should be able to see all those interfaces and checkout the routing table. I can also get to the Internet via the default gateway.# ssh ubuntu@10.0.20.68SNIP!ubuntu@testnetworks:~$ ifconfig | grep etheth0      Link encap:Ethernet  HWaddr 52:54:00:2d:52:8d  eth1      Link encap:Ethernet  HWaddr 52:54:00:72:db:0e  eth2      Link encap:Ethernet  HWaddr 52:54:00:93:1b:07  eth3      Link encap:Ethernet  HWaddr 52:54:00:76:61:ee  eth4      Link encap:Ethernet  HWaddr 52:54:00:72:79:48 ubuntu@testnetworks:~$ netstat -rnKernel IP routing tableDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface0.0.0.0         192.168.122.1   0.0.0.0         UG        0 0          0 eth010.0.10.0       0.0.0.0         255.255.255.0   U         0 0          0 eth210.0.20.0       0.0.0.0         255.255.255.0   U         0 0          0 eth110.0.30.0       0.0.0.0         255.255.255.0   U         0 0          0 eth4192.168.100.0   0.0.0.0         255.255.255.0   U         0 0          0 eth3192.168.122.0   0.0.0.0         255.255.255.0   U         0 0          0 eth0ubuntu@testnetworks:~$ ping -c 1 news.google.comPING news.l.google.com (74.125.228.100) 56(84) bytes of data.64 bytes from iad23s08-in-f4.1e100.net (74.125.228.100): icmp_seq=1 ttl=52 time=96.2 ms--- news.l.google.com ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 96.265/96.265/96.265/0.000 msSo now I have five networks to play with!Let’s boot one more to see if we have connectivity between two vms.# ./generic.sh trusty testnetworks2# cat /var/lib/libvirt/dnsmasq/internal.leases 1398535915 52:54:00:84:c2:b9 10.0.20.27 testnetworks2 *1398535697 52:54:00:72:db:0e 10.0.20.68 testnetworks *So we should be able to ping 10.0.20.27 from .68.ubuntu@testnetworks:~$ ping -c 1 10.0.20.27PING 10.0.20.27 (10.0.20.27) 56(84) bytes of data.64 bytes from 10.0.20.27: icmp_seq=1 ttl=64 time=0.284 ms--- 10.0.20.27 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.284/0.284/0.284/0.000 msYup.So now we have five networks in libvirt, and virtual machines that can boot up and get a dhcp address on each of those networks. Hopefully this means I can work on automating testing of Swiftacular, probably by creating a custom inventory script.Please let me know if you see any issues. One question I have is about the forward mode that should be set. I’m not sure it should be nat for my extra networks. Something to look into."
    } ,
  
    {
      "title"    : "Swiftacular - deploy OpenStack Swift with Ansible on CentOS",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/04/12/swiftacular-openstack-swift-with-centos.html",
      "date"     : "2014-04-12 00:00:00 -0400",
      "content"  : "Swiftacular is a project that can deploy an OpenStack Swift cluster using Ansible. In this post I’ll talk about a recent feature I added which is the ability to deploy to RedHat/CentOS 6.ProvidersFor the most part Swiftacular is used for getting used to how OpenStack Swift works and is installed, and will usually be deployed using Vagrant and Virtualbox.One of the features I hope to add is the ability to use multiple provisioners, such as IaaS providers like as Digital Ocean, OpenStack, etc. Having said that, it’s not required to use Vagrant, you could easily change some IPs around in the ansible hosts file, make a couple changes in other spots, and deploy to any servers, whether they are provided by Vagrant, virtual servers or even bare metal.But for now, Swiftacular uses Vagrant and Virtualbox.Deploying to CentOSBy default the Vagrantfile that comes with Swiftacular will use the Ubuntu 12.04 Precise 64bit box that the Vagrant project provides.But, if you would like to try deploying OpenStack Swift to CentOS 6 Swiftacular supports that as well, and doing so is as easy as changing which box the Vagrantfile points to.#config.vm.box = \"centos65\"#config.vm.box_url = \"http://puppet-vagrant-boxes.puppetlabs.com/centos-65-x64-virtualbox-nocm.box\"config.vm.box = \"precise64\"config.vm.box_url = \"http://files.vagrantup.com/precise64.box\"So if you would like to deploy to CentOS, simply make the Vagrantfile look like this:config.vm.box = \"centos65\"config.vm.box_url = \"http://puppet-vagrant-boxes.puppetlabs.com/centos-65-x64-virtualbox-nocm.box\"#config.vm.box = \"precise64\"#config.vm.box_url = \"http://files.vagrantup.com/precise64.box\"Then, once vagrant up is run and the virtual machines are created, the site.yml playbook can be run and OpenStack Swift will be deployed using the RDO packages. The playbooks will detect that it is a RedHat-like operating system and deploy the right packages and files for that operating system.About RedHat RDOApparently RDO is a meaningless acronym, but I tend to think of it as “RedHat’s Distribution of OpenStack.”One thing I found while using RDO is that the rdo-release.repo has a priorities setting. Note the “priority=98” option below.[vagrant@swift-proxy-01 yum.repos.d]$ cat rdo-release.repo [openstack-havana]name=OpenStack Havana Repositorybaseurl=http://repos.fedorapeople.org/repos/openstack/openstack-havana/epel-6/enabled=1skip_if_unavailable=0gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RDO-Havanapriority=98This means that it will be used before other repos that don’t have a priority setting (as I believe the default is 99, and the lower the number the higher the priority).But, this requires the yum priorities plugin to work.[vagrant@swift-proxy-01 yum.repos.d]$ rpm -qa | grep prioritiesyum-plugin-priorities-1.1.30-17.el6_5.noarchThus that plugin must be installed in order to use the priority setting, otherwise the wrong packages may be installed.Run SwiftacularOnce the Vagrantfile has been changed to use a CentOS 6 box and vagrant up has been run, swiftacular can install the Swift cluster.Below I show that the servers are all CentOS 6.5.curtis$ ansible -a \"cat /etc/redhat-release\" all192.168.100.20 | success | rc=0 &gt;&gt;CentOS release 6.5 (Final)192.168.100.30 | success | rc=0 &gt;&gt;CentOS release 6.5 (Final)192.168.100.200 | success | rc=0 &gt;&gt;CentOS release 6.5 (Final)192.168.100.100 | success | rc=0 &gt;&gt;CentOS release 6.5 (Final)192.168.100.50 | success | rc=0 &gt;&gt;CentOS release 6.5 (Final)192.168.100.202 | success | rc=0 &gt;&gt;CentOS release 6.5 (Final)192.168.100.201 | success | rc=0 &gt;&gt;CentOS release 6.5 (Final)Now to run the site playbook. Note that occasionally I’ve had to run the site playbook twice because of a bug. So below is on the second run.curtis$ #second runcurtis$ ansible-playbook site.yml SNIP!PLAY RECAP ******************************************************************** 192.168.100.100            : ok=26   changed=1    unreachable=0    failed=0   192.168.100.20             : ok=22   changed=0    unreachable=0    failed=0   192.168.100.200            : ok=42   changed=3    unreachable=0    failed=0   192.168.100.201            : ok=42   changed=3    unreachable=0    failed=0   192.168.100.202            : ok=45   changed=8    unreachable=0    failed=0   192.168.100.30             : ok=17   changed=2    unreachable=0    failed=0   192.168.100.50             : ok=35   changed=0    unreachable=0    failed=0   Once the Swiftacular playbook has completed successfully, swift is up and running.curtis$ vagrant ssh swift-package-cache-01working on swift-package-cache-01 with ip of 192.168.100.20Last login: Sat Apr 12 09:26:12 2014 from 192.168.100.1Welcome to your Vagrant-built virtual machine.[vagrant@swift-package-cache-01 ~]$ . testrc [vagrant@swift-package-cache-01 ~]$ swift list[vagrant@swift-package-cache-01 ~]$ #nothing there yet, let's add 100 files[vagrant@swift-package-cache-01 ~]$ mkdir test[vagrant@swift-package-cache-01 ~]$ for i in $(seq 1 100); do echo \"swift $i\" &gt; \\test/swift$i.txt; done[vagrant@swift-package-cache-01 ~]$ swift upload test testtest/swift76.txttest/swift28.txttest/swift79.txttest/swift27.txtSNIP![vagrant@swift-package-cache-01 ~]$ swift listtest[vagrant@swift-package-cache-01 ~]$ swift list test | headtest/swift1.txttest/swift10.txttest/swift100.txttest/swift11.txttest/swift12.txttest/swift13.txttest/swift14.txttest/swift15.txttest/swift16.txttest/swift17.txt[vagrant@swift-package-cache-01 ~]$ swift list test  |wc -l100Now that I’ve uploaded 100 files, and we have replicas set to 2, I can do a little digging around to see where files ended up.curtis$ ansible -m shell -a \"sudo find /srv | grep data | wc -l\" storage192.168.100.202 | success | rc=0 &gt;&gt;0192.168.100.200 | success | rc=0 &gt;&gt;100192.168.100.201 | success | rc=0 &gt;&gt;100Above it can be seen that there are 100 files on two of the three servers, which makes sense if we want two copies of each file. Though I still have a lot to learn about what OpenStack Swift is actually doing in the background.ConclusionSwiftacular now has the basic ability to deploy OpenStack Swift to RedHat 6 based operating systems using the RDO packages. There is a lot more to be done, but now with Ubuntu 12.04 and RedHat 6 support I can move onto adding other interesting features.If you have any suggestions or run into errors, please do let me know and I’ll fix them. I’d love to get some testers. :)"
    } ,
  
    {
      "title"    : "Use squid to cache RedHat/CentOS yum repositories",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/03/29/squid-cache-yum.html",
      "date"     : "2014-03-29 00:00:00 -0400",
      "content"  : "Today I began working on adding RedHat/CentOS support to my Ansible based project Swiftacular which deploys OpenStack Swift.Because, right now, it takes several Vagrant/Virtualbox virtual machines to run swiftacular, I like to make sure that I configure a local package caching server so that I don’t kill my Internet connection by downloading the same package multiple times.When configuring Ubuntu vms I use apt-cacher-ng. But with RedHat I couldn’t find, at least in a quick google search, a similar system for RedHat. (Though apparently apt-cacher-ng can cache rpms, but I haven’t tried it with Redhat repos.)I opted just to use squid. Surprisingly I couldn’t find much for blog posts on using squid to proxy yum so I thought I’d post what I did, which is very simple, in case anyone else was trying to do the same thing.I imagine there are better ways to cache rpms locally, ie. on a laptop, but this is what I did. :)Setup squidFirst, I installed squid on the package caching server.cache# yum install squidThe one change I have made to the squid configuration file is to set a cache_dir.cache# grep cache_dir /etc/squid/squid.conf#cache_dir ufs /var/spool/squid 100 16 256cache_dir ufs /var/spool/squid 7000 16 256The first uncommented cache_dir is the default, which is not set unless uncommented. The second is my setting, which just has a larger maximum size for the cache directory. You could set that to whatever is appropriate for your environment.You’ll also have to allow connections to port 3128 via iptables, or just shut iptables down. (Normally I wouldn’t shut iptables down, but this is just for a test system.)Also note I started squid.cache# netstat -ant | grep 3128tcp        0      0 :::3128                     :::*                        LISTEN cache# service iptables stopSNIP!cache# service squid startSo far so good.Configure the servers that will use the package cacheOn all the servers that need to use the cache, I set the proxy configuration in their /etc/yum.conf file to be the cache server on port 3128.server# head -12 yum.conf[main]cachedir=/var/cache/yum/$basearch/$releaseverkeepcache=0debuglevel=2logfile=/var/log/yum.logexactarch=1obsoletes=1gpgcheck=1plugins=1installonly_limit=5proxy=http://192.168.100.20:3128Also, I commented out the mirrorlist lines in CentOS-Base.repo and uncommented the baseurl, eg.[vagrant@swift-storage-02 yum.repos.d]$ grep \"mirrorlist\\|baseurl\" CentOS-Base.repo # If the mirrorlist= does not work for you, as a fall back you can try the # remarked out baseurl= line instead.#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=osbaseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updatesbaseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extrasbaseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplusbaseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=contribbaseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/Normally you wouldn’t want to do this, but because of the mirror list there will be many squid cache misses. So baseurl is better, and a closer mirror would be better still.Next I removed the fastestmirror plugin:server# rm -f /etc/yum/pluginconf.d/fastestmirror.conf As an example, if I install php-common on one of the servers, then install it on another, it will hit the squid cache and just download it from there. So we only download the package once from the Internet, even if it’s installed on several servers.1396152104.396     10 192.168.100.202 TCP_HIT/200 537778 GET \\ http://mirror.centos.org/centos/6/updates/x86_64/Packages/php-common-5.3.3-27.el6_5.x86_64.rpm \\ - NONE/- application/x-rpmThat’s all it takes.One thing I’m not sure about is the cache expiry time. I’m sure there are some other changes I will come across while using squid to cache yum repos, as I’ve only started using it in the last few hours, but I will ensure to come back and edit this post with new information.If anyone has any comment, questions, concerns, or criticisms, do let me know. :)AnsiblePS. Everything I’m doing above is automated with Ansible.Updates  Added removing fastestmirror plugin"
    } ,
  
    {
      "title"    : "boot2docker on OSX",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/03/26/boot2docker.html",
      "date"     : "2014-03-26 00:00:00 -0400",
      "content"  : "Containers are cool. There I said it. LXC is production ready, Docker is a new and very popular project. Containers are hip and that’s Ok with me. (Note that when I say containers I pretty much just mean LXC and systems based on LXC, there are other container systems out there, and have been for some time.)In this post I’ll look at using boot2docker on OSX. In a previous post I was using boot2docker via libvirt and kvm, but now I’ll use it on OSX, mostly because I have an Apple macbook and really need to learn more about docker.So…what is boot2docker?From the README.md file:  boot2docker is a lightweight Linux distribution based on Tiny Core Linux made specifically to run Docker containers. It runs completely from RAM, weighs ~24MB and boots in ~5s (YMMV).boot2docker will use virtualbox on OSX to boot a tiny core linux based operating system that has docker already installed. Using tiny core to provide access to docker is a really interesting model, and is necessary because OSX doesn’t have a “container”-like system (yet), so in order to use containers locally on an OSX laptop or workstation you need to boot a Linux virtual machine.Docker is:  …an open-source project to easily create lightweight, portable, self-sufficient containers from any application. The same container that a developer builds and tests on a laptop can run at scale, in production, on VMs, bare metal, OpenStack clusters, public clouds and more. – DockerInstall boot2docker on OSXThanks to brew, installation is easy.curtis$ brew updateSNIP!curtis$ brew install boot2dockerSNIP!These are the versions I have:curtis$ brew list --versions | grep dockerboot2docker 0.7.0docker 0.9.0Thanks brew!Use boot2dockerNow that boot2docker is installed, we can initialize it.curtis$ boot2docker init[2014-03-25 17:31:47] Creating VM boot2docker-vmVirtual machine 'boot2docker-vm' is created and registered.UUID: 1e60ddea-8794-4c32-b488-ef6b054628e8Settings file: '/Users/curtis/VirtualBox VMs/boot2docker-vm/boot2docker-vm.vbox'[2014-03-25 17:31:48] Apply interim patch to VM boot2docker-vm (https://www.virtualbox.org/ticket/12748)[2014-03-25 17:31:48] Setting VM settings[2014-03-25 17:31:48] Setting VM networking[2014-03-25 17:31:48] boot2docker.iso not found.[2014-03-25 17:31:49] Latest version is v0.7.0, downloading...  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed100   342  100   342    0     0    508      0 --:--:-- --:--:-- --:--:--   508100 24.0M  100 24.0M    0     0  1111k      0  0:00:22  0:00:22 --:--:-- 1332k[2014-03-25 17:32:11] Done[2014-03-25 17:32:11] Setting VM disks[2014-03-25 17:32:11] Creating 40000 Meg hard drive...Converting from raw image file=\"stdin\" to file=\"/Users/curtis/.boot2docker/boot2docker-vm.vmdk\"...Creating dynamic image with size 41943040000 bytes (40000MB)...[2014-03-25 17:32:11] Done.[2014-03-25 17:32:11] You can now type boot2docker up and wait for the VM to start.Now we can start the boot2docker vm.curtis$ boot2dockerUsage /usr/local/bin/boot2docker {init|start|up|save|pause|stop|restart|status|info|delete|ssh|download}curtis$ boot2docker up[2014-03-25 17:33:58] Starting boot2docker-vm...[2014-03-25 17:34:18] Started.To connect the docker client to the Docker daemon, please set:export DOCKER_HOST=tcp://localhost:4243Now that the vm is up and running via boot2docker, we can ssh into the vm. For this image, at this time, the password to login is “tcuser.”curtis$ boot2docker sshdocker@localhost's password: #enter \"tcuser\" as the password                        ##        .                  ## ## ##       ==               ## ## ## ##      ===           /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\___/ ===      ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ /  ===- ~~~           \\______ o          __/             \\    \\        __/              \\____\\______/ _                 _   ____     _            _| |__   ___   ___ | |_|___ \\ __| | ___   ___| | _____ _ __| '_ \\ / _ \\ / _ \\| __| __) / _` |/ _ \\ / __| |/ / _ \\ '__|| |_) | (_) | (_) | |_ / __/ (_| | (_) | (__|   &lt;  __/ ||_.__/ \\___/ \\___/ \\__|_____\\__,_|\\___/ \\___|_|\\_\\___|_|boot2docker: 0.7.0docker@boot2docker:~$ ifconfig eth0eth0      Link encap:Ethernet  HWaddr 08:00:27:B8:BB:5C            inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0          inet6 addr: fe80::a00:27ff:feb8:bb5c/64 Scope:Link          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1          RX packets:88 errors:0 dropped:0 overruns:0 frame:0          TX packets:60 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:1000           RX bytes:10303 (10.0 KiB)  TX bytes:8361 (8.1 KiB)Check it out, there’s the cool docker whale guy.We’ll do as boot2docker suggests and export the DOCKER_HOST variable.curtis$ export DOCKER_HOST=tcp://localhost:4243# also put it in my .profile, and add an alias so I don't have to# type docker all the time :)curtis$ tail -2 ~/.profile export DOCKER_HOST=tcp://localhost:4243alias d='docker'Now we can user the docker command. Note that I’m running this from OSX.# note I'm using my alias d=dockercurtis$ d infoContainers: 0Images: 0Driver: aufs Root Dir: /mnt/sda1/var/lib/docker/aufs Dirs: 0Debug mode (server): trueDebug mode (client): falseFds: 10Goroutines: 13Execution Driver: native-0.1EventsListeners: 0Kernel Version: 3.13.3-tinycore64Init Path: /usr/local/bin/dockerOk, so here it is important to note that we are running the docker command on OSX, but the docker server is running inside a boot2docker based virtual machine created by Vagrant an the boot2docker scripts.We can also check the status of the boot2docker vm.NOTE: I have noticed that when I move from home to work sometimes virtual machines network stops working. I haven’t looked into this much, but a couple of times I had to restart the docker virtual machine with “boot2docker stop; boot2docker start.”curtis$ boot2docker status[2014-03-26 12:51:09] boot2docker-vm is running.Lets download the busybox image to play with. This is part of the hello world example on the docker documentation website.curtis$ d pull busyboxPulling repository busybox769b9341d937: Download complete                                                 511136ea3c5a: Download complete bf747efa0e2f: Download complete 48e5f45168b9: Download complete Unfortunately at this point I seem to be running into a bug. When I run a docker command I get a “Coudn’t send EOF: use of a closed network connection” error.curtis$ d run -i -t busybox echo \"hi\"hi# have to hit enter...[error] client.go:2264 Couldn't send EOF: use of closed network connectionI’ll have to come back to this blog post once I find out more about this bug.I did find it entered into a github issue so it should be fixed soon, and once it is I’ll come back and finish up this blog post.I can still ssh into the boot2docker vm and run commands without that issue, of course.curtis$ boot2docker sshSNIP!docker@boot2docker:~$ docker run -i -t busybox echo \"hi\"hiIn reality we haven’t come very far in this blog post, partially because there seems to be a bug in the particular versions of boot2docker and docker I am using, but at least we got boot2docker installed, and are ready to move onto actually using docker straight from OSX. Once I figure out that bug I’ll come back and update this post with some more things I learned about Docker."
    } ,
  
    {
      "title"    : "Swiftacular - deploy OpenStack Swift with Ansible",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/03/21/swiftacular-openstack-swift-with-ansible.html",
      "date"     : "2014-03-21 00:00:00 -0400",
      "content"  : "A few months ago I put a good amount of working into learning more about OpenStack Swift and how to deploy it. I used Ansible as my configuration management system, and called the whole project Swiftacular.Recently I realized that I forgot to blog about it, so I decided it was time to fix that. :)OpenStack SwiftOpenStack Swift is an object storage service made up of many components. From their documentation  Swift is a highly available, distributed, eventually consistent object/blob store. Organizations can use Swift to store lots of data efficiently, safely, and cheaply.In several ways it is analogous to Amazon’s S3 service, which has billions and billions of files/blobs/objects (whatever you want to call them) stored in it.Most people who use the Internet or smartphones with mobile apps use object storage, they just don’t know it because it’s used in the background by the applications, such as ones that need to store many, many files, like pictures.New features in Swift: Replication network and regionsSwift has a couple new features which are used in Swiftacular. Though I should note that by default Swiftacular only sets up one region.Replication network  Swift’s replication process is essential for consistency and availability of data. By default, replication activity will use the same network interface as other cluster operations. However, if a replication interface is set in the ring for a node, that node will send replication traffic on its designated separate replication network interface.The idea is that you could do some quality of service on the replication network, or split it off entirely from the standard Swift network, as likely the replication network would go across the link between regions, which are in most cases going to be in different data centers. The link would likely be lower bandwidth and higher latency.Regions  Whereas Zones are designed to distribute replicas among nodes and drives such that there is no single point of hardware / networking failure, Regions are conceptually designed to distribute those replicas among different geographical areas.Regions are great because you can then deploy one large OpenStack Swift cluster across multiple geographically separated data centers. The organization I currently work for is deploying Swift in Calgary, Alberta and Kelowna, B.C. Actually these are in different timezones too!Deploying OpenStack Swift with AnsibleI like to use Ansible because it’s straightforward to understand and executes over ssh instead of some custom RPC type system requiring a client running on the remote system and certificates. ssh is awesome. Ansible is awesome. Swift is awesome. Radical!First off, you can find the repository containing all the Ansible playbooks and roles needed to deploy Swift here. That repository also contains a README file that will likely be more up to date than this blog post.Requirements:  Vagrant  Virtualbox  Ansible  Internet connectionOnce you have those requirements, this is how to quickly deploy OpenStack Swift:$ git clone git@github.com:ccollicutt/swiftacular.git$ cd swiftacular$ mkdir library# Checkout some modules to help with managing openstack $ git clone https://github.com/openstack-ansible/openstack-ansible-modules \\library/openstack$ vagrant up $ cp group_vars/all.example group_vars/all$ vi group_vars/all # ie. edit the CHANGEMEs in the file, if desired# Source aliases, etc$ . ansiblerc# Test connectivity to virtual machines$ ans -m ping all# Run the playbook to deploy Swift!$ pb site.ymlAfter those commands have completed, you should end up with several virtual machines running.$ vagrant statusSNIP!swift-package-cache-01    running (virtualbox)swift-keystone-01         running (virtualbox)swift-lbssl-01            running (virtualbox)swift-proxy-01            running (virtualbox)swift-storage-01          running (virtualbox)swift-storage-02          running (virtualbox)swift-storage-03          running (virtualbox)SNIP!Those virtual machines comprise an OpenStack Swift cluster. While there is only one region setup by default, the storage servers are setup with a replication network. Both regions and the replication network are new features of OpenStack Swift. In /etc/swift of each of the storage servers there is both a replication configuration file and a standard server configuration.root@swift-storage-01:~# ls /etc/swift/*-server/etc/swift/account-server:account-replication.conf  account-server.conf/etc/swift/container-server:container-replication.conf  container-server.conf/etc/swift/object-server:object-replication.conf  object-server.confThe replication server configuration file looks like this:root@swift-storage-01:~# cat /etc/swift/object-server/object-replication.conf [DEFAULT]devices = /srv/nodebind_ip = 10.0.30.200workers = 2[pipeline:main]pipeline = object-server[app:object-server]use = egg:swift#objectreplication_server = True[object-replicator][object-updater][object-auditor]As can be seen above the replication server is listening on 10.0.30.200 where 10.0.30.0/24 is the replication network configured. Also the “replication_server” option is set to True.This is what the regular server config file looks like:root@swift-storage-01:~# cat /etc/swift/object-server/object-server.conf      [DEFAULT]devices = /srv/nodebind_ip = 10.0.20.200workers = 2[pipeline:main]pipeline = object-server[app:object-server]use = egg:swift#object[object-replicator][object-updater][object-auditor][object-expirer]It’s listening on 10.0.20.200 and is not setup as a replication only server.All three of the object, container, and account servers are setup in the same fashion.Using Ansible delegation to setup the ringsPart of configuring OpenStack Swift involves adding devices to the ring. Thankfully Ansible supports delegating commands. So while some configuration management systems need to have centralized metadata (think PuppetDB) in order to configure all of the devices, with Ansible we can simply use a delegation command, which means that when a storage server is being configured, we can actually delegate a configuration command to run on the proxy server.That sounds complicated but it’s fairly simple. Maybe a better example is if using Ansible to configure a webserver, and then using a delegation command to add the webserver to a loadbalancer.As an example, when a storage server is configured we can tell the proxy to add it’s devices to the ring. Below is an Ansible task that is part of Swiftacular that delegates configuration of a ring device when a storage node is being configured. The command actually runs on the Swift proxy.(Sorry the example swift-ring-builder command below will probably stretch across the screen. It’s long and kinda complicated and is really meant to be invoked programatically.)  - name: build account ring   command: swift-ring-builder account.builder \\   add r{{ region }}z{{ zone }}-{{ ansible_eth3.ipv4.address }}:6002R{{ ansible_eth4.ipv4.address }}:6002/{{ disk_prefix }}{{ item }} 100           chdir=/etc/swift    delegate_to: \"{{ swift_proxy_server }}\"   with_sequence: count={{ disks }}   when: \"losetup.rc &gt; 0\"    Delegation is very handy, especially with OpenStack Swift where the proxy needs to know what devices each storage server has.Documentation of SSL/TLS in OpenStackI don’t know why, but much of the OpenStack documentation, especially around the authentication system Keystone, avoids discussing how to deploy SSL/TLS enabled services. I’m not talking about Swift here specifically, rather the rest of OpenStack–authentication, endpoints, and other services that should be TLS enabled.On one hand most organizations deploying OpenStack know that there is going to be a layer of TLS termination in front of most services, but on the other it’s not obvious from that general documentation that this layer should exist. The OpenStack Security Guide goes into more detail about the TLS layer, but don’t go very deep:  OpenStack endpoints are HTTP services providing APIs to both end-users on public networks and to other OpenStack services within the same deployment operating over the management network. It is highly recommended these requests, both those internal and external, operate over SSL.I guess what I’m saying is that it would be good to know how larger OpenStack providers are securing OpenStack services with TLS. :)Swiftacular sets up Keystone and a loadbalancer with TLS enabled to front swift-proxy. Certainly it’s not setup as you would do it in production (likely with with a dedicated set of TLS termination servers) but it’s a good example. I don’t think the OpenStack documentation should show deploying systems such as Keystone without SSL.ConclusionUsing the Swiftacular Ansible playbooks and roles, and using the provided Vagrantfile, it’s fairly easy to get a small OpenStack cluster going on a good laptop or workstation. It also includes regions and a replication network setup. This could be a good basis for starting out with OpenStack Swift.As usual–if you have any questions, concerns, comments or criticism, do let me know. It’s quite likely I’ve made a mistake somewhere here, small or large. :)Updates  Tried to clarify SSL/TLS section"
    } ,
  
    {
      "title"    : "ssh read from socket failed",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/03/20/ssh-connection-reset-by-peer.html",
      "date"     : "2014-03-20 00:00:00 -0400",
      "content"  : "UPDATE: This is likely because cloud-init is failing to get information from the various sources it can get information from.For the people of the future, the ones who get an error message about connections being reset by peer when trying to ssh into a server, check to see if the server does indeed have server keys in /etc/ssh.I spent like an hour or more looking into this. I’m not sure why, but an Ubuntu image I was using wouldn’t create the servers ssh keys, though the files were there…they were of zero size.Below is an example of an attempted connection.root@client# ssh -vvvv ubuntu@192.168.122.217OpenSSH_5.9p1 Debian-5ubuntu1.1, OpenSSL 1.0.1 14 Mar 2012debug1: Reading configuration data /root/.ssh/configdebug1: /root/.ssh/config line 1: Applying options for 192.168.122.*debug1: Reading configuration data /etc/ssh/ssh_configdebug1: /etc/ssh/ssh_config line 19: Applying options for *debug2: ssh_connect: needpriv 0debug1: Connecting to 192.168.122.217 [192.168.122.217] port 22.debug1: Connection established.debug1: permanently_set_uid: 0/0debug1: identity file /root/.ssh/id_rsa type -1debug1: identity file /root/.ssh/id_rsa-cert type -1debug1: identity file /root/.ssh/id_dsa type -1debug1: identity file /root/.ssh/id_dsa-cert type -1debug1: identity file /root/.ssh/id_ecdsa type -1debug1: identity file /root/.ssh/id_ecdsa-cert type -1debug1: Remote protocol version 2.0, remote software version OpenSSH_6.5p1 Ubuntu-4debug1: match: OpenSSH_6.5p1 Ubuntu-4 pat OpenSSH*debug1: Enabling compatibility mode for protocol 2.0debug1: Local version string SSH-2.0-OpenSSH_5.9p1 Debian-5ubuntu1.1debug2: fd 3 setting O_NONBLOCKdebug3: load_hostkeys: loading entries for host \"192.168.122.217\" from file \"/dev/null\"debug3: load_hostkeys: loaded 0 keysdebug1: SSH2_MSG_KEXINIT sentRead from socket failed: Connection reset by peerTo fix this I regenerated the keys on the server (by logging into the console) and restarted. Then I was able to ssh into the server.root@server# ssh-keygen -q -f /etc/ssh/ssh_host_key -N '' -t rsa1root@server# ssh-keygen -f /etc/ssh/ssh_host_rsa_key -N '' -t rsaroot@server# ssh-keygen -f /etc/ssh/ssh_host_dsa_key -N '' -t dsaroot@server# service ssh restartI have no idea why this is happening in this image. Obviously something is broken."
    } ,
  
    {
      "title"    : "Boot Ubuntu Trusty Tahr 14.04 with libvirt",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/03/17/trusty-libvirt.html",
      "date"     : "2014-03-17 00:00:00 -0400",
      "content"  : "Then next Ubuntu long term service (LTS) operating system, Trusty Tahr or version 14.04, is currently expected to be released on April 27th of 2014, about a month away from the writing of this post. So note that right now it is still beta.Below I’ll take a quick look at how to deploy the cloud image prepared by Ubuntu with libvirt. We’ll use libvirt + kvm on Ubuntu Precise to boot the Trusty Tahr image. The server running libvirt is Ubuntu 12.04, and is setup in a default manner, which means vms should IPs from dnsmasq and get natted access to the Internet.Download the imageFirst we download the cloud image from Ubuntu’s site. Thankfully Ubuntu provides ready-to-go images.root# cd /var/lib/libvirt/imagesroot# wget https://cloud-images.ubuntu.com/trusty/current/trusty-server-cloudimg-amd64-disk1.imgroot# mv trusty-server-cloudimg-amd64-disk1.img trusty-server-cloudimg-amd64-disk1.img.distroot# qemu-img convert -O qcow2 trusty-server-cloudimg-amd64-disk1.img.dist \\trusty-server-cloudimg-amd64-disk1.imgroot# qemu-img resize trusty-server-cloudimg-amd64-disk1.img +8GAbove I downloaded the image, converted it from compressed to uncompressed, and then increase it’s size by 8G, making it about a 10G image. If it’s not resized it will be about 2G.Next create a snapshotted backing file which will allow us to keep the original image pristine.root# qemu-img create -f qcow2 -b trusty-server-cloudimg-amd64-disk1.img trusty1.imgThat’s all it takes to grab a pre-built Ubuntu Trusty Tahr image!Checkout cloud-localdsWe need to get cloud-utils from launchpad which includes a cloud-localds script. Using this script we can easily create an ISO file that can be used locally by the vm with cloud-init to configure the vm.root# cd ~root# bzr branch lp:cloud-utilsroot# ls cloud-utils/bin/cloud-localdscloud-utils/bin/cloud-localdsUsing that script we can create a user-data image that can be attached to our virtual machine.Create a user-data fileEssentially what this allows us to do is have a local disk file attached to the vm which cloud-init can use to setup the system as we would like it, at least by setting a password and/or an ssh key.I’m not showing my public key below just because it doesn’t fit well on the page. But just paste your public ssh key in after the “-“. The password for the ubuntu user is set to “passw0rd” which is in fact not a good password. It might be better to either not set the password at all and just use and ssh key, or to set a very good password. This is just an example.#cloud-configpassword: passw0rdchpasswd: { expire: False }ssh_pwauth: Truessh_authorized_keys:  - &lt;enter your public key&gt;Cloud-init has a ton more features and options so I suggest checking out the documentation. That said, it is a rather new and fast moving project.Build the user-data imageNow we convert the user-data file to an ISO file.root# ~/cloud-utils/bin/cloud-localds user-data.img user-dataroot# cp user-data.img /var/lib/libvirt/imagesWith that file created, we can now setup a libvirt xml file to boot the virtual machine.Prepare a libvirt.xml fileHere is an example libvirt xml file, in this case called trusty1.xml.root# cat trusty1.xml&lt;domain type='kvm'&gt;    &lt;name&gt;trusty1&lt;/name&gt;    &lt;memory&gt;1048576&lt;/memory&gt;    &lt;os&gt;        &lt;type&gt;hvm&lt;/type&gt;        &lt;boot dev=\"hd\" /&gt;    &lt;/os&gt;    &lt;features&gt;        &lt;acpi/&gt;    &lt;/features&gt;    &lt;vcpu&gt;1&lt;/vcpu&gt;    &lt;devices&gt;        &lt;disk type='file' device='disk'&gt;            &lt;driver type='qcow2' cache='none'/&gt;            &lt;source file='/var/lib/libvirt/images/trusty1.img'/&gt;            &lt;target dev='vda' bus='virtio'/&gt;        &lt;/disk&gt;        &lt;disk type='file' device='disk'&gt;            &lt;source file='/var/lib/libvirt/images/user-data.img'/&gt;            &lt;target dev='vdb' bus='virtio'/&gt;        &lt;/disk&gt;        &lt;interface type='network'&gt;            &lt;source network='default'/&gt;                &lt;model type='virtio'/&gt;        &lt;/interface&gt;    &lt;/devices&gt;&lt;/domain&gt;Now that we have a libvirt xml file we can define and start the vm.Define and start the virtual machineLets define and start the vm based on the images we created and the libvirt xml file.root# virsh define trusty1.xmlDomain trusty1 defined from trusty1.xmlroot# virsh start trusty1Now that the vm is booted, by default it should get an IP from dnsmasq, and has a random mac address. Figuring out the IP is not that easy with libvirt (if you know a way, let me know) but generally I look at the leases file or would set specific mac addresses to get specific IPs.root# cat /var/lib/libvirt/dnsmasq/default.leases1395108553 52:54:00:6a:f1:c8 192.168.122.103 cloudimg *From dnsmasq know that the instance has the 192.168.122.103 IP, we can ssh in. Note that because I put my ssh public key in I can ssh into the server without a password. I removed some content for brevity.root# ssh ubuntu@192.168.122.103Warning: Permanently added '192.168.122.103' (ECDSA) to the list of known hosts.Welcome to Ubuntu Trusty Tahr (development branch) (GNU/Linux 3.13.0-14-generic x86_64)SNIP!Last login: Sun Mar 16 18:46:59 2014 from 192.168.122.1ubuntu@cloudimg:~$ cat /etc/lsb-releaseDISTRIB_ID=UbuntuDISTRIB_RELEASE=14.04DISTRIB_CODENAME=trustyDISTRIB_DESCRIPTION=\"Ubuntu Trusty Tahr (development branch)\"ubuntu@cloudimg:~$ df -h | grep vda1/dev/vda1        11G  898M  8.7G  10% /That is about the minimum required to get a Trusty image up and running via libvirt. Certainly there are other considerations one might have to make in a production environment, such as passwords and keys, actual production Trusty Tahr image, etc.Now to explore the new features of Trusty Tahr!Don’t forget it won’t be officially released until late April of 2014."
    } ,
  
    {
      "title"    : "boot2docker and libvirt",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/03/13/boot2docker-qemu.html",
      "date"     : "2014-03-13 00:00:00 -0400",
      "content"  : "Docker is so hot right now. Well, containers in general are. LXC just hit version 1.0 and the developers have declared it production ready.Here is part of the LXC 1.0 release announcement:  LXC 1.0 is the first production ready release of LXC and it comes with a commitment from upstream to maintain it until at least Ubuntu 14.04 LTS reaches end of life in April 2019. That’s slightly over 5 years of support!So what is docker? From the website:  Docker is an open-source project to easily create lightweight, portable, self-sufficient containers from any application. The same container that a developer builds and tests on a laptop can run at scale, in production, on VMs, bare metal, OpenStack clusters, public clouds and more.boot2dockerThe idea behind boot2docker is to be able to use docker quickly, mostly on OSX. But OSX doesn’t support containers (yet, maybe someday), so running docker natively isn’t possible.To use docker on OSX it has to be done inside a Linux virtual machine running in a hypervisor (like Virtualbox or VMWare Fusion) on top of OSX. This is what boot2docker does–provides a small Linux vm with docker installed, and helps get it all configured and provides a command line interface.From the github repo for boot2docker:  boot2docker is a lightweight Linux distribution based on Tiny Core Linux made specifically to run Docker containers. It runs completely from RAM, weighs ~24MB and boots in ~5s (YMMV).boot2docker comes with helpful commands and setup to get this running easily and quickly on OSX, but I’m not going to use it on OSX…I’m going to use it with libvirt and KVM.Use boot2docker with libvirtFirst I downloaded the boot2docker iso from github.root# wget \\https://github.com/boot2docker/boot2docker/releases/download/v0.7.0/boot2docker.isoThen I created a qemu disk image from that iso.root# qemu-img convert -O qcow2 boot2docker.iso /var/lib/libvirt/images/boot2docker.imgroot# file /var/lib/libvirt/images/boot2docker.img: QEMU QCOW Image (v2), 25165824 bytesboot2docker boot scriptNow that I have a base backing file made from the boot2docker ISO file, I can boot virtual machines off it.I wrote a script that I am still in the process of refining (ie. this is still pretty ugly) but using it I can start several boot2docker based virtual machines from libvirt.I’ve also added a second drive for each vm and in the script the drive image gets partitioned and ext4 formatted with a label that boot2docker recognizes and mounts automatically.I’m using sfdisk to partition a second file, and the partitioning may not be setup properly. I haven’t done enough testing, but it’s working so far. :)root# cat boot2docker.sh #!/bin/bashvmtype=boot2dockernum_vms=4backing_image=boot2docker.imgfor ((i=1; i&lt;=num_vms; i++)); do  virsh destroy ${vmtype}$i &gt; /dev/null  virsh undefine ${vmtype}$i &gt; /dev/null  rm -f ./${vmtype}0${i}.xml &gt; /dev/null  rm -f /var/lib/libvirt/images/${vmtype}$i.img &gt; /dev/null  rm -f /var/lib/libvirt/images/${vmtype}$i-persist.img &gt; /dev/null## Setup partitions for image#cat &lt;&lt;-SFDISKOUT &gt; /var/tmp/sfdisk.out.${vmtype}${i}# partition table of boot2docker1-persist.imgunit: sectors${vmtype}${i}-persist.img1 : start=     2048, size= 10483712, Id=83${vmtype}${i}-persist.img2 : start=        0, size=        0, Id= 0${vmtype}${i}-persist.img3 : start=        0, size=        0, Id= 0${vmtype}${i}-persist.img4 : start=        0, size=        0, Id= 0SFDISKOUT  #  # Create images  #   pushd /var/lib/libvirt/images &gt; /dev/null    qemu-img create -f qcow2 -b ${backing_image} ${vmtype}${i}.img &gt; /dev/null    qemu-img create -f raw ${vmtype}${i}-persist.img 5G    sfdisk --force ${vmtype}${i}-persist.img &lt; /var/tmp/sfdisk.out.${vmtype}${i}    losetup --offset 1048576 /dev/loop0 ${vmtype}${i}-persist.img    mkfs.ext4 -F -L boot2docker-data /dev/loop0    losetup -d /dev/loop0  popd &gt; /dev/null  rm -f /var/tmp/sfdisk.out.${vmtype}${i}  chown libvirt-qemu:kvm /var/lib/libvirt/images/*.img  vm_uuid=`uuid`  #  # Build the libvirt xml file  # cat &lt;&lt;-LIBVIRTXML &gt; ${vmtype}${i}.xml    ${uuid}    ${vmtype}${i}    4194304                hvm                                    1                                                                      \t                                                                         LIBVIRTXML  #  # Define and start the vm  #  virsh define ${vmtype}${i}.xml &gt; /dev/null  if virsh start ${vmtype}${i} &gt; /dev/null; then\techo \"${vmtype}${i} started\"  fi  sleep 1donevirsh list --allexit 0Create some boot2docker virtual machinesIf I run that script, which is a little noisy, I end up with four virtual machines running the boot2docker OS (which is based on Tiny Linux).The script ends off by listing all the running vms on the box.root# ./boot2docker.sh SNIP!boot2docker4 started Id Name                 State----------------------------------127 boot2docker1         running128 boot2docker2         running129 boot2docker3         running130 boot2docker4         runningThe vms are getting IPs from dnsmasq which is configured by default by libvirt.root# cat /var/lib/libvirt/dnsmasq/default.leases 1394830266 fa:16:3e:18:89:04 192.168.122.246 * 01:fa:16:3e:18:89:041394830262 fa:16:3e:18:89:03 192.168.122.245 * 01:fa:16:3e:18:89:031394830263 fa:16:3e:18:89:02 192.168.122.244 * 01:fa:16:3e:18:89:021394830255 fa:16:3e:18:89:01 192.168.122.243 * 01:fa:16:3e:18:89:01I set the mac adresses in the script to be fa:16:3e:18:89:0X.Knowing the IPs the vms received from libvirt/dnsmasq, I can ssh into them. (The default user/pass is docker/tcuser.)root# ssh docker@192.168.122.243Warning: Permanently added '192.168.122.243' (ECDSA) to the list of known hosts.docker@192.168.122.243's password:                         ##        .                  ## ## ##       ==               ## ## ## ##      ===           /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\___/ ===      ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ /  ===- ~~~           \\______ o          __/             \\    \\        __/              \\____\\______/ _                 _   ____     _            _| |__   ___   ___ | |_|___ \\ __| | ___   ___| | _____ _ __| '_ \\ / _ \\ / _ \\| __| __) / _` |/ _ \\ / __| |/ / _ \\ '__|| |_) | (_) | (_) | |_ / __/ (_| | (_) | (__|   &lt;  __/ ||_.__/ \\___/ \\___/ \\__|_____\\__,_|\\___/ \\___|_|\\_\\___|_|boot2docker: 0.7.0And we can see that the second device, which is /dev/vdb, has indeed been mounted.docker@boot2docker:~$ df -hFilesystem                Size      Used Available Use% Mounted onrootfs                    3.5G    223.4M      3.3G   6% /tmpfs                     1.9G         0      1.9G   0% /dev/shm/dev/vdb1                 4.8G     25.3M      4.5G   1% /mnt/vdb1cgroup                    1.9G         0      1.9G   0% /sys/fs/cgroupUse dockerWe can run docker version to see if it works.docker@boot2docker:~$ docker versionClient version: 0.9.0Go version (client): go1.2.1Git commit (client): 2b3fdf2Server version: 0.9.0Git commit (server): 2b3fdf2Go version (server): go1.2.1Last stable version: 0.9.0And also run a docker command. The first time we run a container type it’ll have to be downloaded.docker@boot2docker:~$ docker run ubuntu /bin/echo hello worldUnable to find image 'ubuntu' locallyPulling repository ubuntu9f676bd305a4: Download complete 9cd978db300e: Download complete eb601b8965b8: Download complete 5ac751e8d623: Download complete 9cc9ea5ea540: Download complete 511136ea3c5a: Download complete 6170bb7b0ad1: Download complete 1c7f181e78b9: Download complete f323cf34fd77: Download complete 321f7f4200f4: Download complete 7a4f87241845: Download complete hello worldWith boot2docker we can have many small vms that quickly boot and are ready to run docker right away. Not sure how practical this is, but it’s interesting none-the-less.Now I need to fix up the script a bit, and also figure out how to setup ssh keys so that I don’t have to enter a password to login."
    } ,
  
    {
      "title"    : "hipache",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/03/12/hipache.html",
      "date"     : "2014-03-12 00:00:00 -0400",
      "content"  : "I’ve been trying out using hipache as a routing web proxy. It uses redis as a configuration store of sorts, so you can add webroutes (not sure what to call them…vhost routes?) without having to reload or restart hipache.Running it on Ubuntu 12.04 was fairly straight forward, other than I had to install a much newer version of nodejs and npm than what comes with the distro by default.Install new npm, nodejs, and hipacheSo first, get a recent version of nodejs.# get a newer version of noderoot# grep DESC /etc/lsb-release DISTRIB_DESCRIPTION=\"Ubuntu 12.04.4 LTS\"root# sudo apt-get updateroot# sudo apt-get install -y python-software-properties python g++ makeroot# sudo add-apt-repository ppa:chris-lea/node.jsroot# sudo apt-get updateroot# sudo apt-get install nodejsroot# node --versionv0.10.26Next get a new npm…using npm.root# apt-get install npm# now update npm using npmroot# npm install npm -g --ca=nullroot# npm config set ca=\"\"root# npm --version1.3.26Now install hipache.root# npm install hipache -gInstall redishipache uses redis as a data store for webroutes.I installed version 2.8.7 stable from source.root# wget http://download.redis.io/releases/redis-2.8.7.tar.gzroot# tar zxf redis-2.8.7.tar.gzroot# cd redis-2.8.7root# makeroot# make installroot# cd utils# Setup redis init stuffroot# ./install_server.shAt this point I had to edit the /etc/init.d/redis_6379 file because it had a bunch of “\\n\\n” newlines that weren’t converted for some reason. I’ll have to check into that later on.The head of that file should look like this.root# head redis_6379 #/bin/sh#Configurations injected by install_server below....EXEC=/usr/local/bin/redis-serverCLIEXEC=/usr/local/bin/redis-cliPIDFILE=/var/run/redis_6379.pidCONF=\"/etc/redis/6379.conf\"REDISPORT=\"6379\"###############case \"$1\" inI configured redis to only to listen on localhost.root# grep bind 6379.conf # interfaces using the \"bind\" configuration directive, followed by one or# bind 192.168.1.100 10.0.0.1bind 127.0.0.1Now we can start it.Start redisUse service to start redis.root# service redis_6379 startIt’s now listening on 6379.root# netstat -ant  |grep 6379  | grep LISTENtcp        0      0 127.0.0.1:6379          0.0.0.0:*               LISTEN    Start hipacheHipache provides an upstart script.# hipache - distributed HTTP and websocket proxystart on runlevel [2345]stop on runlevel [06]respawnrespawn limit 15 5script  hipache --config /etc/hipache.jsonend scriptI placed that in /etc/init/hipache.confroot# ls /etc/init/hipache.conf /etc/init/hipache.confFinally we need a hipache config file.root# cat /etc/hipache.json {    \"server\": {        \"accessLog\": \"/var/log/hipache_access.log\",        \"port\": 80,        \"workers\": 5,        \"maxSockets\": 100,        \"deadBackendTTL\": 30,        \"address\": [\"127.0.0.1\"],        \"address6\": [\"::1\"]    },    \"redisHost\": \"127.0.0.1\",    \"redisPort\": 6379,    \"redisDatabase\": 0,}Obviously this is a test config with no password, but it is only listening on localhost.We can start hipache.root# service hipache startConfigure vhostsvhosts for hiapche to proxy are entered into redis.Here’s an example taken from the README:$ redis-cli rpush frontend:www.dotcloud.com mywebsite(integer) 1$ redis-cli rpush frontend:www.dotcloud.com http://192.168.0.42:80(integer) 2$ redis-cli rpush frontend:www.dotcloud.com http://192.168.0.43:80(integer) 3$ redis-cli lrange frontend:www.dotcloud.com 0 -11) \"mywebsite\"2) \"http://192.168.0.42:80\"3) \"http://192.168.0.43:80\"Now accessing a hostname/URL that is associated with the IP hipache is listening on will redirect to the backend servers set in redis, assuming the vhost dns name is configured properly, and we can keep adding them and reconfiguring without taking down hipache.There isn’t a ton of documentation on using hipache, so I’m betting that I’ll eventually run into a wall with it, but for now it’s a pretty interesting project that not only gets me introduced to nodejs but also redis.Also I should setup an ansible role to do this. Not too happy about the packaging here either, upstart scripts all over the place."
    } ,
  
    {
      "title"    : "/dev/random, OSX and Yarrow",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/03/04/yarrow.html",
      "date"     : "2014-03-04 00:00:00 -0500",
      "content"  : "I’ve been doing some research on “hardening” or securing workstations–I prefer to call desktops and laptops workstations–specifically OSX Mavericks.When considering information security cryptography is extremely important. If you subscribe to the CIA triad (confidentiality, integrity, and availability), then cryptography can help with both the “C” and the “I.”Usually the randomness used in cryptography on workstations comes from /dev/random and /dev/urandom. On Linux /dev/random and /dev/urandom are different devices. If the entropy during cryptographic key creation is not high, then the resulting keys will not be as good.  When read, the /dev/random device will only return random bytes within the estimated number of bits of noise in the entropy  pool. /dev/random should be suitable for uses that need very high quality randomness such as one-time pad or key generation. When the entropy pool is empty, reads from /dev/random will block until additional environmental noise is gathered.A read from the /dev/urandom device will not block waiting for more entropy. As a result, if there is not sufficient entropy  in  the  entropy pool, the returned values are theoretically vulnerable to a cryptographic attack on the algorithms used by the driver. Knowledge of how to do this is not available in the current unclassified literature, but it is theoretically possible that such an attack may exist. If this is a concern in your application, use /dev/random instead.If  you are unsure about whether you should use /dev/random or /dev/urandom, then probably you want to use the latter. As a general rule,/dev/urandom should be used for everything except long-lived GPG/SSL/SSH keys. NOTE: from man 4 random on Ubuntu 12.04 PreciseBut on OSX they are essentially the same device, and don’t use a hardware random number generator.  /dev/urandom is a compatibility nod to Linux. On Linux, /dev/urandom will produce lower quality output if the entropy pool drains, while /dev/random will prefer to block and wait for additional entropy to be collected.  With Yarrow, this choice and distinction is not necessary, and the two devices behave identically. You may use either. NOTE: from man 4 random on OSX MavericksIn fact, the devices use the Yarrow algorithm invented by Bruce Schneier and friends!  Yarrow is a PRNG; it generates cryptographically secure pseudo-random numbers on a computer. It can also be used as a real random number generator, accepting random inputs from analog random sources. We wrote Yarrow because after analyzing existing PRNGs and breaking our share of them, we wanted to build something secure.So on one hand, it’s slightly disappointing that my workstation doesn’t have a hardware random number generator (despite the potential that hrngs have been polluted by various government agencies), but on the other the prng algorithm was created in part by a personal information security hero of mine, Bruce Schneier."
    } ,
  
    {
      "title"    : "Book Review - The Phoenix Project",
      "category" : "",
      "tags"     : "",
      "url"      : "/2014/02/21/review-the-phoenix-project.html",
      "date"     : "2014-02-21 00:00:00 -0500",
      "content"  : "NOTE: May contain spoilers.Recently I have been reading books about DevOps–its (short) history and (old) influences. That meant reading The Goal by Eliyahu M. Goldratt, which is a business novel about managing a factory using lean manufacturing concepts. The Goal is heavily referenced in The Phoenix Project, and in fact Kim et al’s book mirrors the style and format of Goldratt’s book. (Though it leaves out much of the home life drama of the main character that is included in Goldratt’s book.)In short the allegorical novel is about Bill Palmer, an IT manager who is suddenly promoted to VP of IT Operations in a large, struggling auto parts manufacturing company. Every IT department and project is a mess and will be familiar to anyone who has worked in the field. The book also uses the same Socratic method as The Goal in that there is an eccentric Guru character, Erik, who helps Bill along the path to success using questions that force critical thinking along the DevOps paradigm.The first part of the book is about connecting with the reader.  …the first 170 pages of the book is really designed to create the response of “holy cow, this is me that they’re describing in the book,”  regardless of your role in the organization.  Why?  It’s because it happens everywhere where DevOps practices and culture isn’t embraced.  – Gene Kim, infoqComing from an information security background, I was struck by how the Chief Information Security Officer (CISO) was depicted in the book, but I was not surprised, because I’ve also read Kim et al’s book Visible Ops Security, in which statements such as the following are made:  [P]eople may use the following words to describe information security: hysterical, irrelevant, bureaucratic, bottleneck, difficult to understand, not aligned with the business, immature, shrill, and perpetually focused on irrelevant technical minutiae.In the context of the Visible Ops Security book I absolutely agree with the above statement, and have always felt that something needs to change in information security in order for it to become more aligned with the business, and instead of saying no all the time, be able to say yes to new, valuable projects, and to help those projects in achieving reasonable security.In The Phoenix Project the CISO, John, hits rock bottom after a series of security project failures. In fact there is a scene with him extremely drunk in a bar, with all his belongings packed in a U-haul attached to his Volvo (of course he drives a Volvo). The VP IT Operations gets him a cab home and afterwards John doesn’t come into the office for two weeks. When he finally returns to work he is a “new man,” complete with a shaved head, new attitude, “Euro discotheque” style, and game to implement security in a way that is acceptable to his peers, as well as Erik, the wealthy, eccentric, lean business (and auditing) Guru. While the transformation is a bit obvious, this is an allegorical novel, and the point’s well taken. I assume at some point he sells the Volvo and buys a Porsche.I don’t think the book is as compelling as Goldratt’s novel. What I do like about The Phoenix Project is its focus on information security, and to a lesser extent auditing. I think there are a lot of improvements that can be made in the way information security practitioners integrate into their organizations culturally, and that this book begins giving examples as to how to accomplish that. However, it does not go far enough down that road, and I would love to read another information security specific book from these authors, one advancing the work done in Visible Ops Security and The Phoenix Project, though I think it will be some time before they can get to it as they are still working on the book The DevOps Cookbook.In the end, The Phoenix Project is a good read which is differentiated from other business and technical books by its storytelling approach, and it is about DevOps, which likely makes it required reading for anyone in, or wanting to know more about, the difficult to define field."
    } ,
  
    {
      "title"    : "Deploy Pound with Ansible or Puppet",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/12/20/deploy-pound-ansible-puppet.html",
      "date"     : "2013-12-20 00:00:00 -0500",
      "content"  : "I just thought I would mention that I’ve put up Puppet and Ansible modules for deploying the Pound proxy and load-balancer.I’ve been working on learning Puppet lately, so that is the reason for that module, and then just yesterday AnsibleWorks released their “playbook” site, called Galaxy, which performs a similar function to the Puppet Forge, so I thought I would try putting together a playbook/module and uploading it.So if you use puppet you can download my Pound module with:$ puppet module install serverascode/poundor if you use ansible you can do:$ ansible-galaxy install serverascode.poundNext up is looking at SaltStack and then onto figuring out how to do testing of the modules. I’m thinking that testing will be the same with every module, so it should be easy to add that to each one, even if I end up supporting Ansible, Puppet, Chef, and SaltStack."
    } ,
  
    {
      "title"    : "Deploy Swift All in one with Puppet",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/12/09/swift-all-in-one-puppet.html",
      "date"     : "2013-12-09 00:00:00 -0500",
      "content"  : "My current employer is deploying OpenStack Swift object storage. I’m a big fan of object storage, and the deployment we are working on is one that will have two regions in separate timezones, so it’s very interesting in terms of having a geo-replicated Swift cluster.We would like–or at least some of us would like–to deploy it with the Puppet configuration management system. While I am quite familiar with Ansible I have hardly used Puppet, so in order to deploy our Swift cluster I need to learn how to use it.Learning PuppetPuppet is a mature configuration management system, which means it has a lot of what I would call “best practices”…things like style guides, blog posts on writing good modules, etc, etc.The way I decided to get into Puppet was to configure puppet manifests to deploy OpenStack Swift All-in-one. Swift All-in-one is a way to run the an entire Swift system off of one virtual machine, and to deploy the Swift code from Git.Deploy Swift-all-in-one with the puppet-saio moduleI called the module SAIO and it can be found on the Puppet Forge.Usually I would write a bit about how to use it, but that should all be in the README which should be much more up to date than this blog post.VagrantOne part that is not included in the Puppet module on the forge site is that there is also a Vagrant configuration file and some Puppet bootstrapping that will allow a simple vagrant up and a Swift All-in-one virtual machine will be built automatically.$ git checkout https://github.com/ccollicutt/puppet-saio$ cd puppet-saio$ vagrant upSNIP!Notice: Finished catalog run in 195.17 seconds$ vagrant ssh# run remakerings# then run startmainAt this point you can start testing out Swift.Incremental puppetism…So far I have moved the module from a single init.pp file to breaking it out into a couple of functions, as well as adding params.pp file (which seems to be a best practice). Also I have run puppet-lint on it and fixed most of the warnings, though I have some work to do in my text editor to make sure that it lets me know about code style issues so I can fix them as I’m editing.Surprisingly, even with a small module (likely less than 400 lines once it fully matures) there is a lot of work in terms of determining order of operations and in breaking it out into ever smaller chunks, both of which are very puppety things to do, and in fact pretty much define the puppet paradigm.I’m not sure Puppet would be considered officially object oriented but it is certainly object-like, and I have spent more time thinking about order, “chunking”, and best practices than I did writing the tasks. That said, the theory is this will lead to better modules."
    } ,
  
    {
      "title"    : "apt-cacher-ng",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/11/16/apt-cacher-ng.html",
      "date"     : "2013-11-16 00:00:00 -0500",
      "content"  : "apt-cacher-ng is an package caching system for apt packages. And I suppose it must be the “next generation” version. :)I find it to be an indespensible system–especially when I am creating complex multi-virtual machine systems on my laptop–because it allows me to only have to download each package once from the Internet, and every vm can just grab the package from apt-cacher-ng. So if you have one, five or ten vms needing the same package, it’s still only downloaded once from the Internet but many times from your local cache server.Configuring it is easy!package_cache_srv$ sudo apt-get install apt-cacher-ngpackage_cache_srv$ sudo vi /etc/apt-cacher-ng/acng.conf# edit config, change the bind address to: BindAddress: 0.0.0.0package_cache_srv$ sudo service apt-cacher-ng restartThen on each system that you want to use the apt-cache server and a proxy server configuration file for apt:vm_1$ cat /etc/apt/apt.conf.d/01proxyAcquire::http { Proxy \"http://:3142\"; };&lt;/code&gt;&lt;/pre&gt;Now each vm configured with the proxy file will use the apt-cache-ng server to obtain packages. "
    } ,
  
    {
      "title"    : "OpenStack Keystone with SSL",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/11/13/openstack-keystone-havana-ssl.html",
      "date"     : "2013-11-13 00:00:00 -0500",
      "content"  : "In this post I want to quickly go over getting SSL enabled in OpenStack Keystone, specifically the Havana release, and on Ubuntu 12.04. I am going to setup SSL with self-signed certificates for testing.Havana Ubuntu cloud repo and packages:$ cat /etc/apt/sources.list.d/ubuntu_cloud_archive_canonical_com_ubuntu.list deb http://ubuntu-cloud.archive.canonical.com/ubuntu precise-updates/havana main$ dpkg --list | grep keystoneii keystone 1:2013.2-0ubuntu1~cloud0   OpenStack identity service - Daemonsii python-keystone 1:2013.2-0ubuntu1~cloud0   OpenStack identity service - Python libraryii python-keystoneclient 1:0.3.2-0ubuntu1~cloud0 Client library for OpenStack Identity APITo create test SSL key file, run this command:$ keystone-manage ssl_setup --keystone-user keystone --keystone-group keystoneThen add an ssl section to the keystone.conf file:[ssl]enable = Truecertfile = /etc/keystone/ssl/certs/keystone.pemkeyfile = /etc/keystone/ssl/private/keystonekey.pemca_certs = /etc/keystone/ssl/certs/ca.pemca_key = /etc/keystone/ssl/certs/cakey.pemSetup at least the keystone “Identity Service” publicurl endpoint to use https, eg:https://$keystone_srv:5000/v2.0Finally, when using the keystone command line client, use the insecure option (again, this is for testing):$ cat adminrc export OS_SERVICE_ENDPOINT=https://$keystone_srv:5000/v2.0export OS_SERVICE_TOKEN=$your_admin_token$ . adminrc$ keystone --insecure user-list+----------------------------------+--------+---------+-------+|                id                |  name  | enabled | email |+----------------------------------+--------+---------+-------+| d83ff7c66b4a4086b498c960fa3096fe | admin  |   True  |       |+----------------------------------+--------+---------+-------+The above assumes there is at least the admin user in keystone. Otherwise, with no users it will just complete with no results.If you don’t use the insecure option you will get this error:$ keystone user-list&lt;attribute 'message' of 'exceptions.BaseException' objects&gt; (HTTP Unable to establish connection to https://$keystone_srv/v2.0/users)Please do let me know if I’ve made any errors by commenting, but so far this is working for me as a basic test of keystone with ssl support.Finally, note that in most production situations, I believe keystone would be fronted by a separate SSL termination system of some kind (eg. OpenBSD’s relayd). So this is just for testing, and perhaps getting to know keystone a bit better."
    } ,
  
    {
      "title"    : "MetalOps - IPMI serial-over-lan and Supermicro systems",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/11/13/ipmi-serial-over-lan-supermicro.html",
      "date"     : "2013-11-13 00:00:00 -0500",
      "content"  : "IPMI access is important for people who admister bare metal…which is something that I do from time to time. I administer a small openenstack cluster of eight nodes, based on Dell C6220 chassis, and also a ten node openstack swift cluster based on Supermicro hardware.Usually people get a console, ie. bios access, via some Java applet. I find that really difficult to use because often the applet only runs on one OS, so it means installing a VM with that OS, firing up a browswer and downloading the applet.IPMI serial-over-lan is much easier and works with both the Dell and the Supermicro hardware.Getting to the bios with SOLUsually I open two terminals:Terminal to use access the SOL console withTerminal to control the power of the serverNOTE: Probably a good idea to change the ADMIN password. :)First, establish an SOL connection. Below I’m assuming the server has already been setup with it’s IPMI static IP of 10.0.0.10.terminal_1$ ipmitool -I lanplus -H 10.0.0.10 -U ADMIN -P ADMIN sol activateNext, power down the server.terminal_2$ ipmitool -I lanplus -H 10.0.0.10 -U ADMIN -P ADMIN chassis power offThen set it to boot into bios. I find this easier then trying to hit a key on startup.terminal_2$ ipmitool -I lanplus -H 10.0.0.10-U ADMIN -P ADMIN chassis bootdev biosAnd finally start the server again.terminal_2$ ipmitool -I lanplus -H 10.0.0.10 -U ADMIN -P ADMIN chassis power onIn a few seconds, or a minute perhaps, takes a while for these systems to boot up, you should see the server start to come up in terminal_1, and eventually it will drop you into the text based bios, an example of which you can see in the picture at the very top of this blog post.To exit, hit enter and then the “~” (tilde) key twice, and finally a “.”. If you just enter one tilde then when you exit you will exit right out of your ssh session to the server that has access to the management network on which the IPMI interfaces are places, assuming that is, you have a secure management network.Note that IPMI interfaces are notoriously insecure, and using them definitely requires some careful thought and resources, if available, to secure them."
    } ,
  
    {
      "title"    : "Truncate command and sparse disks",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/11/11/truncate-and-sparse-disks.html",
      "date"     : "2013-11-11 00:00:00 -0500",
      "content"  : "Just a quick post on creating sparse disks on Linux.Currently I am working on deploying OpenStack Swift. I am using Vagrant and Virtualbox virtual machines to create an Ansible playbook to deploy and manage Swift. To do that I have been using sparse disks to mimic real disks without actually having all the disk space required.$ cd /var/tmp# Create a sparse file using the truncate command$ truncate --size 500G sparse_disk1.img$ ls -la sparse_disk1.img -rw-rw-r-- 1 vagrant vagrant 536870912000 Nov 11 06:20 sparse_disk1.img# Check if there is a loop0 already...$ sudo losetup /dev/loop0loop: can't get info on device /dev/loop0: No such device or address# Ok good lets setup the loop device with the sparse disk image$ sudo losetup /dev/loop0 /var/tmp/sparse_disk1.img # Make a file system on that loop device$ sudo mkfs.xfs -i size=1024 /dev/loop0meta-data=/dev/loop0             isize=1024   agcount=4, agsize=32768000 blks         =                       sectsz=512   attr=2, projid32bit=0data     =                       bsize=4096   blocks=131072000, imaxpct=25         =                       sunit=0      swidth=0 blksnaming   =version 2              bsize=4096   ascii-ci=0log      =internal log           bsize=4096   blocks=64000, version=2         =                       sectsz=512   sunit=0 blks, lazy-count=1realtime =none                   extsz=4096   blocks=0, rtextents=0# And mount it$ sudo mkdir /mnt/sparse_disk1$ sudo mount -o noatime,nodiratime,nobarrier /dev/loop0 /mnt/sparse_disk1$ df -h /dev/loop0Filesystem      Size  Used Avail Use% Mounted on/dev/loop0      500G   33M  500G   1% /mnt/sparse_disk1To unmount/remove…just go in reverse. :)$ sudo umount /mnt/sparse_disk1$ sudo losetup -d /dev/loop0$ rm -f /var/tmp/sparse_disk1.img If you haven’t used Vagrant and configuration management systems such as Chef, Puppet, Ansible, Salt, etc, I highly suggest it. Putting up and tearing down servers and clusters of servers gets addicting."
    } ,
  
    {
      "title"    : "MetalOps - OpenStack Swift reference hardware",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/11/11/metalops-swift-reference-hardware.html",
      "date"     : "2013-11-11 00:00:00 -0500",
      "content"  : "I can’t remember where I first heard the phrase metalops but I think it’s an interesting term, much like devops and all the other ops (opops?). Technology rapidly changes, and so must definitions and labels and job titles, etc, etc. Whether these labels are correct or not, they are often useful. (As an aside, I really like the Gartner Hype Cycle.)I think metalops is interesting because it defines an important role in “the cloud” which is that there is always hardware running beneath it, and someone needs to take care of it. Certainly one of the major features of “the cloud” is that the users don’t have to worry about the underlying hardware. But cloud providers do.My current employer has an interesting role in that while it’s an advocate for the use of “the cloud” (ie. in most cases users not running the physical servers) we also provide several small cloud systems based on OpenStack. This means that we also have to maintain and administer the underlying hardware. Thus, while with one hand I am  using devops tools and methodologies, with the other I am trying to figure out where to get smaller hard-drive screws and whether or not serial-over-lan is going to work on the new hardware so that I don’t have to load up a virtual machine and run a GUI java interface. While I can delete a semi-colon I can’t remove 1.5mm of metal from 80 too long hdd screws.But enough about that, let’s talk OpenStack Swift hardware.Swift hardwareFirst let me note that we are not yet in production with this hardware. I’ll come back and update this post once we are.We bought two types of servers:4x proxy nodes6x storage nodesWe will be deploying the small cluster in two separate geographical areas. We purchased the hardware from Silicon Mechanics who have been extremely helpful throughout the process, especially with regards to getting us parts (the parts we forgot) fast, usually in a couple of days. Their servers are based on Supermicro gear.Proxy NodesThe proxy nodes are simple 1U servers that will act at the front-ends to the Swift system. Each region will have two proxy nodes, and while we haven’t exactly determined how they will be used, each region will likely end up with the pairs being highly available in an active/passive setup, though it’s possible we may change our mind and have them active/active by being load balanced by a third system.Proxy node hardware:  Silicon Mechanics Rackform iServ R335.v3 “Proxy Nodes”  CPU:  1x Intel Xeon E5-2630L, 2.0GHz (6-Core, HT, 15MB Cache, 60W) 32nm  RAM:  64GB (4 x 16GB DDR3-1600 ECC Registered 2R DIMMs) Operating at 1600 MT/s Max  8x 2.5” hot swap drive slots  2x GB on-board NICS and 1x IPMI/BMC shared LAN port  PCIe 3.0 x16 - 2:  Intel X520-DA2 10GbE Dual-Port Server Adapter (82599ES) 10GBASE-CR - SFP+ Direct AttachTo start we only put one CPU and 64GB of RAM in the proxy nodes, but if we find out they are underpowered we can add a CPU and double the RAM quite easily. We also decided to use a chassis with eight drive slots in case we decide to re-use these servers in the future for a completely different purpose. With eight slots they could easily become OpenStack compute nodes. If they only had four drive slots they might not be as useful.Storage nodesThe storage nodes are interesting beasts. Each 4U box has 36x 3.5” drive slots. There are 24x slots on the front of the server and 12x in the back. This is dense storage.To start we only loaded each storage node with 10x 3TB drives, so we can add 26x more drives as we require more storage.  Silicon Mechanics Swift Storage Node  CPU: 2 x Intel Xeon E5-2630L, 2.0GHz (6-Core, HT, 15MB Cache, 60W) 32nm  RAM: 128GB (8 x 16GB DDR3-1600 ECC Registered 2R DIMMs)  2x GB on-board NICS and 1x IPMI/BMC shared LAN port  Controller: I350 Dual-Port Ethernet, 2 Ports 6Gb/s SATA, and 8 Ports 3Gb/s SATA  LP PCIe 3.0 x16: LSI 9207-8i (8-Port Int, PCIe 3.0)  LP PCIe 3.0 x8 - 1: Intel X520-DA2 10GbE Dual-Port Server Adapter (82599ES) - 10GBASE-CR - SFP+ Direct Attach  Front Drive Set: 10 x 3TB Seagate Constellation CS (6Gb/s, 7.2K RPM, 64MB Cache) 3.5-inch SATAWe went with 128GB of memory and lower wattage CPUs that still have 6 cores. We will be using 2x of the hot swap slots for the OS drive. These chassis have 2x internal hard drive slots for OS drives, but getting them out requires pulling the entire server out of the rack to get at them, so we aren’t going to use them.Silicon Mechanics also offers an SSD cache-drive option, but we aren’t going to deploy Swift using cache drives, though I think some organizations do. Perhaps we will in the future. SSD caching is certainly on our list of technologies to investigate.MetalOps IssuesWe did have some issues with these servers, though not due to the vendor whatsoever.  We are reusing some 2.5” hard drives from another project as the OS drives for all these servers. I forgot to order 2.5” adapters for the 3.5” hot swap sleds, so we couldn’t install the OS drives.  The hdd screws that come with the 3.5” sleds in the storage nodes and the 2.5” sleds in the proxy nodes are too long for our 2.5” drives. So we had to order smaller screws.  We didn’t order c13-c14 power cables to plugin to the standard rack power. Fortunately our datacenter was able to provide them or we wouldn’t have been able to power up the servers.  The proxy rack rails are identical for the left and right rails. This means that in order to pull the server out of the rack the left rail release has to be pushed up, and the right release down. It took us a few minutes to figure this out. The storage node rails have distinct left and right rails."
    } ,
  
    {
      "title"    : "A year with OpenStack Essex",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/10/21/year-with-openstack-essex.html",
      "date"     : "2013-10-21 00:00:00 -0400",
      "content"  : "I’ve been using an OpenStack Essex installation (cluster? I never know what to call it) for about a year now. OpenStack has gone through several new versions, Essex to Folsom to Grizzly and now Havana was released only a few days ago. But here I am back on Essex. I believe it has been possible to upgrade OpenStack since Folsom, but because I’m running Essex I think upgrading would mean forklift style. My current workplace has a few OpenStack clouds running, and one of them has gone from Folsom to Grizzy and will eventually go to Havana and beyond, but for this one, we’re stuck on Essex.The system is made up of eight Dell C6220s with one “cloud controller” (ie. not highly available) and seven compute nodes. As of right now we have had almost 3500 virtual machines booted. That’s not bad considering this OpenStack cloud is only used by one application (Apache VCL).mysql&gt; select id from instances order by id DESC limit 1;+------+| id   |+------+| 3355 |+------+1 row in set (0.01 sec)Striped solid state drivesBecause we use OpenStack to essentially provide a VDI-lite service, one in which the VMs don’t store any state, and Windows instances are heavy IOPS users, we moved to using striped solid state drives. We’ve been running the compute nodes that way for about three months and so far so good (prior to that they were running on spinning disks).curtis@c2:~$ cat /proc/mdstat Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] md127 : active raid0 sda5[0] sdc5[2] sdb5[1]      2597588736 blocks super 1.2 256k chunks      md0 : active raid1 sda1[0] sdb1[1]      524224 blocks [2/2] [UU]      md1 : active raid0 sdb3[1] sda3[0] sdc3[2]      106953984 blocks super 1.2 256k chunksunused devices: curtis@c2:~$ sudo smartctl -i /dev/sdasmartctl 5.41 2011-06-09 r3365 [x86_64-linux-3.2.0-51-generic] (local build)Copyright (C) 2002-11 by Bruce Allen, http://smartmontools.sourceforge.net=== START OF INFORMATION SECTION ===Device Model:     Crucial_CT960M500SSD1Serial Number:    1324093FD9ABLU WWN Device Id: 5 00a075 1093fd9abFirmware Version: MU02User Capacity:    960,197,124,096 bytes [960 GB]Sector Sizes:     512 bytes logical, 4096 bytes physicalDevice is:        Not in smartctl database [for details use: -P showall]ATA Version is:   8ATA Standard is:  ATA-8-ACS revision 6Local Time is:    Mon Oct 21 13:57:51 2013 MDTSMART support is: Available - device has SMART capability.SMART support is: Enabled&lt;/code&gt;&lt;/pre&gt;## IssuesSo far we have only had a couple issues.The first is there was a security patch added to the nova package that double checked the virtual size of an image. I updated OpenStack's packages and we couldn't boot Windows images, so I had to roll back to a previous version. I haven't had time to figure out what the issue is, so we have frozen our OpenStack version while I find time to determine the cause. I'm quite sure that this is not an OpenStack issue, rather a configuration issue in terms of the Windows image size.I also have a problem in which it seems the Windows disk images are mounted via ndb, I assume for some kind of file injection, but then not released. This can cause deleted image files to be hung onto by the file system, so disk space can fill up, but du will not be able to explain why (had to use lsof and look for deleted files).curtis@c2:~$ mount | grep nbd/dev/mapper/nbd15p1 on /tmp/openstack-disk-mount-tmp0lvNeR type fuseblk (rw,nosuid,nodev,allow_other,blksize=4096)/dev/mapper/nbd14p1 on /tmp/openstack-disk-mount-tmpMrUDJF type fuseblk (rw,nosuid,nodev,allow_other,blksize=4096)/dev/mapper/nbd13p1 on /tmp/openstack-disk-mount-tmp5p_Jcy type fuseblk (rw,nosuid,nodev,allow_other,blksize=4096)/dev/mapper/nbd12p1 on /tmp/openstack-disk-mount-tmpOv9v9X type fuseblk (rw,nosuid,nodev,allow_other,blksize=4096)SNIP!While this does seem like some kind of bug, the held nbd mounts/images are limited to 16 by a configuration option, and while this shouldn't be happening it doesn't seem to be affecting normal operation, at least in our limited use case. This is another issue that needs more investigation. Maybe someone will read this post and let me know what is going on. I did email the OpenStack list about the issue but no one replied. Not sure how many people are using OpenStack Essex with Windows 7.## Essex has worked greatWhile I've got a couple issues to look into, OpenStack Essex has worked great. Certainly no major issues to report, but I'm still looking forward to eventually upgrading this system."
    } ,
  
    {
      "title"    : "Dark days in information security",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/10/18/dark-days-infosec.html",
      "date"     : "2013-10-18 00:00:00 -0400",
      "content"  : "To be honest it’s been a while since I’ve worked in a security conscious environment. When I first started out my career I was extremely interested in security (and I still am) but the organizations I worked in were not as interested in the end-result as much as the process itself–ie. meeting and checking off certain requirements rather than actually being secure.As my career continued it just became too hard to do the things I thought were required to achieve reasonable information security, so in effect I gave up, and instead of being a security system administrator, or security analyst, whatever the title, I became a plain old systems administrator and started to focus more on storage and research systems, and now “cloud” technology (such as OpenStack), and the devops mindset.However, I still consider information security my true calling in terms of my IT career, so you can imagine how disappointed I was to read wave after wave of articles and news items regarding the various privacy invasions, alleged illegal spying, etc, etc.Even though it’s been a few months since the revelations brought on by the leaks made by Edward Snowden and others, and the snowball effect that has created more interest and information around global information security, I find it difficult to properly analyze and discuss the events. It will likely take some time to sort out the damage. Suffice it to say that I hope that as we continue to use more and more technology that politicians, entrepreneurs, teenagers, security experts, government employees, police and security agencies…everyone…takes some time to consider how things are changing and what security, sovereignty, freedom, privacy, patriotism, and democracy really mean to them.Certainly societal norms can, do, and should change over time, but I think it’s important to occasionally take a step back and try to make sure that we are consciously working towards achieving our true, long term goals as a society. It’s easy to slip into a reactionary posture and make changes that–while seemingly beneficial in the near term–may do more harm than good over a longer period of time. On one hand the future is hard to predict, but on the other…we do make our own future."
    } ,
  
    {
      "title"    : "Georgia Tech Coursera SDN MOOC",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/08/12/sdn-mooc.html",
      "date"     : "2013-08-12 00:00:00 -0400",
      "content"  : "Recently I completed the Software Defined Networking (SDN) course presented by Dr. Nick Feamster of the Georgia Institute of Technology. The course was a massive open online course, otherwise known as a MOOC. (Some of the earliest MOOCs were put on by researchers associated with Athabasca University, an Alberta based post secondary institution specializing in distance learning.)MOOCsI have a Bachelor of Education which means I could have been a teacher in Alberta. However I went in another direction and have a career in information technology, but one that has been almost entirely spent either working at a university as support staff, or at organizations that support universities in one form or another. Therefore I’m aware–perhaps more than the average person–of MOOCs and how they are affecting, or not affecting, post secondary education. Interestingly, the University of Alberta has just began offering their first class using the Coursera platform: DINO 101.SDN CourseBut what I want to talk about here is the Dr. Feamster’s SDN course. One of the people helping to teach the course wrote this thank you note to the students, and mentioned that it’s a good idea to document the learning in some fashion, so that is what I’m attempting to do in this post.  I encourage you to document your learning, which is a more insightful way of providing evidence to your learning….I hope you all will try and apply your new knowledge in SDN at work or in your studies.What I likedIn general, I like the idea of MOOCs. While getting a masters degree of some kind has always been in the back of my mind, I really feel like I am done with school, but being in technology I can never be done with learning. I don’t think I am an autodidact but learning on my own is something that I do all the time (mostly via articles or blog posts on the Internet), have to do in order to be employable, and enjoy doing, so MOOCs fit in with that style of learning well.A couple other people at work also took the course at the same time, and we had some discussions about it. I think we all felt that overall the course was a good introduction to SDN, but that it wasn’t in-depth enough for our tastes. Having said that, I don’t see how the team behind the course could have done it differently, as there were so many different people taking the MOOC, from people with little or no networking experience or programming experience, to highly experience networking and programming professionals.Here are a few things I liked about the class:  Learning about new SDN tools  Having a bit of structure around learning a new technology  The professor’s positive attitude  The MOOC being a good overview of SDN, where it came from and (maybe) where it’s goingI also enjoyed the interviews though didn’t quite have time to watch all of them. Perhaps at some point I can go back and watch them.What I didn’t likeThe most unusual thing, I felt, was the quiz system. Essentially you could take each quiz at least 3 times, most of them more, and you could review your answers, and mistakes, from the previous attempts, and retake the quiz. I’m not sure what the value of doing quizzes in this fashion is, unless you were to take an average of the first quiz and subsequent quizzes, because the easiest thing to do with the quiz is to take it once and even if you score poorly you can simply review your answers and pass the next attempt. Sometimes the question’s changed answers in each attempt, but mostly they did not. I don’t think the quizzes tested students knowledge very well.Another thing was that the instructions for the assignments were occasionally incorrect, at least in terms of cutting and pasting commands. Because I’ve been using the command line for a long time it was no problem to adjust, but if I was a student with little command line experience, I would imagine cutting and pasting commands would have been frustrating…possibly as difficult as the programming assignments themselves.If you did cut and paste exactly what was shown, you would still end up having to futz around because you would find yourself in the wrong working directory, and command examples would not execute. I think part of the reason for this was the the assignments would change as the course was ongoing, but the documentation couldn’t keep up.Also I was never quite sure what mark was required to pass the tests. At least one had to be 100% to pass, I believe, but even now I’m not sure. I think it’s 70% on each quiz to pass. It may very well turn out that I didn’t pass the course. No idea.Finally, unless I’m mistaken, it wasn’t possible to download the slides used in the video lectures. I would have been great to have access to the slides, and notes as well. Given this is the first MOOC I’ve taken I’m not sure if this is part of the general MOOC design or not. I didn’t enjoy the lectures themselves, mostly because I just don’t like the video-over-slides approach.What I learnedThe course covered 6 weeks and 8 modules. There were 10 quizzes and 4 programming assignments.The modules were:  Module 1: History and Evolution of Software Defined Networking  Module 2: Control and Data Plane Separation  Module 3: Virtual Networking  Module 4: SDN Nuts and Bolts - Control Plane  Module 5: SDN Nuts and Bolts - Data Plane  Module 6: Programming SDNs  Module 7: SDN in The Wild  Module 8: The Future of SDN (and Wrap-Up)Mostly I learned about some new and interesting SDN tools. I had used Mininet and Pox, and knew of Nox (related to Pox) and Floodlight, but everything else was new to me.  Ryu seems very interesting, is written in Python (my favorite programming language) and is supported in OpenStack. The small OpenStack cloud I run is based on Essex, so there is no Quantum/Neutron networking, but someday we will upgrade and it will be interesting to try out some SDN capabilities. I suppose I should try some out in test, just toss up Devstack and see what I can find out.  One of the people I work with is working on Open Daylight. Again, not part of the course, but something that I stumbled on while doing some extra reading.  While Trema wasn’t used in the course, one of my co-workers came across it. It’s not in Python, so that is likely why it wasn’t in the course. Instead it’s written in Ruby and some C.Interestingly  Don’t be surprised that Trema has an integrated OpenFlow network emulator and you do not need to prepare OpenFlow switches and end-hosts for testing controller applications!The above is useful because most of the others work in combination with Mininet, which is an extra step.  Click modular router project: I’m not clear on what this actually is.  NetFGPA project I had come across this before–hardware data plane.  Pyretic A SDN programming language, or more specifically a domain specific language, aka DSL.  Resonance  Interview with Jennifer Rexford I thought this was a great interview.What now?So, the question is: Now that I have had a good introduction to SDN, where do I go from here?I don’t really know. It’s unlikely that I will be handed a real network to experiment with, other than perhaps one associated with a small OpenStack cluster, so this means I will have to work with virtualized networks, perhaps using Mininet. Trema is interesting to me because it (apparently) doesn’t require something like Mininet, and can create virtualized networks on its own.One area I’m interested in doing some more research on is creating broken networks and seeing how that affects systems, especially distributed networking systems."
    } ,
  
    {
      "title"    : "Deploying a boundary.com meter with ansible",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/06/27/boundary-meters.html",
      "date"     : "2013-06-27 00:00:00 -0400",
      "content"  : "Lately we have started using Boundary at work. While puppet  and chef recipes and a shell script already exist to deploy a boundary meter onto a node/server, there was not a way to easily deploy one using ansible, which is my preferred configuration management and orchestration tool.Creating a boundary meter on a server is not complicated, but it was not really possible to do it with ansible without simply using ansible to copy a shell script up to the server. And, as ansible’s documentation suggests, if you’re pushing a script up to the server to run it with ansible, then it might be time to turn that bash script into an ansible module.So, I have written a basic boundary meter module for ansible, and it is currently sitting in the pull request queue waiting to be reviewed, and hopefully added to the many ansible modules in core.NOTE: Both boundary and ansible move pretty fast, so it’s likely that things will have changed even by the time I post this. :)Obtaining the ca.pem fileNOTE: It seems this file may be in the ubuntu package now, and is installed in /etc/bprobe/ca.pem.dkpg-distOne file that the bprobe package and my ansible module do not provide is the ca.pem file that is necessary for bprobe to contact boundary’s api server.The easiest way to get that file is to grab it from the bprobe_cookbook github repo.$ wget https://raw.github.com/boundary/bprobe_cookbook/master/files/default/ca.pem$ md5sum ca.pem11f809a92ed1cc029c3ac86b42460a10  ca.pemor I believe it is also available in the official tar.gz release.That file needs to go into /etc/bprobe and would likely be put there using a configuration management system of some kind, be it chef, ansible, puppet, saltstack, etc… :)Getting the bprobe clientWhile you don’t need the bprobe client to create a meter, you do need bprobe in order to send data. So let’s start by installing bprobe.I’m installing the bprobe client on ubuntu 12.04. Below I’ll simply show the parts of my ansible playbook that setup the official boundary repository and install bprobe. Even if you don’t use ansible it’s pretty straightforward to understand what’s happening.#Snippet of an ansible playbook- name: ensure boundary repository is installed  action: apt_repository repo='deb https://apt.boundary.com/ubuntu/ precise universe'  - name: ensure boundary repository gpg key is installed  action: apt_key url=https://apt.boundary.com/APT-GPG-KEY-Boundary state=present- name: install boundary bprobe  action: apt pkg=bprobe state=installed force=yesOnce those three actions have run, bprobe will be installed.$ which bprobe/usr/local/bin/bprobeCreating a meterIn order to start sending data to boundary’s api, that’s all done with a restful api, we need to register a meter. In order to do that with ansible, you’ll need the boundary_meter module I (initially) wrote, which by now, if I’m lucky, will be in ansible’s core set of modules.Once that module is available to ansible, using it to create a meter looks like the below:- name: register boundary meter  action: boundary_meter apikey=AAAAAAA apiid=BBBBBB state=present \\  name=${ inventory_hostname }  notify: restart bprobehandlers:  - name: restart bprobe    action: service name=bprobe state=restartedwhere AAAAAA and BBBBBB are your organizations api id and api key for boundary.MonitoringOnce the meter is registered and bprobe up and running connecting back to boundary’s api server, data should be flowing, and you can login to the boundary web gui and check out the node.Thanks, and as always, if there are questions, suggestions, comments or criticisms, do let me know. :)"
    } ,
  
    {
      "title"    : "Where to find vagrant boxes",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/04/25/where-to-find-vagrant-boxes.html",
      "date"     : "2013-04-25 00:00:00 -0400",
      "content"  : "Update 2: Vagrant Cloud has launched and I believe is now the best way to find Vagrant boxes.Update: Added Phusion Passenger url.This is just a quick post on a couple of places I know to find “vagrant”:htp://vagrantup.com boxes.Ubuntu cloud imagesToday ubuntu 13.04, aka raring ringtail, was released But did you know that ubuntu actually provides vagrant specific boxes, ones that are built every day? They sure do!  http://cloud-images.ubuntu.com/vagrant/raring/current/So it’s quite simple to try out raring just by using vagrant and ubuntu’s cloud images.$ mkdir raring; cd raring$ vagrant init# Edit Vagrantfile and add the below$ grep box Vagrantfile | grep -v \"#\"  config.vm.box = \"raring\"  config.vm.box_url = \"http://cloud-images.ubuntu.com/vagrant/raring/current/raring-server-cloudimg-amd64-vagrant-disk1.box\"$ vagrant upBringing machine 'default' up with 'virtualbox' provider...SNIP!## Now we can ssh into the box#$ vagrant sshWelcome to Ubuntu 13.04 (GNU/Linux 3.8.0-19-generic x86_64)SNIP!vagrant@vagrant-ubuntu-raring-64:~$ cat /etc/lsb-release DISTRIB_ID=UbuntuDISTRIB_RELEASE=13.04DISTRIB_CODENAME=raringDISTRIB_DESCRIPTION=\"Ubuntu 13.04\"Nice. That was easy.Vagrantbox.esMost vagrant users will know about this site, but I add it here for completeness.  http://www.vagrantbox.es/Obviously for testing/development using these images is just fine, but most shops will want to build their own production images. I think.Opscode BentoThis github repo has tons of boxes, and also Packer templates to create your own.Phusion PassengerPhusion Passenger also creates and distributes some vagrant boxes.Make your ownVeewee is a good way to automate image creation. I’ve used it quite a bit, but that was a few months ago. It can take a bit of work to get it up and running.Puppet labs also publishes some information on creating vagrant boxes, as well as several pre-built boxes.FedoraUnfortunately I can’t seem to find official fedora or redhat vagrant boxes, which is too bad.But, fedora is working on it!  http://fedoraproject.org/wiki/Features/FirstClassCloudImagesI’ll try to update this page as I find more resources. Please feel free to comment with suggestions. :)"
    } ,
  
    {
      "title"    : "More over committing with kvm",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/04/12/more-overcommitting-kvm.html",
      "date"     : "2013-04-12 00:00:00 -0400",
      "content"  : "Previously I wrote about overcommiting with kvm.In this post I’m still doing the exact same thing, but now I’m keep track of load and iops.Basic environmentWhat we have is:  a single Dell C6220 node  32 threads  128GB of memory  two Intel 520 SSDs in a stripe  ubuntu precise64 cloud image  qcow2 image snapshots  open vSsitch  dnsmasq providing ip addresses  “ansible”:ansible.cc for running stress testsWhat we’re going to do is boot 300 2GB instances based off the same image.Get setupFirst thing we do is reboot, just to start fresh. It doesn’t really matter, but I like to start with no swap being used, just to show exactly what happens when we boot hundreds of vms.First set ksm to do more scanning faster.root# echo \"20000\" &gt; /sys/kernel/mm/ksm/pages_to_scanroot# echo \"20\" &gt; /sys/kernel/mm/ksm/sleep_millisecsNext–re-setup vswitch, as I have a kind of rigged-up configuration going, which needs some care after a reboot, so obviously this is just for testing, not production. :)$ sudo ifconfig br-int$ sudo ifconfig br-int up$ sudo ifconfig br-int 192.168.100.10 netmask 255.255.255.0That ip is what dnsmasq is set to listen on.I was running into an error where some taps existed already, and the boot script would start failing. I’m not sure why, and I just ended up deleting all the ports in the switch for that particular bridge, br-int.## Show/count all the ports# $ sudo ovs-vsctl list-ports br-int | wc -l300## Delete some ports#$ sudo for i in $(seq 1 300); do ovs-vsctl del-port br-int tap$i; done# This takes a while...Now restart to make sure dnsmasq is listening on 102.168.100.10.$ sudo service dnsmasq stop   * Stopping DNS forwarder and DHCP server dnsmasq [ OK ] ## Now with the right br-int config#$ sudo dnsmasq startNext–time to start hundreds of vms!Start virtual machinesOk, now that we have networking (hopefully) all set up, we’re going to boot 300 vms, 10 seconds apart.Thankfully these are linux vms so they don’t really cause a boot storm, unlike windows 7.If I booted 30 (note: 30, not 300) windows 7 vms the server’s load would get so high that the system would grind to a halt. I know because I’ve tried it. Even though this is an ubuntu cloud image, which hopefully is specifically setup to use less iops, and knowing that is perhaps an unfair advantage, I still have no problem saying that windows images use more resources than linux images.Start instances!root# ./kvm_ubuntu_openvswitch.sh After that completes there are ~300 vms running.$ ps ax | grep \"kvm -drive\" | wc -l301We can run tests on those vms.NOTE: I haven’t taken the time to tell dnsmasq to send dhcp information for more than a /24, so we only have 240 vms with an ip address.$ wc -l /var/lib/misc/dnsmasq.leases 240 /var/lib/misc/dnsmasq.leasesSo when I run the stress test with ansible, we can only run it across 240 vms, even though 300 are running, it’s just that 60 of them don’t have ips.Stress testIn my previous post a commenter suggested running stress, so that’s what I’m doing.Using ansible, I’ll run this stress command:shell stress --cpu 1 --io 1 --vm 1 --vm-bytes 1024M --timeout 10sacross 20 vms at a time, running over all the vms that are reported by an inventory script that looks at the ips in the dnsmasq.leases file.Here’s an example of running ansible’s ping module across all those hosts.$ ansible all -c ssh -i ./inventory.py -m ping -u ubuntu SNIP! 192.168.100.98 | success &gt;&gt; {    \"changed\": false,     \"ping\": \"pong\"}192.168.100.99 | success &gt;&gt; {    \"changed\": false,     \"ping\": \"pong\"}192.168.100.97 | success &gt;&gt; {    \"changed\": false,     \"ping\": \"pong\"}Note that the ips aren’t in order, so .97 is the last host in this run. Suffice it to say that all 240 hosts “ponged” back. :)Here’s the simple ansible playbook I’ll be running. These vms don’t have access to the internet, and don’t have stress installed, so I’m just copying over the package and installing it “manually”, and then running the stress command.$ cat load.yml ---- hosts: all  user: ubuntu  sudo: yes  tasks:  - name: check if stress is already installed    action: shell which stress    register: stress_installed    ignore_errors: True  - name: copy stress deb to server    action: copy src=files/stress_1.0.1-1build1_amd64.deb \\    dest=/tmp/stress_1.0.1-1build1_amd64.deb    only_if: ${stress_installed.rc} &gt; 0  - name: install stress    action: shell dpkg -i /tmp/stress_1.0.1-1build1_amd64.deb    only_if: ${stress_installed.rc} &gt; 0  - name: run stress    action: shell stress --cpu 1 --io 1 --vm 1 --vm-bytes 1024M --timeout 10sLet’s run it across 20 vms at a time and see what happens.$ ansible-playbook -u ubuntu -c ssh -i ./inventory.py -f 20 ./load.ymlPLAY [all] ********************* GATHERING FACTS ********************* ok: [192.168.100.110]ok: [192.168.100.113]ok: [192.168.100.117]SNIP!    192.168.100.97                 : ok=3    changed=2    unreachable=0    failed=0    192.168.100.98                 : ok=3    changed=2    unreachable=0    failed=0    192.168.100.99                 : ok=3    changed=2    unreachable=0    failed=0  # Done!Ansible can be fun. :)GraphsBelow are a rather poor set of graphs. Forgive me as I’m a newbie with gnuplot.As soon as the load starts going up, that is when the test starts, and as soon as it’s on its way down, that’s when it ends. :)First run:!https://raw.github.com/ccollicutt/ccollicutt.github.com/master/img/kvm_overcommitting_load_1.png!Second run:!https://raw.github.com/ccollicutt/ccollicutt.github.com/master/img/kvm_overcommitting_load_2.png!Last load run:!https://raw.github.com/ccollicutt/ccollicutt.github.com/master/img/kvm_overcommitting_load_3.png!So with three runs we see a load of about 30, where a load of 32 would be Ok with me, given we have 32 threads in this server.Also, let’s watch some iops.I gathered iops data using iostat.!https://raw.github.com/ccollicutt/ccollicutt.github.com/master/img/kvm_overcommitting_io_1.png!Ooof, that’s a misleading graph, isn’t it? An increase of one iop is absolutely nothing. A rounding error perhaps.So–the iops don’t change during the test run, I guess because stress isn’t running any io test, even though we are running with –io 1. That said, I’m not sure what an io setting of 1 with stress does, something to look into. Perhaps running some tests with fio is something to do in the future.But that graph sure looks like something’s happening, even though the iops only increase by one. I probably shouldn’t include that graph here, but part of what I’m doing is learning about how to display the results of a performance test.ConclusionFirst off, I’m not a scientist, didn’t take statistics, etc. So I’m not sure what kind of conclusions can be made here. All I can say for sure is that fours runs of the stress command across 20 virtual machines in parallel, over a total of 240 vms all running on the same KVM-based host, seems to bring load up to somewhere around 30 to 35, which is acceptable to me.Mostly this has generated more questions–such as what exactly is stress doing? How do we know when the vms are too unresponsive? What kind of overcommitting numbers do we want? What would happen if we used fio instead of –io 1 with stress? Do red lines in a graph make things seem worse? :)As usual, if anyone has any suggestions, questions, or critiques let me know in the comments!"
    } ,
  
    {
      "title"    : "Vagrant and vmware",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/04/11/vagrant-and-vmware.html",
      "date"     : "2013-04-11 00:00:00 -0400",
      "content"  : "After IRC, vagrant is probably my most important development tool, mostly because I like to use and investigate openstack, which means using a lot of virtual machines.Recently Hashicorp released Vagrant 1.1 which introduces the idea of providers. Previously vagrant only supported virtualbox, but now, with 1.1, plugins can be written to support almost any virtualization system that has a command line or API interface of some sort.For example:  VMWare Fusion (note that this is a paid plugin)  AWS (github repo)  RackSpace  OpenStack (based on the rackspace plugin)The provider I’m going to focus on here is vmware fusion.vmware_fusionOne of the things I’ve learned about using brand new technologies is that they often don’t work and/or don’t have any documentation, which frankly are about the same thing to me. That sounds like a kind of grumpy thing to say, but I’m kind of grumpy today. :)Regardless, I went ahead and bought vmware fusion (which is cheap, BTW, at $49) and also the vagrant vmware_fusion plugin (which is $79).I think this is the first time that I’ve encountered a plugin that was more expensive than the actual application it was plugging into, but I can understand the pricing because Fusion is probably under-valued, or at least under-priced. Plus the $79 goes towards the development of vagrant, which I use a lot.Recently I deployed packstack via vagrant and virtualbox, and I wanted to do the same with vmware_fusion, but I ran into a few problems, which I’m going to spend the rest of the post detailing.NOTE: I should say that nothing here is the vmware_fusion plugins fault. I’m not blaming the plugin at all. Rather just detailing some of the pain points I’ve encountered, which will no doubt disappear as more people use vmware fusion and vagrant together, and as I get my act together. I’ll try to update this post as I find out new information. :)Routes collide!I have both vmware fusion and virtualbox installed on my macbook retina. Unfortunately, virtualbox has an iron grip on its networks.  VirtualBox hangs on to its network devices (“vboxnet”) for dear life. I haven’t figured out yet how to actually get rid of them except restarting your computer. – Mitchell HashimotoIf you encounter the below error, either change subnets (perhaps in virtualbox, perhaps in the vagrantfile, not sure) or reboot.$ vagrant up apis --provider=vmware_fusionBringing machine 'apis' up with 'vmware_fusion' provider...[apis] Verifying vmnet devices are healthy...The VMware network device 'vmnet2' can't be started becauseits routes collide with another device: 'vboxnet'. Pleaseeither fix the settings of the VMware network device or stop thecolliding device. Your machine can't be started while VMwarenetworking is broken.Again, not vmware_fusion’s fault, but still a pain. I can’t simply un-install virtualbox…yet.vmx settingsOften we want to change the settings in the virtual machine, settings such as memory, number of cpus, etc.Unfortunately vmx is an undocumented format.  VMX is an undocumented format. You’ll have to google, unfortunately. :) – Mitchell HashimotoBut at the very least here is how to set memory:config.vm.provider :vmware_fusion do |p|  p.vmx['memsize'] = '2048'endAs more people use vmware_fusion there will be better documentation on vmx settings.Centos6 boxWhile Hashicorp has conveniently provided a base precise64 box for vagrant, there isn’t an official centos box. I have previously tried to create a centos6 box for vagrant, but haven’t had much luck, and that was with vagrant &lt; 1.1 and there is even less documentation on the process now.Then I noticed that vagrantbox.es (which is a very handy site!) has a centos6 box for vmware_fusion, so I grabbed that:  CentOS 6.4 x86_64 Minimal VMware Fusion (VMware Tools, Chef 11.4.0, Puppet 3.1.1)Unfortunately it doesn’t seem to work when multiple interfaces are specified in the vagrantfile, so that doesn’t help me much on my quest to run packstack in vmware_fusion. If anyone knows of a good centos6 box, or notices that I’m doing something wrong, please let me know!Here’s the networking part of the config:config.vm.network :private_network, ip: \"172.10.0.200\", :netmask =&gt; \"255.255.0.0\"config.vm.network :private_network, ip: \"10.10.0.200\", :netmask =&gt; \"255.255.0.0\" Let’s boot it:$ vagrant up --provider=vmware_fusionBringing machine 'default' up with 'vmware_fusion' provider...[default] Cloning Fusion VM: 'centos65fusion'. This can take some time...[default] Verifying vmnet devices are healthy...[default] Preparing network adapters...[default] Starting the VMware VM...[default] Waiting for the VM to finish booting...[default] The machine is booted and ready![default] Forwarding ports...[default] -- 22 =&gt; 2222[default] Configuring network adapters within the VM...The following SSH command responded with a non-zero exit status.Vagrant assumes that this means the command failed!/sbin/ifup eth1 2&gt; /dev/nullOoops, shouldn’t be seeing the failed command.What’s the networking like?$ vagrant ssh[vagrant@vagrant-centos-6 ~]$ ifconfigeth0      Link encap:Ethernet  HWaddr 00:0C:29:24:6A:AD            inet addr:192.168.134.146  Bcast:192.168.134.255  Mask:255.255.255.0          inet6 addr: fe80::20c:29ff:fe24:6aad/64 Scope:Link          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1          RX packets:432 errors:426 dropped:0 overruns:0 frame:0          TX packets:293 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:1000           RX bytes:48690 (47.5 KiB)  TX bytes:38046 (37.1 KiB)          Interrupt:19 Base address:0x2024 lo        Link encap:Local Loopback            inet addr:127.0.0.1  Mask:255.0.0.0          inet6 addr: ::1/128 Scope:Host          UP LOOPBACK RUNNING  MTU:16436  Metric:1          RX packets:0 errors:0 dropped:0 overruns:0 frame:0          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:0           RX bytes:0 (0.0 b)  TX bytes:0 (0.0 b)Nope that’s not what I wanted at all.Ok, now let’s use the exact same vagrantfile but with the offical vmware_fusion ubuntu box.config.vm.box = \"precise64\"Vagrant up!## Destroy the old one# $ vagrant destroy[default] Stopping the VMware VM...[default] Deleting the VM...## Edit the vagrantfile to use precise64 basebox#$ vi Vagrantfile## Boot it# $ vagrant up --provider=vmware_fusionBringing machine 'default' up with 'vmware_fusion' provider...[default] Cloning Fusion VM: 'precise64'. This can take some time...[default] Verifying vmnet devices are healthy...[default] Preparing network adapters...[default] Starting the VMware VM...[default] Waiting for the VM to finish booting...[default] The machine is booted and ready![default] Forwarding ports...[default] -- 22 =&gt; 2222[default] Configuring network adapters within the VM...[default] Enabling and configuring shared folders...[default] -- vagrant-root: /Users/curtis/working/vagrant/grizzly## SSH into the box...# $ vagrant sshWelcome to Ubuntu 12.04.1 LTS (GNU/Linux 3.2.0-29-virtual x86_64) * Documentation:  https://help.ubuntu.com/Last login: Thu Jan 31 13:48:53 2013vagrant@precise64:~$ ifconfigeth0      Link encap:Ethernet  HWaddr 00:0c:29:29:dc:aa            inet addr:192.168.134.139  Bcast:192.168.134.255  Mask:255.255.255.0          inet6 addr: fe80::20c:29ff:fe29:dcaa/64 Scope:Link          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1          RX packets:375 errors:367 dropped:0 overruns:0 frame:0          TX packets:241 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:1000           RX bytes:41473 (41.4 KB)  TX bytes:33220 (33.2 KB)          Interrupt:18 Base address:0x2024 eth1      Link encap:Ethernet  HWaddr 00:0c:29:29:dc:b4            inet addr:172.10.0.200  Bcast:172.10.255.255  Mask:255.255.0.0          inet6 addr: fe80::20c:29ff:fe29:dcb4/64 Scope:Link          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1          RX packets:1 errors:1 dropped:0 overruns:0 frame:0          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:1000           RX bytes:209 (209.0 B)  TX bytes:468 (468.0 B)          Interrupt:16 Base address:0x20a4 eth2      Link encap:Ethernet  HWaddr 00:0c:29:29:dc:be            inet addr:10.10.0.200  Bcast:10.10.255.255  Mask:255.255.0.0          inet6 addr: fe80::20c:29ff:fe29:dcbe/64 Scope:Link          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1          RX packets:1 errors:1 dropped:0 overruns:0 frame:0          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:1000           RX bytes:209 (209.0 B)  TX bytes:468 (468.0 B)          Interrupt:17 Base address:0x2424 lo        Link encap:Local Loopback            inet addr:127.0.0.1  Mask:255.0.0.0          inet6 addr: ::1/128 Scope:Host          UP LOOPBACK RUNNING  MTU:16436  Metric:1          RX packets:0 errors:0 dropped:0 overruns:0 frame:0          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:0           RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)That’s what I expect to see.Example vagrantfilesUPDATE (April 18th, 2013): This Vagrantfile now doesn’t seem to work with Vagrant 1.2I have been searching for good examples of vagrantfiles that use vmware_fusion.So far I’ve just found this one:  https://github.com/uksysadmin/OpenStackCookBook-1/blob/master/VagrantfileBut I will keep an eye out for other examples.UPDATE (April 18th, 2013): New network problemThis is a new one…now this can’t be virtualbox’s fault.$ vagrant up --provider=vmware_fusionBringing machine 'percona0' up with 'vmware_fusion' provider...Bringing machine 'percona1' up with 'vmware_fusion' provider...Bringing machine 'percona2' up with 'vmware_fusion' provider...Bringing machine 'haproxy0' up with 'vmware_fusion' provider...Bringing machine 'haproxy1' up with 'vmware_fusion' provider...[percona0] Cloning Fusion VM: 'precise64'. This can take some time...[percona0] Verifying vmnet devices are healthy...The VMware network device 'vmnet1' can't be started becauseits routes collide with another device: 'vmnet13'. Pleaseeither fix the settings of the VMware network device or stop thecolliding device. Your machine can't be started while VMwarenetworking is broken.Instructions from Mitchell, edit /Library/Preferences/VMware\\ Fusion/networking and:bq.. Get rid of all the lines in that file except the ones that start with “answer VNET_1” or “answer VNET_8”. We want to keep those, as they’re the default networks that ship with Fusion. After that, open VMware Fusion.app, then run these commands in a separate terminal:sudo /Applications/VMware\\ Fusion.app/Contents/Library/vmnet-cli –stopsudo /Applications/VMware\\ Fusion.app/Contents/Library/vmnet-cli –configuresudo /Applications/VMware\\ Fusion.app/Contents/Library/vmnet-cli –startThen runsudo /Applications/VMware\\ Fusion.app/Contents/Library/vmnet-cli –statusAnd tell me the output. Should only have the vmnet1/vmnet8 devices. AfterTHAT you shoudl be good to go again.VMware networking is an absolute nightmare. - MitchellIt sucks that it’s an edge case, but I still hope that there is some code added to help in situations like this. Thanks to Mitchell for responding on the mailing list, as now I can continue on with other problems. :)ConclusionI love vagrant, but am having a heck of a time with vmware_fusion and centos6. Having said that, I KNOW that things are going to get better as I learn and as more people start using vagrant and vmware_fusion.Hats off to Mitchell for creating a great development tool, one that I use every day!"
    } ,
  
    {
      "title"    : "Vagrant and openstack",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/04/11/vagrant-and-openstack.html",
      "date"     : "2013-04-11 00:00:00 -0400",
      "content"  : "Earlier I wrote a post on using vmware fusion and vagrant.Now I’m going to use vagrant and the vmware_fusion plugin to create a precise64 virtual machine, in which I will install devstack, and then I will use the vagrant and openstack plugin to boot a cirros vm inside the devstack vm. Meta…inception…whatever you want to call it. :)NOTE: Make sure your precise64 vm has more than the default memory of 512–I set mine to 2048. A bit more memory might be nice too, if you’ve got it available.NOTE: Here is a great post to follow on using devstack and grizzly and quantum, much of which I am reusing here.Why?There is no spoon.Install devstackDevstack is a really useful development environment for openstack. If you want to try out the new features in openstack grizzly, this is an easy way.## Create the vm#$ vagrant up --provider=vmware_fusionBringing machine 'default' up with 'vmware_fusion' provider...SNIP!## Login to the vm#$ vagrant sshWelcome to Ubuntu 12.04.1 LTS (GNU/Linux 3.2.0-29-virtual x86_64) * Documentation:  https://help.ubuntu.com/Last login: Thu Apr 11 10:14:43 2013 from 192.168.134.1vagrant@precise64:~$ sudo apt-get updateIgn http://security.ubuntu.com precise-security InReleaseIgn http://us.archive.ubuntu.com precise InReleaseIgn http://us.archive.ubuntu.com precise-updates InReleaseSNIP!$ sudo apt-get install gitReading package lists... DoneBuilding dependency tree       Reading state information... DoneThe following extra packages will be installed:  git-man libcurl3-gnutls liberror-perl librtmp0 rsyncSNIP!$ git clone git://github.com/openstack-dev/devstack.gitCloning into 'devstack'...## Setup devstack with a localrc file# vagrant@precise64:~$ cd devstackvagrant@precise64:~/devstack$ cat localrcENABLED_SERVICES=g-api,g-reg,key,n-api,n-crt,n-obj,n-cpu,n-sch,n-cauth, \\horizon,mysql,rabbit,sysstat,cinder,c-api,c-vol,c-sch,n-cond,quantum,q-svc, \\q-agt,q-dhcp,q-l3,q-meta,q-lbaas,n-novnc,n-xvnc,q-lbaasDATABASE_PASSWORD=passwordRABBIT_PASSWORD=passwordSERVICE_TOKEN=passwordSERVICE_PASSWORD=passwordADMIN_PASSWORD=password## Run stack.sh# vagrant@precise64:~/devstack$ ./stack.shTraceback (most recent call last):  File \"\", line 2, in ImportError: No module named netaddrTraceback (most recent call last):  File \"\", line 2, in ImportError: No module named netaddrSNIP!# That error doesn't look good...oh well let's continue...# hit enter a few times# Go for a walk, get a coffee, do some vacuuming...Horizon is now available at http://192.168.134.139/Keystone is serving at http://192.168.134.139:5000/v2.0/Examples on using novaclient command line is in exercise.shThe default users are: admin and demoThe password: ed5cb213364bb0fd15a9This is your host ip: 192.168.134.139stack.sh completed in 694 seconds.&lt;/code&gt;&lt;/pre&gt;Now that devstack seems to have completed the install, check and see if basic openstack commands are working.# # Source the user, password file generated by devstack#vagrant@precise64:~/devstack$ source openrc ## And lets see what's running and is available#vagrant@precise64:~/devstack$ nova list+----+------+--------+----------+| ID | Name | Status | Networks |+----+------+--------+----------++----+------+--------+----------+vagrant@precise64:~/devstack$ nova image-list+--------------------------------------+---------------------------------+--------+--------+| ID                                   | Name                            | Status | Server |+--------------------------------------+---------------------------------+--------+--------+| 73f320dd-5769-4ec2-a0e7-e44979070e8c | cirros-0.3.1-x86_64-uec         | ACTIVE |        || 4af449b1-a70b-4857-93ea-9690bc5db779 | cirros-0.3.1-x86_64-uec-kernel  | ACTIVE |        || 017e58df-27bc-4bb4-89d3-f133760a3f0e | cirros-0.3.1-x86_64-uec-ramdisk | ACTIVE |        |+--------------------------------------+---------------------------------+--------+--------+## Oooh, we have quantum too!#vagrant@precise64:~$ quantum net-list+--------------------------------------+---------+--------------------------------------------------+| id                                   | name    | subnets                                          |+--------------------------------------+---------+--------------------------------------------------+| 5a39203e-3d83-4d47-a75e-9ec98f5ed595 | private | dae29b88-1562-42e4-8e30-0ecce7b40f47 10.0.0.0/24 || a608d79d-ace8-4335-81c3-3490393d7700 | public  | cc058059-b342-41d9-8c68-98d6feedcfbd             |+--------------------------------------+---------+--------------------------------------------------+vagrant@precise64:~$ quantum subnet-list+--------------------------------------+------+-------------+--------------------------------------------+| id                                   | name | cidr        | allocation_pools                           |+--------------------------------------+------+-------------+--------------------------------------------+| dae29b88-1562-42e4-8e30-0ecce7b40f47 |      | 10.0.0.0/24 | {\"start\": \"10.0.0.2\", \"end\": \"10.0.0.254\"} |+--------------------------------------+------+-------------+--------------------------------------------+## Using vagrant with openstackFirst, get the vagrant-openstack plugin.$ vagrant plugin install vagrant-openstackInstalling the 'vagrant-openstack' plugin. This can take a few minutes...Installed the plugin 'vagrant-openstack (0.0.2)'!$ vagrant plugin listvagrant-openstack (0.0.2)vagrant-vmware-fusion (0.4.2)Before we get too far, let's create a keypair in devstack. vagrant@precise64:~$ source ~/devstack/openrc vagrant@precise64:~$ nova keypair-add --pub-key ~/.ssh/authorized_keys vagrantvagrant@precise64:~$ nova keypair-list+---------+-------------------------------------------------+| Name    | Fingerprint                                     |+---------+-------------------------------------------------+| vagrant | dd:3b:b8:2e:85:04:06:e9:ab:ff:a8:0a:c0:04:6e:d6 |+---------+-------------------------------------------------+I'm going to create a new local directory to work with vagrant out of.$ cd ~/working/vagrant$ mkdir vagrant-openstack$ cd vagrant-openstack$ vagrant init$ vi Vagrantfile# Add config information...We need to insert some information into the vagrantfile for openstack.First get the image ID. Devstack automatically adds an image, but each time devstack is run the ID will be different.vagrant@precise64:~$ nova image-list+--------------------------------------+---------------------------------+--------+--------+| ID                                   | Name                            | Status | Server |+--------------------------------------+---------------------------------+--------+--------+| 0cf481ad-482e-441c-b8a6-49e792ae0dfb | cirros-0.3.1-x86_64-uec         | ACTIVE |        || 2630cd9e-c375-49d0-81bd-ffbfc638e752 | cirros-0.3.1-x86_64-uec-kernel  | ACTIVE |        || 7375ddbc-51c7-4492-bd2b-de30f10210db | cirros-0.3.1-x86_64-uec-ramdisk | ACTIVE |        |+--------------------------------------+---------------------------------+--------+--------+In this example we want the _0cf481ad-482e-441c-b8a6-49e792ae0dfb_ image ID.Also, we probably want to add a smaller flavor for the cirros image. By default the smallest flavor uses 512MB of ram.## Default flavors#vagrant@precise64:~$ nova flavor-list+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+-------------+| ID | Name      | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | extra_specs |+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+-------------+| 1  | m1.tiny   | 512       | 0    | 0         |      | 1     | 1.0         | True      | {}          || 2  | m1.small  | 2048      | 20   | 0         |      | 1     | 1.0         | True      | {}          || 3  | m1.medium | 4096      | 40   | 0         |      | 2     | 1.0         | True      | {}          || 4  | m1.large  | 8192      | 80   | 0         |      | 4     | 1.0         | True      | {}          || 5  | m1.xlarge | 16384     | 160  | 0         |      | 8     | 1.0         | True      | {}          |+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+-------------+## Add a smaller flavor#  vagrant@precise64:~$ nova-manage flavor create --name=m1.teeny --memory=64 \\--cpu=1 --root_gb=0 --ephemeral_gb=0 --flavor=6 --swap=0 --is_public yes2013-04-11 11:36:08    DEBUG [nova.openstack.common.lockutils] Got semaphore \\\"dbapi_backend\" for method \"__get_backend\"...m1.teeny created## Now we have a 6th flavor!# vagrant@precise64:~$ nova flavor-list+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+-------------+| ID | Name      | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | extra_specs |+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+-------------+| 1  | m1.tiny   | 512       | 0    | 0         |      | 1     | 1.0         | True      | {}          || 2  | m1.small  | 2048      | 20   | 0         |      | 1     | 1.0         | True      | {}          || 3  | m1.medium | 4096      | 40   | 0         |      | 2     | 1.0         | True      | {}          || 4  | m1.large  | 8192      | 80   | 0         |      | 4     | 1.0         | True      | {}          || 5  | m1.xlarge | 16384     | 160  | 0         |      | 8     | 1.0         | True      | {}          || 6  | m1.teeny  | 64        | 0    | 0         |      | 1     | 1.0         | True      | {}          |+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+-------------+Flavor 6 is what we'll use.Next, check the OS vars in devstack to see what to put into the vagrantfile:vagrant@precise64:~$ env | grep \"^OS\"OS_PASSWORD=passwordOS_AUTH_URL=http://192.168.134.139:5000/v2.0OS_USERNAME=demoOS_TENANT_NAME=demoOS_CACERT=/opt/stack/data/CA/int-ca/ca-chain.pemOS_NO_CACHE=1Now with all that information we can fill out the vagrantfile. Mine looks like this:$ cat Vagrantfile # -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure(\"2\") do |config|  # All Vagrant configuration is done here. The most common configuration  # options are documented and commented below. For a complete reference,  # please see the online documentation at vagrantup.com.  # Every Vagrant virtual environment requires a box to build off of.  config.vm.box = \"base\"    config.vm.provider :openstack do |os|    os.url = \"http://192.168.134.139:5000/v2.0\"    os.tenant = \"demo\"    os.user = \"demo\"    os.password = \"password\"    os.flavor = \"6\"    os.keypair = \"vagrant\"    os.image = \"0cf481ad-482e-441c-b8a6-49e792ae0dfb\"    # Not sure why but I feel like calling this vm whitney    os.name = \"whitney\"    os.ssh_username = \"cirros\"    os.ssh_private_key = \"~/.ssh/id_dsa.pub\"  endendUsing vagrant, boot the vm.## Boot it# $ vagrant up --provider=openstackBringing machine 'default' up with 'openstack' provider...[default] New VM created 419e5940-e068-42a4-bb28-68ad72f85d8a =&gt; whitney## Check status#$ vagrant statusCurrent machine states:default                  running (openstack)The nova instance is running. To stop this machine, you can run`vagrant halt`. To destroy the machine, you can run `vagrant destroy`.Let's see what's happening in devstack.## What does virsh know?# vagrant@precise64:~$ sudo virsh list Id Name                 State----------------------------------  1 instance-00000001    running## And lets ask openstack... #vagrant@precise64:~$ source ~/devstack/openrc vagrant@precise64:~$ nova list+--------------------------------------+---------+--------+------------------+| ID                                   | Name    | Status | Networks         |+--------------------------------------+---------+--------+------------------+| 419e5940-e068-42a4-bb28-68ad72f85d8a | whitney | ACTIVE | private=10.0.0.3 |+--------------------------------------+---------+--------+------------------+vagrant@precise64:~$ nova show 419e5940-e068-42a4-bb28-68ad72f85d8a+-----------------------------+----------------------------------------------------------------+| Property                    | Value                                                          |+-----------------------------+----------------------------------------------------------------+| status                      | ACTIVE                                                         || updated                     | 2013-04-11T18:40:43Z                                           || OS-EXT-STS:task_state       | None                                                           || private network             | 10.0.0.3                                                       || key_name                    | vagrant                                                        || image                       | cirros-0.3.1-x86_64-uec (0cf481ad-482e-441c-b8a6-49e792ae0dfb) || hostId                      | cbfc5a689eaff0c72de8f66161efb06270322d48baf6d9120f612c42       || OS-EXT-STS:vm_state         | active                                                         || flavor                      | m1.teeny (6)                                                   || id                          | 419e5940-e068-42a4-bb28-68ad72f85d8a                           || security_groups             | [{u'name': u'default'}]                                        || user_id                     | 26c0f9a23e9c44f6b660557122119171                               || name                        | whitney                                                        || created                     | 2013-04-11T18:40:31Z                                           || tenant_id                   | bb54c65c4aba482f8f6d363e0730df95                               || OS-DCF:diskConfig           | MANUAL                                                         || metadata                    | {}                                                             || accessIPv4                  |                                                                || accessIPv6                  |                                                                || progress                    | 0                                                              || OS-EXT-STS:power_state      | 1                                                              || OS-EXT-AZ:availability_zone | nova                                                           || config_drive                |                                                                |+-----------------------------+----------------------------------------------------------------+Nice.Now, unless we give this vm a \"public ip\" we won't be able to ssh in without hopping into the devstack host first.But first...one. More. Step.By default, with devstack, it seems the default security group is pretty restrictive. So we need to add a couple rules.## Default secgroup rules#vagrant@precise64:~$ nova secgroup-list-rules default+-------------+-----------+---------+----------+--------------+| IP Protocol | From Port | To Port | IP Range | Source Group |+-------------+-----------+---------+----------+--------------+|             | -1        | -1      |          | default      ||             | -1        | -1      |          | default      |+-------------+-----------+---------+----------+--------------+## Add ping# vagrant@precise64:~$ nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0+-------------+-----------+---------+-----------+--------------+| IP Protocol | From Port | To Port | IP Range  | Source Group |+-------------+-----------+---------+-----------+--------------+| icmp        | -1        | -1      | 0.0.0.0/0 |              |+-------------+-----------+---------+-----------+--------------+## Add ssh# vagrant@precise64:~$ nova secgroup-add-rule default tcp 22 22 0.0.0.0/0+-------------+-----------+---------+-----------+--------------+| IP Protocol | From Port | To Port | IP Range  | Source Group |+-------------+-----------+---------+-----------+--------------+| tcp         | 22        | 22      | 0.0.0.0/0 |              |+-------------+-----------+---------+-----------+--------------+## New secgroup rules#vagrant@precise64:~$ nova secgroup-list-rules default+-------------+-----------+---------+-----------+--------------+| IP Protocol | From Port | To Port | IP Range  | Source Group |+-------------+-----------+---------+-----------+--------------+|             | -1        | -1      |           | default      ||             | -1        | -1      |           | default      || icmp        | -1        | -1      | 0.0.0.0/0 |              || tcp         | 22        | 22      | 0.0.0.0/0 |              |+-------------+-----------+---------+-----------+--------------+## And now we should be able to ping and ssh in to whitney# vagrant@precise64:~$ ping -c 1 -w 1 10.0.0.3PING 10.0.0.3 (10.0.0.3) 56(84) bytes of data.64 bytes from 10.0.0.3: icmp_req=1 ttl=63 time=72.2 ms--- 10.0.0.3 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 72.261/72.261/72.261/0.000 msvagrant@precise64:~$ ssh cirros@10.0.0.3cirros@10.0.0.3's password: # enter \"cubswin:)\"$ uname -aLinux cirros 3.2.0-37-virtual #58-Ubuntu SMP Thu Jan 24 15:48:03 UTC 2013 x86_64 GNU/Linux_NOTE: I'm skipping the part about using the authorized_keys file cirros sets up for itself based on the keypair specified. But you can ssh into the cirros instance without a password if everything is setup right, ssh -A, ssh-agent, etc._## ConclusionAnd that concludes our brief look at booting a vm inside of vm, using:- macbook retina- vagrant- vmware fusion- vagrant-openstack- devstack- cirros- openstack grizzlyEverything works!We can even delete the vm we just created:$ vagrant destroy[default] Deleting the instance...Check in with openstack...## vm gone!# vagrant@precise64:~$ nova list+----+------+--------+----------+| ID | Name | Status | Networks |+----+------+--------+----------++----+------+--------+----------+"
    } ,
  
    {
      "title"    : "Thoughts on &quot;no reliable cloud&quot;",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/04/10/no-reliable-cloud.html",
      "date"     : "2013-04-10 00:00:00 -0400",
      "content"  : "Recently Hendrik Volkmer put up a blog post entitled There will be no reliable cloud. Part of it was based on a presentation I watched at the last OpenStack summit (wish I was going to the Portland summit, but alas is not to be).The Cloud Scaling presentation was one I enjoyed and considered thought provoking. I wrote a few notes on that presentation last year.No reliable cloudHere’s a quote from the top of the first post  Stop wasting your time trying to [find a reliable cloud]. Stop wasting your time (and money) trying to build one. If you find a service provider that claims that they have it: Maybe question their understanding of cloud - and business.I put that there to remind me of the point of the series of posts, and because it essentially defines the attention grabbing headline. :)tl;drMy thoughts on these posts come down to this:  He’s mostly talking web-scale applications  A single zone will not be reliable  But still have to make zones as reasonably reliable as possible (where’s the line?)  We should design reliable applications on top of unreliable zones (but how?)  Contain failure!  HA pairs are probably not the direction to go in to gain reliability  Clustering software often brings in complexity that can destroy reliability gains  Stateless systems are a lot more fun :)  Keep the stateful part of an application or system smallThinking about reliability in a cloud, especially an OpenStack cloud, is an interesting thought experiment. Fortunately, the OpenStack cloud I help to run, which is the back-end for a single application, is actually mostly stateless–except for machine images, the OpenStack database, and the application database. Not a lot of stateful information, except those darn windows images that are many tens of times the size of a standard Linux cloud image.Notes from the part one postFor a short post it sure goes over a lot of information and links!  HA pairs fail catastrophically  HA pairs don’t scale  Classic HA example: NFS + DRBD and clustering, such as Pacemaker…then problems?  HA pairs often end up cheating CAP theorem  Cluster software causes more system outages than hardware failures of software bugs (this I can attend to having used clustered LVM)  Distributed systems** Eg. Percona Xtradb Cluster      Availability vs reliability** HA systems that need to go down for maintenance are a joke    How to build a reliable cloud  A cloud is a distributed system  Use the stateless (from Cloud Scaling presentation) approach for stateless parts  Distributed data stores for the stateful parts (eg. distributed mysql, distributed file systems such as ceph)  But the distributed stateful part is often what fails (eg. EBS in Amazon)  Notes from blog post comments (notably Randy Bias of Cloud Scaling)  On OpenStack  Move to MySQL Cluster with the NBDEngine running 2-4 mysql instances, and load balancing across them  Or perhaps OpenStack will get rid of the RDBMS and replace with K/V store  Even with 1000s of nodes, metadata use is still low in OpenStack, could be put in memory and persist data using any appropriate back-end  No point in having highly redundant hardware for stateless services  Build reliable applications on unreliable cloudsOk, now on to part two.Notes from part twoThe second post builds on the basic information provided in the first.  Complexity + Scale =&gt; Reduced Reliability + Increased Chance of catastrophic failures  Complexity  Complex system fail catastrophically  Failure domains  OpenStack example  Single controller, single cloud (or zone)  HA setup – two controllers in an HA mode of some kind  Single controller, multiple cloud (or multiple zones)  A single zone is unreliable  If both HA nodes fail, still unreliable, and HA is more complex  Two zones is two failure domains, which is more reliable than a single HA-enabled zone  (But of course you should make each zone as reliable as possible)  Reliability engineering (aka math)  Reliability engineering matters except when it doesn’t  “The higher the number of dependent components =&gt; the lower the overall availability and the bigger the impact of failure”  In a cloud with many nodes, adding the ability for live migration will actually decrease reliability, because all nodes are now tied together  Many reliability calculations come from mechanical engineering, which is much different than software engineering  Many complex systems fail by cascading, failure starts small and grows big, until it engulfs the entire system  General approach is to make failure local and contained  Partial failure is desirable  Business side  Software reliability is cheaper  Most web scale applications consist of a large stateless part and a small stateful piece  It does not make business sense to  provide a super-reliable cloud  A single compute node or even zone will never be reliable  Best not to consider virtual machines, such as those in EC2, as serversNOTE: There will eventually be a part three post, but as of this writing it’s not up yet.ConclusionTo me, it boils down to building reliable applications on unreliable clouds, which I think is what a lot of people are doing, and is what seems to come out every time AWS fails.The first issue that pops into my mind though is RDBMS systems, and how to replicate data between zones, which is often a network concern. Actually, replicating any data between zones could be a problem, which is why, I’m guessing, that he’s (perhaps) suggesting to keep stateful pieces small."
    } ,
  
    {
      "title"    : "First look at PackStack",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/03/13/first-look-packstack.html",
      "date"     : "2013-03-13 00:00:00 -0400",
      "content"  : "I am a big fan, and user, of OpenStack. I also really like the Ansible configuration management and orchestration system. In fact, I use Ansible to deploy OpenStack, and all kinds of other things as well.Recently on the Ansible mailing list the lead developer (and now CTO of Ansibleworks) suggested that it was time to bring together everyone who is working, or wants to work with, both Ansible and OpenStack.One of the suggestions was to follow what PackStack has done–it’s based on puppet–and port it over to Ansible. (NOTE: I’m not even sure that’s the right git repo, things are moving so fast!)While I had heard of PackStack before, I had never used it, so I decided it was time I took a look so that I can perhaps help with the OpenStack + Ansible project. Frankly, it was on my list of technology to check out because I always need to have some virtual machines running OpenStack, and it would be nice if there was an easy way to quickly build a multi-host OpenStack install (especially if I would like to contribute code back to the community at some point).Also–I’m sure Ansible will soon be one of Vagrants supported deployment systems, and when that happens it will be very easy to deploy OpenStack with Vagrant.So I spent a couple hours creating an Ansible playbook that would simply fire off the PackStack command with a generic answer file. So I haven’t ported anything from PackStack to Ansible–I’m simply using Ansible and Vagrant to create an environment for PackStack to do it’s work.RedHat/CentOSOne big note–PackStack currently only supports RedHat/CentOS. I’m using CentOS 6.Host organizationThat repository also contains a Vagrant file, and an Ansible hosts file. I have four virtual machines making up a small OpenStack cluster:$ cat ansible_hosts [openstack]apis ansible_ssh_host=192.168.100.130scheduler ansible_ssh_host=192.168.100.131compute01 ansible_ssh_host=192.168.100.132compute02 ansible_ssh_host=192.168.100.133And all of those IPs and names are reflected in the PackStack.yml, files/packstack.cfg, and Vagrantfile files. I don’t know if they are the most descriptive names, but this is how I’ve currently organized it.$ grep \"100\\.13\" Vagrantfile apis_config.vm.network :hostonly, \"192.168.100.130\" # nic3scheduler_config.vm.network :hostonly, \"192.168.100.131\" # nic3compute01_config.vm.network :hostonly, \"192.168.100.132\" # nic3compute02_config.vm.network :hostonly, \"192.168.100.133\" # nic3The apis host is going to run horizon (not that I need it) and the front-facing APIs, and the scheduler will run the mysql server and nova-scheduler…and some cinder related services as well (which likely need more investigation as I have really only have experience thus far with OpenStack Essex–but at least I can experiment with other versions of OpenStack now, thanks to PackStack).For example, here is what nova services are running where:[root@apis packstack(keystone_admin)]$ nova-manage service listBinary           Host       Zone  Status     State Updated_Atnova-scheduler   scheduler  nova  enabled    :-)   2013-03-13 21:15:11nova-cert        apis       nova  enabled    :-)   2013-03-13 21:15:15nova-consoleauth apis       nova  enabled    :-)   2013-03-13 21:15:15nova-compute     compute01  nova  enabled    :-)   2013-03-13 21:15:13nova-compute     compute02  nova  enabled    :-)   2013-03-13 21:15:12Smiley faces are good. :-)Deploying PackStackDeploying this specific example only requires a couple of commands (I’m running OSX, and using Virtualbox).First, tell Vagrant to build the servers.NOTE: You could have IP collisions if you have other virtual machines running–the IPs used here are hard-coded.$ vagrant up[apis] Importing base box 'centos6'...[apis] The guest additions on this VM do not match the install version ofVirtualBox! This may cause things such as forwarded ports, sharedfolders, and more to not work properly. If any of those things fail onthis machine, please update the guest additions and repackage thebox.Guest Additions Version: 4.1.6VirtualBox Version: 4.2.6[apis] Matching MAC address for NAT networking...[apis] Clearing any previously set forwarded ports...[apis] Fixed port collision for 22 =&gt; 2222. Now on port 2200.SNIP!Once those are built you should have four vms running:$ vagrant statusCurrent VM states:apis                     runningscheduler                runningcompute01                runningcompute02                runningThis environment represents multiple VMs. The VMs are all listedabove with their current state. For more information about a specificVM, run `vagrant status NAME`.Once that’s done, you can simply run the packstack.yml playbook.$ ansible-playbook -k -u root PackStack.yml SSH password: #enter the vagrant password of 'vagrant', just have to do this oncePLAY [openstack] ********************* GATHERING FACTS ********************* ok: [compute01]ok: [scheduler]ok: [compute02]ok: [apis]SNIP! #Tons of stuff happens here, mostly done by PackStackTASK: [run PackStack] ********************* skipping: [compute01]skipping: [compute02]skipping: [scheduler]changed: [apis]PLAY RECAP ********************* apis                           : ok=13   changed=11   unreachable=0    failed=0    compute01                      : ok=7    changed=5    unreachable=0    failed=0    compute02                      : ok=7    changed=5    unreachable=0    failed=0    scheduler                      : ok=7    changed=5    unreachable=0    failed=0  When it completes, there should be a multi-server OpenStack cluster running!Login to the apis server:$ ssh root@192.168.100.130root@192.168.100.130's password: #vagrant password againLast login: Wed Mar 13 16:34:03 2013 from 192.168.100.1            [root@apis ~]# source keystonerc_admin [root@apis ~(keystone_admin)]$ nova list# nothing will appear here, but at least you should get no error messagesBecause the Ansible playbook downloaded and installed the Cirros image, @nova image-list@ should give some output:[root@apis ~(keystone_admin)]$ nova image-list+--------------------------------------+--------+--------+--------+| ID                                   | Name   | Status | Server |+--------------------------------------+--------+--------+--------+| 84eebd30-953f-4ffe-b9f9-afb7099f1e75 | cirros | ACTIVE |        |+--------------------------------------+--------+--------+--------+At any rate, it’s a good start.Once Vagrant supports Ansible and VMWare Fusion, it will be crazy easy to create a working OpenStack cluster on my laptop.AcknowledgmentsNothing I ever do is novel, and this blog post is no different. I based this work off the following: Installing a 4 node Fedora 18 OpenStack Folsom cluster with PackStack.Issues/Caveats/Questions  I had at least one problem getting this going…PackStack doesn’t seem to want to install puppet on CentOS 6. I had to make sure the Ansible playbook setup the EPEL repository so that the puppet RPMs were available.  AFAIK, Virtualbox doesn’t support nested virtualization, so you won’t be able to boot any Inception styled OpenStack instances, ie. vms within vms. Though, again AFAIK, VMWare Fusion on OSX does supported nested virtualization, which is why I’m excited about Vagrant supporting Fusion. I’d prefer to use KVM, but I need to run OSX for work.  Don’t think Cinder is working.  It can take a good 20 to 30 minutes for this entire build process to complete, mostly because there are a ton of puppet modules to download.  I’m not even sure what version of OpenStack this installs…something to look into. Might have to jump to a more recent version of Fedora to get something like OpenStack Grizzly.[root@apis ~(keystone_admin)]$ yum info openstack-nova-commonLoaded plugins: fastestmirror, prioritiesLoading mirror speeds from cached hostfile * base: www.muug.mb.ca * epel: www.muug.mb.ca * extras: mirror.its.sfu.ca * updates: www.muug.mb.caInstalled PackagesName       : openstack-nova-commonArch       : noarchVersion    : 2012.2.2Release    : 1.el6Size       : 115 kRepo       : installedFrom repo  : epelSummary    : Components common to all OpenStack Nova servicesURL        : http://openstack.org/projects/compute/License    : ASL 2.0Description: OpenStack Compute (codename Nova) is open source software designed to           : provision and manage large networks of virtual machines, creating a           : redundant and scalable cloud computing platform. It gives you the           : software, control panels, and APIs required to orchestrate a cloud,           : including running instances, managing networks, and controlling access           : through users and projects. OpenStack Compute strives to be both           : hardware and hypervisor agnostic, currently supporting a variety of           : standard hardware configurations and seven major hypervisors.           :            : This package contains scripts, config and dependencies shared           : between all the OpenStack nova services."
    } ,
  
    {
      "title"    : "Software defined networking, Openvswitch, and Ubuntu 12.04",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/02/21/openvswitch.html",
      "date"     : "2013-02-21 00:00:00 -0500",
      "content"  : "Recently I’ve been testing over committing on KVM. Once I started running hundreds of virtual machines (vms) on a single node, I realized that in order to get them to do anything I have to access them over the network to run something like Ansible playbooks designed to test load.In order to provide networking resources to the vms, I decided to take a look at Openvswitch and what it takes to get it up and running with KVM and Ubuntu 12.04/Precise.Software defined networkingLike cloud and big-data, software defined networking (SDN) is a loaded term. But, like those terms, I feel I need to at least try to get a grasp of what it means.  “A good working definition of SDN is the separation of the data and control functions of today’s routers and other layer two networking infrastructure with a well-defined programming interface between the two.” – Via ArstechnicaSDN is a big part of OpenStack as well. Starting with the Folsom release, networking was split out into it’s own *as-a-Service capability called Quantum, whereas previously it was a sub-component of Nova. So given I’m a big fan, and user, of OpenStack, it’s important for me to get a good grasp of SDN.Openflow is also an important technology in SDN that requires some research time.But, having said all that, basically I’m just going to install and use Openvswitch on a single compute node. :)Building on other’s workI followed these blog posts on configuring Openvswitch on Ubuntu 12.04/Precise:  How to build a SDN Lab without needing Openflow hardware  Openvswitch and OpenFlow: Let’s get started  Installing KVM and Openvswtich on UbuntuI’m not really doing anything new here–though I hope to at some point… :)InstallationI put together an ansible playbook to install Openvswitch in Ubuntu 12.04. There is no easy, direct way (that I’m aware of) to install Openvswitch in Precise…unfortunately I just can’t do @apt-get install openvswitch@ and have everything work like magic. I guess building the module is the only unusual thing, and this will disappear in future versions of Ubuntu–perhaps it’s already not necessary in 12.10, not sure, haven’t looked it up.I’m not going to directly cut and paste my ansible playbook into this post, but suffice it to say that most dependencies can be installed via apt-get, but there is one step required to build and module.# Install these packages:#   - openvswitch-datapath-source #   - bridge-utils#   - module-assistant  #   - openvswitch-brcompat#   - openvswitch-common#   - openvswitch-switch#   - linux-headers-3.2.0-23-generic#   - linux-headers-generic-pae# Then build the module:$ module-assistant auto-install openvswitch-datapathNext we set BRCOMPAT=yes in /etc/default/openvswitch-switch and restart openvswitch-switch.PoxPox, among other things, is an Openflow controller.  “At its core, it’s a platform for the rapid development and prototyping of network control software using Python.  Meaning, at a very basic level, it’s one of a growing number of frameworks…for helping you write an OpenFlow controller.” – Via Pox websiteHey, it’s Python, it’s Openflow…what else do I need. Sign me up. :)$ cd /usr/local/src$ git clone http://github.com/noxrepo/pox$ zdaemon -p 'python /usr/local/src/pox/pox.py \\--no-cli forwarding.l2_learning' -d startAnd now Pox should be listening on port 6633:$ netstat -ant | grep 6633tcp        0      0 0.0.0.0:6633            0.0.0.0:*               LISTEN     $ sudo lsof -i | grep 6633python   34150   root    3u  IPv4 39211055      0t0  TCP *:6633 (LISTEN)More information about pox can be found on the Openflow site.Configure the bridgeNow we can add a bridge to the openv switch.$ ovs-vsctl add-br br-intAnd then configure br-int (or whatever you’ve called the bridge), and I’m using the eth2 interface in this example.$ ovs-vsctl add-port br-int eth2; ifconfig eth2 0; ifconfig br-int  \\netmask 255.255.255.0&lt;/code&gt;&lt;/pre&gt;Let's tell Openvswitch to use the Pox controller that is running.$ ovs-vsctl set-controller br-int tcp::6633&lt;/code&gt;&lt;/pre&gt;Finally we need interface up and down scripts.ifdown:$ cat /sbin/ovs-ifdown#!/bin/sh switch='br-int' /sbin/ifconfig $1 0.0.0.0 down ovs-vsctl del-port ${switch} $1ifup:$ cat /sbin/ovs-ifup#!/bin/sh switch='br-int' /sbin/ifconfig $1 0.0.0.0 up ovs-vsctl add-port ${switch} $1 Now we can boot some vms!## Booting a vmI'm booting vms via a script, and part of the `kvm` command line options is the network tap, which uses the ifup/ifdown scripts:-net tap,script=/sbin/ovs-ifup,downscript=/sbin/ovs-ifdownAnd, here is a running instance:$ ps ax | grep \"kvm -drive\" | head -1  9831 ?        Sl    21:01 kvm -drive if=virtio,file=/mnt/intel/1.img -m 2048 \\  -boot a -net nic,macaddr=52:54:00:a1:c0:fd \\  -net tap,script=/sbin/ovs-ifup,downscript=/sbin/ovs-ifdown -nographic -vnc :1 \\   -chardev file,id=charserial0,path=/mnt/intel/1.console.log \\   -device isa-serial,chardev=charserial0,id=serial0 -chardev pty,id=charserial1 \\   -device isa-serial,chardev=charserial1,id=serial1 \\   -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5Right now there's about 300 vms running on this single compute node.$ ps ax  |grep \"kvm -drive\" | wc -l301Fun stuff. :)## What now?Well, I've achieved my goal of getting Openvswitch up and running to enable networking between vms on a single compute node. ubuntu@ubuntu:~$ ifconfig eth0 | grep 192          inet addr:192.168.100.111  Bcast:192.168.100.255  Mask:255.255.255.0ubuntu@ubuntu:~$ ping -c 1 192.168.100.23PING 192.168.100.23 (192.168.100.23) 56(84) bytes of data.64 bytes from 192.168.100.23: icmp_req=1 ttl=64 time=0.452 ms--- 192.168.100.23 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.452/0.452/0.452/0.000 msI think my next step will be to work in multiple virtual machines to see if I can do some of the interesting and useful things that Openflow is capable of, and to find out how I can work with the Pox system to learn more about SDN.Another important thing to do is to get a test environment of OpenStack Folsom (or Grizzly) up and running to see how Quantum utilizes Openflow and SDN.I'm hoping to spend the next few days learning about [Pox](http://www.noxrepo.org/pox/about-pox/). Wish me luck. :)"
    } ,
  
    {
      "title"    : "Over committing with KVM",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/02/20/overcommitting-with-kvm.html",
      "date"     : "2013-02-20 00:00:00 -0500",
      "content"  : "It’s quite possible to over commit resources with the KVM hypervisor.I should say first that most of the work I’ve been doing around over committing in KVM is based on a project I am working in where the virtual machines are stateless. This makes over committing easier because if we run out of resources on a compute node and this causes a vm to crash, it’s not the end of the world–the end-user can just restart their session.To me over committing means having virtual machines running on a node where the total CPU, main memory (ie. ram), and disk that is available to the virtual machines is much larger than the actual physical resources available on the node. Hopefully five or ten times larger.HardwareI am running these tests on a single Dell C6220 node. It has 32 cores (including hyperthreading), 128GB of main memory, and SSD drives.test_host:~$ cat /proc/cpuinfo | grep processor | wc -l32The SSD drive types we are testing with are:  2x Standard Dell 300GB SSD  2x Samsung 830 512GB  2x Intel 520 480GBEach of them is in a stripe/RAID0 configuration, so we have three striped devices, md2 (Samsung) and the poorly named md126 (Intel) and md127 (Dell).test_host:~$ cat /proc/mdstat Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4][raid10] md0 : active raid1 sdb1[1] sda1[0]      307136 blocks [2/2] [UU]      md2 : active raid0 sdb5[1] sda5[0]      882155520 blocks super 1.2 512k chunks      md1 : active raid1 sdb2[1] sda2[0]      41910144 blocks super 1.2 [2/2] [UU]      md126 : active raid0 sdd[1] sdc[0]      937702400 blocks super 1.2 512k chunks      md127 : active raid0 sdf[1] sde[0]      586072064 blocks super 1.2 512k chunks      unused devices: &lt;/code&gt;&lt;/pre&gt;The striped Samsungs are really fast--84K IOPS from the stripe!## Memory over committing: KSM (not KVM)KSM, or Kernel Samepage Merging, is a memory de-duplication feature that is present in most Linux systems that support KVM. I wrote a bit about it in a [previous blog post](http://serverascode.com/2012/11/11/ksm-kvm.html). Basically if you up the scanning rate it will de-duplicate more memory. This means if I'm running 300 Ubuntu Precise images there is a lot of memory that can be de-duplicated.## CPU over committingI don't have a lot of information on how over committing CPU works in KVM, but there are some best practices documented by a couple organizations, which I have listed below. This is an area I need to do more research in.- [IBM says:](http://publib.boulder.ibm.com/infocenter/lnxinfo/v3r0m0/index.jsp?topic=%2Fliaat%2Fliaatbpprocmem.htm)** Target system use at max 80%--ie. each vm shouldn't be using 100% of their CPU, and in fact should max out at 80%** Allocate minimum VCPUs per vm--ie. if the vm doesn't need four CPUs, rather say, only one, then just give it one- [RedHat says](https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Administration_Guide/sect-Virtualization-Tips_and_tricks-Overcommitting_with_KVM.html:)** _Virtualized CPUs are over committed best when each guest only has a single VCPU. The Linux scheduler is very efficient with this type of load. KVM should safely support guests with loads under 100% at a ratio of five VCPUs. Over committing single VCPU guests is not an issue._Notice that RedHat is saying a 5:1 ratio for CPU over committing.## Disk space over committingThis is straight forward: image snapshots. qcow2 images can be created as snapshots of a backing file.test_host:/mnt/intel$ file 263.img 263.img: QEMU QCOW Image (v2), has backing file (path /mnt/intel/precise-server-cloudimg-amd64-disk1.qcow2), 2147483648 bytesThis is also how OpenStack handles images. It puts one base image on the compute node and each instance based on that same image is backed by a qcow2 snapshot. I suppose we could do the same with LVM, if desired, using snapshots.## Can't over commit IOPSThe part that is difficult in terms of running hundreds of vms on a single node is the amount of IOPS each vm takes up. In my experiments the Ubuntu cloud image doesn't really do anything at all in terms of IOPS--maybe one or two IOPS per image when they are idle. So if I'm running 300 images, they don't even use 300 IOPS when idle. Even during boot they hardly do anything. (Of course when they are working they could be using a lot of IOPS.)But, if I boot 100 Windows 7 images five minutes apart, I need 5000 IOPS. Yup: *5000 IOPS*. Six SATA drives in RAID10 are going to barely provide 400 IOPS let alone 5000. So, SSDs, or at least some kind of faster storage, are required. Pretty much any SSD is capable of 5000 IOPS. IMHO, it's a lot more efficient to run the Ubuntu cloud image than a standard Windows image.## LoadRight now I have 300 Ubuntu images running, each being given 2048MB of memory. test_host:~$ ps ax | grep \"kvm -drive\" | wc -l300And load and memory usage are just fine. A bit of swapping, but that's Ok.top - 10:58:13 up 8 days, 23:55,  2 users,  load average: 1.35, 1.29, 1.92Tasks: 845 total,   2 running, 843 sleeping,   0 stopped,   0 zombieCpu(s):  0.8%us,  3.1%sy,  0.0%ni, 95.9%id,  0.1%wa,  0.0%hi,  0.0%si,  0.0%stMem:  131997760k total, 122223236k used,  9774524k free,   142448k buffersSwap: 33554424k total,   242172k used, 33312252k free, 42323472k cachedSNIP!They have been running for a few days, and are not using much in terms of IOPS as they are completely idle. Below is the output from [iostat](http://linuxcommand.org/man_pages/iostat1.html). All the vms are running off qcow2 images on the Intel 520 based stripe, md126.avg-cpu:  %user   %nice %system %iowait  %steal   %idle           0.67    0.00    3.08    0.05    0.00   96.20Device:            tps    MB_read/s    MB_wrtn/s    MB_read    MB_wrtnsda              16.40         0.00         0.17          0          1sdb              16.40         0.00         0.17          0          1sdc              44.70         0.00         0.16          0          1sde               0.00         0.00         0.00          0          0sdd              44.20         0.00         0.15          0          1sdf               0.00         0.00         0.00          0          0md127             0.00         0.00         0.00          0          0md126            60.10         0.00         0.31          0          3md1              35.20         0.00         0.17          0          1md2               0.00         0.00         0.00          0          0md0               0.00         0.00         0.00          0          0dm-0              0.00         0.00         0.00          0          0nb0               0.00         0.00         0.00          0          0It would be great to have IOPS usage on a per-process basis, but I couldn't find a tool that would do that. The best I could do is iostat with IOPS per storage device.## Running testsEach vm gets its IPs from dnsmasq running on an [Openvswitch](http://openvswitch.org/) bridge (topic of a future blog post). So we have 300 tap devices.test_host:~/performance_testing$ sudo ovs-vsctl showSNIP!        Port \"tap166\"            Interface \"tap166\"        Port \"tap0\"            Interface \"tap0\"    ovs_version: \"1.4.0+build0\"test_host:~/performance_testing$ ifconfig | grep tap | wc -l300I like using the [Ansible](http://ansible.cc) configuration management system. I've written a custom inventory script that pulls the IPs out of the dnsmasq lease file.test_host:~/performance_testing$ ansible all -c ssh -i ./inventory.py \\-m ping -u ubuntuSNIP 298 hosts!192.168.100.98 | success &gt;&gt; {    \"changed\": false,     \"ping\": \"pong\"}192.168.100.99 | success &gt;&gt; {    \"changed\": false,     \"ping\": \"pong\"}Then, using that inventory and Ansible I can run whatever load tests I want across however many virtual instances I want. For example I could run [fio](http://linux.die.net/man/1/fio) tests across 10% of the vms and watch the IOPS usage on the compute node.If you have any questions, criticisms or concerns, please let me know by posting in the comments. :)"
    } ,
  
    {
      "title"    : "Canadian OpenStack Users Group - CanStack!",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/02/20/canstack.html",
      "date"     : "2013-02-20 00:00:00 -0500",
      "content"  : "Recently I have started helping out the Canadian OpenStack Users Group. With the support of my employer, I’ve put up a website called CanStack and a blog for that site as well. And, of course, the obligatory twitter account. There is even a github repository for the blog posts, so contributing a post is only a git commit away! :)The fun thing about the website and blog is that the website is hosted in a beta Canadian OpenStack-based cloud, and the blog is hosted in Amazon S3. I am a big proponent of object storage systems such as S3 and OpenStack Swift, so it was a good experience to get a static blog up on S3.I use jekyll to generate the blog, just like I do this site, though serverascode.com is hosted by github. Thanks github!My hope with the CanStack website and blog is that they will help to find more members to bring together in our monthly meetings, and that hopefully we can provide some useful information about OpenStack as well.I just put up a blog post on how we are using OpenStack in a virtual classroom project, complete with what server and network hardware we are using, and even what data center!So, if you are interested in what Canadians are doing with OpenStack, canstack.ca might be a good place to start.As always, comments, suggestions, and criticisms are welcome. :)"
    } ,
  
    {
      "title"    : "tcpflow",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/02/08/tcpflow.html",
      "date"     : "2013-02-08 00:00:00 -0500",
      "content"  : "This is just a quick little post on tcpflow.I’ve been using graphite as a a graphing solution for system statistics. I’ve also been using carbon-relay-ng to relay packets from different OpenStack projects to a central graphite server.However, I’ve been having an issue (and am still having an issue actually) with concatenated packets arriving at the graphite server, despite the fact that the clients aren’t sending them.In order to troubleshoot this I eventually started running tcpflow because it gave me cleaner output than tcpdump did…at least by default anyways. I’m sure that tcpdump can give me pretty much any output I need, but I liked the symplicity of tcpflow, for example:[root@stats ~]# tcpflow -c port 2003tcpflow[11791]: listening on eth0010.000.000.001.39216-010.000.000.006.02003: debbuild.cputotals.user 0 1360364000010.000.000.001.39216-010.000.000.006.02003: debbuild.cputotals.nice 0 1360364000010.000.000.001.39216-010.000.000.006.02003: debbuild.cputotals.sys 0 1360364000010.000.000.001.39216-010.000.000.006.02003: debbuild.cputotals.wait 0 1360364000010.000.000.001.39216-010.000.000.006.02003: debbuild.cputotals.idle 99 1360364000010.000.000.001.39216-010.000.000.006.02003: debbuild.cputotals.irq 0 1360364000010.000.000.001.39216-010.000.000.006.02003: debbuild.cputotals.soft 0 1360364000010.000.000.001.39216-010.000.000.006.02003: debbuild.cputotals.steal 0 1360364000SNIP!vs.[root@stats ~]# tcpdump -nnvvXSs 1514 -i eth0 port 2003tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 1514 bytes22:59:32.650673 IP (tos 0x0, ttl 64, id 41032, offset 0, flags [DF], proto TCP (6), length 103)    10.0.0.1.39216 &gt; 10.0.0.6.2003: Flags [P.], cksum 0xab18 (correct),     seq 1882211360:1882211411, ack 2847074948, win 115, options     [nop,nop,TS val 804219404 ecr 2428141312], length 51\t0x0000:  4500 0067 a048 4000 4006 8642 0a00 0001  E..g.H`.`..B....\t0x0010:  0a00 0006 9930 07d3 7030 4420 a9b2 ea84  .....0..p0D.....\t0x0020:  8018 0073 ab18 0000 0101 080a 2fef 6a0c  ...s......../.j.\t0x0030:  90ba 7f00 636f 6c6c 6563 7464 3031 2d63  ....collectd01-c\t0x0040:  6c69 656e 742d 7465 7374 2e74 6573 742e  lient-test.test.\t0x0050:  6669 7273 7420 2033 3530 3420 3133 3630  first..3504.1360\t0x0060:  3336 3433 3733 0a                        364373.22:59:32.650750 IP (tos 0x0, ttl 64, id 53955, offset 0, flags [DF], proto TCP (6), length 52)    10.0.0.6.2003 &gt; 10.0.0.1.39216: Flags [.], cksum 0xc56c (correct),     seq 2847074948, ack 1882211411, win 501, options [nop,nop,TS val     2428143328 ecr 804219404], length 0\t0x0000:  4500 0034 d2c3 4000 4006 53fa 0a00 0006  E..4..`.`.S.....\t0x0010:  0a00 0001 07d3 9930 a9b2 ea84 7030 4453  .......0....p0DS\t0x0020:  8010 01f5 c56c 0000 0101 080a 90ba 86e0  .....l..........\t0x0030:  2fef 6a0c                                /.j.SNIP!One thing to note is the -c switch on tcpflow, it means print to the console instead of files in the local directory.I would love to hear about other uses of tcpflow or ways to alter the output of tcpdump, so please let me know of anything interesting by commenting. :)"
    } ,
  
    {
      "title"    : "Converting VMWare Windows images to OpenStack with virt-v2v",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/11/26/converting-vmware-windows-to-openstack.html",
      "date"     : "2012-11-26 00:00:00 -0500",
      "content"  : "As I’ve mentioned in previous posts, I’m currently working on a project that uses Apache VCL to provide university students with the ability to remotely login to a virtual machine and use specialized software to complete their classwork–a virtual computer lab if you will.Because we are moving off our current backend to OpenStack, we need to convert Windows images from VMWare to something that will work in OpenStack/KVM.Note about Windows…Let me note that I am not a “windows guy”–I haven’t really used windows in the last 10 years. Because I have usually been employed as a Unix/Linux admin, I’ve always either ran OpenBSD, Linux, or OSX on my desktop, and literally never had to work with Windows. Until now of course. ;)Moving the backend to OpenStackCurrently we are in the process of moving the backend of our VCL system from a VMWare ESXi based cluster to OpenStack (Essex to be precise.) There are a lot of reasons for this move, and I won’t get into them here.Unfortunately you can’t just take an ESXi image and drop it into OpenStack and have it work. Certainly we can actually import the image into glance, as vmdk images are supported, but the OS on the image will not have the virtio drivers for disk and network installed, drivers that OpenStack uses by default.Neither is it as easy as just installing the drivers into the OS image while it’s running in ESXi. While I believe it’s possible to do this in Windows Server 2008, I don’t believe you can install drivers into Windows 7 without having the actual “hardware” accessible to the image (please correct me if I’m wrong–I’d love to hear that I am), not without some registry and other hacks outside of my purview.This means we needed to find a way to convert the images from ones that work in VMWare ESXi, to ones that will work in OpenStack + KVM, preferably a way that doesn’t require me to learn anything about the Windows registry. ;)virt-v2v to the rescueFortunately RedHat provides a system called virt-v2v. This allows the cross-conversion of images for several different types of hypervisors.The main things I found out about RedHat and virt-v2v are:Some of the packages are not available in CentOS, or at least I couldn’t find them, so you need a RedHat license. If you are converting images from VMWare ESXi to OpenStack, then you probably have enough money in the project to buy a RedHat license. ;)It doesn’t work in a virtual machine–it expects hardware virtualization, so you need a hardware server. You don’t need much, just enough to support hardware virtualization with KVM.Currently, and this might change, you have to download the image from the ESXi server each time you try a conversion, so if it takes a long time to download, then the conversion will also take a long time to finish.I’m sure most of the above could be changed with a bit of Perl hacking, but as far as I can tell, without changing any of the RedHat code you do need to meet those requirements.Note–RedHat has pretty good documentation on the subject, a few quick google searches will tell you as much as I know. :)Installing virt-v2vFirst, as I’ve mentioned, you need a hardware server with RedHat Enterprise 6.x on it.[root@localhost ~]# cat /etc/redhat-release Red Hat Enterprise Linux Server release 6.3 (Santiago)It needs to be registered with the RedHat Network and have the “RHEL Server Supplementary” and “RHEL V2VWIN” channels enabled, as per the below image.[root@localhost ~]# yum repolistLoaded plugins: product-id, rhnplugin, subscription-managerUpdating certificate-based repositories.Unable to read consumer identityrepo id       repo name     statusrhel-x86_64-server-6 Red Hat Enterprise Linux Server (v. 6 for 64-bit x86_64) 8,712rhel-x86_64-server-supplementary-6 RHEL Server Supplementary (v. 6 64-bit x86_64) 311rhel-x86_64-server-v2vwin-6 RHEL V2VWIN (v. 6 for 64-bit x86_64) 2repolist: 9,025Once those are configured, install the following packages.[root@localhost ~]# yum install virt-v2v virtio-win libguestfs-winsupport libvirtAnd then reboot.Once the server has rebooted, start libvirtd if it isn’t already.[root@localhost ~]# service libvirtd startNext we need to add a storage pool to libvirt. In this example, just because of the way RHEL defaulted my temporary install, we have a huge /home/images directory in which to put the converted images.[root@localhost ~]# df -h /homeFilesystem            Size  Used Avail Use% Mounted on/dev/mapper/VolGroup-lv_home                      5.4T  235G  4.9T   5% /home[root@localhost ~]# cat pool.xml           virtimages                  /home/images              Using that pool.xml file, we’ll configure the pool.[root@localhost ~]# virsh pool-create --file pool.xml[root@localhost ~]# virsh pool-listName                 State      Autostart -----------------------------------------virtimages           active     no        Finally, we configure a .netrc file, obviously entering the proper ESXi host, login, and password.[root@localhost ~]# cat .netrcmachine  login  password &lt;/code&gt;&lt;/pre&gt;Now we can run virt-v2v![root@localhost images]# virt-v2v -ic esx:///?no_verify=1 -o libvirt -os virtimages : 100% [========================]D 0h51m16svirt-v2v: WARNING: No mapping found for bridge interface public in config file. The converted guest may not start until its network interface is updated.virt-v2v:  configured with virtio drivers.&lt;/code&gt;&lt;/pre&gt;I don't worry about the \"No mapping\" message.## Bring that image into OpenStackNow that we have an image in the libvirt pool we configured, it's time to:# Convert the raw/vmdk image to qcow2 using \"qemu-img\".# Import the converted qcow2 image into glance.# Boot the image once and manually login to finish off virt-v2v's \"firstboot\" scripts. This means you need an admin login to the image.# \"Set\" the hardware, make any changes to the instance such as updating and the like.# Shut down the instance.# Create a new OpenStack image from that instance using _nova image-create_.# Then delete the original image, and instance booted from it, if you want. It's not much use except for archival purposes.# Boot a new instance from the new image, the one created with _nova image-create_, just to make sure it all works out Ok.At this point you should, hopefully, have a working Windows image, one that was converted from VMWare ESXi!"
    } ,
  
    {
      "title"    : "KSM and KVM",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/11/11/ksm-kvm.html",
      "date"     : "2012-11-11 00:00:00 -0500",
      "content"  : "UPDATE: I did some more research and have a better idea of what pages_saved and pages_saving means. So we are saving quite a bit of memory!I recently found out about the ksm technology that is in Ubuntu 12.04 + kvm by default. ksm is a memory de-duplication process. As far as I understand it, ksm can allow virtual machines (actually any application, not just virtualization) to share memory pages–it finds all duplicated memory pages and merges them, thereby saving memory in some situations.One of the projects I am working on is a classroom as a service, or virtual classrooms. Students can login to a web gui and request a reservation to a virtual machine image which they can then access with a RDP client.In this project all of the images are based on–unfortunately–Windows 7. One would think that if we are running many similar Windows 7 images ksm could do a lot of de-duplication.I have been doing a few experiments in my spare time to see if ksm can help to over-commit memory. If I can I’d rather be able to run 400 virtual machines than 200. If we can over-commit on memory 1:2 or 1:4 there could be substantial cost savings for the project.The TestI have a basic Windows 7 image in qcow2 format.root@ksm_test:/mnt/ksm-test$ file win7-base.qcow2win7-base.qcow2: QEMU QCOW Image (v2), 21474836480 bytesI am going to run 30 Windows 7 images with four gigs of ram and two virtual cpus each, based off a qcow2 snapshot from the original backing image.The server I am running this test on is a Dell c6220 with 32 HT cores and 128 gigs of main memory.ASIDE: /mnt/ksm-test is an xfs file system. I found that this test on a ext4 based filesystem used considerably more IOPs than xfs because the jdb2 process was doing a lot of journaling. There are likely some settings I should be using with ext4 to get better performance, but instead I just hopped over to xfs and haven’t gone back to ext4 yet.This is the little script I use to boot the vms:root@ksm_test:~$ cat test_ksm.sh #!/bin/bash# How much memory to boot withMEM=4048BACKING_DIR=/mnt/ksm-testBACKING_FILE=win7-base.qcow2SLEEP=60pushd $BACKING_DIRfor i in {1..30}; do\techo \"====&gt; Starting a new instance...\"\t# Remove the old backing file\trm -f win7-$i.qcow2\t# Create a new backing file that is a qcow2 snapshot of the original file\tqemu-img create -f qcow2 -b $BACKING_FILE win7-$i.qcow2\t# Actually start the intstance\t/usr/bin/kvm \\\t-M pc-1.0 \\\t-smp 2,sockets=2,cores=1,threads=1 \\\t-enable-kvm \\\t-m $MEM \\\t-drive file=win7-$i.qcow2,if=virtio \\\t-boot d \\\t-net nic,model=virtio \\\t-net user \\\t-nographic \\\t-vnc :$i \\\t-device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 \\\t-daemonize\t# Let's just sleep for a few seconds...\techo \"====&gt; Sleeping for $SLEEP...\"\tsleep $SLEEP donepopdexit 0After that script runs we have 30 kvm Win7 instances running:root@ksm_test:/mnt/ksm-test$ ps ax  |grep \"bin\\/kvm\" | wc -l30For the first while things are a little crazy on the host because 30 Windows 7 vms just booted in 30 minutes. After a few hours, or rather overnight, the vms settle down quite a bit to just doing a few IOPs each.As far as what these vms are doing–I login to a couple every once and a while just to make sure they are up, but otherwise they are doing nothing but whatever they do by default.The Defaultsksm is enabled in Ubuntu by default when using kvm. However, the defaults are fairly conservative:root@ksm_host:~$ cat /sys/kernel/mm/ksm/pages_to_scan 100root@ksm_host:~$ cat /sys/kernel/mm/ksm/sleep_millisecs 200ksm will scan 100 pages, sleep for 200 milliseconds and then scan 100 more, and so on. But with millions of pages it will take a long, long time to scan all of them.I set the pages_to_scan to 20000 and sleep_millisecs to 20–I’m guessing these are pretty aggressive settings.root@ksm_host:~$  echo \"20000\" &gt; /sys/kernel/mm/ksm/pages_to_scanroot@ksm_host:~$  echo \"20\" &gt; /sys/kernel/mm/ksm/sleep_millisecsThe ResultsI suppose saying “results” sounds scientific. :)The reality is that I’m really just cutting and pasting the ksm information that has been recorded after several days of running 30 Windows 7 virtual machines that should all be very close in terms of memory use.From the ksm.txt file:A high ratio of pages_sharing to pages_shared indicates good sharing,but a high ratio of pages_unshared to pages_sharing indicates wasted effort. pages_volatile embraces several different kinds of activity, but a high proportion there would also indicate poor use of madvise MADV_MERGEABLE.And the results of ksm after a few days of running 30 vms…&lt;/pre&gt;root@ksm_test:~$ for i in `ls -1 /sys/kernel/mm/ksm`; \\do echo \"===&gt; $i\"; cat /sys/kernel/mm/ksm/$i;  done===&gt; full_scans5417===&gt; pages_shared443355===&gt; pages_sharing26704343===&gt; pages_to_scan20000===&gt; pages_unshared3164064===&gt; pages_volatile183552===&gt; run1===&gt; sleep_millisecs20So, if I understand these numbers correctly, pages_shared is the amound of memory ksk is actually using. Thus in this example, ksm is using 1.7GB of memory:root@ksm_test:~$ getconf PAGESIZE4096So if we have 443355 pages shared, ksm is using this many bytes:root@ksm_test:~$ echo \"443355 * 4096\" | bc1815982080which is about 1.7GB.However, saved memory, ie. pages_sharing, is quite high! So this is good. :)root@ksm_test:~$ echo $((26704343*`getconf PAGE_SIZE`/1024/1024/1024)) GB101 GBI’m not sure how this equates to all the memory being used on the machine, but as far as ksm is concerned it’s saving us about 100 gigs. Nice.root@ksm_test:~$ free             total       used       free     shared    buffers     cachedMem:     131997772  131556896     440876          0     108448  108129628-/+ buffers/cache:   23318820  108678952Swap:     41943032     302836   41640196Update: Now that I have a better understanding of what ksm is doing and what it’s numbers mean, using a modified version of this script we can see some interesting results, though in the below example I am running 60 2 gig 1 vcpu instances instead of 30 4 gig 2 vcpu instances like the rest of this post:root@ksm_test:~$ ./ksm_stat.sh Shared memory is 2071 MBSaved memory is 95514 MBUnshared memory is 21336 MBVolatile memory is 2549 MBShared pages usage ratio is 46.11Unshared pages usage ratio is .22So thanks for reading, and if you have any suggestions as to what I might be doing incorrectly, be it settings or my math or my general assumptions about ksm :), please let me know in the comments."
    } ,
  
    {
      "title"    : "OpenStack 2012 Summit Day &amp;#35;4",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/10/18/openstack-summit-day-4.html",
      "date"     : "2012-10-18 00:00:00 -0400",
      "content"  : "Security Track!Having been a security administrator at a medium sized University; having taking fairly extensive education in Information Security; having attended security conferences like CanSec West and H.O.P.E I am happy that all of Thursday has a dedicated security track. Unfortunately I can only attend a couple of sessions, but at least I can do that much. :)Creating an OpenStack Security GroupBryan D. Payne from Neubula and Robert Clark from HP talk about the need for a OpenStack Security Group. The OSSG is “hiring”, ie. need volunteers especially security engineers, technical writers, and security experts that operate OpenStack clouds.Computer Security  Design for security from the start  Understand your threats  Understand your goals  Pervasive security culture (not just “that paranoid guy has it under control”)Security Challenges for OpenStack  Security as an afterthought  Security as silos  Security by non-expertsThere was some mention of the Cloud Security Alliance which Bryan is a co-lead on a workgroup for and that the CSA is a more oriented to high-level theory whereas the OSSG will need to be directed at applied security.Delivering Secure OpenStack IaaS for SaaS Products“Everyone has a great plan until they get punched in the mouth”–Mike TysonAndrew Hay, is the Chief Evangelist at CloudPassage, Inc. where he serves as the public face of the company and its cloud server security product portfolio.Three places to “put security” in OpenStack  Quantum** Coolest thing about Quantum is the ability to inject 3rd party products  Keystone  Nova  Security groups/firewalling with iptables  VLANS  Initial configuration of novaIssues in Cloud Security  Host security  Security of images** Network-based security is only so good in multitenant cloud  The ultimate target is the endpoint–so secure it  Cloud servers are more exposed  De-provisioning of servers releases public IPs that might have the wrong firewall rulesHe then went over some basic host security concepts, quite a few of which I disagree with, or at least he wasn’t able to really go into depth with what he actually meant by them.Generally speaking I think host security has been reduced mostly because in OpenStack most people use images that have already been provided to them and don’t run any specific configuration management (ie. chef/puppet/ansible) “hardening” processes.Nor do people usually build their own OpenStack images, which I think is an important skill to have. Though if more OS vendors provided base OpenStack images things would be a little easier. As far as I know, at this time, only Ubuntu publishes OpenStack compatible cloud images.SDSCUnfortunately, or fortunately, depending on your perspective, that was all the OpenStack security talks I was able to attend on Thursday because I had to go to a meeting at the San Digeo Super Computing Center, or the SDSC, and wasn’t able to come back to the summit.I was excited to go to find out more about what they are doing, especially because they have one of the largest OpenStack Swift installations in the world. I think about five petabytes worth.They were also very excieted about their new supercomputer Gordon, which has a ton of SSD storage (ie. flash–get it…gordon + flash).Also they had some old tape systems they were retiring. Tape is not dead yet though!(old tape silo being retired; sad to see it go)"
    } ,
  
    {
      "title"    : "OpenStack 2012 Summit Day &amp;#35;3",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/10/17/openstack-summit-day-3.html",
      "date"     : "2012-10-17 00:00:00 -0400",
      "content"  : "KeynotesHP says something enterprise business-like. Not too sure. 100s of billions of “spend” available. Capex -&gt; Opex. Many, many slides with business pictographs.Troy Toman from Rackspace talks about how proud they are of OpenStack. Brags about how Rackspace’s contribution percentage has actually reduced from 54% in Essex and 30% in Folsom and how that is a good thing, which it is. :)Notes from Rackspace’s production usage of OpenStack:  They have Quantum and Melange in production  Glance is backed by Swift  They have deployed the Cells code; at least three cells in each regon  Three regions worldwide  All of the “control” pieces, eg. API servers, are running on a private OpenStack cloud; an internal infrastructure called inova  Continuous delivery model for building and deploying to cloud–they pull from trunk at least once a day, and do this in under and hour, and have been deploying this once a week or so.  Fail fast fix fast  Coming soon: block storage and network products  Releasing PHP and Java cloud SDKsCisco Webex talks about what they do. Lots of open source technologies…puppet, salt, cobbler, etc to run a private cloud so they have “one throat to choke”.Service Resiliency Doesn’t Always mean “HA” or “Cluster”The guys from CloudScaling talk about redundancy patterns. This was actually pretty fascinating, and looks like they have taken care of a lot of scale-out redundancy, specifically not “HA-mmer” pairs, all the way up to the DB level, at which part they fall back to MMR MySQL.“HA” pairs are not the only type of redundancy  Airplanes require seven catastrophic failures to fall out of the sky, whereas with HA pairs you just need one layer to fail  Create small failure domains that don’t propagate = scale up    Alternative redundancy patterns    Redundancy patterns in Open Source Cloud Software  CloudScaling have come up with a methodology for “HA” they call Service Distribution.Service Distribution  Resilient  Stateless  Scale-out  Implementation  OSPF (quagga), Anycast (zebra), Load-balancing proxy (pound)  Perfect for site resiliency  Traditional HA pairs don’t support this  Use ZeroMQ (peer-to-peer) vs RabittMQ and CloudScaling contributed the code back to OpenStack for ZeroMQ  Making data centers look like ISP backbonesCirrOS, cloud-init: the future of cloud guestsI think the dev sessions are really about starting conversations that will be finished elsewhere, and if you aren’t fairly deep into the topic there’s not a lot of ways to get your head into the session. I’ve felt the pain of the lack of Cloud-init in Redhat. I’m surprised that there is just one person creating Cirros.Topics Covered  Cirros  Cloud-init  Config drive 2  Execute code from metadata in various waysAdding OpenVZ support to NovaRackspace has code that will eventually make it’s way into trunk for using OpenVZ in OpenStack. The session leader, Devananda van der Veen,  has created a way to build an OpenVZ kernel on Ubuntu. Rackspace uses OpenVZ to run their cloud database system because they get better performance than using something like KVM for virtualization.  devstack-gate  Etherpad for this sessionMost of the discussion was about how to get the OpenVZ driver into the OpenStack continuous integration system.Cloud Hosted Desktops and OpenStackKen Ringdahl, Vice President Engineering, Desktone discusses Desktop as a Service. I’m interested in this topic not because I’m interested in the topic, but because the project I’m currently working on is essentially desktop in the cloud via Apache VCL and I’m constantly thinking about how to get off VCL and away from it’s 40k lines of perl, and just use OpenStack with a thin layer on top of it. The reality is that OpenStack has 95% of the code required to implement a service similar to VCL.So Ken goes through his slides. Mentions VDI and that VDI is hard! Capex, buy more hardware, can’t fit it in their data center, SAN administrators, etc. VDI is tough. But DaaS is not VDI.The point of the session is that DaaS is a three billion dollar market and is a new business area that OpenStack can access. Further he talks about the fact that running a desktop resource manager/broker on top of OpenStack can create operational and other cost savings. It also allows and organization like Desktone to focus on user experience versus running infrastructure.Desktops are Different:  Storage is key  Random IO  Write intensive IO  Scale: 500 servers is a lot, 500 desktops is not  Optimize OpenStack for DaaS  HA for desktops is different from servers  The future of VDI is non-persistence…nothing stored in VMs      of VMs under management requires a different approach    Eg. VM and guest state changes  Monitoring is critically important  DaaS encompasses all parts of OpenStack"
    } ,
  
    {
      "title"    : "OpenStack 2012 Summit Day &amp;#35;2",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/10/16/openstack-summit-day-2.html",
      "date"     : "2012-10-16 00:00:00 -0400",
      "content"  : "KeynotesThe keynotes mostly focused on the increased sized of the OpenStack community and the creation of the foundation that now runs OpenStack. Except Shuttleworth’s in which he used Juju to live upgrade from Essex to Folsom which turned some heads. It’s the first time I’ve heard him speak, and he seemed exactly like the kind of person that can sit in-between business and open source. Chris Kemp was good, and took a couple of shots at VMWare which I always think is fun. ;)High AvailabilityFlorian Haas of Hastexo presented an update on high-availability with OpenStack. The company I currently work for has, in the past, contracted Florian/Hastexo to consult on OpenStack deployments and I like the things that he says. He’s the go to person for HA + OpenStack.He talked at length about Pacemaker which is a cluster resource manager. Interestingly he notes that it runs air traffic control systems. Also he mentions that it is extremely friendly to 3rd party functionality via plugins/resource agents,He also went through the OpenStack High Availability Guide which he was largely responsible for creating, and mentioned that it is in source control and thus people can contribute patches.The slides for this talk are online.Frameworks and APIs for Advanced Service InsertionThis was a developer session on the future methodologies of inserting things like firewalls, load balancers and such into Quantum. It was interesting to hear some of the thoughts on the topologies of Quantum and how OpenStack is trying not just to replicate existing networking technologies, but create something new. It was nice to hear the word router with an Italian accent. OpenStack is a global project.LunchNot much to say, on this. There was tortilla soup though. :)The Future of Infrastructure AutomationThis session was not what I thought it was going to be. I didn’t realize it was part of the strategy track until I was sitting down listening, and so before knowing that I figured it would be about what comes after puppet/chef/etc…which it was not.I don’t have a lot to say about this session, other than the speaker liked webhooks and push notifications, that it would be nice to for a guest in OpenStack to be able to write metadata instead of just reading, and that some organizations are working on this functionality. I definitely would like to be able to write metadata from guests so that was good to hear.Surviving your first check-in: An engineers guide to contributing to OpenStackThis was a great session on lessons learned by an engineer (specifically not a developer), Collin McNamera, who wanted to contribute code to OpenStack. To get his code into OpenStack he had a ratio of 100:1 in terms of time spent figuring out how to contribute and waiting for answers vs actually coding. That was for his first contribution so the ratio is better now, but it was a tough slog at first.Lessons learned:  Join, or start, a local meetup group  Make sure that according to your employer you can contribute code to OpenSource projects  Execute your OpenStack CLA (contributor license agreement)  Setup your dev environment, probably using virtual machines  Devstack  With devstack you can point it to a different OpenStack git repo if you want  Use vm snapshotting to help you keep using your dev environment  Configure git with your name, email, etc and this will save time in the future. Do it right first!  Install git-review  Clone a project, or use the directories that came with Devstack  Add your ssh key to OpenStack Code Review  Create a topic branch  Change code  Test code: run_tests.sh  Commit changes - make sure to put a bug id or blueprint # in the first line  Then give back to the community by teaching others! :)While it’s disappointing to hear how hard it is to contribute code, Collin was a great speaker is obviously passionate about doing good open source work and contributing back to the community.Swift Project UpdateAs I mentioned in my previous post I am a big fan of object storage, so Swift is an important project to me. :)  New features in the last six months  Folsom version of Swift is 1.7.4  Unique-as-possible - Allows more flexible growth, and more harddrives as hand-off nodes  Deep statsd integration - Enables notifications that things like graphite can graph  SSD optimization - It turns out that in large clusters the account and container servers can become limited by IOPs.  Versioned writes - This feature is “almost” there  Moved swift client into it’s own project  Code golf for the last six months  37 people have contributed  20 have provided their first patch  Three new core developers  170 total commits  17 is the most files touched by a single commit (statsd integration)  3466 is the most lines removed in a single patch (moving swift-client out to a new project)  What’s next?  Global clusters - ie. Geographic replication, will hopefully be in Grizzly  CORS Support - Better integrate with the browser security model  Optimize concurrent reads(Nebula logo projected on the pool at the club)PartiesThis night I went to both the Rackspace and Nebula parties, though I came back to the hotel pretty early. At the Rackspace party I learned that Miramar, the location of the flight school in Top Gun, is only a few minutes away and actually watched some military jets maneuvering out over the ocean. Surprised no one sang You’ve Lost That Loving Feeling. ;)"
    } ,
  
    {
      "title"    : "ipmitool and BIOS Access",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/10/16/dell-ipmi-bios.html",
      "date"     : "2012-10-16 00:00:00 -0400",
      "content"  : "When I put my systems administrator hat on one of the things that bothers me is getting remote access to the BIOS. I love to use the command line, and any time I have to fire up a GUI of some kind to do work I get an icky feeling.So when I had to remotely access some Dell C6220s to turn on virtualization, which is off by default in these Dell servers, I didn’t want to have to go through the rigamarole of getting the right OS configuration that would allow me to remotely access the console via a web-based Java GUI run out of Firefox. Even writing that sentence takes too long. So I thought I would try out serial over lan (SOL) access.And it works!server2 $ ipmitool -I lanplus -H server1-ipmi -U root -P password sol activate[SOL Session operational.  Use ~? for help]Ubuntu 12.04.1 LTS server1 ttyS1server1 login: If we reboot the server we can see all the usual boot screens and can access them all without having to fire up the Java based console, instead I can just ssh into a second server that has access to the ipmi network and connect with ipmitool.Here are some of the key mappings that will help to use ipmitool. The main thing to note is how to exit from the SOL session, and to do that you basically hit ENTER the a tilde, then a period. (For some reason this will usually log me not only out of the SOL session, but also the ssh session. Something to look into.)server2 $ ipmitool -I lanplus -H 10.10.10.10 -U username -P password sol activate[SOL Session operational.  Use ~? for help]Here are some useful commands:        KEY MAPPING FOR CONSOLE REDIRECTION:         Use the &lt;0&gt; key sequence for     \tUse the &lt;!&gt; key sequence for     \tUse the &lt;@&gt; key sequence for         Use the  key sequence for     \tUse the  key sequence for     \tUse the  key sequence for     \tUse the  key sequence for         Use the  key sequence for , where x is any letter    \tkey, and X is the upper case of that key        Use the  key sequence for Help commands from ~?:Supported escape sequences:    \t~.  - terminate connection    \t~^Z - suspend ipmitool    \t~^X - suspend ipmitool, but don't restore tty on restart    \t~B  - send break    \t~?  - this message    \t~~  - send the escape character by typing it twice    \t(Note that escapes are only recognized immediately after newline.)&lt;/code&gt;&lt;/pre&gt;There are all kinds of things one can do with ipmitool other than terminal based BIOS access, and I won't list all of them here, but one nice thing is the ability to set the boot device, for example setting the boot device to disk and doing so for all future reboots.server2 $ ipmitool -H 10.10.10.10 -U root -P password chassis bootdev disk \\options=persistentHere are the kernel configs I used to tell Ubuntu to use serial. Note that I added the \"\\\" in the text below, so you can't just cut and paste it; needs to be all on one line.pxeboot-server:/tftp/pxelinux.cfg# cat default # D-I config version 2.0DEFAULT serverprompt 0timeout 1# serial consoleconsole 0serial 0 115200 0LABEL serverkernel ubuntu-installer/amd64/linuxappend ramdisk_size=14984 locale=en_US keyboard-configuration/layoutcode=us \\console-keymaps-at/keymap=us locale=en_US console-setup/layoutcode=en_US \\netcfg/wireless_wep= netcfg/choose_interface=eth0 netcfg/get_hostname=c01-07 \\url=http://10.10.10.10/node.preseed vga=normal \\initrd=ubuntu-installer/amd64/initrd.gz -- \\console=ttyS1,115200 earlyprint=serial,ttyS1,115200Finally I'll show a screenshot of a text install happening, and I am watching that via text-based SOL access.![](https://raw.github.com/ccollicutt/ccollicutt.github.com/master/img/sol_install.png)So give SOL a shot, and let me know if I've made any mistakes in this post, or if there are other interesting things that can be done with ipmitool. :)"
    } ,
  
    {
      "title"    : "OpenStack 2012 Summit Day &amp;#35;1",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/10/15/openstack-summit-day-1.html",
      "date"     : "2012-10-15 00:00:00 -0400",
      "content"  : "This week I’m at the OpenStack 2012 Summit. It’s by far the biggest conference I’ve been to. Usually I go to conferences on security, such as H.O.P.E or CanSec or even smaller library related conferences. Given that OpenStack is one of the fastest growing open source projects of all time, it’s no surprise to find out that the conference has grown from 75 people a couple years ago, to about 1300 this year, up from 700 only a year ago. It’s massive and it’s growing.tl;dr  Need to get into Ceph  Database as a service too  Upgrading is a big issue in OpenStack  San Diego is nice  OpenStack is kinda a big dealOpening “keynote”Not much to say here, some summit housekeeping with trippy techno rave music to wake me up. :)Open ComputeThe first session I attended was by Cole Crawford of the Open Compute Foundation. Basically this was an overview of the history of things like the 19” rack (it came from waaaaay back from the train system) and how we need to change our standards to allow things like interoperability and other good stuff.I was hoping to find out how a small organization–like the one that I work for–could be involved in Open Compute, specifically how we could access similar hardware as Facebook and others are using. I didn’t quite get that out of the presentation, but hopefully in the future I will be able to get us into Open Compute hardware in some fashion or another.At the very least I signed up for the mailing list. :)Intercloud Object Storage: ColonyThe Colony session was lead by Shigetoshi Yokoyama of the Japan National Institute of Informatics and there is a copy of the slides  online. Colony is described as “federated swifts” for intercloud object storage services.Having worked in a world class library that was interested in running petabytes of storage, I’m enthralled with object storage. It’s an important storage paradigm, and surprisingly, for reasons unknown, one that doesn’t seem to get a lot of attention in Canada. Perhaps it’s because startups and other organizations just use S3 and haven’t run into any privacy issues as of yet. Speaking of Amazon S3: it has a trillion+ objects stored now.I think that in situations where we need to store a lot of replicated data–such as what a library or researcher would like to, or rather should like to store–object storage such as swift is a great way to go. (Though that said, perhaps systems like Amazon’s Glacier make more sense for those use cases, but maybe not.) I was at a presentation by the then CIO of the University of Alberta who figured there was three to five petabytes of research data on campus that needs to be preserved. That’s a lot of storage, especially when factoring in replication, and it really needs to happen at some point–and it can’t just be a huge tape system.Further, considering that libraries and researchers  (ie. their respective Universities and organizations), are supposed to work together it would make sense to be able to have some kind of interoperability between object storage clouds, private or otherwise, even if it’s just for some geographic separation.Certainly Colonly is going to be a project to keep an eye on as object storage evolves and we try to do it across data centers and organizations.Operating your OpenStack Private CloudThe next session I attended was Operating your OpenStack Private Cloud, which is good because that is one of the things that I do. I manage a small, eight node, OpenStack cluster that will eventually backend a Apache VCL setup for virtual classrooms. So a very small private cloud, but I’ve still hit a few “pain points” that larger installations do.A few points the speaker made:  Monitoring: statsd, graphite, etc  Operations tools don’t exist for OpenStack (yet)–we need better ops tools so he wrote Pulsar  Would be nice to see an operations dashboard (perhaps in horizon)  Eg. can’t get a list of all instances on a node and their IP addresses  Some tools need hostnames and some need ids…why  DSH for Ubuntu, PSH for Redhat?  Don’t forget your bashfu; still working with Linux :)  Database backups  Holland  Performance and scale considerations  Block storage solution of some kind (cinder, or cinder + some vendor)  Local disk - raw image is only slightly faster than qcow2, but qcow2 may make better business sense  IO will degrade on local disks when glance copies images between machines  Scheduling  CFQ made the most sense  You have the power to change all this! You have the power to change your scheduler! Benchmark workloads and plan accordingly.  Lots of things you can do with glance performance-wise  Image caching  If you’re using qcow2 it doesn’t have to move the whole image (something to check into)  Don’t want swift to become unbalanced  They use chef for automated deployment  controller in 15 minutes  compute in two  Day to day tasks  Made up of mostly dealing with new issues  Eg. resizing  Hardware failures…still have all this hardware  Don’t forget–it’s not (always) you, it’s a bug  Nice thing about a private cloud is you can pick the architecture that matches your requirements  OpenStack/We need to provide an upgrade path (I hear this 20 times today)  Metrics  Load average is actually quite telling  How long does an API call take?Lunch!Lunch was Ok. There are so many people here it’s tough to move around. And given OpenStack gave out giant bags, everyone (like me) has their own personal giant bag, plus the one they got from OpenStack (though I gave mine back). Giant bags galore!Then I had a beer at the bar downstairs:Database as a ServiceThis session is specifically about RedDwarf. RedDwarf is taking MySQL and treating it like a “first class citizen” in OpenStack. It’s a managed MySQL database service.I believe that SQL databases should be separated out from compute. That said, there are many people that would disagree with me. There’s a theory out there that storage and compute should be together on the same nodes, and if that’s the case then I assume so would database. Right now I would prefer to separate out storage, compute, and database. Maybe it doesn’t fit perfectly with the cloud paradigm, but sometimes you have to apply technology not just theorize about it. SQL isn’t going anywhere, and neither is its unique workload.HP and Rackspace have slightly different implementations of RedDwarf, and both of their services are in production. Each are trying to keep their system compatible with OpenStack, and have committed themselves to using the same CLI: python-reddwarfclient. Further, they are both using OpenStack-common and Swift for securely storing database snapshots.The team has a desire to bring RedDwarf forward, and they want to involve more community usage and get it into incubation in OpenStack and at that point a lot of doors open, such as a GUI in Horizon.You can get a test install using the below commands, and it will “fake” a nova backend.$ git clone https://github.com/hub-cap/reddwarf_lite$ ./reddwarf_lite/bin/start_server.shThere was also a lot of discussion as to the fact that RedDwarf could be used to build Other Things as a Service and also the fact that they are running it on top of OpenVZ versus another virtualization hypervisor and did so for performance reasons (up to 30% better was a number mentioned).Extending OpenStack for Fun and Profit: Creating New Functionality with the Enhancements APIThis session is about having two sides–one side is OpenStack, and the other side is “your innovation”. It’s about getting OpenStack and your innovation to talk to one another; about getting your innovation to market and make money.The speaker, Tim Smith, is the co-founder of GridCentric which has essentially implemented fork() for virtual machines. The main idea is spin up replica instances quickly and efficiently. They are reinventing boot, and in order to do that they need to extend Nova. GridCentric’s code for their extension is up on github and would be a good example to work from. (I’ve looked at GridCentric’s offerings before in relation to virtual classrooms–ie. not having a bootstorm when a bunch of students startup instances for a class. With something like GridCentric’s technology you don’t have to boot the instance.)Nova is a framework providing:  A standardized API  Messaging/RPC  Database-backed ORMNova can be extended by:  API extensions  Custom “services”  Nova CLI extensions  Dashboard extensionsTim suggests that there are a few business challenges such as the fast evolution of OpenStack but generally seems positive about extending OpenStack. Certainly GridCentric has based some of their business on OpenStack.A commenter noted that there are actually pre-action and post-action boot hooks in Folsom which may help to extend Nova.Storing VMs with Cinder and Ceph RBDConsidering how interested I am in storage, this project, Ceph, is one of the most important I know of. So when Josh from Inktank talks about Ceph, I’m here to listen. As a note, this room is standing room only for this session!Ceph is designed for scalability–it has no single point of failure, and you don’t have to be locked into a single hardware vendor. It’s software based and self-managing. There is also the ability to run custom functions on the storage node–such as create a thumbnail from an image file.CRUSH is basically what sets Ceph apart from other storage systems. It’s a pseudo-random placement algorithm and it ensures even distribution across the cluster and has a rules-based system. Basically it avoids look up tables, which eventually kill distributed storage. Further, unlike RAID the entire cluster can recover in parallel.RBD, the Rados Block Device, is the part of Ceph used to access block storage. KVM has a native driver for RBD. They are thin-provisioned, striped across nodes, and support snapshots and cloning. With RBD you can spin up thousands of virtual machines with the same base image.Why use block storage in OpenStack?  Persistent  Not tied to a single host–decouple storage from compute  Enables live migrationNow with Folsom, Cinder has learned how to talk to Glance, which can respond with Images–ie. boot from volume. But still involves a large copy from Glance. However, images can be stored in RBD, and have been able to for some time! So we can skip the copy and just use RBD directly.That said, it isn’t quite perfect yet. There are enhancements being made in Grizzly. Eg. is not in Horizon, but is in the API and CLI.NOTES:  The Ceph file system is not recommended for production, but everything else is, such as RBD. If you don’t use the Ceph file system you don’t need metadata servers.  According to Josh, in terms of running OSDs on the same node as the hypervisor, as long as you have enough memory you should be Ok. Everything is running in userspace. This runs a bit counterintuitive to me, and also is contrary to one of the positive points made about block storage–decoupling. But it’s a valid use case.From Folsom to Grizzy: A DevOps Upgrade PatternThis was the last session of the day, and I was quite tired, so apologies. Suffice it to say: Even imagining upgrading OpenStack is difficult and requires a lot of bullet points.Misc Notes from the Day  There’s free sunshine and stout outside on the balcony  Probability of data loss in Swift (this is not to say it’s high, rather an analysis)  Infiniband doesn’t have a lot of traction in OpenStack AFAIK, need to look into this  San Diego has basically the same weather all the time  Japan’s summers are very hot and humid  !https://fbcdn-sphotos-e-a.akamaihd.net/hphotos-ak-ash4/430171_10150245244739982_1571995757_n.jpg!"
    } ,
  
    {
      "title"    : "My OpenBSD Lab",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/06/13/openbsd-lab.html",
      "date"     : "2012-06-13 00:00:00 -0400",
      "content"  : "Above you can see my little OpenBSD lab. For some reason my workplace has several different kinds of small form factor servers, such as the big silver Netcom box, a couple of Soekris boxes, and I can’t even tell what make/model the little blue boxes are. They are all essentially small, fanless servers with at least two ethernet ports. Since they weren’t being used for anything and I needed to setup a few test installs of OpenBSD, I went to work. :)The “lab” is comprised of two carped firewalls (the blue boxes; note the red cable between them is a cross over cable for pfsync traffic), a couple of cheap desktop switches, and the square silver thing is an OpenBSD bridge that goes in between the two switches. The carped firewalls are connected to another part of my test network, which eventually leads out to the Internet. I also have a couple of OpenBSD virtual machines running on my RHEL 5 xen dom0. One of them provides an OpenBSD pxe boot solution to install OpenBSD onto systems like the Soekris that can’t boot from USB.When testing I plug my laptop into the internal switch (the green cable), so that in order to get out to the Internet it has to go over the bridge and through the firewalls. “Over the Bridge and through the Firewall”…sounds like a book Hemingway would have written if he was a Unix security administrator. ;)Using this little lab I can test out all kinds of interesting OpenBSD functionality such as packet filtering with pf, virtual IP address failover with carp, bridging, and network authentication using authpf, along with anything else that I need to work on or try out in the OpenBSD world, or just try to stay familiar with OpenBSD in general.Bridge up!$ ifconfig bridge0 up"
    } ,
  
    {
      "title"    : "36 hot swappable hard-drive bay Supermicro server specs",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/06/07/36-hot-swappable-day-supermicro-chassis.html",
      "date"     : "2012-06-07 00:00:00 -0400",
      "content"  : "The above is the front of four Supermicro 36-bay chassis servers racked but not completely filled with drives.This is the back of the server. As you can see, there are 12 hot swappable slots in the back. The motherboard has room for seven low profile boards of varying PCIe speeds.Sorry for the poor picture quality–I don’t get paid enough to have a phone with a good camera. ;)This is output from the bottom server that has four drives lit up:# ls /dev/sd?/dev/sda  /dev/sdb  /dev/sdc  /dev/sdd  /dev/sde  /dev/sdfso with the two internal hard-drives that makes six, which is what we see above. If the server was filled up with drives, there would be 38.First things First: The SpecsWhy not get this out of the way? That’s what you’re here for, isn’t it? :) The below would be for one server:            Item      Part or Part #      Quantity              4U 36 bay Chassis      SM SC847E16-R1400LPB      1              iPass cable      SM CBL-0281L      1              iPass cable      SM 0108L-02      1              Internal HD Brackets      SM MCP-220-84701-0N      2              Motherboard      SM X8DT6-F      1              CPU      Xeon E5645 (6 cores)      2              Heat Sink/CPU Fan      Dynatron G666      2              Kingston 24GB Pack      SM KVR1066D3Q8R7SK3/24G      2              SATA power splitter      SM CBL-0082L      1      The total cost for the above is around $4000.What you get is one 4U, dual CPU server (24 cores counting hyperthreading), with 48GB of RAM, the MB supports up to 196GB I believe, and room for 36 hot swappable drives, PLUS two internal OS drives that are actually bracketed deep inside the chassis, ie. difficult to get out.%{color:blue}Note:% I’ve done my best to make sure the above is correct, and I do have exactly that hardware, but don’t base your business on those specs without doing some testing. Order one server and see how it goes before ordering a dozen+.%{color:blue}Note:% You would need to find a local vendor to put the pieces together, or you could do it yourself–but I don’t recommend that. I’d put 5% of the total cost aside for professional assembly. We were lucky enough that our vendor was willing to put them together for free–at least on the first order. :)(Terrible image…but you can kind of see the two internal hard-drives.)So with those two internal slots, plus 24 bays in the front, and 12 in the back is a total of 38 3.5” slots, 36 of them hot swappable. Or, put another way, if using 3TB SATA drives…114TB of raw storage. (Also I think you can fit four 2.5” drives internally instead of two 3.5” drives. But don’t quote me on that.)Bulk storage costsThe amount of data an organization has to store never seems to go down, only up up UP!–probably at least 50% per year. High-end, “enterprise storage”, can cost between $50K-$100K per terabyte, where K is thousands.I’m not kidding. It’s really that much, and there are few organizations that can afford to have their storage increase by 50% per year at that cost. Everywhere I’ve worked has started out with a large Enterprise SAN and quickly realized they can’t afford it.Of course, there are reasons why enterprise storage costs so much. It’s not easy to design and build a large, production, high-performance SAN, and in most cases trying to replicate one with commodity hardware is not a good idea, and could cost people their jobs. Not a nice thought, but it’s true.But in some specialized cases, such as bulk data storage, not using an Enterprise SAN might be an option. However, unless you work at Facebook, or another company that somehow has access to hardware based on Open Compute’s Open Vault, what do you do for bulk storage hardware?One possibility is the above 36-bay Supermicro chassis based server. I know there are people out there using them. Lots of them.Now you can afford test infrastructureOne note I feel important to make is that when you purchase cost effective hardware there is often money on the table to buy more hardware for test. I always like to have test hardware running so that I don’t have to worry about “trying something out” in production. When running production systems I never want to be afraid to make a change, and the way to do that is to have test infrastructure to experiment and…test with.If your enterprise hardware is so expensive that test infrastructure is completely out of the question, then I wonder how easy it will be to try things out; to make changes without fear of brining down the production system?To me there is massive value in test hardware.Also, test hardware can be moved from test to production in case of failure.SupportObviously if you spec your own hardware, versus say buying from a tier one vendor, support means that if it breaks you pull a new part off the shelf or out of test and replace it in production yourself.Further, tier one vendors will usually certify their hardware for particular operating systems and applications. You won’t have that if you spec your own. But you certainly can start small (and inexpensively) and test out your specs/OS/applications, then go bigger when you know it all works well together.Personally, I don’t like spending 3 hours on the phone with the tier one support, going through generic call scripts with someone who doesn’t care about my problem in the least, and then having the local support tech either be a “cell phone with hands” or cancel four times in a row before simply replacing the whole motherboard because they have no idea what is actually wrong. I suppose that sounds kind of negative, but those are real anecdotes. :) PS. We have lots of tier one vendor servers…BonusThe Java IPMI remote console gui works great on Linux! Some tier one vendors don’t…and they make you pay extra to actually use the remote console. (cough HP cough)That said…don’t look at how they store passwords in the ssh interface. Just don’t.ConclusionI feel that in specialized applications systems like this supermicro server can be very effective. Especially if it leaves money on the table for test infrastructure. I’d rather have twelve (nine production, three in test) of these boxes than two four-socket tier one servers.I recommend using 5% of the budget for parts to go on the shelf, and 5%-25% for test.If you have any questions/concerns, please comment."
    } ,
  
    {
      "title"    : "Installing IBM high-iops FusionIO Cards in Redhat/Centos 6",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/05/23/installing-ibm-fusionio-rhel_centos6.html",
      "date"     : "2012-05-23 00:00:00 -0400",
      "content"  : "In a previous post I had described how I deployed a IBM branded FusionIO drive on Redhat Enterprise 5.I am now running that same card on CentOS 6, and am using the new version (2.2.3) of IBM’s version of the driver.Actually I think there is a new-new version (3) of the driver now out for some people. I’m not sure if IBM has put out this driver or not for their high-iops cards.CentOS version:[root@srv ~]# cat /etc/redhat-release CentOS release 6.2 (Final)The kernel I am running is stock RHEL 6:[root@srv ~]# uname -aLinux example.com 2.6.32-220.17.1.el6.x86_64 #1 \\SMP Wed May 16 00:01:37 BST 2012 x86_64 x86_64 x86_64 GNU/LinuxThis is what I see in terms of PCI devices for the FusionIO cards:[root@srv ~]# lspci | grep -i fusion8f:00.0 Mass storage controller: Fusion-io ioDimm3 (rev 01)90:00.0 Mass storage controller: Fusion-io ioDimm3 (rev 01)So the card is physically installed in the server, but the driver has not been loaded, so they are not usable at this point. Also should note that one 640GB cards actually looks like 2x 320GB devices to the OS.First, we download the zip file containing the RPMs from IBM.%{color:red}Warning:% These drivers are for the IBM version of the FusionIO cards. If you are not running the IBM version you probably need different drivers and RPMs.# wget ftp://download2.boulder.ibm.com/ecc/sar/CMA/XSA/ibm_dd_highiop_ssd-2.2.3_rhel6_x86-64.zipSNIP!Inside that zip are several RPMs:[root@srv tmp]# mkdir fio[root@srv tmp]# cd fio/[root@srv fio]# unzip ../ibm_dd_highiop_ssd-2.2.3_rhel6_x86-64.zip Archive:  ../ibm_dd_highiop_ssd-2.2.3_rhel6_x86-64.zip  inflating: rhel6/fio-common-2.2.3.66-1.0.el6.x86_64.rpm    inflating: rhel6/fio-firmware-highiops-101583.6-1.0.noarch.rpm    inflating: rhel6/fio-snmp-agentx-1.1.1.5-1.0.el6.x86_64.rpm    inflating: rhel6/fio-sysvinit-2.2.3.66-1.0.el6.x86_64.rpm    inflating: rhel6/fio-util-2.2.3.66-1.0.el6.x86_64.rpm    inflating: rhel6/high_iops-gui-2.3.1.1874-1.1.noarch.rpm    inflating: rhel6/iomemory-vsl-2.2.3.66-1.0.el6.el6.src.rpm    inflating: rhel6/iomemory-vsl-2.6.32-71.el6.x86_64-2.2.3.66-1.0.el6.el6.x86_64.rpm    inflating: rhel6/libfio-2.2.3.66-1.0.el6.x86_64.rpm    inflating: rhel6/libfusionjni-1.1.1.5-1.0.el6.x86_64.rpm So far when I’ve been running these servers I haven’t installed all of those RPMs, only a subset.So lets install those RPMs:[root@srv rhel6]# yum localinstall --nogpg \\ fio-common-2.2.3.66-1.0.el6.x86_64.rpm \\ libfio-2.2.3.66-1.0.el6.x86_64.rpm fio-util-2.2.3.66-1.0.el6.x86_64.rpm \\ fio-sysvinit-2.2.3.66-1.0.el6.x86_64.rpm \\ fio-firmware-highiops-101583.6-1.0.noarch.rpm \\ iomemory-vsl-2.6.32-71.el6.x86_64-2.2.3.66-1.0.el6.el6.x86_64.rpmSNIP!Transaction Test SucceededRunning Transaction  Installing     : fio-util-2.2.3.66-1.0.el6.x86_64                   1/6   Installing     : fio-common-2.2.3.66-1.0.el6.x86_64                 2/6   Installing     : iomemory-vsl-2.6.32-71.el6.x86_64-2.2.3.66-1.0.e   3/6   Installing     : libfio-2.2.3.66-1.0.el6.x86_64                     4/6   Installing     : fio-sysvinit-2.2.3.66-1.0.el6.x86_64               5/6   Installing     : fio-firmware-highiops-101583.6-1.0.noarch          6/6 Installed:  fio-common.x86_64 0:2.2.3.66-1.0.el6                                      fio-firmware-highiops.noarch 0:101583.6-1.0                               fio-sysvinit.x86_64 0:2.2.3.66-1.0.el6                                    fio-util.x86_64 0:2.2.3.66-1.0.el6                                        iomemory-vsl-2.6.32-71.el6.x86_64.x86_64 0:2.2.3.66-1.0.el6.el6           libfio.x86_64 0:2.2.3.66-1.0.el6                                        As you can see the sysvinit RPM contains a couple of init.d files.[root@srv rhel6]# rpm -qpl fio-sysvinit-2.2.3.66-1.0.el6.x86_64.rpm /etc/init.d/iomemory-vsl/etc/sysconfig/iomemory-vslLet’s chkconfig this on permanently.[root@srv rhel6]# chkconfig iomemory-vsl onWe also need to enable iomemory-vsl in /etc/sysconfig/iomemory-vsl.[root@srv init.d]# cd /etc/sysconfig[root@srv sysconfig]# grep ENABLED iomemory-vsl # If ENABLED is not set (non-zero) then iomemory-vsl init script will not be#ENABLED=1[root@srv sysconfig]# vi iomemory-vsl [root@srv sysconfig]# grep ENABLED iomemory-vsl # If ENABLED is not set (non-zero) then iomemory-vsl init script will not beENABLED=1[root@srv sysconfig]#And we can start or restart iomemory-vsl:[root@srv sysconfig]# service iomemory-vsl restartStopping iomemory-vsl: Unloading module iomemory-vsl                                                           [FAILED]Starting iomemory-vsl: Loading module iomemory-vslAttaching: [                    ] (  0%) /Attaching:[                    Attaching: [====================] (100%) \\fioaAttaching: [====================] (100%)fiob                                                           [  OK  ]At this point I’m going to reboot the server as well, just to make sure everything is going to get loaded if the server spontaneously restarts, which they have been known to do. ;)[root@srv sysconfig]# rebootNow after the reboot there are a couple more block storage devices on this server:[root@srv ~]# ls /dev/fio?/dev/fioa  /dev/fiobWe want to create a lvm physical volume (pv) on that block device:[root@srv ~]# pvcreate /dev/fioa  Device /dev/fioa not found (or ignored by filtering).Ooops. Error message. What went wrong? Well, the “or ignored by filtering” is where to start looking. This FusionIO knowledge base entry (which you have to login to see, how annoying is that) shows that we need to add an entry to the lvm.conf on the server:Locate and edit the /etc/lvm/lvm.conf configuration file.Add an entry similar to the following to that file:types = [ \"fio\", 16 ]That is precisely what I will do.[root@srv lvm]# grep types lvm.conf    # List of pairs of additional acceptable block device types found     # types = [ \"fd\", 16 ]    types = [ \"fio\", 16 ]And now:# let's see if the types were loaded[root@srv ~]# lvm dumpconfig | grep types  \ttypes=[\"fio\", 16][root@srv ~]# pvcreate /dev/fioa  Physical volume \"/dev/fioa\" successfully created[root@srv ~]# pvcreate /dev/fiob  Physical volume \"/dev/fiob\" successfully createdAnd create a volume group and add the pvs to it.[root@srv ~]# vgcreate hiops /dev/fioa  Volume group \"hiops\" successfully created[root@srv ~]# vgextend hiops /dev/fiob  Volume group \"hiops\" successfully extended[root@srv ~]# vgs  VG     #PV #LV #SN Attr   VSize   VFree    hiops    2   0   0 wz--n- 504.91g 504.91g  system   1   9   0 wz--n-  58.56g  36.66g  vm       1  11   2 wz--n-   1.31t 228.09gI should note at this point that there is only 504g in the hiops volume group when there should be about 600g.Previously, using the fio-format command, I had formatted these drives to only 80% capacity. But that was on another server, and I’m not sure it’s really necessary to do that unless you are looking for extreme performance or perhaps additional reliability.I believe that in some cases with SSD, PCIe or otherwise, it’s not a bad idea to use less than 100% of the drive. That said, if you are looking to max out these drives performance-wise, I’d suggest talking to your vendor rather than just listening to me. :)(AFAIK, these cards can actually take an external power source to increase performance even more. But we don’t use that functionality.)So I’m going to reformat these drives to 100% usage. Just for fun. Why not get back that 100g because the performance/endurance at 100% is going to be fine for our usage.%{color:blue}Note:% Brand new drives won’t have to be formatted. I’m only doing this because I had formatted the drives when they were in the other server.%{color:red}Warning:% Reformatting will obviously delete any data on these drives!# first detach the /dev/fioa[root@srv ~]# fio-detach /dev/fct0Detaching: [====================] (100%) -[root@srv ~]# fio-format -s 100% /dev/fct0Creating a device of size 322.55GBytes (300.40GiBytes).  Using block (sector) size of 512 bytes.WARNING: Formatting will destroy any existing data on the device!Do you wish to continue [y/n]? yFormatting: [====================] (100%) \\Formatting: [====================] (100%)Format successful.# then attach...[root@srv ~]# fio-attach /dev/fct0Attaching: [====================] (100%) -fioaAnd we can add that device back with pvcreate and then we should see a larger drive:[root@srv ~]# pvcreate /dev/fioa  Physical volume \"/dev/fioa\" successfully created[root@srv ~]# pvs /dev/fioa  PV         VG    Fmt  Attr PSize   PFree    /dev/fioa  hiops lvm2 a-   300.40g 300.40gI reformatted the other side of the drive back to 100% as well. (With new drives this shouldn’t be necessary.)And the fio-status now is:[root@srv ~]# fio-statusFound 2 ioDrives in this system with 1 ioDrive DuoFusion-io driver version: 2.2.3 build 66Adapter: ioDrive Duo\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:XXXXX\tExternal Power: NOT connected\tPCIE Power limit threshold: 24.75W\tSufficient power available: Unknown\tConnected ioDimm modules:\t  fct0:\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:XXXXX\t  fct1:\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:XXXXXfct0\tAttached as 'fioa' (block device)\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:XXXXX\tAlt PN:68Y7382\tLocated in slot 0 Upper of ioDrive Duo SN:XXXXX\tPCI:8f:00.0\tFirmware v5.0.6, rev 101583\t322.55 GBytes block device size, 396 GBytes physical device size\tSufficient power available: Unknown\tInternal temperature: avg 50.2 degC, max 51.2 degC\tMedia status: Healthy; Reserves: 100.00%, warn at 10.00%fct1\tAttached as 'fiob' (block device)\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:XXXXX\tAlt PN:68Y7382\tLocated in slot 1 Lower of ioDrive Duo SN:XXXXX\tPCI:90:00.0\tFirmware v5.0.6, rev 101583\t322.55 GBytes block device size, 396 GBytes physical device size\tSufficient power available: Unknown\tInternal temperature: avg 46.3 degC, max 46.8 degC\tMedia status: Healthy; Reserves: 100.00%, warn at 10.00%Finally we can create a logical volume (lv) to use.[root@srv ~]# vgs hiops  VG    #PV #LV #SN Attr   VSize   VFree    hiops   1   0   0 wz--n- 300.40g 300.40g[root@srv ~]# lvcreate -n test -L10.0G /dev/hiops  Logical volume \"test\" createdIf you have any corrections or other comments, please let me know!"
    } ,
  
    {
      "title"    : "Bottle, Elixir, Bootstrap and Datatables - Instant Admin Backend",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/05/15/instant-admin-backend.html",
      "date"     : "2012-05-15 00:00:00 -0400",
      "content"  : "UPDATE: I’ve recently stopped using Elixir and have moved to using straight SQLAlchemy.Recently I have been working on a web-based administrative backend, and have found doing so unusually easy, mostly because of the combination of Bottle, a python micro-framework, Elixir, a wrapper over top of SQLAlchemy (which is itself an SQL toolkit and ORM), Twitter’s bootstrap, scaffolding for websites, and finally Datatables, which enables advanced interactions with HTML tables.The only difficulty I had, and it was slight, was integrating bootstrap and datatables together, but this post helped out quite a bit.I would certainly suggest to anyone looking to create an administrative backend, or any web application really, to look into these four technologies, as it would take very little time to create a minimum viable product, or a demo, using a combination of Bottle, Elixir, Bootstrap, and Datatables.In future posts I will present an example application."
    } ,
  
    {
      "title"    : "OCZ Z-Drive R4 Installation and Performance",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/05/08/ocz-zdrive-r4-installation-performance.html",
      "date"     : "2012-05-08 00:00:00 -0400",
      "content"  : "In a previous post I mentioned how we had purchased 11 300GB OCZ Z-Drive R4 PCIe-SSD cards. (Please note that this was a special case purchase–the cards didn’t meet any specific requirements we had other than that they were easily available, PCIe-SSD, and low profile.)We bought the low profile version because these drives are going into the Supermicro SC847E16-R1400LPB chassis (the subject of future posts), which have room for seven low profile cards. I believe the full height zdrive R4s are faster, so this is a compromise.InstallationEach of our servers is going to get one zdrive. I placed them in a x8 slot.Once the OS is up and installed (these cards are not bootable, ie. the OS can’t be installed onto the cards) the proprietary kernel module needs to be loaded.There is an installation guide.I’m running Centos 6.2 on these servers.# cat /etc/redhat-release CentOS release 6.2 (Final)# uname -aLinux ocz_server 2.6.32-220.el6.x86_64 #1 SMP Tue Dec 6 19:48:22 GMT 2011 x86_64 x86_64 x86_64 GNU/LinuxI’m using the:Red Hat Enterprise Linux 6.x, CentOS 6.x 64-bit\t1.0.0.1480\tMar 2, 2012version of the driver.When that tar file is downloaded and unzipped all there is inside is the ocz10xx.ko kernel module.# wget http://www.oczenterprise.com/files/drivers/OCZ%20RHEL-Centos_6.x_64-Bit_r1480.tar.gz --2012-05-08 21:40:23--  http://www.oczenterprise.com/files/drivers/OCZ%20RHEL-Centos_6.x_64-Bit_r1480.tar.gzResolving www.oczenterprise.com... 74.52.187.58Connecting to www.oczenterprise.com|74.52.187.58|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 4072553 (3.9M) [application/x-gzip]Saving to: “OCZ RHEL-Centos_6.x_64-Bit_r1480.tar.gz”100%[======================================&gt;] 4,072,553   1.03M/s   in 4.0s    2012-05-08 21:40:27 (991 KB/s) - “OCZ RHEL-Centos_6.x_64-Bit_r1480.tar.gz” saved [4072553/4072553]# tar zxvf OCZ\\ RHEL-Centos_6.x_64-Bit_r1480.tar.gz ocz10xx.kowhich can be loaded by:# insmod ocz10xx.ko# lsmod | grep oczocz10xx               479350  1 When that module is loaded the following is reported to dmesg:ocz10xx: module license 'Proprietary' taints kernel.Disabling lock debugging due to kernel taintocz10xx module is older than RHEL 6.2 ... applying fixups  alloc irq_desc for 26 on node -1  alloc kstat_irqs on node -1ocz10xx 0000:07:00.0: PCI INT A -&gt; GSI 26 (level, low) -&gt; IRQ 26ocz10xx 0000:07:00.0: setting latency timer to 64OCZ Storage Controller is found, using IRQ 26, driver version 2.0.0.1480.OCZ Linux driver ocz10xx, driver version 2.0.0.1480.OCZ DRIVE LEVEL=OCZ_FAST, STATE=ONLINEscsi5 : OCZ Storage Controllerscsi 5:0:126:0: Direct-Access     ATA      OCZ Z-DRIVE R4 C 2.15 PQ: 0 ANSI: 5sd 5:0:126:0: Attached scsi generic sg8 type 0sd 5:0:126:0: [sdg] 586135549 512-byte logical blocks: (300 GB/279 GiB)sd 5:0:126:0: [sdg] Write Protect is offsd 5:0:126:0: [sdg] Mode Sense: 41 00 00 00sd 5:0:126:0: [sdg] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA sdg: unknown partition tablesd 5:0:126:0: [sdg] Attached SCSI diskand I now have a /dev/sdg to use.Loading the kernel module at bootFirst, let me say that I don’t have a lot of experience with kernel modules. I’m hoping that if I’ve made a mistake that someone will alert me in the comments. Or perhaps I missed where this is documented by OCZ.Running insmod is great for the first time one tries out the zdrive, but what happens after a reboot?Usually kernel modules go in /lib/modules/uname -r but this module doesn’t seem to be tied to a particular kernel version. While I could put it in that directory, each time I get a new kernel I’d have to move it. This would not be good for maintainability. Assuming the module works with all 6.x kernels–which is what the OCZ drivers page suggests–it should be Ok to put this module in a more permanent location.What I did was build and RPM with three files:# rpm -qf /etc/depmod.d/ocz-zdrive-r4.conf ocz-zdrive-r4-r1480-2.el6.x86_64# rpm -qf /etc/modprobe.d/ocz-zdrive-r4.conf ocz-zdrive-r4-r1480-2.el6.x86_64# rpm -qf /usr/share/ocz-zdrive-r4/module/ocz10xx.ko ocz-zdrive-r4-r1480-2.el6.x86_64The .conf files contain:# cat /etc/depmod.d/ocz-zdrive-r4.conf search /usr/share/ocz-zdrive-r4/module# cat /etc/modprobe.d/ocz-zdrive-r4.conf alias ocz10xx oczwhich will ensure that the ocz10xx.ko module is loaded with all the other kernel modules, so that you can put file systems on the zdrive into fstab and have them mounted at boot:# uptime 23:05:24 up 15 min,  2 users,  load average: 0.00, 0.02, 0.00# lsmod | grep oczocz10xx               479350  1 # mount | grep ocz/dev/mapper/ocz-test on /mnt/ocz-xfs-test type xfs (rw)# cat /etc/fstab | grep ocz/dev/mapper/ocz-test /mnt/ocz-xfs-test          xfs    defaults        1Please let me know if there is something wrong with the methodology. :)Performance testingAs I’ve said before, good performance testing is hard to do. All I can really do at this point is run the same tests that FusionIO (GAH! Behind a support login now! Bad FusionIO, bad!) suggests running on their drives.%{color:red}WARNING:% The write tests will destroy data on the drive!%{color:blue}NOTE:% A little bird told me that you need to run the write tests first, otherwise the flash drive–if it’s empty–may (depending on the vendor, perhaps) know it’s empty and return zeroes and you’ll be testing RAM instead of the card.Write bandwidth test# fio --filename=/dev/sdg --direct=1 --rw=randwrite --bs=1m \\--size=5G --numjobs=4 --runtime=10 --group_reporting --name=file1file1: (g=0): rw=randwrite, bs=1M-1M/1M-1M, ioengine=sync, iodepth=1...file1: (g=0): rw=randwrite, bs=1M-1M/1M-1M, ioengine=sync, iodepth=1fio 2.0.7Starting 4 processesJobs: 4 (f=4): [wwww] [100.0% done] [0K/1021M /s] [0 /974  iops] [eta 00m:00s]file1: (groupid=0, jobs=4): err= 0: pid=2444  write: io=9281.0MB, bw=948572KB/s, iops=926 , runt= 10019msec    clat (usec): min=679 , max=81086 , avg=4111.26, stdev=4974.85     lat (usec): min=848 , max=81251 , avg=4313.21, stdev=4974.67    clat percentiles (usec):     |  1.00th=[ 1704],  5.00th=[ 1928], 10.00th=[ 2064], 20.00th=[ 2672],     | 30.00th=[ 2736], 40.00th=[ 2800], 50.00th=[ 2960], 60.00th=[ 3408],     | 70.00th=[ 3568], 80.00th=[ 3760], 90.00th=[ 4960], 95.00th=[ 9664],     | 99.00th=[35584], 99.50th=[36608], 99.90th=[38144], 99.95th=[38656],     | 99.99th=[81408]    bw (KB/s)  : min=139912, max=273338, per=25.14%, avg=238498.74, stdev=28729.62    lat (usec) : 750=0.09%, 1000=0.03%    lat (msec) : 2=8.60%, 4=76.87%, 10=9.69%, 20=2.80%, 50=1.90%    lat (msec) : 100=0.03%  cpu          : usr=4.53%, sys=2.98%, ctx=9287, majf=0, minf=120  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     issued    : total=r=0/w=9281/d=0, short=r=0/w=0/d=0Run status group 0 (all jobs):  WRITE: io=9281.0MB, aggrb=948572KB/s, minb=948572KB/s, maxb=948572KB/s, mint=10019msec, maxt=10019msecDisk stats (read/write):  sdg: ios=83/18421, merge=578/0, ticks=13/69730, in_queue=69712, util=99.21%Read IOPS test# fio --filename=/dev/sdg --direct=1 --rw=randread --bs=4k \\--size=5G --numjobs=64 --runtime=10 --group_reporting --name=file1file1: (g=0): rw=randread, bs=4K-4K/4K-4K, ioengine=sync, iodepth=1...file1: (g=0): rw=randread, bs=4K-4K/4K-4K, ioengine=sync, iodepth=1fio 2.0.7Starting 64 processesJobs: 64 (f=64): [rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrJobs: 64 (f=64): [rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrJobs: 64 (f=64): [rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrJobs: 64 (f=64): [rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrJobs: 64 (f=64): [rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrJobs: 64 (f=64): [rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrJobs: 64 (f=64): [rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrJobs: 64 (f=64): [rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrJobs: 64 (f=64): [rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr] [100.0% done] [374.1M/0K /s] [91.4K/0  iops] [eta 00m:00s]file1: (groupid=0, jobs=64): err= 0: pid=2465  read : io=3589.4MB, bw=367442KB/s, iops=91860 , runt= 10003msec    clat (usec): min=100 , max=283036 , avg=693.42, stdev=1539.29     lat (usec): min=101 , max=283036 , avg=693.61, stdev=1539.29    clat percentiles (usec):     |  1.00th=[  262],  5.00th=[  378], 10.00th=[  438], 20.00th=[  506],     | 30.00th=[  556], 40.00th=[  604], 50.00th=[  652], 60.00th=[  700],     | 70.00th=[  756], 80.00th=[  828], 90.00th=[  948], 95.00th=[ 1064],     | 99.00th=[ 1400], 99.50th=[ 1592], 99.90th=[ 2288], 99.95th=[ 2832],     | 99.99th=[56064]    bw (KB/s)  : min=  816, max= 7032, per=1.55%, avg=5706.40, stdev=599.67    lat (usec) : 250=0.81%, 500=17.90%, 750=50.29%, 1000=23.77%    lat (msec) : 2=7.05%, 4=0.16%, 10=0.01%, 20=0.01%, 50=0.01%    lat (msec) : 100=0.01%, 250=0.01%, 500=0.01%  cpu          : usr=0.56%, sys=6.49%, ctx=919527, majf=0, minf=2240  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     issued    : total=r=918880/w=0/d=0, short=r=0/w=0/d=0Run status group 0 (all jobs):   READ: io=3589.4MB, aggrb=367441KB/s, minb=367441KB/s, maxb=367441KB/s, mint=10003msec, maxt=10003msecDisk stats (read/write):  sdg: ios=914696/0, merge=0/0, ticks=612829/0, in_queue=607382, util=98.84%Read bandwidth test# fio --filename=/dev/sdg --direct=1 --rw=randread --bs=1m --size=5G \\ --numjobs=4 --runtime=10 --group_reporting --name=file1file1: (g=0): rw=randread, bs=1M-1M/1M-1M, ioengine=sync, iodepth=1...file1: (g=0): rw=randread, bs=1M-1M/1M-1M, ioengine=sync, iodepth=1fio 2.0.7Starting 4 processesJobs: 4 (f=4): [rrrr] [100.0% done] [1599M/0K /s] [1524 /0  iops] [eta 00m:00s]file1: (groupid=0, jobs=4): err= 0: pid=2543  read : io=16475MB, bw=1647.2MB/s, iops=1647 , runt= 10002msec    clat (usec): min=828 , max=79515 , avg=2423.79, stdev=1154.98     lat (usec): min=828 , max=79515 , avg=2424.04, stdev=1154.98    clat percentiles (usec):     |  1.00th=[ 1528],  5.00th=[ 1768], 10.00th=[ 1912], 20.00th=[ 2064],     | 30.00th=[ 2160], 40.00th=[ 2256], 50.00th=[ 2320], 60.00th=[ 2416],     | 70.00th=[ 2544], 80.00th=[ 2736], 90.00th=[ 2992], 95.00th=[ 3280],     | 99.00th=[ 3856], 99.50th=[ 4128], 99.90th=[ 6176], 99.95th=[13120],     | 99.99th=[78336]    bw (KB/s)  : min=369211, max=526336, per=25.10%, avg=423322.49, stdev=33254.52    lat (usec) : 1000=0.18%    lat (msec) : 2=14.73%, 4=84.40%, 10=0.63%, 20=0.04%, 100=0.02%  cpu          : usr=0.19%, sys=5.89%, ctx=16488, majf=0, minf=1151  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     issued    : total=r=16475/w=0/d=0, short=r=0/w=0/d=0Run status group 0 (all jobs):   READ: io=16475MB, aggrb=1647.2MB/s, minb=1647.2MB/s, maxb=1647.2MB/s, mint=10002msec, maxt=10002msecDisk stats (read/write):  sdg: ios=32621/0, merge=0/0, ticks=71360/0, in_queue=71316, util=99.09%Write IOPS test# fio --filename=/dev/sdg --direct=1 --rw=randwrite --bs=4k --size=5G \\--numjobs=64 --runtime=10 --group_reporting --name=filefile: (g=0): rw=randwrite, bs=4K-4K/4K-4K, ioengine=sync, iodepth=1...file: (g=0): rw=randwrite, bs=4K-4K/4K-4K, ioengine=sync, iodepth=1fio 2.0.7Starting 64 processesJobs: 64 (f=64): [wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwJobs:  64 (f=64): [wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwJobs: 64 (f=64): [wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwJobs: 64 (f=64): [wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwJobs: 64 (f=64): [wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwJobs: 64 (f=64): [wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwJobs: 64 (f=64): [wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwJobs: 64 (f=64): [wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwJobs: 64 (f=64): [wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww] [100.0% done] [0K/408.8M /s] [0 /99.8K iops] [eta 00m:00s]file: (groupid=0, jobs=64): err= 0: pid=2556  write: io=3670.1MB, bw=374777KB/s, iops=93694 , runt= 10030msec    clat (usec): min=40 , max=302579 , avg=677.08, stdev=1765.33     lat (usec): min=40 , max=302580 , avg=678.03, stdev=1765.34    clat percentiles (usec):     |  1.00th=[  117],  5.00th=[  390], 10.00th=[  450], 20.00th=[  506],     | 30.00th=[  548], 40.00th=[  580], 50.00th=[  620], 60.00th=[  652],     | 70.00th=[  692], 80.00th=[  748], 90.00th=[  820], 95.00th=[  892],     | 99.00th=[ 1064], 99.50th=[ 1144], 99.90th=[31616], 99.95th=[32640],     | 99.99th=[33536]    bw (KB/s)  : min= 2208, max= 9448, per=1.56%, avg=5834.54, stdev=562.51    lat (usec) : 50=0.25%, 100=0.58%, 250=1.49%, 500=16.44%, 750=62.05%    lat (usec) : 1000=16.98%    lat (msec) : 2=1.91%, 4=0.06%, 10=0.09%, 20=0.02%, 50=0.11%    lat (msec) : 100=0.01%, 250=0.01%, 500=0.01%  cpu          : usr=0.68%, sys=6.54%, ctx=942753, majf=0, minf=2070  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     issued    : total=r=0/w=939753/d=0, short=r=0/w=0/d=0Run status group 0 (all jobs):  WRITE: io=3670.1MB, aggrb=374776KB/s, minb=374776KB/s, maxb=374776KB/s, mint=10030msec, maxt=10030msecDisk stats (read/write):  sdg: ios=609/926539, merge=2759/0, ticks=337/599297, in_queue=594561, util=99.08%ConclusionFrom a cursory look these drives seem to perform well. At least when they are brand new. :) We’ll see how they perform over time.If anyone would like to see specific tests, please let me know."
    } ,
  
    {
      "title"    : "What 11 OCZ Z-Drive R4 Cards Look Like",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/04/13/11-ocz-zdrive-r4s.html",
      "date"     : "2012-04-13 00:00:00 -0400",
      "content"  : "…in one big box.Recently we picked up 11 300GB OCZ Z-Drive PCIe SSD cards to put in some new servers we bought. Don’t ask me to explain what and why because this was somewhat of an unusual purchase, but suffice it to say we’re going to be running a lot of fast VMs on top of this storage. I would have liked to get the full height cards, but because of the servers they are going in we have to use the half-height version.Look for some performance and configuration posts in the future, along with updates on using FusionIO on RHEL 6 as I’ve updated the server those are installed in to RHEL 6."
    } ,
  
    {
      "title"    : "Deploying Ruby-on-Rails applications using RPM packaging",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/01/17/Deploying-ruby-on-rails-applications-via-rpm-packaging.html",
      "date"     : "2012-01-17 00:00:00 -0500",
      "content"  : "It’s been a long time between posts but the time has come!In this post I hope to take a good look at one way to deploy a working ruby on rails (RoR) application by packaging it in an RPM.In this example all of the gems the application requires are downloaded and built/compiled at the same time the RPM is, and thus the RPM contains all the required gems (100+ in this example). The best way to deploy an application, in my opinion, would be to standardize on a set of gems that is available at the OS level–so the RPM would not contain any gems, rather would require the general OS level gems.Unfortunately, for many reasons, which I won’t get into, that is just not possible for me at this time. Maybe in the future when all gems can easily be built into RPMs, and also when internal developers can agree on a set of gems. Someday…EnvironmentWe’re deploying to a specific RHEL6 server environment.Ruby versionWe’ll be deploying the RoR application to Redhat Enterprise 6 (RHEL6) virtual machine which has, and likely always will have, @ruby 1.8.7@ (with backported security patches of course!).[root@RoR-TEST ~]# ruby -vruby 1.8.7 (2010-06-23 patchlevel 299) [x86_64-linux][root@RoR-TEST ~]# cat /etc/redhat-release Red Hat Enterprise Linux Server release 6.1 (Santiago)This will likely be a problem in the future, as it seems that Rails 3.2 will be the last version that supports ruby 1.8.X (where X seems to be 7+ as 1.8.6 is specifially not supported). At some point the dev team may want to go to a Rails version that will not run on Ruby 1.8.7.Apache and passengerWe’ll also be deploying the RoR app using apache and passenger.RequirementsA few things are required to build and deploy an RPM.The application code in some kind of version control system and hopefully that VCS supports tagging…svn, mercurial, and git all support tags.A build server that is the same as OS and arch as the production server being deployed to. In this case, RHEL6 and X86_64.** A spec file for the application.** This build server needs bundle and gem available in the binary PATH because currently the example spec file needs it to be there.** A working rpmbuild environment, configured as appropriate.A test server to test the RPM deployment, ie. a place to actually install the RPM into.The spec fileBuilding a RPM requires, among other things, a spec file. This file is the heart of a RoR RPM deployment.I have put an example spec file up on github to peruse and abuse. Again, it’s not going to work out of the box, but it’s a good example, or will be at some point. :)The build portion of the spec file is what is interesting in terms of deploying a RoR app with RPM.Prior to the build section the code has been pulled out of a git repository into a local build directory by the rpmbuild process.In the build section, which I’m cutting and pasting examples out of, we are going to cd into that checked out repository and use bundle to compile and install all the gems into ./vendor/bundle.%buildpushd %{name}# Install all required gems into ./vendor/bundle using the handy bundle commmandbundle install --deploymentOnce that has completed, which could be quite a long process depending on the number and complexity of the gems required, we remove the assets and recompile them.# Compile assets, this only has to be done once AFAIK, so in the RPM is finerm -rf ./public/assets/*bundle exec rake assets:precompileThen we need to also build bundler into the RPM as well, which requires a smidge of trickery:# For some reason bundler doesn't install itself, this is probably right,# but I guess it expects bundler to be on the server being deployed to# already. But the rails-helloworld app crashes on passenger looking for# bundler, so it would seem to me to be required. So, I used gem to install# bundler after bundle deployment. :) And the app then works under passenger.PWD=`pwd`cat &gt; gemrc &lt;&lt;EOGEMRCgemhome: $PWD/vendor/bundle/ruby/1.8gempath:- $PWD/vendor/bundle/ruby/1.8EOGEMRC        #gem --source %{gem_source} --config-file ./gemrc install bundler        gem --config-file ./gemrc install bundler# Don't need the gemrc any more...rm ./gemrcFinally, it seems that some of the gems have a funny location for ruby set, which we need to change because the rpmbuild process will mark that as a requirement. This issue may be fixed now.# Some of the files in here have /usr/local/bin/ruby set as the bang# but that won't work, and makes the rpmbuild process add /usr/local/bin/ruby# to the dependencies. So I'm changing that here. Either way it prob won't# work. But at least this rids us of the dependencie that we can never meet.for f in `grep -ril \"\\/usr\\/local\\/bin\\/ruby\" ./vendor`; do         sed -i \"s|/usr/local/bin/ruby|/usr/bin/ruby|g\" $f         head -1 $fdonepopdBasically, three major things happen in the build section:Use the handy bundler application to install all the required gemsAlso install bundler itselfWork around other issues as foundOnce that is done, we have a nice spec file that can be built and then installed!rpmbuildNow we build our RPM. In this example I’m building a RoR application called special_collections. rhel6b is my RHEL6 build server/environment.[curtis@rhel6b SPECS]$ rpmbuild -ba special_collections.spec Executing(%prep): /bin/sh -e /var/tmp/rpm-tmp.J1hbLc+ umask 022+ cd /home/curtis/rpmbuild/BUILD+ rm -rf ./special_collections+ git clone https://code.example.com/git/special_collectionsInitialized empty Git repository in /home/curtis/rpmbuild/BUILD/special_collections/.git/SNIP!Checking for unpackaged file(s): /usr/lib/rpm/check-files /home/curtis/rpmbuild/BUILDROOT/special_collections-0.1.4-1.el6.ualib.x86_64Wrote: /home/curtis/rpmbuild/SRPMS/special_collections-0.1.4-1.el6.ualib.src.rpmWrote: /home/curtis/rpmbuild/RPMS/x86_64/special_collections-0.1.4-1.el6.ualib.x86_64.rpmExecuting(%clean): /bin/sh -e /var/tmp/rpm-tmp.VOkPMU+ umask 022+ cd /home/curtis/rpmbuild/BUILD+ rm -rf /home/curtis/rpmbuild/BUILDROOT/special_collections-0.1.4-1.el6.ualib.x86_64+ exit 0NOTES:  The above rpmbuild could take a long time depending on the number of gems that the application requires. It’s important to rembember that in this process all the gems are being downloaded from rubygems.org and then also compiled on the build server, each and every time the rpm is built. So it’s slow. There are some things I’m looking at doing to reduce the time it takes to build the RPM, but that’s where it is right now. Maybe someone will read this blog and give me some comments on what I can be doing better!  The resulting RPM is quite large…in this case about 80MB compressed. This is because it has 100+ gems in it.Installing the RPM on a brand new serverI have a brand new server all ready for this ruby application to be deployed. It’s a minimal install.[root@RoR-TEST ~]# rpm -qa | grep -i \"apache\\|ruby\\|passenger\"[root@RoR-TEST ~]# # Nothing! No ruby, passenger, or apache currently installed.[root@RoR-TEST ~]# rpm -qa | wc -l293# And only 293 RPMs!Normally I install a RPM from a custom yum repository, but in this example I will use @yum localinstall@ so I copy the RPM from the build server to the new server.Note that I have several 3rd party repositories configured on this server, including epel, rpmforge, and the passenger repository. Obviously one has to trust a 3rd party repository to use it. Configuring yum priorities might be a good idea as well to try to avoid unwanted collisions.So, to install:[root@RoR-TEST tmp]# yum localinstall special_collections-0.1.4-1.el6.ualib.x86_64.rpm SNIP! rubygem-passenger-native-libs  x86_64  1:3.0.11-1.el6_1.8.7.352   passenger                                       29 k rubygem-rack                   noarch  1:1.1.0-2.el6              epel                                           446 k rubygem-rake                   noarch  0.8.7-2.1.el6              optional                                       403 k rubygems                       noarch  1.3.7-1.el6                optional                                       206 k sgml-common                    noarch  0.6.3-32.el6               base                                            43 kTransaction Summary========================================================================================================================Install      73 Package(s)Total size: 234 MTotal download size: 74 MInstalled size: 413 MIs this ok [y/N]: ySNIP!  rubygem-daemon_controller.noarch 0:0.2.6-1.el6                   rubygem-fastthread.x86_64 0:1.0.7-2.el6               rubygem-passenger.x86_64 1:3.0.11-1.el6                          rubygem-passenger-native.x86_64 1:3.0.11-1.el6        rubygem-passenger-native-libs.x86_64 1:3.0.11-1.el6_1.8.7.352    rubygem-rack.noarch 1:1.1.0-2.el6                     rubygem-rake.noarch 0:0.8.7-2.1.el6                              rubygems.noarch 0:1.3.7-1.el6                         sgml-common.noarch 0:0.6.3-32.el6                               Complete!Configure the applicationCurrently the RPM will create a directory in /etc/ that contains the database.yml file for the rails app:[root@RoR-TEST special_collections]# pwd/etc/railsapps/special_collections[root@RoR-TEST special_collections]# lsdatabase.ymlEdit that to set the proper database information.Configure apacheNow that apache has been installed because it is required by the custom RPM it needs to be configured.First let’s make sure it’ll start on a reboot. Don’t want to have to login on the weekend three months from now after a spontaneous reboot now do we? :)[root@RoR-TEST yum.repos.d]# chkconfig httpd onNow to setup the apache rails environment for this particular application. Note that in this case, we’re doing one RoR app per virtual host. It’s just easier for me because there are some variables that need to be set in the virtual host config file.I also always configure a /etc/httpd/conf.d/vhost.d directory for virtual host files, and tell httpd to check there for *.conf files.[root@RoR-TEST vhost.d]# grep vhost.d /etc/httpd/conf/httpd.conf Include conf.d/vhost.d/*.confThe vhost config file looks like this:[root@RoR-TEST vhost.d]# cat specialcollections.example.com.conf &lt;VirtualHost *:80&gt;   ServerName specialcollections.example.com   DocumentRoot /usr/share/railsapps/special_collections/public   # Because of the way we're deploying rails apps, ie. by using bundler during the rpm   # build process to install all the required gems into $RAILSAPP/$NAME/vendor/bundle/ruby/1.8   # this has to be set here. Otherwise the app will not have the required gems to run.   SetEnv GEM_HOME /usr/share/railsapps/special_collections/vendor/bundle/ruby/1.8/   &lt;Directory /usr/share/railsapps/special_collections/public&gt;        Options -MultiViews    &lt;/Directory&gt;&lt;/VirtualHost&gt;Startup apache:[root@RoR-TEST vhost.d]# service httpd configtest[root@RoR-TEST vhost.d]# service httpd startDone with apache.RakeNow to configure the initial database.First, the paths need to be setup. I create a file called special_collectionsrc that has path information setup. Note that this rc file is someting I created specifically for this application because each rails app will have it’s own paths and gems. Then, when wanting to use rake with the specific application that file is sourced to ensure the correct rake and other gems are used.[root@RoR-TEST ~]# which rake/usr/bin/rake# oops not the right one![root@RoR-TEST ~]# which bundle/usr/bin/which: no bundle in (/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin)# oops isn't on the path![root@RoR-TEST ~]# cat special_collectionsrc #!/bin/bashexport GEM_HOME=/usr/share/railsapps/special_collections/vendor/bundle/ruby/1.8PATH=/usr/share/railsapps/special_collections/vendor/bundle/ruby/1.8/bin:$PATHexport RAILS_ENV=productionOnce that file is sourced, we should be able to find rake on the path:[root@RoR-TEST ~]# source special_collectionsrc [root@RoR-TEST ~]# which rake/usr/share/railsapps/special_collections/vendor/bundle/ruby/1.8/bin/rake[root@RoR-TEST ~]# which bundle/usr/share/railsapps/special_collections/vendor/bundle/ruby/1.8/bin/bundlecd to /usr/share/railsapps/special_collections/ and load the db:[root@RoR-TEST special_collections]# rake db:load/usr/share/railsapps/special_collections/vendor/bundle/ruby/1.8/gems/curb-0.7.16/lib/curb_core.so: warning: already initialized constant CURL_SSLVERSION_DEFAULT-- create_table(\"collections\", {:force=&gt;true})   -&gt; 0.4194s-- create_table(\"gallery_images\", {:force=&gt;true})   -&gt; 0.0040s-- initialize_schema_migrations_table()   -&gt; 0.0077s-- assume_migrated_upto_version(20111104163654, [\"db/migrate\"])   -&gt; 0.0048sWhenever working with this particular RoR app the rc file should be sourced.Done raking.That’s…itAt this point the rails app should be available at the virtual host URL that was configured in the vhost. :)While it’s a long process to get that intial spec file and rpmbuild working, once it’s done the application can be deployed in a few minutes, and now the developers can simply worry about commiting and tagging code, and let the sysadmin deal with deploying the actual application in a replicable manner. Of course there will be some back and forth, new gems might not compile, etc, but the general structure is in place. Further, the deployment is quite automatable–a new tag could mean a new RPM build and deployment to test."
    } ,
  
    {
      "title"    : "Quickly building command line apps to create files from templates in python",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/10/13/quickly-building-command-line-apps-to-create-files-from-templates-in-python.html",
      "date"     : "2011-10-13 00:00:00 -0400",
      "content"  : "First, let me say that I am not an expert in Python. :) That said, I have been working on a script for a while now that creates a kickstart file from a command line application that uses:a configuration file to get defaults,command line options to add or override options, andcheetah template files.I think I finally have a good system for doing this, and could see it being useful to others in terms of writing their own quick application to generate some kind of text file from a command line application using the above three points, and what I have done is commited a set of skeleton files so that you could do this yourself very easily.Dependenciescli-template-generator was written to run on RHEL5 which means python 2.4.3.It also requires python-argparse and python-cheetah if you are on RHEL5.I think it will work in later python versions.Using the cli-template-generatorFirst, clone the cli-template-generator repository.$ git clone git@github.com:ccollicutt/cli-template-generator.gitCloning into cli-template-generator...remote: Counting objects: 8, done.remote: Compressing objects: 100% (6/6), done.remote: Total 8 (delta 0), reused 8 (delta 0)Receiving objects: 100% (8/8), done.$ cd cli-template-generator/$ lsREADME  skeleton.conf  skeleton.py  skeleton.tplThere are three files that work together to create a text file from a template: skeleton.{conf,py,tpl}.The tpl file contains the Cheetah template information:$ cat skeleton.tpl Hello $hello Which means the hello variable will be replaced with the string it’s set to in either the config file or the command line argument.The conf file holds the default configuration option(s).$ cat skeleton.conf [default]## Add default configuration options in this file# Eg.# Key:\tValuehello:\tWorld!By default, the skeleton.conf file sets the variable hello to the string World!. So if we run skeleton.py we will see:$ ./skeleton.py Hello World! Also by default we have one command line configuration option, --hello, that we can set to whatever we want. We can see what options are available using --help.$ ./skeleton.py --helpusage: skeleton.py [-h] [-c CONFIGFILE] [--hello HELLO]optional arguments:  -h, --help            show this help message and exit  -c CONFIGFILE, --config-file CONFIGFILE                        Use a different config file than ./skeleton.conf  --hello HELLO         Who are you saying hello to?So if we run skeleton.py with the --hello option we should get different results.$ ./skeleton.py --hello Curtis!Hello Curtis! So, as you can see, it should be fairly easy to copy the skeleton files, replace some of the default locations for the files, add some configuration options to the configuration file and also to the parser in the py file, and edit the template so that it uses your new variables.Bam! You have a custom cli text file generator! Given the amount of text files in Unix/Linux, there could be a lot of good uses for this."
    } ,
  
    {
      "title"    : "Getting the number of commits in mercurial, git, and svn",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/09/28/Getting-the-number-of-hg-git-svn-commits.html",
      "date"     : "2011-09-28 00:00:00 -0400",
      "content"  : "This is a short post on one way to get the number of commits in hg, git, and svn.One of the things that I wanted to graph with Cacti is the number of commits that happen in our git, mercurial, and svn repositories. Yup we use all three.I know that the number of commits isn’t the best metric in terms of figuring out how much our repos are being used, but it’s certainly one of the numbers to look at, and it’s easy to start with. I’m aware of things like churn in hg, but haven’t looked into them fully. Obviously one could make one large commit, or many smaller ones. I prefer many smaller ones, but that’s just me. Basically I’m saying I’ll add more metrics later.In order to graph the number of commits, I need to find the number of commits.At this point I’m most interested in the number of commits that happened in the last 24 hours, one day ago, or yesterday, which I’m aware are not necessarily all the same thing. ;) I’ll run the cronjob that checks commits just after midnight, so the numbers should be kinda accurate.hg$ cd some_hg_repo$ hg log --template '{rev}:{node|short}\\n'  --date -1 | wc -lgit$ cd some_git_repo$ git log --since=\"24 hours ago\" | grep \"^commit\" | wc -lsvn$ cd some_svn_repo# and SVN_REPO=`pwd` or something like that# Where YESTERDAY=`date --date yesterday +\\{\\%Y-\\%m-\\%d\\}`$ svn log -q -r $YESTERDAY file:///$SVN_REPO | grep \"^r\" | wc -lNote that with svn, if there hasn’t been a commit in since YESTERDAY it will return that last commit before that–could be two days ago or more–so unless there are no commits, the number of commits will be at least one, which may not be what you are expecting."
    } ,
  
    {
      "title"    : "OpenBSD pf and set limit states",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/09/12/openbsd-pf-set-limit-states.html",
      "date"     : "2011-09-12 00:00:00 -0400",
      "content"  : "So you have a OpenBSD firewall. Actually you have at least two because you are doing carp for high availability (not load balancing but HA), right?Awesome! It’s fun isn’t it? I suppose I have to admit it’s more fun testing it in a lab environment than in production. :)One thing I noticed when doing a bit of non-scientific load testing on a pair of small carped firewalls is that in OpenBSD the size of the state table is limited to 10000 entries by default. I would imagine that most people won’t run into the limit, but I was surprised at how easy it was to hit 10000 sessions using something like siege.Using a client laptop–an older core 2 duo with 4 gigs of ram and a 60 gig SSD drive (a Lenovo T61 specifically) on one side of the firewall and a virtualized web server with 512MB of RAM and one CPU on the other–I was able to hit the state limit in a couple of seconds with a command such as:laptop$ siege -b -c 40 -r 100 http://testserver/testpagewhere the resulting test page looks like:hi thereWhen the siege command is run I watch the state tables on the OpenBSD firewalls with a command such as:openbsd_fw{1,2}#  while true; do pfctl -s info; sleep 1; doneWith nothing happening the result of that command looks about like this:openbsd_fw1$ pfctl -s infoStatus: Enabled for 0 days 00:22:05              Debug: errState Table                          Total             Rate  current entries                       38                 searches                          215881          162.9/s  inserts                            30364           22.9/s  removals                           46342           35.0/sCounters  match                              30804           23.2/s  bad-offset                             0            0.0/s  fragment                               0            0.0/s  short                                362            0.3/s  normalize                              0            0.0/s  memory                                 0            0.0/s  bad-timestamp                          0            0.0/s  congestion                             0            0.0/s  ip-option                              0            0.0/s  proto-cksum                            0            0.0/s  state-mismatch                         0            0.0/s  state-insert                           0            0.0/s  state-limit                            0            0.0/s  src-limit                              0            0.0/s  synproxy                               0            0.0/sWe can see that the current limit is 10000:openbsd_fw1$ pfctl -sm     states        hard limit    10000src-nodes     hard limit    10000frags         hard limit     5000tables        hard limit     1000table-entries hard limit   200000So lets fire up that siege command and see what happens by watching the current entries on the firewall that has the master carp IP. (Note that with pfsync all the states will be transferred to the backup firewall as well, but for simplicity let’s focus on the master.)openbsd_fw1$ ifconfig | grep -i master        carp: MASTER carpdev fxp2 vhid 1 advbase 1 advskew 0        status: master        carp: MASTER carpdev fxp0 vhid 2 advbase 1 advskew 0        status: masteropenbsd_fw1$ while true; do pfctl -s info | grep \"current entries\"; sleep 1; done  current entries                       17                 current entries                       15                 current entries                       13                 current entries                       12                 current entries                       12 # siege starts up here                current entries                     4820                 current entries                    10000                 current entries                    10000                 current entries                    10000                 current entries                    10000                 current entries                    10000 ^CSo you can see it only takes a couple seconds to hit that limit.Let’s up it to 200000 and see what happens.openbsd_fw1$ grep \"set limit\" /etc/pf.confset limit states 200000openbsd_fw1$ pfctl -nf /etc/pf.confopenbsd_fw1$ pfctl -f /etc/pf.confopenbsd_fw1$ pfctl -smstates        hard limit   200000src-nodes     hard limit    10000frags         hard limit     5000tables        hard limit     1000table-entries hard limit   200000Run the siege command on the client:openbsd_fw1$ while true; do pfctl -s info | grep \"current entries\"; sleep 1; done   current entries                       40                 current entries                       40                 current entries                       40                 current entries                       40  # siege starts up here               current entries                      560                 current entries                     6686                 current entries                    12480                 current entries                    17728                 current entries                    23060                 current entries                    27116                 current entries                    28332                 current entries                    29498                 current entries                    29884                 current entries                    29884                 current entries                    29884                 current entries                    29884                 current entries                    29884                 current entries                    29884                 current entries                    29884                 current entries                    29884               ^CAnd looks like we max out the siege command now at about 30k sessions. Nice!To conclude, this was just a quick look at session limits on OpenBSD. If you’re running a pf firewall it may be something to consider looking at to make sure you’re not hitting the limit which would reduce the effectiveness of your firewall.Note that I haven’t shown any memory usage from the firewall, but the small boxes have 512MB of RAM and even at 200K sessions the memory usage only went up very slightly so I don’t think it’s constrained for memory reasons."
    } ,
  
    {
      "title"    : "Cacti, Better Cacti Graphs, and SSH Original Command",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/09/12/better-cacti-graphs-and-ssh-original-command.html",
      "date"     : "2011-09-12 00:00:00 -0400",
      "content"  : "So you’re at the point where you want to monitor your servers performance. Actually let’s take it a step further and you want to do this with Cacti. Actually let’s take another step and say that you’re going to use Cacti and Better Cacti Graphs (BCG)…and ssh.We’re getting pretty specific here aren’t we?So you setup Cacti, RPM up old BCG, and you configure ssh logins for BCG so that it can grab stats off the client system.But, and here’s the kicker–you want to make sure that you limit what the cacti user can do with a ssh login. (Possibly because your security analyst wants it to be “more secure” and yet has never actually used ssh.)Well, there is fairly little known functionality built into OpenSSH that will allow you to lock down what the user can do with a ssh key based login. The best thing to do at this point to learn more about this functionality would be to google SSH_ORIGINAL_COMMAND. Sorry, can’t help but use lmgtfy.com, no offense intended. :) It’s just fun to say.The point is that there are many (little known) options that can be put in a authorized_keys file to limit what the user can do with they login with that key.eg. The beginning of the key that I put in the cacti user’s authorized_keys file on the client server:command=\"/usr/share/cacti-ssh-auth/ssh_commands_check.sh\",from=\"SOME_IP_ADDRESS\",no-port-forwarding,no-X11-forwarding ssh-dss SNIP_REST_OF_KEY!What this setting does is:  Only allows the user to run the script that comes after command, meaning they can only run ssh_commands_check.sh, and it runs by default.  Only allow logins from SOME_IP_ADDRESS (eg. 10.0.4.30 or something), ie. the monitoring server where cacti is installed. Authentication via IP addresses isn’t the best idea, but why not.  Disable port forwarding and X11 forwarding for the session, always.The contents of the ssh_commands_check.sh script look like this:#!/bin/sh# When using $SSH_ORIGINAL_COMMAND to watch what commands we get out# of better-cacti-graphs, this is what we see:# cat /proc/diskstats# cat /proc/stat# wget -U Cacti/1.0 -q -O - -T 5 \"http://localhost/server-status?auto\"# uptime# free -obcase \"$SSH_ORIGINAL_COMMAND\" in        'cat /proc/diskstats')                cat /proc/diskstats                ;;        'wget -U Cacti/1.0 -q -O - -T 5 http://localhost/server-status?auto')                wget -U Cacti/1.0 -q -O - -T 5 \"http://localhost/server-status?auto\"                ;;        'uptime')                uptime                ;;        'free -ob')                free -ob                ;;        *)                # Then essentially do nothing b/c only the above                # commands are allowed to run. :)                # I don't really want to echo the actual command                # until I can find some way to escape anything                 # malicious. For another day!                logger -i \"$0 ERROR: disallowed command attempted\"                ;;esacGreat you say. But how did you find out exactly what commands cacti is trying to run? Well SSH_ORIGINAL_COMMAND to the rescue!I used something like:command=\"echo $SSH_ORIGINAL_COMMAND &gt;&gt; /var/tmp/ssh_check_cmd.txt; $SSH_ORIGINAL_COMMAND\",from=\"SOME_IP_ADDRESS\",no-port-forwarding,no-X11-forwarding ssh-dss SNIP_REST_OF_KEY!and then @tail -f /var/tmp/ssh_check_cmd.txt@ and saw this:client_server$ tail -f ssh_orig_cmd.txt cat /proc/diskstatscat /proc/statwget -U Cacti/1.0 -q -O - -T 5 \"http://localhost/server-status?auto\"uptimefree -obso we can be fairly sure that those are the commands the cacti monitor server is asking the client to run.NOTE that I would suggest checking what commands the ssh cacti scripts are running and not blindly using the ones I put above because things could have changed since I put up this post. And in fact could be completely wrong. It will take a bit of time to figure this out.Also you can review the code for BCG’s use of ssh to find out exactly what commands are running, but note that bash/ect might interpret things differently, so it’s best to check with SSH_ORIGINAL_COMMAND."
    } ,
  
    {
      "title"    : "Packaging code is about sharing",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/08/18/packaging-code-is-about-sharing.html",
      "date"     : "2011-08-18 00:00:00 -0400",
      "content"  : "I have been packaging code into RPMs for internal use for about five years now. I’m not going to say I’m an expert at it–I’m still learning–but I’m getting better at it and I think that’s a good thing.Recently I packaged a piece of software that didn’t have an existing RPM (AFAIK). I was then able to share that RPM, and installation instructions, with a partner institution so that they could easily install the software as well–in fact with one command. :)From one perspective it’s obvious that packaging software is about sharing. But often I spend so much time just getting the RPM built, which can mean the gruelling process of pulling requirements out of developers who think packaging is…not important or even intrusive, that I forget how it’s not just about easing sysadmin maintenance of servers; that it’s about being able to share systems and software with peers.I often wonder where some Linux users think packages come from. Certainly not a big white stork in the middle of the night. Dedicated volunteers (and I don’t mean me) are building thousands of packages every day! So hat tip to all those volunteers. Licensing is important, but so is packaging. :)"
    } ,
  
    {
      "title"    : "Booting partitions bigger than 2TB on a HP DL160 G6 with RHEL5",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/08/09/dl160-g6-2tb-partitions.html",
      "date"     : "2011-08-09 00:00:00 -0400",
      "content"  : "Yesterday I was working on a HP DL160 G6 server. Originally it had two 160GB hard-drives, but of course, I wanted more storage…a lot more. :) So we ordered four 2TB drives to put in it. Then I realized the backplane would only support two drives, so I had to order a backplane that can support four drives.Once all the parts arrived I replaced the backplane and put the four drives in. It was fairly simple actually. Then when I booted the server with the new backplane and disks the P410 RAID card noticed the new drives, and suggested configuring RAID 1+0, a suggestion I accepted. That leaves me with about 4TB usable.By default, the system creates one large drive of 4TB, which Redhat Enterprise 5 sees as /dev/sda.However, mbr, the default partition type on RHEL5, cannot boot partitions larger than 2TB. So, after the first install via kickstart I was missing 2TB. Not cool! Well, actually it’s fine, the computer did what it was told, but I wanted to use the rest.The solution? It was actually fairly easy. Maybe too easy. But it’s working.Because, it seems, the HP BIOS in this server supports UEFI/EFI/GPT/whatever, in the %pre section of the kickstart we can create a gpt partition.%pre# B/c sda on this server is 4.0TB we need to try to use gpt instead of mdr./usr/sbin/parted -s /dev/sda mklabel gptAlso make sure that if you have a clearpart command in your kickstart to comment it out or delete it.# Removing b/c of the parted in %pre#clearpart --drives=sda --all --initlabelThen run your kickstart as usual and hopefully your system will boot with whatever partitions you configured. In my case, I created two partitions, one 60GB for the system and the rest for virtual machines, and placed logical volumes over top:4TB_RAID10_SERVER$ pvs  PV         VG     Fmt  Attr PSize  PFree   /dev/sda2  system lvm2 a-   58.56g 36.66g  /dev/sda3  vm     lvm2 a-    3.58t  3.58tAnd running parted we can see that it is indeed a gpt layout:4TB_RAID10_SERVER$ parted /dev/sda printModel: HP LOGICAL VOLUME (scsi)Disk /dev/sda: 4001GBSector size (logical/physical): 512B/512BPartition Table: gptNumber  Start   End     Size    File system  Name  Flags 1      1049kB  525MB   524MB   ext4               boot 2      525MB   63.4GB  62.9GB                     lvm 3      63.4GB  4001GB  3937GB                     lvmSo now I can create that rsync backup server I’ve always wanted, and I have up to 3.58TB to store the backups on. Good times and I’m glad it all worked out. Now if only they were 3TB drives… ;)"
    } ,
  
    {
      "title"    : "ksplice bought out by Oracle, RHEL desupported",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/07/21/oracle-buys-ksplice.html",
      "date"     : "2011-07-21 00:00:00 -0400",
      "content"  : "I am extremely saddened to see that ksplice has been bought out by Oracle and will no longer be supporting Redhat Enterprise. I can only hope that the Linux community can find a way to provide this service outside of the limited confines of Oracle Corp."
    } ,
  
    {
      "title"    : "Basic infrastructure to support production linux servers",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/07/20/basic-linux-infrastructure.html",
      "date"     : "2011-07-20 00:00:00 -0400",
      "content"  : "Every IT group providing Linux servers will require some infrastructure services. What I mean by that is that there are services that Linux sysadmins need that help them to run their servers in a efficient, scalable way.Production services need support from infrastructure services.This post lists a basic collection of services that a Linux sysadmin might require. For me, this would be the minimum services required to run a group of Linux servers, be it 10 or 1000.Notes:I mention specific solutions, but there are many different ways to obtain the same basic infrastructure.Most of what I discuss below is geared towards Redhat servers, but is also completely applicable to any Linux disto that has a packaging system, which is pretty much all of them.1. Write documentation with mediawikiOne of the most important things a sysadmin does is document what they did so that they can take a relaxing, rejuvenating vacation. By this I mean that they have documented their systems so that the other admins taking over can use it to fix things, to understand what is installed, where it is, what it does, who owns it, ect, when the primary sysadmin is unavailable because he/she is in downtown Tokyo and is having a hard time connecting to the local wireless because they can’t read Japanese.Documentation is also important so you can remember what you did six months ago to get iscsi working, or how to configure a xen dom0 to use a bridge that comes from a vlan, or that command to dd a logical volume from one server to another over ssh without having to figure it out all over again. I suppose you could blog about it too. :)Regardless of what is being documented, usually the system I will use is a Mediawiki instance. Mediawiki supports searching (also full text searching if you use sphinx and the sphinx search plugin so that you can search in pre tags too), file uploads, categories, and many, many other features, especially via the diverse plugin/extension community.When people don’t want to use mediawiki I ask them what they want to do, how they want to work, and inevitably mediawiki can do it.2. Serve packages using mrepoMost Linux distros use some form of package management to install software. For example, in Debian/Ubuntu a deb file and in Redhat it’s an rpm.These packages, somewhat akin to an advanced zip file, contain all the files, requirements, metadata, ect, for a particular piece of software or service. The @./configure; make; make install@ dance is only done on the build servers.In most of the environments I work in, we strive to package all code. This means that all software and applications are installed on a server in a package. Configuration can be done by hand, or via a centralized configuration management system, but the code is actually installed via a package. This goes for Perl CPAN modules too. :) EVERYTHING.The package server provides a central spot that the servers obtain packages from, eg. a mrepo server.The packages server will do three major things:** Download RPM packages from external repositories, eg. the official Redhat repositories, EPEL, RPMForge, ect.Also it will allow us to store our own custom packages.This means our servers don’t go to the internet to get updates, they go to the packages server.** Serve those packages to all of our Linux servers.** Allow for the ability to  “freeze” the repositories so that we can install software updates in test environments, test them, and then install the exact same version of the packages/software in production, thus being a somewhat more sure that everything is going to work OK after the update.3. Centralize syslogging with rsyslogThere should be a central syslog server somewhere in your infrastucture, and all production servers should send syslog packets to that server.This will allow for a central spot for gathering syslog messages from all servers. We can then run scripts/processes to analyze these logs looking for issues.Also, should a server get hacked the first thing malicious users usually do is (try) to delete logs. But, if the logs have been sent to a central log server then they (probably) can’t do that.I usually replace, when possible, syslog with rsyslog and at minimum use TCP delivery instead of UDP.4. Centralize root email with sendmail or postfix and dovecotThere are many scripts on servers that will send email to root if something breaks, eg. cron or logwatch. So all of these emails should be sent to a central email address that sysadmins have access to.I’m not a huge fan of what is essentially logging over email, but it’s nearly impossible to avoid.Currently I configure each server’s root alias to email a central address which is delivered to a Maildir and serve that up over IMAPS with dovecot. Dovecot is pretty great.(Note that while sendmail is default on RHEL5, postfix is default on RHEL6!)5. Serve kickstarts over httpThe kickstart server is simply a plain http server that serves kickstart files, and in conjunction with the packaging server allows for rapid, repeatable installation of Redhat/CentOS. Or you could do it over nfs, or pop it on the USB key. I always use a web server to serve up the kickstarts.ks=http://example.com/ks/newserver.ks if you know what I mean. :)6. Centralize configuration managementThis is a central server that can be used to access all the Linux/Unix servers via ssh and to try automate with custom scripts or things like pdsh, chef, puppet, fabric, ect.Also stores a copy of every Linux servers RPM database so that if a server gets hacked you can check what files have changed, if any, on the hacked server. (Though that should be in backups too.)7. Build packages with rpmbuildTo create custom RPM packages a build server for each OS and architecture is required.  So if you run RHEL5 and RHEL 6 on x86_64 then you’ll need two build servers, one for each OS and arch.This is where @rpmbuild -ba some.spec@ will be run to build a RPM. Then the RPM will be copied to the packaging server.8. Login from anywhere securely with ssh(Of course with ssh!)Linux/Unix admins should be able to access a ssh gateway from the Internet to be able to access servers and possibly their workstations or just the central management server.Only public key authentication would be allowed to this server (meaning no password based authentication) which makes it very secure in terms of auth.Some workplaces will want to put this behind a commercial VPN. Try to avoid this…IMHO ssh is one of the, if not the, most secure network applications on the planet–certainly better than some million SLOC ssl “vpn”.PS. Did you know you can create a ssh-based vpn with something like sshuttle If you did sysadmin 2pts for you! :)9. Monitor uptime with nagiosEssentially a Nagios server, or similar, that monitors production services and servers and will let you know when they go down. I have also used hobbit, which is apparently called Xymon now.(But they won’t go down, right? In fact, the monitoring system will go down more than the production serivces, won’t it. :) I’ve always thought that was the hard part of uptime monitoring.)10. Manage code with version control systemsEvery IT workplace should have a central code repository. Sure, hg and git are distributed, but I still think it’s nice to have a central location. “Github”:github.com seems to be successful at centralizing distributed revision control, so I think centralizing a local git or hg instance (with hgweb.cgi for example) will work for most workplaces as well. :)I have a github account and have used svn and hg at work. Use whatever you want–just do it now!11. Backup with rdiff-backupThis would be a server used to rsync snapshot-type backups over ssh to disk for rapid restores. I like rdiff-backupIt would likely compliment that huge Netbackup, or Commvault (calmvault?), or other backup system you have that doesn’t work half the time and requires the installation of a gigantic root-running tarball or monolithic 600MB RPM which doubles the size of your 500MB minimal Redhat OS install. Good times in commercial backup land.12. Test with jmeterIf you have dev, test, production environments then you should also automate testing. This would be the service that helps you do that, running Jmeter and/or Selenium for example; do everything from one location, perhaps when a RPM/package is built then it’s automatically tested from here. Wouldn’t that be nice!?13. Monitor performance with cactiEveryone wants performance. Or do they? How does one know how much performance one needs? You have to do performance testing, and you have to monitor performance. You can’t just buy performance, you have to put some work in.That said, for monitoring performance I usually use either Nagios or something like Cacti using snmp with some custom scripts.14. Secure remote bios level access via a remote KVM or IPMIBy this I mean the secure network you attach your remote KVM to the hardware dom0 servers, and/or to the IPMI interfaces that most teir 1 servers come with (not that I endorse only tier 1 vendors) so that you don’t have to hang out in that cold, loud, unfriendly server room."
    } ,
  
    {
      "title"    : "Using fusion-io drives on Redhat Enterprise 5",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/06/27/fusionio-drives-on-redhat-enterprise-5.html",
      "date"     : "2011-06-27 00:00:00 -0400",
      "content"  : "%{color:red}Update:% Please note that this post is getting a bit old. Currently I am running these IBM FusionIO drives on RHEL 6. I’ll be posting about that and a few other PCIe-SSD subjects in the next short while. - 24 Apr 2012FusionIO IODrive OverviewSo at work we have a rather large IBM x3850 x5 server. It has 4 sockets each with six cores and hyperthreading (not that I’m necessarily a fan of hyperthreading–really I haven’t done enough research to make up my mind) which ends up with RHEL5 seeing 48 CPUS.$ cat /proc/cpuinfo | grep proc | wc -l48Fun.But the important part of this post is that this server also has three 640GB fusion-io drives which I have installed and configured as a volume group called fio$ ls /dev/fiofio/  fioa  fiob  fioc  fiod  fioe  fiof  $ vgs fio  VG   #PV #LV #SN Attr   VSize VFree  fio    6   4   0 wz--n- 1.76T 1.08Tand where the fio[a,b,c,d,e,f] are the drives, with each 640 gig card actually appearing as 2 320 gig disks.$ dmesg  |grep -i \"found device\"fioinf IBM 640GB High IOPS MD Class PCIe Adapter 0000:89:00.0: Found device 0000:89:00.0fioinf IBM 640GB High IOPS MD Class PCIe Adapter 0000:8a:00.0: Found device 0000:8a:00.0fioinf IBM 640GB High IOPS MD Class PCIe Adapter 0000:93:00.0: Found device 0000:93:00.0fioinf IBM 640GB High IOPS MD Class PCIe Adapter 0000:94:00.0: Found device 0000:94:00.0fioinf IBM 640GB High IOPS MD Class PCIe Adapter 0000:98:00.0: Found device 0000:98:00.0fioinf IBM 640GB High IOPS MD Class PCIe Adapter 0000:99:00.0: Found device 0000:99:00.0ResourcesThe most important resource for using these FusionIO drives is the official knowledge base which has several articles specifically for linux. I would suggest reading all of them. :)InstallOnce the cards were put into the server, which is somewhat harrowing given their individual cost, and the server was booted, the software drivers that were downloaded from the IBM website were installed. This server runs RHEL5$ cat /etc/redhat-release Red Hat Enterprise Linux Server release 5.6 (Tikanga)as that RHEL version that is what IBM supports for drivers.$ rpm -i iodrive-driver-1.2.7.5-1.0_2.6.18_164.el5.x86_64.rpm \\iodrive-firmware-1.2.7.6.43246-1.0.noarch.rpm \\iodrive-jni-1.2.7.5-1.0.x86_64.rpm \\iodrive-snmp-1.2.7.5-1.0.x86_64.rpm \\iodrive-util-1.2.7.5-1.0.x86_64.rpm \\Currently I am using the drivers as they were downloaded, which means using a specific matching kernel to match. The drivers do come with a source RPM so that you can rebuild them for your latest kernel, but I have opted not to do that yet. So install the matching kernel$ yum install kernel-2.6.18-164.el5and reboot.However, I am also using the amazing ksplice service to ensure that depsite the fact that I am using a rather old kernel to match the FusionIO drivers that the kernel is still up to date in terms of security issues:$ uptrack-uname -r2.6.18-238.12.1.el5$ uname -r2.6.18-164.el5The @uptrack-uname -r@ command asks uptrack what security equivalent version of the kernel is. Great stuff that kslplice.Once the drivers are installed we can load the modules$ modprobe fio-driverand now we can see the drives$ ls /dev/fio*fioa  fiob  fioc  fiod  fioe  fiof and at this point we can configure the drives.Worker processesOnce the drivers are installed there is a /etc/init.d/iodrive startup script. One of the things this script does is startup some worker processes which I believe are used to move data around the FusionIO drives to ensure their performance and longevity.$ chkconfig --list iodriveiodrive 0:off\t1:on\t2:on\t3:on\t4:on\t5:on\t6:off$ ps ax | grep worker 5271 ?        S&lt;   1169:51 [fct0-worker] 5588 ?        S&lt;   1168:07 [fct1-worker] 5593 ?        S&lt;   359:01 [fct2-worker] 5598 ?        R&lt;   206:02 [fct3-worker] 5603 ?        S&lt;   203:15 [fct4-worker] 5608 ?        S&lt;   203:12 [fct5-worker]20921 pts/2    S+     0:00 grep workerThese processes will take up some CPU time. Frankly, because there are 48 CPUs in this server, using up one to run these workers is OK. But it was a little confusing at first seeing all this activity–one worker process for each card.ConfigurationGiven that we are going to manage the FusionIO drives via LVM, we will need to configure LVM to allow it. See this knowledge base article.$ grep fio /etc/lvm/lvm.conf    types = [ \"fio\", 16 ]Then add each /dev/fio* drive as a phyical volume and then add them to a volume group.$ pvs | grep fio  /dev/fioa  fio    lvm2 a-   300.31G 320.00M  /dev/fiob  fio    lvm2 a-   300.31G 100.31G  /dev/fioc  fio    lvm2 a-   300.31G 100.31G  /dev/fiod  fio    lvm2 a-   300.31G 300.31G  /dev/fioe  fio    lvm2 a-   300.31G 300.31G  /dev/fiof  fio    lvm2 a-   300.31G 300.31G$ vgs fio  VG   #PV #LV #SN Attr   VSize VFree  fio    6   4   0 wz--n- 1.76T 1.08Tfio-statusUseful way to check the status of the FusionIO drives.$ fio-statusFound 6 ioDrives in this system with 3 ioDrive DuosFusion-io driver version: 1.2.7.5Adapter: ioDrive Duo\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:59518\tPCIE Power limit threshold: 24.75W\tConnected ioDimm modules:\t  fct0:\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:77479\t  fct1:\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:77478fct0\tAttached as 'fioa' (block device)\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:77479\tAlt PN:68Y7382\tLocated in 0 Upper slot of ioDrive Duo SN:59518\tFirmware v43246\t322.46 GBytes block device size, 396 GBytes physical device size\tInternal temperature: avg 56.6 degC, max 59.6 degC\tMedia status: Healthy; Reserves: 100.00%, warn at 10%fct1\tAttached as 'fiob' (block device)\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:77478\tAlt PN:68Y7382\tLocated in 1 Lower slot of ioDrive Duo SN:59518\tFirmware v43246\t322.46 GBytes block device size, 396 GBytes physical device size\tInternal temperature: avg 61.0 degC, max 63.0 degC\tMedia status: Healthy; Reserves: 100.00%, warn at 10%Adapter: ioDrive Duo\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:59507\tPCIE Power limit threshold: 24.75W\tConnected ioDimm modules:\t  fct2:\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:77143\t  fct3:\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:77144fct2\tAttached as 'fioc' (block device)\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:77143\tAlt PN:68Y7382\tLocated in 0 Upper slot of ioDrive Duo SN:59507\tFirmware v43246\t322.46 GBytes block device size, 396 GBytes physical device size\tInternal temperature: avg 62.0 degC, max 65.5 degC\tMedia status: Healthy; Reserves: 100.00%, warn at 10%fct3\tAttached as 'fiod' (block device)\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:77144\tAlt PN:68Y7382\tLocated in 1 Lower slot of ioDrive Duo SN:59507\tFirmware v43246\t322.46 GBytes block device size, 396 GBytes physical device size\tInternal temperature: avg 64.0 degC, max 66.4 degC\tMedia status: Healthy; Reserves: 100.00%, warn at 10%Adapter: ioDrive Duo\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:100366\tPCIE Power limit threshold: 24.75W\tConnected ioDimm modules:\t  fct4:\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:77344\t  fct5:\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:77345fct4\tAttached as 'fioe' (block device)\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:77344\tAlt PN:68Y7382\tLocated in 0 Upper slot of ioDrive Duo SN:100366\tFirmware v43246\t322.46 GBytes block device size, 396 GBytes physical device size\tInternal temperature: avg 68.9 degC, max 71.9 degC\tMedia status: Healthy; Reserves: 100.00%, warn at 10%fct5\tAttached as 'fiof' (block device)\tIBM 640GB High IOPS MD Class PCIe Adapter, Product Number:68Y7381 SN:77345\tAlt PN:68Y7382\tLocated in 1 Lower slot of ioDrive Duo SN:100366\tFirmware v43246\t322.46 GBytes block device size, 396 GBytes physical device size\tInternal temperature: avg 63.0 degC, max 66.0 degC\tMedia status: Healthy; Reserves: 100.00%, warn at 10%XFSPrior to finding out about the official knowledge base, I had decided to purchase a subscription from Redhat for the XFS file system. Then, upon reading this kb article, I found that they heavily recommend XFS as the file system to run on top of a FusionIO driveXFS is currently the recommended filesystem. It can achieve up to 3x the performance of a tuned ext2/ext3 solution. At this time, there is no know additional tuning for running XFS in a single- or multi-ioDrive configuration so that is the file system we use.$ mount | grep fio/dev/mapper/fio-vault1 on /var/lib/vault1 type xfs (rw)Mounting drives after a rebootI’ll admit I hadn’t thought of this during the initial installation. After a few days we moved the server to a new location which thus required a power down and restart.While the server was restarting, and I was standing in the cold, loud server room because the new room didn’t have any networking for IPMI (which is not good), I noticed it took a very long time to get past the udev portion of the boot, and in fact the FusionIO drives failed to mount from fstab. Of course there is a logical reason for that–read about it here.Because we are using the 1.2 driver, I followed the straight forward instructions here.Performance testingPerformance testing is hard. Maybe it’s just me. But testing superdisk like these FusionIO drives on a server with 48 CPUS and 64 gigs of main memory is not easy. Again I will admit I took a shot at benchmarking the FusionIO disk having not read the kb. I messed around with Bonnie++, io-whatever, but nothing quite came out right, partially because I didn’t put a lot of time into it, and because the server has so much memory that it makes it hard to beat the cache (I did try to reduce the memory the OS could see via kernel configuration, but didn’t have a lot of luck with that).Finally I read this kb article which suggested using the fio utility (which I don’t believe is a utility put out by FusionIO, rather just aptly named).The fio tool is not in the RHEL repositories but it is in rpmforge/repoforge.$ cd /var/tmp$ wget http://pkgs.repoforge.org/rpmforge-release/rpmforge-release-0.5.2-2.el5.rf.x86_64.rpm$ rpm -Uvh rpmforge-release-0.5.2-2.el5.rf.x86_64.rpm$ yum repolist | grep forgerpmforge                           RHEL 5Server - RPMforge.net - enabled: 10,636$ yum search fio | grep -i benchmarkfio.x86_64 : I/O benchmark and stress/hardware verification toolHere are a couple of example runs. Please note that at this point I do not know much about fio. Benchmarking disk is a highly technical thing to do, and getting tests right would take a lot of research and consideration, which I have not done.It seems that the fio benchmark utility suports direct=1 which means use non-buffered-io, thereby skipping memory cacheing and going straight to the disk.$ cat fio-randwrite.fio [randwrite[direct=1rw=randwrite bs=1m size=5G numjobs=4 runtime=10 group_reporting directory=/mnt/fio-test-xfs$ fio fio-randwrite.fio randwrite: (g=0): rw=randwrite, bs=1M-1M/1M-1M, ioengine=sync, iodepth=1...randwrite: (g=0): rw=randwrite, bs=1M-1M/1M-1M, ioengine=sync, iodepth=1fio 1.55Starting 4 processesrandwrite: Laying out IO file(s) (1 file(s) / 5120MB)randwrite: Laying out IO file(s) (1 file(s) / 5120MB)randwrite: Laying out IO file(s) (1 file(s) / 5120MB)randwrite: Laying out IO file(s) (1 file(s) / 5120MB)Jobs: 4 (f=4): [wwww] [100.0% done] [0K/522.8M /s] [0 /510  iops] [eta 00m:00s]randwrite: (groupid=0, jobs=4): err= 0: pid=28487  write: io=4556.0MB, bw=466161KB/s, iops=455 , runt= 10008msec    clat (msec): min=1 , max=1692 , avg= 9.83, stdev=22.04     lat (msec): min=1 , max=1692 , avg= 9.84, stdev=22.04    bw (KB/s) : min=  559, max=264126, per=24.79%, avg=115540.55, stdev=20377.90  cpu          : usr=0.10%, sys=14.85%, ctx=59071, majf=0, minf=92  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     issued r/w/d: total=0/4556/0, short=0/0/0     lat (msec): 2=0.53%, 4=1.27%, 10=97.17%, 20=0.59%, 50=0.09%     lat (msec): 100=0.18%, 250=0.15%, 2000=0.02%Run status group 0 (all jobs):  WRITE: io=4556.0MB, aggrb=466161KB/s, minb=477349KB/s, maxb=477349KB/s,  mint=10008msec, maxt=10008msecDisk stats (read/write):  dm-11: ios=0/158802, merge=0/0, ticks=0/55956241, in_queue=55915327,   util=66.05%, aggrios=0/159667, aggrmerge=0/0, aggrticks=0/55932489,  aggrin_queue=55785218, aggrutil=65.96%    fioc: ios=0/159667, merge=0/0, ticks=0/55932489, in_queue=55785218,     util=65.96%And then a similar test using RAID10 SAS disk formated ext3.$ cat fio-randwrite.fio [randwrite[direct=1rw=randwrite bs=1m size=5G numjobs=4 runtime=10 group_reporting directory=/mnt/sas-test$ fio fio-randwrite.fio randwrite: (g=0): rw=randwrite, bs=1M-1M/1M-1M, ioengine=sync, iodepth=1...randwrite: (g=0): rw=randwrite, bs=1M-1M/1M-1M, ioengine=sync, iodepth=1fio 1.55Starting 4 processesrandwrite: Laying out IO file(s) (1 file(s) / 5120MB)randwrite: Laying out IO file(s) (1 file(s) / 5120MB)randwrite: Laying out IO file(s) (1 file(s) / 5120MB)randwrite: Laying out IO file(s) (1 file(s) / 5120MB)Jobs: 4 (f=4): [wwww] [1200.0% done] [0K/0K /s] [0 /0  iops] [eta 1158050441d:07h:00m:05sJobs: 4 (f=4): [wwww] [inf% done] [0K/0K /s] [0 /0  iops] [eta 1158050441d:07h:00m:04s]  Jobs: 4 (f=4): [wwww] [1300.0% done] [0K/0K /s] [0 /0  iops] [eta 1158050441d:07h:00m:04sJobs: 4 (f=4): [wwww] [inf% done] [0K/0K /s] [0 /0  iops] [eta 1158050441d:07h:00m:03s]  Jobs: 1 (f=1): [___w] [66.1% done] [0K/0K /s] [0 /0  iops] [eta 00m:19s]               randwrite: (groupid=0, jobs=4): err= 0: pid=28586  write: io=4096.0KB, bw=112369 B/s, iops=0 , runt= 37326msec    clat (usec): min=12140K, max=37183K, avg=32696578.04, stdev= 0.00     lat (usec): min=12140K, max=37183K, avg=32696579.88, stdev= 0.00    bw (KB/s) : min=   27, max=   83, per=31.61%, avg=34.46, stdev= 0.00  cpu          : usr=0.00%, sys=51.90%, ctx=9598, majf=0, minf=102  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     issued r/w/d: total=0/4/0, short=0/0/0     lat (msec): &gt;=2000=100.00%Run status group 0 (all jobs):  WRITE: io=4096KB, aggrb=109KB/s, minb=112KB/s, maxb=112KB/s,   mint=37326msec, maxt=37326msecDisk stats (read/write):  dm-12: ios=128/4721384, merge=0/0, ticks=5582/602531980, in_queue=602926524,  util=97.85%, aggrios=129/87424, aggrmerge=0/4634618, aggrticks=5631/10828734,  aggrin_queue=10826088, aggrutil=98.01%    sdb: ios=129/87424, merge=0/4634618, ticks=5631/10828734, in_queue=10826088,    util=98.01%That’s a pretty big difference: io=4556.0MB for the FusionIO drives versus io=4096.0KB for the SAS RAID10. I’m going to have to look into this more! :)PS. I found this list of device bandwidths interesting."
    } ,
  
    {
      "title"    : "Installing chef on Centos 5",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/05/11/Installing-chef-on-centos-5.html",
      "date"     : "2011-05-11 00:00:00 -0400",
      "content"  : "Mirroring the FrameOS RPM repositoryInstalling Chef is pretty easy given that FrameOS has created all of the RPMs for us. See this blog post for more information.I mirror their repository on a local, centralized server using mrepo. Below is an example of my configuration.[root@repos etc]# cd /etc/mrepo.conf.d/[root@repos mrepo.conf.d]# cat chef.conf [chef]# FRAMEOS builds RPMS for chefhere: # http://blog.frameos.org/2011/04/14/announcing-rbel-frameos-org/## This might also be a good repo to use:# - http://download.elff.bravenet.com/5/x86_64/name = FrameOS rbel Chef RPMs $release ($arch)release = 5#arch = x86_64 i386arch = x86_64metadata = repomd repoview yum### Additional repositorieschef = http://rbel.frameos.org/stable/el$release/$arch/This repo is then available to my servers at http://repos/mrepo/chef-x86_64/RPMS.chef/.Installing chef-serverThen I created a CentOS 5 virtual machine called chef-server. I enabled the repo I mention above on that server, and then ran:[root@chef-server yum.repos.d]# yum install rubygem-chef-serverLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile * extras: ftp.telus.net * updates: reposSetting up Install ProcessResolving Dependencies--&gt; Running transaction checkSNIP!I added these iptables rules to /etc/sysconfig/iptables.# Chef# -- web interface-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 4040 -j ACCEPT# -- chef-server-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 4000 -j ACCEPT# -- amqp server-A RH-Firewall-1-INPUT -m state --state NEW -m multiport -p tcp --dport 5672,4369,50229 -j ACCEPT# -- search indexes (solr)-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 8983 -j ACCEPT# data store (couchdb)-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 5984 -j ACCEPTAnd then ran the setup script (after taking a look at what it does :) ).[root@chef-server sbin]# setup-chef-server.shChecking RabbitMQ...RabbitMQ not running. Starting...Starting rabbitmq-server: SUCCESSrabbitmq-server.Configuring RabbitMQ default Chef user...Starting CouchDB...Starting couchdb:                                          [  OK  ]Enabling Chef Services...Starting Chef Services...Starting chef-server:                                      [  OK  ]Starting chef-server-webui:                                [  OK  ]Starting chef-solr:                                        [  OK  ]Starting chef-expander:                                    [  OK  ]At which point I had a chef-server running, to which I can add clients and nodes.Installing chef-clientInstalling chef-client is also easily done with the provided rpms. I created another CentOS 5 virtual machine, called chef-client. (Actually I created many of them. It’s fun once you get it automated. :) )[root@chef-client ~]# yum install chef-client(Note that this may not be the preferred way to bootstrap a chef-client, but it has been working for me.)Then create a client.rb file in /etc/chef, where chef-server.example.com is the fqdn of your chef-server and is accessible on port 4000 from your chef-client.[root@chef-client2 chef]# cat client.rb log_level        :info    log_location     STDOUT    chef_server_url  'http://chef-server.example.com:4000'Next, copy the validation.pem file from the chef-server to /etc/chef on the chef-client, likely using scp (or, do it when the server is built in a kickstart file :) ).[root@chef-client2 chef]# lsclient.rb  validation.pemThen start chef-client.[root@chef-client2 chef]# service chef-client startBut in the /var/log/chef/client.log you will see an error that says client.pem is not present. This is good–chef-client will create the client.pem file.# Logfile created on [Date] 1 by logger.rb/22285 INFO: Daemonizing.. INFO: Forked, in 1762. Priveleges: 0 0 INFO: *** Chef 0.10.0 *** INFO: Client key /etc/chef/client.pem is not present - registering WARN: Failed to read the private key /etc/chef/validation.pem: #&lt;Errno::ENOENT: No such file or directory - /etc/chef/validation.pem&gt; ERROR: Chef::Exceptions::PrivateKeyMissing: I cannot read /etc/chef/validation.pem, which you told me to use to sign requests! FATAL: Stacktrace dumped to /var/chef/cache/chef-stacktrace.out ERROR: Sleeping for 1800 seconds before trying again FATAL: SIGTERM received, stoppingNow, restart chef-client so that new client.pem file can be used in conjuncation with the validation.pem file to register the node/client with the chef-server.[root@chef-client2 chef]# service chef-client restartAs long as the client.pem is there, the validation.pem is there, and the networking is OK, you should connect: INFO: *** Chef 0.10.0 *** INFO: Run List is [] INFO: Run List expands to [] INFO: Starting Chef Run for chef-client2.example.com INFO: Loading cookbooks [] WARN: Node chef-client2.example.com has an empty run list. INFO: Chef Run complete in 6.815418 seconds INFO: Running report handlers INFO: Report handlers complete FATAL: SIGTERM received, stopping INFO: Daemonizing.. INFO: Forked, in 2032. Priveleges: 0 0And now the client should appear in the node and client lists. (Note that I have not detailed how to add a user/client to the chef system, you’ll have to do that to use knife.)[someuser@chef-server ~]$ knife node list  chef-client2.example.comSNIP![someuser@chef-server ~]$ knife client list  chef-client2.example.comSNIP!(The SNIP!s mean I’ve removed some items for brevity.)Finally, run @chkconfig chef-client on@ on the chef-client to ensure the service starts at boot.Installing chef-client from a kickstart fileWhen building new vms I install chef-client from a kickstart file. This is also easily done!The first important option in the kickstart file is the repo option.repo --name=chef --baseurl=http://your_repo_server/mrepo/chef-x86_64/RPMS.chef/and then, in the %packages section simply add:rubygem-chefwhich will be installed from the chef repo configured in the repo option.Also, enable the service:services --enabled chef-clientFinally, in the %post section I add the below. Note the [PASTE THE CONTENTS OF YOUR VALIDATION.PEM HERE!!!] portion–that means put the results of @cat /etc/chef/validation.pem@ there, not that actual phrase. :)%post# chef-clientif [ ! -e /etc/chef ]; then        mkdir /etc/chefficat &gt; /etc/chef/client.rb &lt;&lt; EOCLRBlog_level        :info    log_location     STDOUT    chef_server_url  'http://chef-server.example.com:4000'EOCLRBchmod 600 /etc/chef/client.rbcat &gt; /etc/chef/validation.pem &lt;&lt; EOVALPEM[PASTE THE CONTENTS OF YOUR VALIDATION.PEM HERE!!!]EOVALPEMchmod 600 /etc/chef/validation.pemWhen the server built from this kickstart boots chef-client will startup. However, it will fail the first time it starts up because the client.pem had to be generated. But, the next time it starts up it will connect to the chef-server and register. If you want it to register right away, then ssh into the server and run @service chef-client restart@ and it should register."
    } ,
  
    {
      "title"    : "Installing Jekyll on Ubuntu 10.04",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/05/09/Installing-jekyll-on-ubuntu-10-04.html",
      "date"     : "2011-05-09 00:00:00 -0400",
      "content"  : "Another small post…given that I’ve recently changed jobs and thus have new workstation(s) to install and configure as I like them, and that I recently purchased a used Lenovo T61 laptop (which BTW is running very well on Ubuntu 10.04/Lucid, perhaps fodder for another post) I’ve repeated a several software installations lately on Lucid, including getting jekyll running locally so I can review blog posts before I send them up to github to run serverascode.com.When you run gem install jekyll on Lucid, you will recieve this error message:$ grep -i release /etc/lsb-release DISTRIB_RELEASE=10.04$ gem install jekyllERROR:  Error installing jekyll:\tliquid requires RubyGems version &gt;= 1.3.7Lucid comes with @gem 1.3.5@ which is not the version that Jekyll’s gem requires. When searching for the error message I found this post which describes one way of getting jekyll running on lucid, which is to install the gem package from Ubuntu 10.10. Now, obviously installing a package from what essentially is a different version of Ubuntu isn’t usually a recommended way to go, it’s certainly a quick and easy one (duh! :) ). I downloaded the rubygems1.8_1.3.7-2 package and installed it. Then I was able to run gem install jekyll and then run jekyll:$ jekyll --server --autoConfiguration from /ccollicutt.github.com/_config.ymlAuto-regenerating enabled: /ccollicutt.github.com -&gt; ccollicutt.github.com/_site[2011-04-20 13:20:25] regeneration: 12 files changed[2011-04-20 13:20:25] INFO  WEBrick 1.3.1[2011-04-20 13:20:25] INFO  ruby 1.8.7 (2010-01-10) [x86_64-linux][2011-04-20 13:20:30] INFO  WEBrick::HTTPServer#start: pid=7082 port=4000SNIP!It remains to be seen if I’ll run into issues with having the gem from Ubuntu 10.10 running on Ubuntu 10.04. I’ll update this post if I do. :)"
    } ,
  
    {
      "title"    : "Using LVM hosttags",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/04/13/using-lvm-hosttags.html",
      "date"     : "2011-04-13 00:00:00 -0400",
      "content"  : "This is a somewhat minor post, but I thought it would be worthwhile to take a peek at using LVM hosttags to manage dom0 access to logical volumes on top of SAN LUNs because there doesn’t seem to be a lot of documentation on using hosttags online. Perhaps that’s because no one is doing it this way.While it’s not my favorite way of managing SAN disks across servers (I like clvmd but it brings considerable complexity), hosttags are certainly one way to do it. Hosttags are a relatively simple method, and better than LVM filters IMHO. The point of using LVM hosttags is to ensure that only one server is ever writing to a SAN LUN at a time (unless you have something like GFS or similar in use, which is managing that process of multiple writers, and in that case what are you doing here? :) ).In this example we have three Redhat Enterprise 5.x dom0 servers connected to a large–and expensive–storage area network. We’ll call them vmhost1, vmhost2, and vmhost3. The LUNs are provided to the vmhosts via the SANs configuration software. The vmhosts see them as regular disk, but they aren’t regular disk because each of the servers can see them, where see means read and write. (Note that I may be using terms incorrectly, but by SAN LUN I mean the slice of SAN disk provided to the server over fibre channel.)[root@vmhost1 ~]# multipath -l | grep 00000002mpath3 (877880e80144455000001445500000002) dm-12 HITACHI,OPEN-V*4[root@vmhost2 ~]# multipath -l | grep 00000002mpath3 (877880e80144455000001445500000002) dm-11 HITACHI,OPEN-V*4[root@vmhost3 ~]# multipath -l | grep 00000002mpath3 (877880e80144455000001445500000002) dm-11 HITACHI,OPEN-V*4As is shown in the above output, each of the hosts can see the same SAN LUN: 877880e80144455000001445500000002 which in each case is also called mpath3. But I don’t really care about what names the disk is given because I run LVM on top of those disks.[root@vmhost2 ~]# pvs | grep mpath3  /dev/mapper/mpath3 some_volume_group LVM2 a-    96.62G  16.62GSo, we have a SAN LUN, a physical volume (PV) made from that LUN, and a volume group (VG) called some_volume_group created from that PV.So it sort of looks like this in terms of hierarchy:  Fibre Channel SAN LUN  multipathd  PV  VG  Logical volume (LV) which has hosttags assignedUsing hosttagsFirst we make sure we have hosttags configured on each of the vmhosts.[root@vhmost2 lvm]# pwd/etc/lvm[root@vmohost2 lvm]# grep hosttags lvm.conftags { hosttags = 1 }Obviously because I run using LVM hosttags in production, I already have hosttags configured and being used to control vmhost access to LVs.[root@vmohost2 lvm]# uname -nxmhost2.example.com[root@vmhost2 lvm]# lvdisplay @`uname -n` | grep \"LV Name\"SNIP!  lv Name                /dev/some_logical_volume/testIf I want to create a LV on a vmhost, and that VG is configured to use hosttags, then it has to be done properly. This is an example that will fail because the host does not have permission, ie. the LV is not available because of the lack of a hosttag attribute on the LV that names the @uname -n@ host specifically.[root@vmhost2 ~]# lvcreate -n test2 -L10.0G /dev/some_volume_group# Will fail out with error messageBut this next command will work, because we are saying create a LV and assign a hosttag to it which is the same as the hostname that is creating it. The vmhost can’t create a LV if it’s not made available via a hosttag.[root@vmhost2 ~]# uname -avmhost2.example.com[root@vmhost2 ~]# lvcreate --addtag @vmhost2.example.com -n test2 \\-L10.0G /dev/some_volume_groupNow, on vmhost1 we can see that the LV appears in the list, but it is not available:[root@vmhost1 ~]# lvs some_volume_group | grep test2  test2            some_volume_group -wi--- 10.00GBut it’s available on vmhost2, where it was created, and where it has a hosttag attribute of vmhost2.example.com.[root@vmhost2 ~]# lvs some_volume_group | grep test2   test2            some_volume_group -wi-a- 10.00GAnd, if you use @lvdisplay @uname -n@ you can see what LVs have the servers @uname -n@ tag:[root@vmhost2 ~]# lvdisplay @vmhost2.example.com | grep \"LV Name\"  LV Name                /dev/some_volume_group/test2whereas vmhost1 does not see that LV:[root@vmhost1 ~]# lvdisplay @vmhost1.example.com | grep \"LV Name\"# Nothing returns, as expectedSo, while this sounds complicated, it really isn’t. Essentially for a LV to be available to a server that can see the same LUNs as other servers, in terms of LVM, it must have its hosttag added to the LVs metadata. Otherwise, it’s not available and can’t be used on that host.While all three hosts can see the LV, it’s only available to those vmhosts that have had their hostname added to the specific LVs tags. It adds complexity to the vmhost setup and use, but it’s better to do this than to end up having two virtual machines writing to the same LV. Other options include using filtering in LVM, or even going all the way and using clvmd, which I have done, but that’s another story… :)"
    } ,
  
    {
      "title"    : "What is serverascode.com?",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/04/11/what-is-serverascode_com.html",
      "date"     : "2011-04-11 00:00:00 -0400",
      "content"  : "serverascode.com has two major functions:  to document how I am working through the process of learning to treat servers as code, and  to be my systems administration resume.Starting with #2, I believe that in my line of work–Unix/Linux systems administration–in todays “market”, a two page traditional resume will not get me the job I’m looking for. The only thing that will is a good set of open source contributions; that if servers are code, then much of what I do as a systems administrator should be available on-line in a source code repository. As I write this post, I only have a few lines of code available at github but as time goes on that will increase.As far as #1, I love to read about the latest way systems administrator are working; how they are being successful leveraging technology to manage large numbers of often diverse, highly interconnected systems while still ensuring they are highly operational and secure. I read sites like Hacker News and other blogs that describe a sort of “new age” methodology to systems administration; so called “devops” or “agile sysadmin” or just plain 2010+ systems administration.  However, I have not had an opportunity to apply those concepts, and I would like to use this blog, and services like github, to begin treating a @server as code@.For better or for worse, systems administration has changed greatly–and will continue to change–especially with the recent trend in virtualization. I believe at the core of this change is the concept of treating server(s) as code. My feeling is that in 2010+ we don’t admin a server, we code it. From procurement to deployment to maintenance to decommissioning–it’s all code now.On top of every Linux distribution, which can perhaps now be a called a OS framework, comes configuration conventions, and a packaging system. Then, over top of the framework, we add a centralized management server instance (perhaps the only server I should be logging into) which runs configuration management software, such as chef, puppet, and others, which control installation and configuration of applications, alerting things such as change management, and other systems I have not yet determined.Suffice it to say that I am looking forward to working towards treating servers as code and documenting that process in blog posts and, hopefully more-so, as code!"
    } 
  
]