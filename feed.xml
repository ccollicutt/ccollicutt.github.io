<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://serverascode.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://serverascode.com/" rel="alternate" type="text/html" /><updated>2024-10-20T06:38:08-04:00</updated><id>https://serverascode.com/feed.xml</id><title type="html">Server As Code Dot Com</title><subtitle>A techno-blog for our techno-times</subtitle><entry><title type="html">Incus Installation and Use - Setup Storage Pools and Bridged Networking</title><link href="https://serverascode.com/2024/10/19/incus-installation-and-use.html" rel="alternate" type="text/html" title="Incus Installation and Use - Setup Storage Pools and Bridged Networking" /><published>2024-10-19T00:00:00-04:00</published><updated>2024-10-19T00:00:00-04:00</updated><id>https://serverascode.com/2024/10/19/incus-installation-and-use</id><content type="html" xml:base="https://serverascode.com/2024/10/19/incus-installation-and-use.html"><![CDATA[<p>In this post I’ll show you how to install and setup <a href="https://linuxcontainers.org/incus/docs/main/">Incus</a> on a physical host running Ubuntu 24.04. I’ll setup a storage pool and a bridge network, then launch a VM. Once this is all done, I’ll have a nice homelab server that can spin up many virtual machines and do it quickly, putting them on the right storage pool, on a separate network.</p>

<h2 id="what-is-incus">What is Incus?</h2>

<blockquote>
  <p>Incus is a next-generation system container, application container, and virtual machine manager. It provides a user experience similar to that of a public cloud. With it, you can easily mix and match both containers and virtual machines, sharing the same underlying storage and network. - <a href="https://linuxcontainers.org/incus/docs/main/">Incus Docs</a></p>
</blockquote>

<p>Basically, once you install Incus you can ask it for virtual machines or system containers (not Docker containers, but system conatiners) and it will go and build them for you.</p>

<h2 id="physical-host">Physical Host</h2>

<p>I’m using Ubuntu 24.04 on my homelab server.</p>

<pre><code>$ cat /etc/lsb-release 
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=24.04
DISTRIB_CODENAME=noble
DISTRIB_DESCRIPTION="Ubuntu 24.04.1 LTS"
</code></pre>

<p>It’s an older server, but it’s got a lot of memory.</p>

<pre><code>$ free -h
               total        used        free      shared  buff/cache   available
Mem:           188Gi       3.1Gi       185Gi       673Mi       2.5Gi       185Gi
Swap:          8.0Gi          0B       8.0Gi
</code></pre>

<p>And lots of room for disks and such, including a 2TB NVMe drive, which I’ll use for my main storage pool.</p>

<h2 id="install-incus">Install Incus</h2>

<p>I’ll be following the Incus docs - <a href="https://linuxcontainers.org/incus/docs/main/installing/#installing">https://linuxcontainers.org/incus/docs/main/installing/#installing</a></p>

<p>First, install the incus and qemu packages; need qemu for the VM support.</p>

<pre><code>apt install incus qemu-system
</code></pre>

<p>Incus is a small set of packages, qemu is a fair bit larger.</p>

<p>Add your user to the incus group.</p>

<pre><code>$ sudo adduser curtis incus-admin
info: Adding user `curtis' to group `incus-admin' ...
</code></pre>

<p>Log out and log back in to get the new group, or use newgrp, whatever you want.</p>

<pre><code>$ incus ls
+------+-------+------+------+------+-----------+
| NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS |
+------+-------+------+------+------+-----------+
</code></pre>

<h2 id="storage-pool">Storage Pool</h2>

<p>I have a NVMe drive mounted on /mnt/nvme0n1 and I want to use that to back my incus managed virtual machines.</p>

<pre><code>$ sudo mkdir -p /mnt/nvme0n1/incus
$ incus storage create p1 dir source=/mnt/nvme0n1/incus
Storage pool p1 created
$ incus storage ls
+------+--------+--------------------+-------------+---------+---------+
| NAME | DRIVER |       SOURCE       | DESCRIPTION | USED BY |  STATE  |
+------+--------+--------------------+-------------+---------+---------+
| p1   | dir    | /mnt/nvme0n1/incus |             | 0       | CREATED |
+------+--------+--------------------+-------------+---------+---------+
</code></pre>

<p>Some files and directories are created in /mnt/nvme0n1/incus.</p>

<pre><code>$ ls /mnt/nvme0n1/incus/
buckets     containers-snapshots  custom-snapshots  virtual-machines
containers  custom                images            virtual-machines-snapshots
</code></pre>

<p>Very slick and easy to create the storage pool.</p>

<p>Now to build a VM using that storage pool.</p>

<pre><code>$ incus launch images:ubuntu/24.04 test --vm --storage p1
Launching test
                                          
The instance you are starting doesn't have any network attached to it.
  To create a new network, use: incus network create
  To attach a network to an instance, use: incus network attach
</code></pre>

<p>Note that I don’t have a network configured, so this didn’t actually start the VM.</p>

<p>But, files are created for the VM in the storage pool.</p>

<pre><code class="language-bash">$ sudo tree /mnt/nvme0n1/incus/
/mnt/nvme0n1/incus/
├── buckets
├── containers
├── containers-snapshots
├── custom
├── custom-snapshots
├── images
├── virtual-machines
│   └── test
│       ├── agent-client.crt
│       ├── agent-client.key
│       ├── agent.crt
│       ├── agent.key
│       ├── backup.yaml
│       ├── config
│       │   ├── agent.conf
│       │   ├── agent.crt
│       │   ├── agent.key
│       │   ├── agent-mounts.json
│       │   ├── files
│       │   │   ├── hostname.tpl.out
│       │   │   ├── hosts.tpl.out
│       │   │   └── metadata.yaml
│       │   ├── incus-agent
│       │   ├── install.sh
│       │   ├── lxd-agent -&gt; incus-agent
│       │   ├── nics
│       │   ├── server.crt
│       │   ├── systemd
│       │   │   ├── incus-agent.service
│       │   │   └── incus-agent-setup
│       │   └── udev
│       │       └── 99-incus-agent.rules
│       ├── metadata.yaml
│       ├── OVMF_VARS_4M.ms.fd
│       ├── qemu.nvram -&gt; OVMF_VARS_4M.ms.fd
│       ├── root.img
│       └── templates
│           ├── hostname.tpl
│           └── hosts.tpl
└── virtual-machines-snapshots

16 directories, 25 files

</code></pre>

<h2 id="networking">Networking</h2>

<p>OK, I love networking, but it can also be a pain, especially when we’re dealing with bridges and virtual machines, etc, etc. I like to think of networking as moving packets as quickly as possible, not configuring bridges, but there’s just no avoiding it.</p>

<p>The physical host has the below netplan configuration, where I’ve added a VLAN to eth3.</p>

<pre><code>$ sudo cat 50-cloud-init.yaml 
network:
    ethernets:
        eno1: {}
        eth3: {}
    version: 2
    vlans:
        eno1.101:
            addresses:
            - 10.100.1.20/24
            id: 101
            link: eno1
            nameservers:
                addresses:
                - 10.100.1.3
                search: []
            routes:
            -   to: default
                via: 10.100.1.1
        eth3.105:
            id: 105
            link: eth3
        eth3.106:
            id: 106
            link: eth3
</code></pre>

<p>I’m going to tell incus to create a bridge on a network interface that has a VLAN tag on it, eth3.106.</p>

<pre><code>$ incus network create br106 \
  --type=bridge \
  bridge.external_interfaces=eth3.106 \
  ipv4.dhcp=false \
  ipv4.nat=false \
  ipv6.nat=false \
  ipv4.address=none \
  ipv6.address=none
</code></pre>

<p>That command creates this config:</p>

<pre><code>$ incus network show br106
config:
  bridge.external_interfaces: eth3.106
  ipv4.address: none
  ipv4.dhcp: "false"
  ipv4.nat: "false"
  ipv6.address: none
  ipv6.nat: "false"
description: ""
name: br106
type: bridge
used_by:
- /1.0/instances/test
managed: true
status: Created
locations:
- none
</code></pre>

<p>DHCP is actually provided by my physical switch, not incus. So when I launch a VM, it starts with DHCP, but that DHCP address is coming from the upstream switch, not incus. This is what I want.</p>

<p>I can launch a VM with this network on the previously configured storage pool.</p>

<pre><code>$ incus launch images:ubuntu/24.04 test --vm --storage p1 --network br106
</code></pre>

<p>And list the VMs to see the new one, note that we can see the IP address of the VM even though Incus isn’t doing the IP address management.</p>

<pre><code>$ incus ls
+------+---------+---------------------+------+-----------------+-----------+
| NAME |  STATE  |        IPV4         | IPV6 |      TYPE       | SNAPSHOTS |
+------+---------+---------------------+------+-----------------+-----------+
| test | RUNNING | 10.100.6.250 (enp5s0) |      | VIRTUAL-MACHINE | 0         |
+------+---------+---------------------+------+-----------------+-----------+
</code></pre>

<p>Hop onto that VM and ping 1.1.1.1:</p>

<pre><code>$ incus shell test
root@test:~# ping -c 3 1.1.1.1
PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data.
64 bytes from 1.1.1.1: icmp_seq=1 ttl=53 time=5.80 ms
64 bytes from 1.1.1.1: icmp_seq=2 ttl=53 time=3.43 ms
64 bytes from 1.1.1.1: icmp_seq=3 ttl=53 time=3.47 ms

--- 1.1.1.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 3.425/4.232/5.802/1.110 ms
</code></pre>

<p>Network is online!</p>

<p>For various reasons I use Mikrotik switches/routers in my homelab, so this interface might look different on your network. Obviously I don’t have a lot of DHCP going on. :)</p>

<pre><code>[admin@MikroTik] &gt; /ip dhcp-server lease print 
Flags: X - disabled, R - radius, D - dynamic, B - blocked 
 #   ADDRESS                                                                    MAC-ADDRESS       HOST-NAME                                 SERVER                                 RATE-LIMIT                                 STATUS 
 0 D 10.100.6.250                                                                 00:16:3E:4D:15:97 distrobuilder-705ecd65-121a-4b5b-8cdc-... dhcp1                                                                             bound  
[admin@MikroTik] &gt; 
</code></pre>

<p>And we can see the DHCP lease is for the VM.</p>

<h2 id="incus-profiles">Incus Profiles</h2>

<p>Finally, I’ll create a profile for the VM, or rather I’ll edit the default profile to use the bridge network and the storage pool.</p>

<pre><code class="language-bash">$ incus profile show default
config: {}
description: Default Incus profile
devices:
  eth0:
    network: br106
    type: nic
  root:
    path: /
    pool: p1
    type: disk
name: default
used_by:
- /1.0/instances/test
</code></pre>

<h2 id="conclusion">Conclusion</h2>

<p>And there you have it. Incus is now managing my virtual machines, putting them on my storage pool, and giving me a bridge network with IPs from my DHCP server.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[In this post I’ll show you how to install and setup Incus on a physical host running Ubuntu 24.04. I’ll setup a storage pool and a bridge network, then launch a VM. Once this is all done, I’ll have a nice homelab server that can spin up many virtual machines and do it quickly, putting them on the right storage pool, on a separate network.]]></summary></entry><entry><title type="html">Why Aren’t You Using Incus to Create Containers and Virtual Machines?</title><link href="https://serverascode.com/2024/10/12/incus.html" rel="alternate" type="text/html" title="Why Aren’t You Using Incus to Create Containers and Virtual Machines?" /><published>2024-10-12T00:00:00-04:00</published><updated>2024-10-12T00:00:00-04:00</updated><id>https://serverascode.com/2024/10/12/incus</id><content type="html" xml:base="https://serverascode.com/2024/10/12/incus.html"><![CDATA[<h1 id="incus">Incus</h1>

<p>Virtual machines remain the main building block of pretty much all infrastructure. We tend to forget about the technology and just how entrenched it is in our daily technical lives. Ok, that’s a pretty heavy statement for a blog post, but I do think we forget about virtual machines and just how valuable and secure the technology is–most public cloud services are loss leaders for the VM part of their business.</p>

<p>Anyways, what I want to talk about is <a href="https://linuxcontainers.org/incus/introduction/">Incus</a>, a way to easily create containers AND virtual machines.</p>

<blockquote>
  <p>NOTE: I use Incus to exclusively create virtual machines, and don’t use the container functionality that much. I would imagine that most people use the container functionality more. So while I’ll touch on the system container functionality, I use Incus for local VMs.</p>
</blockquote>

<h2 id="what-is-incus">What is Incus?</h2>

<blockquote>
  <p>When using Incus, you can manage your instances (containers and VMs) with a simple command line tool, directly through the REST API or by using third-party tools and integrations. Incus implements a single REST API for both local and remote access. The Incus project was created by Aleksa Sarai as a community driven alternative to Canonical’s LXD. Today, it’s led and maintained by many of the same people that once created LXD. - <a href="https://linuxcontainers.org/incus/introduction/">Incus Docs</a></p>
</blockquote>

<p>With Incus, you can easily create virtual machines and containers.</p>

<p>I don’t know all the history of the project, where it comes from in terms of LXD/LXC, etc, but I do know that I need a way to easily create virtual machines on my local computer, and that I really enjoy using Incus. So easy.</p>

<p>Example of creating a virtual machine:</p>

<pre><code class="language-bash">incus launch ubuntu:22.04 my-server --vm
</code></pre>

<p>It’s that easy. Especially on Ubuntu 24.04, where you can just install the <code>incus</code> package from the default repositories.</p>

<p>I also alias the incus command to this script because I always forget the incus command syntax, and I’m super lazy. So this would create a default sized VM with just <code>vm launch my-vm</code>.</p>

<pre><code class="language-bash">#!/bin/bash

launch_vm() {
  if [ -z "$1" ]; then
    echo "Usage: $0 launch &lt;vm-name&gt;"
    echo "Example: $0 launch my-ubuntu-vm"
    exit 1
  fi

  local name="$1"
  local cpu=2
  local memory="4GiB"
  local disk="40GiB"

  incus launch images:ubuntu/24.04 "$name" --vm \
    --device root,size="$disk" \
    -c limits.cpu="$cpu" \
    -c limits.memory="$memory"
}

list_vms() {
  incus ls
}

show_help() {
  echo "Usage: $0 &lt;command&gt; [options]"
  echo
  echo "Commands:"
  echo "  launch &lt;vm-name&gt;  Launch a new VM"
  echo "  ls                List all VMs"
  echo "  help              Show this help message"
  echo
  echo "For other commands, this script will pass them directly to incus."
}

# Main command handler
case "$1" in
  launch)
    launch_vm "$2"
    ;;
  ls)
    list_vms
    ;;
  help)
    show_help
    ;;
  *)
    if [ -z "$1" ]; then
      show_help
    else
      # If the command isn't recognized, pass it to incus
      incus "$@"
    fi
    ;;
esac
</code></pre>

<h2 id="using-incus">Using Incus</h2>

<p>As mentioned earlier, I almost exclusively use Incus to get a virtual machine.</p>

<p>E.g. with my script I just run:</p>

<pre><code class="language-bash">vm launch a-vm
</code></pre>

<p>Or with the bare Incus command it’s just as easy:</p>

<pre><code class="language-bash">incus launch images:ubuntu/24.04 a-vm --vm
</code></pre>

<p>Now I can shell into the VM very quickly.</p>

<pre><code class="language-bash">$ incus shell a-vm # or with my script, vm shell a-vm
root@a-vm:~# 
</code></pre>

<p>And you are in a nice little virtual machine that you can install anything you want into.</p>

<h2 id="getting-a-container">Getting a Container</h2>

<p>Writing this post was the first time I used the container functionality of Incus! Getting a container is the default mode of operation, and it’s super easy.</p>

<pre><code class="language-bash">$ incus launch images:ubuntu/22.04 ubuntu-container
# Image is downloaded, and the container is created
Launching ubuntu-container
$ incus ls | grep ubuntu-container
| ubuntu-container | RUNNING | 10.57.7.201 (eth0)           | fd42:af1f:b7c8:a36c:216:3eff:fee9:32e4 (eth0)   | CONTAINER       | 0         |
$ vm shell ubuntu-container
</code></pre>

<p>That is lightning fast. But again, important to note: this is a “system container” and not a “application container”, or in simpler terms, it’s not a docker container. If you have ever used LXC, then you will be right at home.</p>

<blockquote>
  <p>Application containers (as provided by, for example, Docker) package a single process or application. System containers, on the other hand, simulate a full operating system similar to what you would be running on a host or in a virtual machine. You can run Docker in an Incus system container, but you would not run Incus in a Docker application container. - <a href="https://linuxcontainers.org/incus/docs/main/explanation/containers_and_vms/">Incus Docs</a></p>
</blockquote>

<p>You may also want to understand the differences between a virtual machine and a system container:</p>

<blockquote>
  <p>Virtual machines create a virtual version of a physical machine, using hardware features of the host system. The boundaries between the host system and virtual machines is enforced by those hardware features. System containers, on the other hand, use the already running OS kernel of the host system instead of launching their own kernel. If you run several system containers, they all share the same kernel, which makes them faster and more lightweight than virtual machines. - <a href="https://linuxcontainers.org/incus/docs/main/explanation/containers_and_vms/">Incus Docs</a></p>
</blockquote>

<h2 id="why-use-incus">Why Use Incus?</h2>

<p>You can see a list of major features <a href="https://linuxcontainers.org/incus/introduction/#features">here</a> but what I like about it might not be the same as what you like about it.</p>

<ol>
  <li>It’s very fast - There is an agent in the image that makes getting a shell into the VM super fast. The images are also small and download like lightning, at least in my experience.</li>
  <li>It’s easy to manage - Incus has a simple syntax for launching VMs and containers</li>
  <li>Image based - Incus uses images, instead of futzing around with qemu backing files and such</li>
  <li>You use Linux as your workstation and need to easily get a VM, or a system container</li>
</ol>

<p>You can also try it online: <a href="https://linuxcontainers.org/incus/try-it/">https://linuxcontainers.org/incus/try-it/</a></p>

<h2 id="why-wouldnt-you-use-incus">Why Wouldn’t You Use Incus?</h2>

<ol>
  <li>It’s not Docker - it’s a different style of containerization, which many people are not used to.</li>
  <li>It’s relatively new, and a lot of work is being done on it - But on Ubuntu 24.04 it’s easy to install and get started.</li>
  <li>I do have some trouble with outbound access from the VMs and have futzed around with Iptables to get it working, but it’s not as easy as one would think–I expect I’m missing something obvious from the docs.</li>
</ol>

<p>That’s about all I can think of.</p>

<h2 id="pairing-incus-with-my-kubernetes-install-script">Pairing Incus with My Kubernetes Install Script</h2>

<p>FYI - If you create an 8GB VM with 4 CPUS, my <a href="https://github.com/ccollicutt/install-kubernetes">single node Kubernetes install script</a> pairs nicely with Incus, and is often what I use it for.</p>

<h2 id="incus-66-was-just-released">Incus 6.6 Was Just Released</h2>

<p>See <a href="https://discuss.linuxcontainers.org/t/incus-6-6-has-been-released/21762">here</a>. There is also a video overview of the new features <a href="https://www.youtube.com/watch?v=gGBEPtQiiQQ">here</a></p>

<p>Install it, try it out. Have fun easily creating VMs and containers!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Incus]]></summary></entry><entry><title type="html">Building an Insecure App…on Purpose (So That GenAI Can Fix It)</title><link href="https://serverascode.com/2024/10/02/building-an-insecure-app.html" rel="alternate" type="text/html" title="Building an Insecure App…on Purpose (So That GenAI Can Fix It)" /><published>2024-10-02T00:00:00-04:00</published><updated>2024-10-02T00:00:00-04:00</updated><id>https://serverascode.com/2024/10/02/building-an-insecure-app</id><content type="html" xml:base="https://serverascode.com/2024/10/02/building-an-insecure-app.html"><![CDATA[<h2 id="tldr">tldr;</h2>

<p>tldr; I built an insecure web application (on purpose) for testing LLMs and here it is: <a href="https://github.com/ccollicutt/insecure-nextjs-guestbook">https://github.com/ccollicutt/insecure-nextjs-guestbook</a>.</p>

<h2 id="dealing-with-technical-debt-using-genai">Dealing with Technical Debt using GenAI</h2>

<p>Is cybersecurity largely a technical issue? An engineering issue? It’s difficult to say. Certainly human psychology plays a big part of it, but, then again, we’re building (insecure) software things and putting them out into the world. We write billions of lines of code, and we can’t do that without making mistakes…so there are billions of mistakes too. That code has bugs, it gets worse over time, and is hard (read: expensive) to maintain. It ends up being a technical liability–a security liability. The reality of software development is an important part of the cybersecurity story. Not the whole story, but an important part.</p>

<p>For this line of thinking, the question is, can Generative Artificial Intelligence (GenAI) help us deal with all this overwhelming technical debt? I believe that GenAI can code, and code well enough to help us get rid of technical debt. And what’s more, this ability can be automated and has the potential to be fast–very fast–so it can potentially take care of a lot of technical debt in a short period of time. Now, not everyone may agree with me, but that’s my opinion, and I’m sticking to it!</p>

<p>So if you believe, or can suspend your disbelief, that GenAI can help you deal with technical debt, then you’ll want to test it, just like I do. But how do you test code that generates code?</p>

<p>I’m going to build an insecure web application and then use GenAI to try to fix it.</p>

<h2 id="what-does-insecure-mean">What Does “Insecure” Mean?</h2>

<p>However, building an insecure web application is a bit of a challenge. On the one hand, we have all kinds of technical debt that’s easy to accumulate in the real world, but on the other hand, when we write a new application, the frameworks, libraries, and tools we use are working behind the scenes to keep us as secure as possible, so in some ways it’s a challenge to build an insecure application, at least out of the gate.</p>

<p>And yet I managed to do it. At least partially.</p>

<p>So, what is an insecure web application? What are common examples of insecurity in a web application?</p>

<h2 id="owasp-top-10">OWASP Top 10</h2>

<p>One way to think about web app vulnerabilities is through the <a href="https://owasp.org/www-project-top-ten/">OWASP Top 10</a>.</p>

<p>Here’s the current OWASP Top 10, as of 2021:</p>

<ul>
  <li>A01:2021-Broken Access Control</li>
  <li>A02:2021-Cryptographic Failures</li>
  <li>A03:2021-Injection</li>
  <li>A04:2021-Insecure Design</li>
  <li>A05:2021-Security Misconfiguration</li>
  <li>A06:2021-Vulnerable and Outdated Components</li>
  <li>A07:2021-Identification and Authentication Failures</li>
  <li>A08:2021-Software and Data Integrity Failures</li>
  <li>A09:2021-Security Logging and Monitoring Failures</li>
  <li>A10:2021-Server-Side Request Forgery (SSRF)</li>
</ul>

<p>Let’s look at A01:2021-Broken Access Control, as defined by OWASP:</p>

<ul>
  <li>Violation of the principle of least privilege or deny by default, where access should only be granted for particular capabilities, roles, or users, but is available to anyone.</li>
  <li>Bypassing access control checks by modifying the URL (parameter tampering or force browsing), internal application state, or the HTML page, or by using an attack tool modifying API requests.</li>
  <li>Permitting viewing or editing someone else’s account, by providing its unique identifier (insecure direct object references)</li>
  <li>Accessing API with missing access controls for POST, PUT and DELETE.</li>
  <li>Elevation of privilege. Acting as a user without being logged in or acting as an admin when logged in as a user.</li>
  <li>Metadata manipulation, such as replaying or tampering with a JSON Web Token (JWT) access control token, or a cookie or hidden field manipulated to elevate privileges or abusing JWT invalidation.</li>
  <li>CORS misconfiguration allows API access from unauthorized/untrusted origins.</li>
  <li>Force browsing to authenticated pages as an unauthenticated user or to privileged pages as a standard user.</li>
</ul>

<p>Forced browsing, as an example, sounds fun and technical–but it’s really just about browsing pages you aren’t supposed to know exist, pages that just happen to have additional permissions or access that the average user doesn’t have.</p>

<blockquote>
  <p>Forced browsing is an attack where the aim is to enumerate and access resources that are not referenced by the application, but are still accessible. An attacker can use Brute Force techniques to search for unlinked contents in the domain directory, such as temporary directories and files, and old backup and configuration files. These resources may store sensitive information about web applications and operational systems, such as source code, credentials, internal network addressing, and so on, thus being considered a valuable resource for intruders. - <a href="https://owasp.org/www-community/attacks/Forced_browsing">https://owasp.org/www-community/attacks/Forced_browsing</a></p>
</blockquote>

<p>And, in a similar vein, (typically SQL) injection, as defined by OWASP:</p>

<ul>
  <li>User-supplied data is not validated, filtered, or sanitized by the application.</li>
  <li>Dynamic queries or non-parameterized calls without context-aware escaping are used directly in the interpreter.</li>
  <li>Hostile data is used within object-relational mapping (ORM) search parameters to extract additional, sensitive records.</li>
  <li>Hostile data is directly used or concatenated. The SQL or command contains the structure and malicious data in dynamic queries, commands, or stored procedures.</li>
</ul>

<p>Some of these are more interesting than others, and for some–it’s hard to believe that they are still happening in 2024.</p>

<h2 id="building-an-insecure-web-app">Building an Insecure Web App</h2>

<p><img src="/img/insecure-webapp-guestbook.jpg" alt="img" /></p>

<p>While there are a handful of “webgoat”-style applications that will help you learn about these vulnerabilities, I decided to build my own so that I would know exactly what problems I was introducing - and thus I would know exactly what problems I was trying to fix with GenAI.</p>

<p>I was working on learning NodeJS and NextJS, so I decided to build my insecure web application using those technologies.</p>

<p>A few points:</p>

<ul>
  <li>
    <p>I wanted to make a guestbook app of all things because it would be easy to build, and the fact that anyone should be able to post to it would make it easier to introduce vulnerabilities.</p>
  </li>
  <li>
    <p>I immediately put the clear text authentication into a SQLite database. However, in the real world, no one would put cleartext authentication in a SQLite database–or even use their own authentication system. There are many, many libraries and SaaS services that provide authentication as a service, which is much, much more secure, and that is what people will use. (That is, they’re not as easy to configure, and they’re error-prone, but they’re still much more secure than doing it yourself). I imagine most people building a new web application would either use a third party or <a href="https://next-auth.js.org/">NextAuth</a>.</p>
  </li>
  <li>
    <p>There is an admin user with a default password.</p>
  </li>
</ul>

<pre><code>// Insert admin user if not exists
db.get(`SELECT * FROM users WHERE username = 'admin'`, (err, row) =&gt; {
  if (!row) {
    db.run(`INSERT INTO users (username, password, admin) VALUES ('admin', 'admin', 1)`);
  }
});
</code></pre>

<ul>
  <li>I wanted it to be susceptible to SQL injection–but interestingly, SQLite does a lot of work to prevent that, so I had to do some work to make it vulnerable in terms of using raw queries. For the most part, SQLite just does the right thing, and you have to do some work to make it vulnerable.</li>
</ul>

<pre><code>// Vulnerable to SQL injection
const query = `SELECT * FROM users WHERE username = '${username}' AND password = '${password}'`;
</code></pre>

<ul>
  <li>
    <p>I also added an admin page that was supposed to be protected, but wasn’t.</p>
  </li>
  <li>
    <p>Originally the app didn’t use a sessionID in the URL, but I added that to make the webapp EVEN MORE VULNERABLE. But you don’t see sessionIDs in the wild, so I’m not sure if that’s realistic.</p>
  </li>
</ul>

<p>What I have so far is a webapp that is vulnerable to a number of attacks, including</p>

<ul>
  <li>SQL injection</li>
  <li>Forced browsing</li>
  <li>Session hijacking</li>
  <li>Probably Cross-Site Scripting (XSS)</li>
</ul>

<h2 id="testing-the-vulnerabilities">Testing the Vulnerabilities</h2>

<p>In addition to building the insecure application, we need to test for the presence of these vulnerabilities. So there is also a script to test for them. Please note that this is not an exhaustive list of vulnerabilities, but rather a set of examples meant to be illustrative, and in fact many of them do not work.</p>
<pre><code>$ ./tests.sh 
Usage: ./tests.sh [test_name]

Available tests:
  login                  Test common logins
  sql_injection          Run SQL Injection Test
  drop_table             Drop messages table with SQL Injection
  xss                    Run Cross-Site Scripting (XSS) Test
  insecure_auth          List all users and get admin password via SQL Injection
  sensitive_data         Run Sensitive Data Exposure Test
  security_misconfig     Run Security Misconfiguration Test
  known_vulnerabilities  Run Known Vulnerabilities Test
  insufficient_logging   Run Insufficient Logging &amp; Monitoring Test
  list_tables_and_entries List all tables and entries in the database
  help                   Display this help message
  list_users             List all users in the database
  list_nonexistent_users List all users in the database
  list_tables_and_entries List all tables and entries in the database
  list_nonexistent_users List all users in the database
</code></pre>

<p>Here’s an example of SQL injection:</p>

<pre><code>$ ./tests.sh sql_injection
###################################################
# Running SQL Injection Test to create admin user #
###################################################
Step 1: Attempting SQL injection to create admin user...
SQL Injection Response: {"message":"Login successful","sessionId":"d86976cace3f01e5ae248e037483d70d","isAdmin":true,"redirectUrl":"/?sessionId=d86976cace3f01e5ae248e037483d70d&amp;username=admin' --&amp;isAdmin=true"}

Step 2: Inserting hacker user with admin privileges...

Step 3: Attempting to login as the new admin user 'hacker'...
Login response: {"message":"Login successful","sessionId":"d2cfc602ff3fd33d201d69f0fac9bdd2","isAdmin":true,"redirectUrl":"/?sessionId=d2cfc602ff3fd33d201d69f0fac9bdd2&amp;username=hacker&amp;isAdmin=true"}
User 'hacker' logged in successfully with admin privileges. SQL Injection successful.

Step 4: Checking database for 'hacker' user...
19|hacker|hackpass|1

Step 5: Listing all users in the database...
1|admin|admin|1
2|test|stsdf|0
3|admin|admin123|0
19|hacker|hackpass|1
</code></pre>

<p>Or drop some tables:</p>

<pre><code>$ ./tests.sh drop_table
##############################################################################################################################
# Running SQL Injection to Drop Table. This will attempt to drop the 'messages' table from the database using SQL injection. #
##############################################################################################################################
Logging in as admin to perform SQL Injection to drop the messages table...
Logging in with admin:admin
SessionId: f86bec743b5e8cd60ec886b4a6e9e3b1
IsAdmin: true


./tests.sh: line 143: get_cookie: command not found
Dropping the messages table with SQL Injection...
Response: {"message":"Entry added successfully","result":{}}
Querying the database to check if the messages table still exists...
users
</code></pre>

<p>Super hacker stuff, I know.</p>

<h2 id="building-insecure-applications-is-a-lot-of-work">Building Insecure Applications is a Lot of Work</h2>

<p>After all, getting a bunch of vulnerabilities into an application is a lot of work. It’s not realistic to deal with every example that OWASP provides. Furthermore, real-world scenarios are often <em>much</em> more complicated and <em>much</em>  more technical and subtle. Most of what we focus on in cybersecurity is the problem of aging code and the vulnerabilities that come with it. There is less focus on the vulnerabilities that come from improper use of libraries and frameworks and their configuration, vulnerabilities that are subtle and harder to detect. The web application I’m building is more like using a sledgehammer instead of a scalpel, if you’ll pardon the mixed metaphors.</p>

<p>I also need to do more research on OWASP, other tools like the Atomic Red Team, and what other “webgoat”-style applications are out there and how they work, and what they do best.</p>

<p>Find the code, such as it is, <a href="https://github.com/ccollicutt/insecure-nextjs-guestbook">here</a>.</p>

<h2 id="next-up">Next Up</h2>

<p>In future posts, I’ll look more at this insecure webapp, how to test and execute the exploitable vulnerabilities, as well as how to fix it, if possible, using GenAI and tools like <a href="https://cursor.sh/">Cursor</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[tldr;]]></summary></entry><entry><title type="html">Easily Create a Single Node Kubernetes Cluster</title><link href="https://serverascode.com/2024/08/22/install-kubernetes-script.html" rel="alternate" type="text/html" title="Easily Create a Single Node Kubernetes Cluster" /><published>2024-08-22T00:00:00-04:00</published><updated>2024-08-22T00:00:00-04:00</updated><id>https://serverascode.com/2024/08/22/install-kubernetes-script</id><content type="html" xml:base="https://serverascode.com/2024/08/22/install-kubernetes-script.html"><![CDATA[<p>I’ve been working with Kubernetes for a long time. Too long, actually. So long, in fact, that I don’t really use it much anymore. Kubernetes has won in terms of being the default way to deploy modern applications. At this point, it’s kind of boring, which is great! We want boring infrastructure. Boring works. If you’re writing a new application today, the target is going to be a container, and that container is probably going to run in good old boring Kubernetes.</p>

<p>For quite a while, the last few years, I have had a bunch of Kubernetes clusters running in my basement. I have half a rack there that used to be filled with servers. Then that changed to just running one larger server with a couple hundred gigs of memory, and that one server was running a bunch of Kubernetes clusters. But recently I shut that down. Mainly because it’s summer here in Toronto and that one big server was heating up the basement, and I wasn’t using it that much. I may turn it on again in the winter. Not sure. Anyways…</p>

<p>Yesterday I needed a small k8s cluster. So I used my good old <code>install-kubernetes.sh</code> script to install it onto a VM running on my local workstation.</p>

<h2 id="tldr">tl;dr</h2>

<ul>
  <li>I have a 500 line bash script that installs Kubernetes on Ubuntu 22.04, usually a small VM, 8 gigs of ram, 2-4 CPUs, 40 gigs of disk.</li>
  <li>The script can create a single node “cluster”</li>
  <li>Or you can deploy a bunch of virtual machines and make one a control plane node and the other workers</li>
  <li>It only takes 2 or 3 minutes to get a k8s cluster. Below is a picture of the test I ran in a github action. Of course, github’s infrastructure is blazing fast–the speed of the installation will largely depend on how fast you can download packages to the host.</li>
</ul>

<p><img src="/img/install-k8s-action.png" alt="quick install in a github action" /></p>

<p>There are other single node k8s tools, but I like mine, of course :)</p>

<h2 id="install-kubernetes">Install Kubernetes</h2>

<p>For a year and a half or so–first commit was March of 2023–I’ve had a script that will deploy a Kubernetes cluster into a virtual machine.</p>

<p>That script can be found here:</p>

<ul>
  <li><a href="https://github.com/ccollicutt/install-kubernetes">https://github.com/ccollicutt/install-kubernetes</a></li>
</ul>

<p>I haven’t used it for a while, and so it was actually broken for the last bit because the upstream Kubernetes project changed where the packages for Ubuntu are located. So I just updated the script, like bumped it to Kubernetes 1.31, fixed a few other things, and now it’s good to go again to create either a cluster of Kubernetes instances or, perhaps more useful, a full Kubernetes deployment running in a single virtual machine instance (where the single node is both a control plane node and a worker node).</p>

<h2 id="building-a-single-node-kubernetescluster">Building a Single Node Kubernetes…“Cluster”</h2>

<p>First, get yourself an Ubuntu 22.04 virtual machine with at least 8 gigs of ram and around 40 gigs of disk. I’d probably also give it 4 CPUs.</p>

<pre><code>root@install-k8s-demo:~# source /etc/lsb-release; echo $DISTRIB_RELEASE
22.04
root@install-k8s-demo:~# nproc
4
root@install-k8s-demo:~# free -h
           	total    	used    	free  	shared  buff/cache   available
Mem:       	7.7Gi   	124Mi   	7.5Gi    	17Mi   	111Mi   	7.4Gi
Swap:         	0B      	0B      	0B
root@install-k8s-demo:~# lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sda  	8:0	0   40G  0 disk
├─sda1   8:1	0  100M  0 part /boot/efi
└─sda2   8:2	0 39.9G  0 part /
</code></pre>

<p>Then, login to that VM and get a root shell.</p>

<p>Next, grab the install-kubernetes script.</p>

<blockquote>
  <p>NOTE: You’ll need git installed.</p>
</blockquote>

<pre><code>root@install-k8s-demo:~# git clone https://github.com/ccollicutt/install-kubernetes
root@install-k8s-demo:~# cd install-kubernetes
</code></pre>

<p>There will be a few files there:</p>

<pre><code>root@install-k8s-demo:~/install-kubernetes# ls
install-kubernetes.sh  makefile  manifests  README.md
</code></pre>

<p>Then, we simply run the <code>install-kubernetes.sh </code>script BUT using the “-s” option to set it so that it deploys a single node control plane + worker node.</p>

<blockquote>
  <p>NOTE: If you forget the “-s”, it is probably best to recreate the virtual machine and reinstall it. This is not idempotent, or at least it hasn’t been tested that way.</p>
</blockquote>

<pre><code>./install-kubernetes.sh -s
</code></pre>

<p>The output of that will look like:</p>

<pre><code>root@install-k8s-demo:~/install-kubernetes# ./install-kubernetes.sh -s
Starting install...
==&gt; Logging all output to /tmp/install-kubernetes-NMxK9WTKim/install.log
Checking Linux distribution
Disabling swap
Removing packages
Installing required packages
Installing Kubernetes packages
Configuring system
Configuring crictl
Configuring kubelet
Configuring containerd
Installing containerd
Starting services
Configuring control plane node...
Initialising the Kubernetes cluster via Kubeadm
Configuring kubeconfig for root and ubuntu users
Installing Calico CNI
==&gt; Installing Calico tigera-operator
==&gt; Installing Calico custom-resources
Waiting for nodes to be ready...
==&gt; Nodes are ready
Checking Kubernetes version...
==&gt; Client version: v1.31.0
==&gt; Server Version: v1.31.0
==&gt; Requested KUBE_VERSION matches the server version.
Installing metrics server
Configuring as a single node cluster
Configuring as a single node cluster
Deploying test nginx pod
Waiting for all pods to be running...
Install complete!

### Command to add a worker node ###
kubeadm join localhost:6443 --token &lt;redact&gt; --discovery-token-ca-cert-hash sha256:&lt;redact&gt;
</code></pre>

<h2 id="now-you-have-a-kubernetes-cluster">Now You Have a Kubernetes Cluster</h2>

<p>At this point, you can run kubectl and access the local cluster.</p>

<p>There’s a kubeconfig in:</p>

<pre><code>root@install-k8s-demo:~# ls ~/.kube/
cache  config
</code></pre>

<p>And, if there is an ubuntu user on the host, the config will be there too.</p>

<pre><code>root@install-k8s-demo:~# ls /home/ubuntu/.kube/
config
</code></pre>

<p>And we can connect to the “cluster”.</p>

<pre><code>root@install-k8s-demo:~# kubectl get pods -A
NAMESPACE      	NAME                                   	READY   STATUS	RESTARTS   AGE
calico-apiserver   calico-apiserver-78d48b5579-j97lc      	1/1 	Running   0      	4m15s
calico-apiserver   calico-apiserver-78d48b5579-kmcvr      	1/1 	Running   0      	4m15s
calico-system  	calico-kube-controllers-7d868b8f66-fldb5   1/1 	Running   0      	4m45s
calico-system  	calico-node-pqfdn                      	1/1 	Running   0      	4m45s
calico-system  	calico-typha-899c7464d-9vqzg           	1/1 	Running   0      	4m45s
calico-system  	csi-node-driver-vqvnx                  	2/2 	Running   0      	4m36s
kube-system    	coredns-6f6b679f8f-tvtjs               	1/1 	Running   0      	4m49s
kube-system    	coredns-6f6b679f8f-zvcdl               	1/1 	Running   0      	4m49s
kube-system    	etcd-install-k8s-demo                  	1/1 	Running   0      	4m57s
kube-system    	kube-apiserver-install-k8s-demo        	1/1 	Running   0      	4m56s
kube-system    	kube-controller-manager-install-k8s-demo   1/1 	Running   0      	4m57s
kube-system    	kube-proxy-9snr9                       	1/1 	Running   0      	4m49s
kube-system    	kube-scheduler-install-k8s-demo        	1/1 	Running   0      	4m56s
kube-system    	metrics-server-5f94f4d4fd-sg2gh        	1/1 	Running   0      	4m35s
tigera-operator	tigera-operator-b974bcbbb-4sjjz        	1/1 	Running   0      	4m49s
</code></pre>

<h2 id="you-can-deploy-many-worker-nodes-if-you-want">You Can Deploy Many Worker Nodes if You Want</h2>

<p>You could also use this script to deploy a single control plane only node, the standard model for deploying K8s where the control plane is separated, and then create and add as many worker nodes to that control plane as makes sense. However, this script does not orchestrate all of this. You would have to log in to each VM and run the script, set it up as a worker node or a control plane node, and then join the worker nodes to the control plane node using the kubeadm join command. So this is not meant to be some kind of high-level k8s cluster creation orchestration mechanism, no magic here. Of course you can create as large a cluster as you want, you just have to set up each node individually.</p>

<h2 id="some-design-decisions">Some Design Decisions</h2>

<p>If you look at the script, here are some design decisions. It’s using:</p>

<ul>
  <li>Ubuntu Kubernetes packages</li>
  <li>containerd</li>
  <li>Calico as the CNI</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>If you need a throwaway Kubernetes cluster that can be created in an Ubuntu 22.04 VM in a few minutes (like two!) I think this is a nice way to do that. Certainly it works for me.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I’ve been working with Kubernetes for a long time. Too long, actually. So long, in fact, that I don’t really use it much anymore. Kubernetes has won in terms of being the default way to deploy modern applications. At this point, it’s kind of boring, which is great! We want boring infrastructure. Boring works. If you’re writing a new application today, the target is going to be a container, and that container is probably going to run in good old boring Kubernetes.]]></summary></entry><entry><title type="html">Leapfrogging: Switching From OpenAI to Claude, and Github Copilot to Cursor</title><link href="https://serverascode.com/2024/08/12/switching-to-claude-and-cursor.html" rel="alternate" type="text/html" title="Leapfrogging: Switching From OpenAI to Claude, and Github Copilot to Cursor" /><published>2024-08-12T00:00:00-04:00</published><updated>2024-08-12T00:00:00-04:00</updated><id>https://serverascode.com/2024/08/12/switching-to-claude-and-cursor</id><content type="html" xml:base="https://serverascode.com/2024/08/12/switching-to-claude-and-cursor.html"><![CDATA[<p>I’m a fan of the recent wave of AI–specifically generative AI, or GenAI for short. I think of GenAI, these large language models, as a kind of compression. They take huge amounts of text–programming code, for example–and they are able to regurgitate it. So we take terrabytes of code, compress/feed it into an LLM that ends up being only a few gigabytes in size, and we can then talk to that LLM in natural language, and it can return code and other text…effectively uncompressing it.</p>

<h2 id="leapfrogging">Leapfrogging</h2>

<p>What I want to focus on here, for this post, is that vendors are getting better and better at building LLMs for decompressing code and, as well, better at building out the user experience for coaxing code out of LLMs. These LLMs, this GenAI, combined with a chatbot interface or an integrated development environment, can do so, so much. We can just ask them for the code, or about the code, or <em>how to code</em>, and they will provide the code or help you figure out how to write it. It’s not always great code, or perfect code, but it’s usually good enough.</p>

<p>A few notes:</p>

<ol>
  <li>There are many companies that make LLMs. A few are building “frontier” LLMs, such as OpenAI and Claude.</li>
  <li>These companies are working to make LLMs and their interfaces better at interacting with humans and creating code.</li>
  <li>There are also companies working on how programmers can best use LLMs to write code. They are not building the LLMs directly, instead finding out how we can best use them.</li>
</ol>

<p>In each of the above situations, at some point, one company will leapfrog another. And that, as far as I’m concerned, is what has happened recently.</p>

<p>For the last few months I’ve been using OpenAI’s ChatGPT and Github’s Copilot via VSCode. But now I have almost completely switched from ChatGPT and CoPilot to Claude and Cursor. I used to pay monthly for ChatGPT and CoPilot, and I stopped paying for them and started paying for Claude and Cursor.</p>

<table>
  <thead>
    <tr>
      <th>What I Used Before</th>
      <th>What I Use Now</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>OpenAI ChatGPT</td>
      <td>Claude 3.5 Sonnet</td>
    </tr>
    <tr>
      <td>GitHub Copilot</td>
      <td>Cursor</td>
    </tr>
  </tbody>
</table>

<p>The difference is amazing…for now!</p>

<h2 id="claude">Claude</h2>

<p>For a long time, OpenAI was great at coding. Over time, in my opinion, it started to slide. Maybe it got nerfed, I don’t know. Then Claude 3.5 Sonnet came out…and it blew me away. It’s just very, very good at spitting out the codez.</p>

<p>But regurgitating code is one thing, doing it in an easy and intuitive way while programming is another. I don’t mind the chatbot style of interacting with LLMs, but it does get tedious. Claude 3.5 Sonnet has helped solve this UX/UI problem with a concept it calls artifacts.</p>

<blockquote>
  <p>…introducing Artifacts on Claude.ai, a new feature that expands how users can interact with Claude. When a user asks Claude to generate content like code snippets, text documents, or website designs, these Artifacts appear in a dedicated window alongside their conversation. This creates a dynamic workspace where they can see, edit, and build upon Claude’s creations in real-time, seamlessly integrating AI-generated content into their projects and workflows.</p>
</blockquote>

<p>Claud is absolutely the best LLM for coding right now. I pay for it. It will save you massive amounts of time.</p>

<p>Find Claude at <a href="https://claude.ai/">https://claude.ai/</a>.</p>

<h2 id="cursor">Cursor</h2>

<p>But no matter how much we tweak the chatbot-style interface, it’s never going to be good enough. We need AI built right into the IDE–the Integrated Developer Environment–which is really just a fancy text editor. AI has to be built in, and totally er…integrated…into the IDE.</p>

<p>This is what cursor is–AI built right into the IDE. It’s not perfect, but it’s certainly a good place to start. The best place right now.</p>

<blockquote>
  <p>Built to make you extraordinarily productive, Cursor is the best way to code with AI.</p>
</blockquote>

<p>Find Cursor at <a href="https://www.cursor.com/">https://www.cursor.com/</a>.</p>

<h2 id="competition">Competition</h2>

<p>I don’t feel bad for Github CoPilot or OpenAI: that’s how competition works. As a consumer, I am in a great position to make choices about what tools I think are best, and given the pace of change, those tools will likely change over time, and perhaps Claude and Cursor will be leapfrogged by other companies and projects. What a fun time!</p>

<h2 id="the-future-looks-fantastic">The Future Looks Fantastic</h2>

<p>I can’t describe where I think things are going with GenAI/LLMs and code better than this video. I heavily suggest watching it all, and perhaps even taking the time to watch the three hour video from which it came.</p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">On using Cursor + Claude 3.5 Sonnet + Tailwind to ship 20x faster (ft. <a href="https://twitter.com/Shpigford?ref_src=twsrc%5Etfw">@Shpigford</a>) <a href="https://t.co/lQ0yTjm8MF">pic.twitter.com/lQ0yTjm8MF</a></p>&mdash; Sahil Lavingia (@shl) <a href="https://twitter.com/shl/status/1821646287290110184?ref_src=twsrc%5Etfw">August 8, 2024</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>]]></content><author><name></name></author><summary type="html"><![CDATA[I’m a fan of the recent wave of AI–specifically generative AI, or GenAI for short. I think of GenAI, these large language models, as a kind of compression. They take huge amounts of text–programming code, for example–and they are able to regurgitate it. So we take terrabytes of code, compress/feed it into an LLM that ends up being only a few gigabytes in size, and we can then talk to that LLM in natural language, and it can return code and other text…effectively uncompressing it.]]></summary></entry><entry><title type="html">Fine Tuning LLMs: Part 1 - Just Getting Started</title><link href="https://serverascode.com/2024/05/16/fine-tuning-llms-part-one-getting-started.html" rel="alternate" type="text/html" title="Fine Tuning LLMs: Part 1 - Just Getting Started" /><published>2024-05-16T00:00:00-04:00</published><updated>2024-05-16T00:00:00-04:00</updated><id>https://serverascode.com/2024/05/16/fine-tuning-llms-part-one-getting-started</id><content type="html" xml:base="https://serverascode.com/2024/05/16/fine-tuning-llms-part-one-getting-started.html"><![CDATA[<p>There are a few ways we can customise a Large Language Model (LLM), and one of those ways is to fine-tune it.</p>

<p>But why fine-tune an LLM?</p>

<blockquote>
  <p>Large language models (LLMs) like GPT-3 and Llama have shown immense promise for natural language generation. With sufficient data and compute, these models can produce remarkably human-like text. However, off-the-shelf LLMs still have limitations. They may generate text that is bland, inconsistent, or not tailored to your specific needs.
This is where finetuning comes in. Finetuning is the process of taking a pre-trained LLM and customizing it for a specific task or dataset. With finetuning, you can steer the LLM towards producing the kind of text you want. - <a href="https://medium.com/@dave-shap/a-pros-guide-to-finetuning-llms-c6eb570001d3">https://medium.com/@dave-shap/a-pros-guide-to-finetuning-llms-c6eb570001d3</a></p>
</blockquote>

<p>As well I just want to make a little disclaimer here on decisions I’ve made. :)</p>

<p><strong>DISCLAIMER</strong></p>

<blockquote>
  <p>Please note that what I’ve done here is really a personal experiment in fine-tuning an LLM. There’s no particular rhyme or reason to the infrastructure and other choices I’ve made. I’m using a particular GPU supplier. I’m using a certain Python notebook. I’ve made some choices that might actually make things more difficult, or that might not make sense to an experienced fine-tuner. Also, in this post, I’m not tuning with a specific set of data or goal in mind. I’m just trying out a set of basic tools.</p>
</blockquote>

<p>Happy hacking!</p>

<h2 id="quick-fine-tuning-example">Quick Fine Tuning Example</h2>

<p>Steps:</p>

<ol>
  <li>Get a GPU from somewhere (I’m using brev.dev)</li>
  <li>Create an instance with proper CUDA and pytorch versioning</li>
  <li>Build a data set to fine-tune with (or use an existing one), NOTE: This step I will build on in later posts</li>
  <li>Use a <a href="https://github.com/unslothai/unsloth">Unsloth iPython notebook</a></li>
  <li>Step through the provided notebook and create a fine-tuned LLM</li>
</ol>

<h2 id="gpu-instance">GPU Instance</h2>

<p>First, we need a GPU.</p>

<blockquote>
  <p>NOTE: The easiest thing to do would just be to use Google Colab and the notebook that Unsloth links to; that would be super easy. Google Colab is a free cloud service to run Jupyter Notebooks and provides access to GPUs. But I’m not using Colab for…some reason. You might want to. Keep that in mind!</p>
</blockquote>

<p>I’m using <a href="https://brev.dev">brev.dev</a> to get access to a GPU instance, but there are tons of “GPU Brokers” out there.</p>

<blockquote>
  <p>NOTE: I have no relationship with brev.dev, I just randomly started using the service. I can’t tell you if it’s good or not, but the combination of the provider plus the docker image for CUDA + pytorch is working for me. Plus if you leave the GUI console for long enough, a cute DVD-style screen saver comes on. lol!</p>
</blockquote>

<p>Here I’m creating a small NVIDIA 4090 instance. Other much larger GPUs are available from brev.dev and other providers.</p>

<p><img src="/img/brev1.jpg" alt="brev.dev" /></p>

<p>Note that I’m using the “advanced container settings” and selecting the docker.io/pythorch/pytorch:2.2.0-cuda12.1-cudnn8-runtime image. This is key because I’ve had lots of problems matching up these versions, especially on my home workstation where I have a NVIDIA 3090.</p>

<p>I find versioning CUDA and Pytorch challenging so this is a really nice feature of brev.dev, though it’s really just about dialing in the right image/settings/etc.</p>

<p><img src="/img/brev2.jpg" alt="brev.dev" /></p>

<p>Once the instance is running there is an option to connect to a notebook.</p>

<p><img src="/img/brev3.jpg" alt="brev.dev" /></p>

<p>And now we can use the notebook.</p>

<p><img src="/img/brev4.jpg" alt="brev.dev" /></p>

<p>Or you can login with the the brev shell. Here my instance is brilliantly named “aaa”.</p>

<pre><code>$ brev shell aaa
⢿ waiting for SSH connection to be available Agent pid 9158
Warning: Permanently added '[provider.pdx.nb.akash.pub]:31314' (ED25519) to the list of known hosts.
Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.0-101-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

This system has been minimized by removing packages and content that are
not required on a system that users do not log into.

To restore this content, you can run the 'unminimize' command.
➜  verb-workspace 
Connection to provider.pdx.nb.akash.pub closed.
</code></pre>

<h2 id="unsloth-notebook">Unsloth Notebook</h2>

<p>Next I’ll upload the unsloth conversational notebook, which I obtained by opening the Colab notebook and downloading the file, then uploading it into the brev.dev instance’s notebook.</p>

<blockquote>
  <p>NOTE: There are a lot of notebooks for getting started training Llama3 out there. For example brev.dev has <a href="https://github.com/brevdev/notebooks/blob/main/README.md">some</a> too. As well, unsloth provides some via <a href="https://huggingface.co/datasets/unsloth/notebooks">huggingface</a>.</p>
</blockquote>

<p>Unsloth - <a href="https://github.com/unslothai/unsloth">https://github.com/unslothai/unsloth</a></p>

<blockquote>
  <p>Unsloth is a lightweight library for faster LLM fine-tuning which is fully compatible with the Hugging Face ecosystem (Hub, transformers, PEFT, TRL). The library is actively developed by the Unsloth team (Daniel and Michael) and the open source community. The library supports most NVIDIA GPUs–from GTX 1070 all the way up to H100s–, and can be used with the entire trainer suite from the TRL library (SFTTrainer, DPOTrainer, PPOTrainer). At the time of writing, Unsloth supports the Llama (CodeLlama, Yi, etc) and Mistral architectures. - <a href="https://huggingface.co/blog/unsloth-trl">https://huggingface.co/blog/unsloth-trl</a></p>
</blockquote>

<p>I’m using the <a href="https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing">conversational notebook</a> they link to in their README. That will bring you to a</p>

<p><img src="/img/unsloth1.jpg" alt="unsloth" /></p>

<h2 id="train-the-model">Train the Model</h2>

<p>Now we can simply step through the notebook and train an example model.</p>

<p><img src="/img/unsloth4.jpg" alt="unsloth" /></p>

<p>After stepping through the cells, we come to the training cell.</p>

<p><img src="/img/unsloth2.jpg" alt="unsloth" /></p>

<p>And we can see the memory usage.</p>

<p><img src="/img/unsloth3.jpg" alt="unsloth" /></p>

<p>If you continue through the notebook you can save the model in various ways, upload it to hugging face, etc.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The technical part of fine-tuning a model is fairly straightforward from a user perspective if you know a bit of Python and understand the concept of a Jupyter notebook and have one to follow through with. Really this blog post is just connecting some dots, like GPU providers and Python notebooks. However, navigating through a notebook and understanding exactly what it does are two different things. Also, once you start understanding the fine tuning process, it becomes a matter of what data we put in and what results we get out. That is the real work. This is just a basic skeleton, but everyone needs a first step!</p>

<h2 id="ps">PS.</h2>

<p>One of the things I love about LLMs right now is just how messy the technology landscape is. There is so much going on, so many niche technologies, libraries, chunks of code, websites, notebooks, on and on. It’s an amazing time.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[There are a few ways we can customise a Large Language Model (LLM), and one of those ways is to fine-tune it.]]></summary></entry><entry><title type="html">Ollama &amp;amp; Llama 3:8b Running Locally</title><link href="https://serverascode.com/2024/04/23/ollama-running-locally.html" rel="alternate" type="text/html" title="Ollama &amp;amp; Llama 3:8b Running Locally" /><published>2024-04-23T00:00:00-04:00</published><updated>2024-04-23T00:00:00-04:00</updated><id>https://serverascode.com/2024/04/23/ollama-running-locally</id><content type="html" xml:base="https://serverascode.com/2024/04/23/ollama-running-locally.html"><![CDATA[<p>First, we start up ollama.</p>

<pre><code>$ ollama serve
</code></pre>

<p>OK, it’s running.</p>

<pre><code>$ curl localhost:11434 &amp;&amp; echo
Ollama is running
</code></pre>

<p>Now, let’s run Meta’s Llama 3:8b.</p>

<blockquote>
  <p>NOTE: This is only the 8b model, the smallest one so far, AFAIK.</p>
</blockquote>

<pre><code>$ ollama run llama3:8b
pulling manifest 
pulling 00e1317cbf74... 100% ▕█████████████████████████████████████████▏ 4.7 GB                         
pulling 4fa551d4f938... 100% ▕█████████████████████████████████████████▏  12 KB                         
pulling 8ab4849b038c... 100% ▕█████████████████████████████████████████▏  254 B                         
pulling c0aac7c7f00d... 100% ▕█████████████████████████████████████████▏  128 B                         
pulling db46ef36ef0b... 100% ▕█████████████████████████████████████████▏  483 B                         
verifying sha256 digest 
writing manifest 
removing any unused layers 
success 
&gt;&gt;&gt; Send a message (/? for help)
</code></pre>

<p>Checkout memory usage. Not bad. Pretty small.</p>

<pre><code>$ nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits -i 0
5308
</code></pre>

<h2 id="intersecting-polygons">Intersecting Polygons</h2>

<p>Lets ask Llama 3:8b to solve a problem: write the code to determine if two polygons are intersecting. This is a problem that some LLMs can’t answer. (However, it is possible that Llama 3 has “learned” how to solve the problem from it being discussed recently on the internet, and given the fact that Llama 3 has new training data.)</p>

<p>Below, it looks like we’ve gotten a pretty good answer, as it uses the Python shapely library, which is the easiest thing to do, and what I’d expect to see.</p>

<pre><code class="language-python">&gt;&gt;&gt; Write a function that calculates the intersection of two polygons. The function is given two polygons
...  represented as a list of points. A third polygon representing the intersection of the two is returne
... d, or an empty list of points if the intersection is empty.
Here's a Python function to calculate the intersection of two polygons:

import shapely.geometry
from typing import List

def polygon_intersection(polygon1: List[List[float]], polygon2: List[List[float]]) -&gt; 
List[List[float]]:
    """
    Calculate the intersection of two polygons.

    Args:
    polygon1 (List[List[float]]): The first polygon represented as a list of points.
    polygon2 (List[List[float]]): The second polygon represented as a list of points.

    Returns:
    List[List[float]]: A list of points representing the intersection of the two polygons, or an 
empty list if the intersection is empty.
    """
    # Convert the input polygons to Shapely Polygon objects
    poly1 = shapely.geometry.Polygon(polygon1)
    poly2 = shapely.geometry.Polygon(polygon2)

    # Calculate the intersection polygon
    intersection = poly1.intersection(poly2)

    # If the intersection is not empty, return it as a list of points
    if not intersection.is_empty:
        return [(point.x, point.y) for point in intersection.exterior.coords]
    else:
        return []

This function uses the Shapely library to work with geometric shapes. It first converts the input 
polygons into Shapely Polygon objects. Then it calculates the intersection of these two polygons 
using the `intersection` method.

If the intersection is not empty, the function returns a list of points representing this polygon 
as a list of coordinates. If the intersection is empty, an empty list is returned.

Here's how you can use this function:

polygon1 = [(0, 0), (0, 2), (2, 2), (2, 0)]
polygon2 = [(1, 1), (1, 3), (3, 3), (3, 1)]

intersection_polygon = polygon_intersection(polygon1, polygon2)
print(intersection_polygon)

This will output the points that make up the intersection of the two polygons.
</code></pre>

<p>However, this code, as is, as provided, will not run.</p>

<pre><code>$ python polygons.py 
Polygon 1:
(0, 0)
(0, 2)
(2, 2)
(2, 0)

Polygon 2:
(1, 1)
(1, 3)
(3, 3)
(3, 1)
Traceback (most recent call last):
  File "/home/curtis/working/intersecting-polygons/polygons.py", line 43, in &lt;module&gt;
    intersection_polygon = polygon_intersection(polygon1, polygon2)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/curtis/working/intersecting-polygons/polygons.py", line 26, in polygon_intersection
    return [(point.x, point.y) for point in intersection.exterior.coords]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/curtis/working/intersecting-polygons/polygons.py", line 26, in &lt;listcomp&gt;
    return [(point.x, point.y) for point in intersection.exterior.coords]
             ^^^^^^^
AttributeError: 'tuple' object has no attribute 'x'
</code></pre>

<p>But I provide llama 3 the error and it gives me this response.</p>

<pre><code>&gt;&gt;&gt; I get this error: AttributeError: 'tuple' object has no attribute 'x'
That's a great error message!

It looks like the `point` variable is actually a tuple (a simple data structure in Python), not an 
object with an `x` attribute. This is because the `intersection.exterior.coords` returns a list of 
tuples, where each tuple represents a point on the polygon.

To fix this, you can simply change the line:

return [(point.x, point.y) for point in intersection.exterior.coords]

to:

return [tuple(point) for point in intersection.exterior.coords]

This will convert each tuple (representing a point) into a new tuple with the same elements.

Alternatively, you can also use a list comprehension to extract the `x` and `y` values from each 
tuple, like this:

return [[p[0], 
p[1][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p]p[1][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p]p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][
</code></pre>

<blockquote>
  <p>NOTE: Interesting additional output (the [p]s). I copied and pasted directly. Not sure what happened there…</p>
</blockquote>

<p>I replaced that line of code as it suggested, and the program ran. For an 8b model this capability seems perfectly fine, at least to me, and was better than what I found ChatGPT 3.5 was capable of a few months ago, in that Llama 3 used the Shapely module, whereas ChatGPT 3.5 would not. I would imagine Llama 3:8b would fail if I told it not to use Shapely.</p>

<p>Results of running the provided code:</p>

<pre><code>$ python polygons.py 
Polygon 1:
(0, 0)
(0, 2)
(2, 2)
(2, 0)

Polygon 2:
(1, 1)
(1, 3)
(3, 3)
(3, 1)

Intersection Polygon:
(2.0, 2.0)
(2.0, 1.0)
(1.0, 1.0)
(1.0, 2.0)
(2.0, 2.0)
</code></pre>

<h2 id="but-it-cant-graph-and-save-an-image">…But It Can’t Graph and Save an Image</h2>

<p>Unfortunately, Llama 3:8b was unable to provide the code to plot the polygons and save them as an image file, which would have looked something like the below image. It may have been able to do this with a different prompt.</p>

<p><img src="/img/intersection.png" alt="img" /></p>

<h2 id="overall-impressive">Overall: Impressive</h2>

<p>This was a very quick test. I only spent a handful of minutes on it.</p>

<p>Llama 3:8B. It’s fast. It can run locally. It’s pretty “smart”, although it would take a bit of manual configuration to get the code it output to work; I didn’t give it much of a chance to get things right. Overall, I’m impressed with this little LLM–its compressed a lot of information.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[First, we start up ollama.]]></summary></entry><entry><title type="html">My Cyberpunk Weekend - Part 3: Using Docker and GPUs</title><link href="https://serverascode.com/2023/12/18/cyberpunk-weekend-3.html" rel="alternate" type="text/html" title="My Cyberpunk Weekend - Part 3: Using Docker and GPUs" /><published>2023-12-18T00:00:00-05:00</published><updated>2023-12-18T00:00:00-05:00</updated><id>https://serverascode.com/2023/12/18/cyberpunk-weekend-3</id><content type="html" xml:base="https://serverascode.com/2023/12/18/cyberpunk-weekend-3.html"><![CDATA[<p>I’m working on running LocalAI. But I feel like running that out of Docker.</p>

<p>So how to use a GPU with Docker (on Linux).</p>

<p>First, need the <code>nvidia-docker2</code> driver. Otherwise you get an error like this:</p>

<pre><code>docker: Error response from daemon: could not select device driver "" with capabilities: [[gpu]].
</code></pre>

<p>So install that.</p>

<pre><code>sudo apt install nvidia-docker2
</code></pre>

<p>I had a fun thing to fix in that I had added some things to the “daemon.json” so had to fix that.</p>

<pre><code>$ sudo dpkg --configure -a
Setting up nvidia-docker2 (2.13.0-1) ...

Configuration file '/etc/docker/daemon.json'
 ==&gt; File on system created by you or by a script.
 ==&gt; File also in package provided by package maintainer.
   What would you like to do about it ?  Your options are:
    Y or I  : install the package maintainer's version
    N or O  : keep your currently-installed version
      D     : show the differences between the versions
      Z     : start a shell to examine the situation
 The default action is to keep your current version.
*** daemon.json (Y/I/N/O/D/Z) [default=N] ? D
--- /etc/docker/daemon.json     2023-04-10 15:23:11.735382489 -0400
+++ /etc/docker/daemon.json.dpkg-new    2023-03-31 09:10:49.000000000 -0400
@@ -1,4 +1,8 @@
 {
-  "registry-mirrors": ["http://10.8.24.123"],
-  "insecure-registries": ["https://some.registry"]
+    "runtimes": {
+        "nvidia": {
+            "path": "nvidia-container-runtime",
+            "runtimeArgs": []
+        }
+    }
 }
</code></pre>

<p>Next, I have two NVIDIA GPUS, one old one and one newer, better one, the 3090, which is what I want to be using for LLMs.</p>

<p>So, locally I have two, as shown below.</p>

<pre><code>$ nvidia-smi 
Mon Dec 18 11:35:39 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.223.02   Driver Version: 470.223.02   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:06:00.0 Off |                  N/A |
|  0%   32C    P8    12W / 350W |     10MiB / 24268MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce ...  Off  | 00000000:07:00.0 N/A |                  N/A |
| 44%   71C    P0    N/A /  N/A |   2574MiB /  3015MiB |     N/A      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1445      G   /usr/lib/xorg/Xorg                  4MiB |
|    0   N/A  N/A      3231      G   /usr/lib/xorg/Xorg                  4MiB |
+-----------------------------------------------------------------------------+
</code></pre>

<p>But we can specify to use “device=0” only in the container, so we should only see one GPU.</p>

<pre><code>$ docker run -it --gpus "device=0" nvidia/cuda:11.4.3-base-ubuntu20.04 nvidia-smi
Mon Dec 18 16:33:29 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.223.02   Driver Version: 470.223.02   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:06:00.0 Off |                  N/A |
|  0%   32C    P8    10W / 350W |     10MiB / 24268MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
</code></pre>

<p>BOOM!</p>

<p>One of the hard parts is figuring out what tag to use on the NVIDIA image. They are all listed here:</p>

<ul>
  <li><a href="https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/supported-tags.md">https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/supported-tags.md</a></li>
</ul>

<p>Examples:</p>

<pre><code>11.4.3-base-ubuntu20.04 (11.4.3/ubuntu20.04/base/Dockerfile)
11.4.3-cudnn8-devel-ubuntu20.04 (11.4.3/ubuntu20.04/devel/cudnn8/Dockerfile)
11.4.3-cudnn8-runtime-ubuntu20.04 (11.4.3/ubuntu20.04/runtime/cudnn8/Dockerfile)
11.4.3-devel-ubuntu20.04 (11.4.3/ubuntu20.04/devel/Dockerfile)
11.4.3-runtime-ubuntu20.04 (11.4.3/ubuntu20.04/runtime/Dockerfile)
</code></pre>

<p>Note that these will change over time, of course. But if Docker reports it can’t find the tag, it’s likely because the tag is wrong, or has changed.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I’m working on running LocalAI. But I feel like running that out of Docker.]]></summary></entry><entry><title type="html">My Cyberpunk Weekend - Part 2: The Llama</title><link href="https://serverascode.com/2023/12/10/cyberpunk-weekend-2.html" rel="alternate" type="text/html" title="My Cyberpunk Weekend - Part 2: The Llama" /><published>2023-12-10T00:00:00-05:00</published><updated>2023-12-10T00:00:00-05:00</updated><id>https://serverascode.com/2023/12/10/cyberpunk-weekend-2</id><content type="html" xml:base="https://serverascode.com/2023/12/10/cyberpunk-weekend-2.html"><![CDATA[<p>Well, last week I picked up the 3090 GPU. This week I need to try to use it. That is not an easy feat because “drivers.”</p>

<p>My good old workstation is on Ubuntu 20.04. I should probably upgrade. I should probably not use this machine for AI work. But, I am.</p>

<p>Currently I’m using the nvidia-driver-470 that I’ve had for a while, as though it’s some sort of cherished antique that I’ll hand down to my children. I do remember it being a pain to get working, back when I only had one GPU.</p>

<pre><code class="language-bash">$ dpkg --list | grep nvidia-driver
ii  nvidia-driver-460                          470.223.02-0ubuntu0.20.04.1                   amd64        Transitional package for nvidia-driver-470
ii  nvidia-driver-470                          470.223.02-0ubuntu0.20.04.1                   amd64        NVIDIA driver metapackage
</code></pre>

<p>But to use a llamafile I need the right CUDA toolkit and driver match up. At first I installed 12.3, but then realized that’s not the driver I have. Need to match those up.</p>

<pre><code class="language-bash">$ ./llava-v1.5-7b-q4-server.llamafile --n-gpu-layers 35
building ggml-cuda with nvcc -arch=native...
nvcc fatal   : Unsupported gpu architecture 'compute_30'
/usr/local/cuda-12.3/bin/nvcc: returned nonzero exit status
building nvidia compute capability detector...
cudaGetDeviceCount() failed: CUDA driver version is insufficient for CUDA runtime version
error: compute capability detector returned nonzero exit status
</code></pre>

<p>Driver:</p>

<pre><code class="language-bash">$ nvidia-smi | grep CUDA
| NVIDIA-SMI 470.223.02   Driver Version: 470.223.02   CUDA Version: 11.4     |
</code></pre>

<p>I didn’t want to break my workstation and thus for now wanted to stay on the 470 driver. So I installed the 11.4 CUDA toolkit.</p>

<p>First I purged the 12.3 CUDA toolkit:</p>

<pre><code class="language-bash">$ dpkg -l | grep -E "cuda|cublas|cufft|cufile|curand|cusolver|cusparse|gds-tools|npp|nvjpeg|nsight|nvvm"
$ # review that list
$ # now remove
sudo apt-get --purge remove "*cuda*" "*cublas*" "*cufft*" "*cufile*" "*curand*" \
 "*cusolver*" "*cusparse*" "*gds-tools*" "*npp*" "*nvjpeg*" "nsight*" "*nvvm*"’’’
</code></pre>

<blockquote>
  <p>NOTE: This requires setting up the NVIDIA repo! Not shown here.</p>
</blockquote>

<p>Then I installed the 11.4 CUDA toolkit:</p>

<pre><code class="language-bash">$ sudo apt-get install cuda-toolkit-11-4
</code></pre>

<p>Added this to my path:</p>

<pre><code class="language-bash">$ which nvcc
/usr/local/cuda-11.4/bin/nvcc
</code></pre>

<p>Next I tried to run the llamafile again:</p>

<pre><code class="language-bash">$ ./llava-v1.5-7b-q4-server.llamafile --n-gpu-layers 35
building ggml-cuda with nvcc -arch=native...
nvcc fatal   : Value 'native' is not defined for option 'gpu-architecture'
/usr/local/cuda-11.4/bin/nvcc: returned nonzero exit status
building nvidia compute capability detector...
building ggml-cuda with nvcc -arch=compute_86...
NVIDIA cuBLAS GPU support successfully loaded
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 2 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6
  Device 1: NVIDIA GeForce GTX 660 Ti, compute capability 3.0

cuBLAS error 3 at /home/curtis/.llamafile/ggml-cuda.cu:6091
current device: 1
</code></pre>

<p>But it was using the wrong card. I believe the error was due to using the old 660Ti and trying to compile for it using CUDA 11.4.</p>

<p>Setting <code>CUDA_VISIBLE_DEVICES=0</code> fixed that:</p>

<pre><code class="language-bash">$ env | grep CUDA
CUDA_VISIBLE_DEVICES=0
$ ./llava-v1.5-7b-q4-server.llamafile --n-gpu-layers 35
NVIDIA cuBLAS GPU support successfully loaded
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6
{"timestamp":1702258585,"level":"INFO","function":"main","line":2650,"message":"build info","build":1500,"commit":"a30b324"}
{"timestamp":1702258585,"level":"INFO","function":"main","line":2653,"message":"system info","n_threads":6,"n_threads_batch":-1,"total_threads":12,"system_info":"AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | "}
Multi Modal Mode Enabledclip_model_load: model name:   openai/clip-vit-large-patch14-336
clip_model_load: description:  image encoder for LLaVA
clip_model_load: GGUF version: 3
clip_model_load: alignment:    32
clip_model_load: n_tensors:    377
clip_model_load: n_kv:         19
clip_model_load: ftype:        q4_0
SNIP!
</code></pre>

<p>That’s about as far as I’m getting this weekend.</p>

<p>Here’s a fun command to watch the GPU:</p>

<pre><code class="language-bash">nvidia-smi --query-gpu=timestamp,name,pci.bus_id,driver_version,pstate,pcie.link.gen.max,pcie.link.gen.current,temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used --format=csv -l 5
</code></pre>]]></content><author><name></name></author><summary type="html"><![CDATA[Well, last week I picked up the 3090 GPU. This week I need to try to use it. That is not an easy feat because “drivers.”]]></summary></entry><entry><title type="html">My Cyberpunk Weekend - Part 1: The Video Card (Or Neural Core, Take Your Pick)</title><link href="https://serverascode.com/2023/12/04/cyberpunk-weekend.html" rel="alternate" type="text/html" title="My Cyberpunk Weekend - Part 1: The Video Card (Or Neural Core, Take Your Pick)" /><published>2023-12-04T00:00:00-05:00</published><updated>2023-12-04T00:00:00-05:00</updated><id>https://serverascode.com/2023/12/04/cyberpunk-weekend</id><content type="html" xml:base="https://serverascode.com/2023/12/04/cyberpunk-weekend.html"><![CDATA[<p>There are a couple of ways to think about this post:</p>

<p>Option 1 (boring):</p>

<blockquote>
  <p><em>I bought a video card and installed it in my computer.</em></p>
</blockquote>

<p>Option 2 (cheesy cliché cyberpunk; more fun):</p>

<blockquote>
  <p><em>In the gray low-rent business suburbs on the edge of the city, where the air hums with the buzz of a thousand illicit transactions, I found myself trudging through a seedy strip mall, its flickering signs casting long shadows over an assortment of massage parlors. Here, amid the cacophony of distant traffic and the murmur of hushed conversation, lay my destination: a dubious, fly-by-night eBay store. The place was a cybernetic bazaar, a maze of used technology and questionable merchandise covered in handwritten labels. Navigating the cramped aisles, I sought a particular treasure–a used AI processor, a critical component for powering my large language model efforts. The store’s operators, engaged in a rapid exchange in a language completely foreign to me, barely acknowledged my presence as their faces, etched with lines of weary experience, hesitated for a brief moment before extracting the neural core straight from the guts of a humming, overworked system host. The device, a relic of technological ambition, was burning hot and singed their fingertips, but not enough to deter them from accepting the cash I offered.</em></p>
</blockquote>

<p>LOL. I’m not sure which one is better. I’ll let you decide.</p>

<h2 id="generative-artificial-intelligence">Generative Artificial Intelligence</h2>

<p>Like most people, I have been surprised by the big changes in Artificial Intelligence (AI) over the last few years…surprised, caught off guard, out of the “know” and out of the loop.</p>

<p>Also, like many people, I’ve been a big user of generative AI, but I don’t have a good understanding of how it works. I hope to change that. I want to be able to run Large Language Models (LLMs) locally, so first, I needed to get a video card–a GPU–capable of running these models.</p>

<h2 id="its-just-a-video-card">It’s Just a Video Card?</h2>

<p>It’s kind of amazing that I can use the phrase “video card” in connection with AI; that there’s any connection between the two at all. What’s a video card for? Connecting to a monitor. Playing video games. But for AI? It’s a bit of a stretch, but it’s true.</p>

<p>So my first step was to find the right video card, the right graphics processing unit (GPU), to work with AI. After a bit of research it seemed like my best bet, the best value card, was to find a used NVIDIA 3090, mostly because it has 24GB of memory and is a good price at this time.</p>

<p>There were a lot of comments and thoughts on sites like Reddit with this kind of advice:</p>

<blockquote>
  <p><em>The 3090 is the best bang for your buck. It comes with 24gb of nvram in a single consumer card with a built-in cooling solution and plugs right into your home rig. It lets you run 33b GPTQ models without fuss.</em></p>
</blockquote>

<h2 id="kijiji---the-canadian-craigslist">Kijiji - The Canadian Craigslist</h2>

<p>Living in Toronto has some advantages in that you can find anything you need used–it’s out there, you just have to search and wait. It’s kind of like what I imagine living in a big city in China would be like - everything is available, you just have to go out and find it, maybe meet some interesting people along the way.</p>

<p>In Canada we have a site called Kijiji (not even sure how to spell it) which is like Craigslist–but a Canadian Craigslist–so I started looking for a used NVIDIA 3090 GPU with 24GB of memory.</p>

<p>Of course, there are all kinds of problems with buying a used video card on Kijiji, or anything else for that matter, but I was willing to take the risk in this case. Plus, it can be fun if you don’t mind possibly losing the money on a bad purchase. I’ve bought quite a few things on Kijiji and never had a problem, it’s really about finding the right person to buy from, like anything else in life. I’ve never been ripped off, but you will find some difficult people. I have a whole story about buying a canoe on Kijiji, but that’s for another time. Of course, you always want to keep your wits about you and meet in a public place.</p>

<p>I set up a search on Kijiji and there are usually a few 3090s for sale, usually around $1000 to $1200. Then I saw a post from a local person, just a few blocks away in fact, who was selling one for $700. “Quick sale,” the ad said. I contacted them, but I wasn’t quick enough, and they sold it in a couple of hours before I could get over there.</p>

<p>Eventually, I saw another ad for a used 3090 that had been pulled from a Dell Alienware workstation for $800 (Canadian) and contacted them about it. They said to give them a few hours notice before coming by to pick it up. Seemed like a good deal, so I said I’d give it a shot. Presumably, if it was from an Alienware computer, it was probably used for gaming, not crypto mining, which is a positive. On the other hand, the people selling it probably knew the value if they were going to part it out, i.e., sell the Alienware box as pieces instead of the entire thing, which means they are professional in some respect.</p>

<p>A day or two later I went to pick it up. Their store was in a strip mall surrounded by massage parlors, which seemed a little seedy at first because there were more than one, but next to the computer store was a regular car dealership, so I figured it couldn’t be that bad. I pulled open the door to the shop, which was so jammed I was not sure I could get it open, and walked into a room completely filled with old computers and a couple of people working feverishly testing them and putting large strips of tape with non-English words on them. Stacks and stacks of computers, half of them falling over.</p>

<p>I told them I was there for the video card and they asked me to wait a few minutes and showed me the card, which looked to be in perfect condition. I asked them if they would benchmark it for me, i.e. put it in a computer and run some tests. They hummed and hawed, but finally agreed to do it. He put it in a computer and ran Furmark and it seemed to work fine. To be honest, I don’t know that much about graphics cards or how they’re supposed to work, I mostly just watched the temperature and made sure the card was working. While the benchmark was running, they were talking to each other in a language I didn’t recognize, so I was never sure exactly what they were saying to each other. Sadly I only speak one language. But they were busy, which means they don’t have time to mess around with people. Frankly, they seemed like exactly the kind of place where you’d buy a used video card pulled from a high-end workstation.</p>

<p>During the benchmark, the temperature of the card went up quite a bit, I think around 85 degrees, but I wasn’t surprised. I asked them where they sold all these computers, the ones stacked around the place, and the elderly gentleman gruffly gave me a one-word answer: “ebay.” Then he went to pull out the card, but didn’t let it cool down and almost burned his fingers, which was a bit worrying; you’d think he’d know it was hot. I sure did.</p>

<p>In the end, I paid them $800 cash and took the card home. Surprisingly, they gave me a 30-day warranty card.</p>

<p>It felt very much like a William Gibson-esque cyberpunk experience, and I was happy to have the card.</p>

<h2 id="power">Power</h2>

<p>In preparation for getting this card, I did some research on maybe building a whole new computer. It was around Black Friday time, so there were a lot of deals. I could have just bought a whole new workstation, but my current one is only a few years old and works just fine. Also, while there was a lot of stuff on sale, there were no good CPUs available; they were all out of stock. Theoretically, I could put the 3090 in my current computer, it would be louder, which is annoying since the computer is in my office, and I would need a new power supply and have to replace it myself, but it should work and it would save some money as well. So for now, I’m just using my existing Linux workstation to host the 3090.</p>

<p>These 3090s can draw up to about 350 watts, which is quite a bit of power. So I had to get a properly sized power supply, as my current workstation only has a 550 watt power supply. I would need a lot more than that, at least 1000 watts. So I started looking for a bigger power supply. I ended up buying a refurbished Corsair RM1000x for $150 from Canada Computers. It’s one of the last remaining computer stores in Toronto. That and Memory Express, which doesn’t even have a Toronto location. <a href="https://www.canadacomputers.com/">Canada Computers</a> is about the best place we have to buy computer parts.</p>

<h2 id="installation">Installation</h2>

<p><img src="/img/cyberpunk-1-3090.png" alt="3090 card" /></p>

<p>I’m a bit of an odd person in that I have a lot of computers, like a lot, so much that I won’t write it down here. It’s just part of what I do for a living, and if you do it for long enough they start to accumulate. However, and I suppose this is the odd part, I don’t particularly like computer hardware, especially desktop computers. I don’t mind network switches for some reason, and rackmount computers, well, they’re okay (albeit desktops in another form). But everything else…not much fun. A lot of people take a considerable pride in their workstation setup, LED lights and all that, but that is not for me. With that in mind, I wasn’t super happy about having to change the power supply and open up the computer and move things around, but I did it. It took me a couple of hours, but I did it.</p>

<p>Honestly, the new power supply went in really easily. There was a <a href="https://www.youtube.com/watch?v=yafbKAuyntw&amp;ab_channel=TheProvokedPrawn">Youtube video</a> that showed my exact power supply and a similar 3090, so that made me feel better about the power swap. I just had to pull three wires and put the new power supply in.</p>

<p>However, my motherboard is a little unusual in that if you use the second M2 slot, the second PCIe slot is disabled, which is where I would put the 3090. I assumed that my NVMe card was in the first slot, so I installed the card and rebooted. But I couldn’t see the 3090 from Linux. Looking at the motherboard again, I realized that the technician who built my computer had put the NVMe card in the second slot, probably to get it farther away from the GPU so it wouldn’t be affected by the card’s heat. As soon as I moved the NVMe card to the first M2 slot, the second PCIe slot was enabled and I could see the 3090!</p>

<pre><code>$ nvidia-smi -L | grep 3090
GPU 0: NVIDIA GeForce RTX 3090
</code></pre>

<p>As you can see, I have an old 660ti as the video card that is connected to my monitors, and the 3090 is the second card. Nice to see the 24GB of memory, which is the whole point of all this “cyberpunk” work!</p>

<pre><code>$ nvidia-smi	 
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.223.02   Driver Version: 470.223.02   CUDA Version: 11.4 	|
|-------------------------------+----------------------+----------------------+
| GPU  Name    	Persistence-M| Bus-Id    	Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|     	Memory-Usage | GPU-Util  Compute M. |
|                           	|                  	|           	MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:06:00.0 Off |              	N/A |
|  0%   30C	P8 	8W / 350W | 	10MiB / 24268MiB |  	0%  	Default |
|                           	|                  	|              	N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce ...  Off  | 00000000:07:00.0 N/A |              	N/A |
| 34%   53C	P8	N/A /  N/A |	976MiB /  3015MiB | 	N/A  	Default |
|                           	|                  	|              	N/A |
+-------------------------------+----------------------+----------------------+
                                                                          	 
+-----------------------------------------------------------------------------+
| Processes:                                                              	|
|  GPU   GI   CI    	PID   Type   Process name              	GPU Memory |
|    	ID   ID                                               	Usage  	|
|=============================================================================|
|	0   N/A  N/A  	1417  	G   /usr/lib/xorg/Xorg              	4MiB |
|	0   N/A  N/A  	2346  	G   /usr/lib/xorg/Xorg              	4MiB |
+-----------------------------------------------------------------------------+
</code></pre>

<h2 id="cooling">Cooling</h2>

<p>I assume I’ll have to find ways to cool this chassis once I start putting the 3090 through its paces.</p>

<h2 id="drivers">Drivers</h2>

<p>Because I had the 660ti installed already, I didn’t have to add any additional drivers to get the 3090 to show up. Finally a nice piece of luck!</p>

<pre><code>$ dpkg --list | grep nvidia-kernel
ii  nvidia-kernel-common-470                   470.223.02-0ubuntu0.20.04.1                   amd64        Shared files used with the kernel module
ii  nvidia-kernel-source-470                   470.223.02-0ubuntu0.20.04.1                   amd64        NVIDIA kernel source package
</code></pre>

<h2 id="conclusion">Conclusion</h2>

<p>So far, I’ve spent about $1000 CDN on this, which isn’t too bad. It remains to be seen if my older computer is up to the task of running the 3090; that it doesn’t get too hot and too loud; that I don’t end up buying a new computer anyway after all this power supply swapping. I might end up doing that if, for example, I decide I want to run multiple GPUs (two 3090s would be optimal) and/or reduce the noise, because I could put the second computer in the basement with all the other computers where I can’t hear it, and leave my trusty old relatively quiet workstation in my office.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[There are a couple of ways to think about this post:]]></summary></entry></feed>