<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://serverascode.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://serverascode.com/" rel="alternate" type="text/html" /><updated>2024-11-02T08:33:38-04:00</updated><id>https://serverascode.com/feed.xml</id><title type="html">Server As Code Dot Com</title><subtitle>A techno-blog for our techno-times</subtitle><entry><title type="html">falcoctl</title><link href="https://serverascode.com/2024/11/01/falcoctl.html" rel="alternate" type="text/html" title="falcoctl" /><published>2024-11-01T00:00:00-04:00</published><updated>2024-11-01T00:00:00-04:00</updated><id>https://serverascode.com/2024/11/01/falcoctl</id><content type="html" xml:base="https://serverascode.com/2024/11/01/falcoctl.html"><![CDATA[<p><em>tl;dr: <a href="https://github.com/falcosecurity/falcoctl">falcoctl</a> is an attempt to make it easier to distribute and upgrade Falco artifacts, such as rules and plugins.</em></p>

<h2 id="what-is-falco">What is Falco?</h2>

<blockquote>
  <p>Falco is a cloud native security tool that provides runtime security across hosts, containers, Kubernetes, and cloud environments. It leverages custom rules on Linux kernel events and other data sources through plugins, enriching event data with contextual metadata to deliver real-time alerts. Falco enables the detection of abnormal behavior, potential security threats, and compliance violations. - <a href="https://falco.org/">Falco</a></p>
</blockquote>

<p><a href="https://falco.org/">Falco</a> is like a security camera for your servers, your Kubernetes nodes, and your cloud environments. It’s a way to detect and respond to security threats in real time.</p>

<p>The underlying technology relies on rules, which are really a set of configuration files. The point of these security rules is that the world around us is changing, and so we need new and updated rules to help us detect and respond to new and emerging threats. If you simply deploy the standard open source Falco rules and never change them, or do not add your own custom rules based on your own unique needs, you’re missing out on much of the power of Falco.</p>

<p>Once you have the base Falco technology, which is of course completely invaluable, the real work is in creating and managing the threat detection rules and then responding to alerts.</p>

<h2 id="distributed-systems">Distributed Systems</h2>

<p>It’s important to note that Falco deployments are like a single security camera. They are not, by default, a distributed collection of security cameras that all send their information to a central location, nor do they by default get their configuration from a central location.</p>

<p>This presents a challenge in that organizations often run many hundreds, thousands, or even tens of thousands of nodes, each with their own Falco installation. Additionally, Falco can also have plugins installed on each node, which further complicates the distribution of configuration.</p>

<ul>
  <li><strong>Configuration files</strong> - As mentioned, Falco is a rules based engine–so it needs the rules to work, and thus we have to manage those rules, and distribute them to potentially tens of thousands of nodes.</li>
</ul>

<p>How do we distribute these rules?</p>

<ul>
  <li><strong>Plugins</strong> - Over the last couple of years Falco has added a plugin framework which allows you in many ways to vastly increase the capacity of what Falco can monitor, i.e. historically Falco has looked at system calls, events from the Linux kernel, but with plugins you can add in other data sources, such as audit logs.</li>
</ul>

<p>So we have rules <em>and</em> plugins to manage, but how?</p>

<p>That is where falcoctl comes in.</p>

<h2 id="what-is-falcoctl">What is falcoctl?</h2>

<p>Here’s a good <a href="https://falco.org/blog/falcoctl-install-manage-rules-plugins/">blog post</a> from the Falco team that explains it well:</p>

<blockquote>
  <p>Since the launch of the plugin framework in January 2022, our adopters have requested an out-of-the-box solution to manage the lifecycle of rules (installation, updates). We heard your request and also created a guide to help you smoothly install the plugins. The Falco maintainers proposed the following solution to help with these issues: falcoctl. Falcoctl is a CLI tool that performs several useful tasks for Falco.</p>
</blockquote>

<p>falcoctl effectively manages artifacts for Falco, including rules, plugins, and configuration.</p>

<ul>
  <li>Install the falcoctl binary CLI application</li>
</ul>

<pre><code>LATEST=$(curl -sI https://github.com/falcosecurity/falcoctl/releases/latest | awk '/location: /{gsub("\r","",$2);split($2,v,"/");print substr(v[8],2)}')
curl --fail -LS "https://github.com/falcosecurity/falcoctl/releases/download/v${LATEST}/falcoctl_${LATEST}_linux_amd64.tar.gz" | tar -xz
sudo install -o root -g root -m 0755 falcoctl /usr/local/bin/falcoctl
</code></pre>

<ul>
  <li>Run it</li>
</ul>

<pre><code>root@falco:~# which falcoctl
/usr/local/bin/falcoctl
root@falco:~# falcoctl

     __       _                _   _ 
    / _| __ _| | ___ ___   ___| |_| |
   | |_ / _  | |/ __/ _ \ / __| __| |
   |  _| (_| | | (_| (_) | (__| |_| |
   |_|  \__,_|_|\___\___/ \___|\__|_|
									 
	
The official CLI tool for working with Falco and its ecosystem components

Usage:
  falcoctl [command]

Available Commands:
  artifact    Interact with Falco artifacts
  completion  Generate the autocompletion script for the specified shell
  driver      Interact with falcosecurity driver
  help        Help about any command
  index       Interact with index
  registry    Interact with OCI registries
  tls         Generate and install TLS material for Falco
  version     Print the falcoctl version information

Flags:
      --config string       config file to be used for falcoctl (default "/etc/falcoctl/falcoctl.yaml")
  -h, --help                help for falcoctl
      --log-format string   Set formatting for logs (color, text, json) (default "color")
      --log-level string    Set level for logs (info, warn, debug, trace) (default "info")

Use "falcoctl [command] --help" for more information about a command.
root@falco:~# 
</code></pre>

<h2 id="manage-falco-artifacts-with-falcoctl">Manage Falco Artifacts with falcoctl</h2>

<p>What we want to do is use falcoctl to manage our Falco artifacts. There are several components and features of falcoctl that we can use to do this.</p>

<ul>
  <li>Add an “index”</li>
</ul>

<pre><code>root@falco:~# sudo falcoctl index add falcosecurity https://falcosecurity.github.io/falcoctl/index.yaml
2024-11-01 19:30:11 INFO  Adding index
                      ├ name: falcosecurity
                      └ path: https://falcosecurity.github.io/falcoctl/index.yaml
2024-11-01 19:30:11 INFO  Index successfully added 
</code></pre>

<ul>
  <li>Review the config</li>
</ul>

<pre><code>root@falco:~# cat /etc/falcoctl/falcoctl.yaml 
artifact:
    follow:
        every: 6h0m0s
        falcoversions: http://localhost:8765/versions
        refs:
            - falco-rules:3
driver:
    hostroot: /
    name: falco
    repos:
        - https://download.falco.org/driver
    type:
        - modern_ebpf
    version: 7.3.0+driver
indexes:
    - name: falcosecurity
      url: https://falcosecurity.github.io/falcoctl/index.yaml
      backend: ""
</code></pre>

<ul>
  <li>Search for Falco artifacts</li>
</ul>

<pre><code>root@falco:~# falcoctl artifact search falco
INDEX        	ARTIFACT              	TYPE     	REGISTRY	REPOSITORY
falcosecurity	falco-incubating-rules	rulesfile	ghcr.io 	falcosecurity/rules/falco-incubating-rules
falcosecurity	falco-rules           	rulesfile	ghcr.io 	falcosecurity/rules/falco-rules
falcosecurity	falco-sandbox-rules   	rulesfile	ghcr.io 	falcosecurity/rules/falco-sandbox-rules
</code></pre>

<ul>
  <li>Search for Kubernetes artifacts</li>
</ul>

<pre><code>root@falco:~# falcoctl artifact search kubernetes
INDEX        	ARTIFACT          	TYPE     	REGISTRY	REPOSITORY
falcosecurity	k8saudit-eks      	plugin   	ghcr.io 	falcosecurity/plugins/plugin/k8saudit-eks
falcosecurity	k8saudit-gke      	plugin   	ghcr.io 	falcosecurity/plugins/plugin/k8saudit-gke
falcosecurity	k8saudit-gke-rules	rulesfile	ghcr.io 	falcosecurity/plugins/ruleset/k8saudit-gke
falcosecurity	k8saudit-rules    	rulesfile	ghcr.io 	falcosecurity/plugins/ruleset/k8saudit
falcosecurity	k8smeta           	plugin   	ghcr.io 	falcosecurity/plugins/plugin/k8smeta
falcosecurity	k8saudit          	plugin   	ghcr.io 	falcosecurity/plugins/plugin/k8saudit
</code></pre>

<ul>
  <li>Install the <code>falco-rules</code> artifact</li>
</ul>

<pre><code>root@falco:~# falcoctl artifact install falco-rules
2024-11-01 19:33:05 INFO  Resolving dependencies ... 
2024-11-01 19:33:05 INFO  Installing artifacts
                      └ refs: [ghcr.io/falcosecurity/rules/falco-rules:latest]
2024-11-01 19:33:05 INFO  Preparing to pull artifact
                      └ ref: ghcr.io/falcosecurity/rules/falco-rules:latest
2024-11-01 19:33:06 INFO  Pulling layer 8da145602705 
2024-11-01 19:33:06 INFO  Pulling layer b3990bf0209c                                            
2024-11-01 19:33:06 INFO  Pulling layer de2cd036fd7f                                            
2024-11-01 19:33:06 INFO  Verifying signature for artifact                                      
                      └ digest: ghcr.io/falcosecurity/rules/falco-rules@sha256:de2cd036fd7f9bb87de5d62b36d0f35ff4fa8afbeb9a41aa9624e5f6f9a004e1
2024-11-01 19:33:07 INFO  Signature successfully verified! 
2024-11-01 19:33:07 INFO  Extracting and installing artifact
                      ├ type: rulesfile
                      └ file: falco_rules.yaml.tar.gz
2024-11-01 19:33:07 INFO  Artifact successfully installed                                       
                      ├ name: ghcr.io/falcosecurity/rules/falco-rules:latest
                      ├ type: rulesfile
                      ├ digest: sha256:de2cd036fd7f9bb87de5d62b36d0f35ff4fa8afbeb9a41aa9624e5f6f9a004e1
                      └ directory: /etc/falco
</code></pre>

<h2 id="following-as-a-daemon">Following as a Daemon</h2>

<blockquote>
  <p>A great feature of falcoctl is its ability to run as a daemon to periodically check the artifacts’ repositories and automatically install new versions. - <a href="https://github.com/falcosecurity/falcoctl">falcoctl</a></p>
</blockquote>

<p>So this would be the key feature of falcoctl, because, again, it’s great to have the technology, but what does it do to make our lives easier?</p>

<p>When falcoctl tracks (follows) an artifact, it will automatically install new versions of that artifact without any human intervention, which I think is a good thing. Of course, we want to make sure that what we deploy works as expected, but that is a whole other can of worms.</p>

<p>So, with all that in mind, let’s create a service to run falcoctl as a daemon.</p>

<ul>
  <li>Create a service</li>
</ul>

<pre><code>cat &lt;&lt; 'EOF' | sudo tee /etc/systemd/system/falcoctl.service
[Unit]
Description=Falcoctl
After=network.target
StartLimitIntervalSec=0

[Service]
Type=simple
Restart=always
RestartSec=1
ExecStart=/usr/local/bin/falcoctl artifact follow
EOF
systemctl enable falcoctl
systemctl start falcoctl
</code></pre>

<ul>
  <li>Check the status now that it is running</li>
</ul>

<pre><code>root@falco:/etc/falcoctl# systemctl status falcoctl
● falcoctl.service - Falcoctl
     Loaded: loaded (/etc/systemd/system/falcoctl.service; static)
     Active: active (running) since Fri 2024-11-01 19:37:32 UTC; 11s ago
   Main PID: 5544 (falcoctl)
      Tasks: 7 (limit: 2296)
     Memory: 25.4M (peak: 25.6M)
        CPU: 674ms
     CGroup: /system.slice/falcoctl.service
             └─5544 /usr/local/bin/falcoctl artifact follow

Nov 01 19:37:33 falco falcoctl[5544]:                       └ artifact: ghcr.io/falcosecurity/r&gt;
Nov 01 19:37:33 falco falcoctl[5544]: 2024-11-01 19:37:33 INFO  Found new artifact version
Nov 01 19:37:33 falco falcoctl[5544]:                       ├ followerName: ghcr.io/falcosecuri&gt;
Nov 01 19:37:33 falco falcoctl[5544]:                       └ tag: 3
Nov 01 19:37:35 falco falcoctl[5544]: 2024-11-01 19:37:35 INFO  Artifact correctly installed
Nov 01 19:37:35 falco falcoctl[5544]:                       ├ followerName: ghcr.io/falcosecuri&gt;
Nov 01 19:37:35 falco falcoctl[5544]:                       ├ artifactName: ghcr.io/falcosecuri&gt;
Nov 01 19:37:35 falco falcoctl[5544]:                       ├ type: rulesfile
Nov 01 19:37:35 falco falcoctl[5544]:                       ├ digest: sha256:de2cd036fd7f9bb87d&gt;
Nov 01 19:37:35 falco falcoctl[5544]:                       └ directory: /etc/falco
root@falco:/etc/falcoctl# systemctl status falcoctl --no-pager
● falcoctl.service - Falcoctl
     Loaded: loaded (/etc/systemd/system/falcoctl.service; static)
     Active: active (running) since Fri 2024-11-01 19:37:32 UTC; 15s ago
   Main PID: 5544 (falcoctl)
      Tasks: 7 (limit: 2296)
     Memory: 25.4M (peak: 25.6M)
        CPU: 674ms
     CGroup: /system.slice/falcoctl.service
             └─5544 /usr/local/bin/falcoctl artifact follow

Nov 01 19:37:33 falco falcoctl[5544]:                       └ artifact: ghcr.io/falcosecu…ules:3
Nov 01 19:37:33 falco falcoctl[5544]: 2024-11-01 19:37:33 INFO  Found new artifact version
Nov 01 19:37:33 falco falcoctl[5544]:                       ├ followerName: ghcr.io/falco…ules:3
Nov 01 19:37:33 falco falcoctl[5544]:                       └ tag: 3
Nov 01 19:37:35 falco falcoctl[5544]: 2024-11-01 19:37:35 INFO  Artifact correctly installed
Nov 01 19:37:35 falco falcoctl[5544]:                       ├ followerName: ghcr.io/falco…ules:3
Nov 01 19:37:35 falco falcoctl[5544]:                       ├ artifactName: ghcr.io/falco…ules:3
Nov 01 19:37:35 falco falcoctl[5544]:                       ├ type: rulesfile
Nov 01 19:37:35 falco falcoctl[5544]:                       ├ digest: sha256:de2cd036fd7f…a004e1
Nov 01 19:37:35 falco falcoctl[5544]:                       └ directory: /etc/falco
Hint: Some lines were ellipsized, use -l to show in full.
</code></pre>

<p>Looks good.</p>

<h2 id="build-your-own-falcoctl-artifact">Build Your Own Falcoctl Artifact</h2>

<p>Let’s push a rules file into an OCI registry.</p>

<ul>
  <li>First create a rules file</li>
</ul>

<pre><code>cat &lt;&lt; 'EOF' | sudo tee ./custom_rules.yaml
- list: falco_binaries
  items: [falcoctl]
EOF
</code></pre>

<ul>
  <li>Login to the registry</li>
</ul>

<pre><code>root@falco:/etc/falcoctl# falcoctl registry auth basic some_registry -u 'your_user' -p 'some_password'
2024-11-01 19:52:43 INFO  Login succeeded registry: some_registry user: your_user
</code></pre>

<ul>
  <li>Now push that single rules file to the registry</li>
</ul>

<blockquote>
  <p>NOTE: We often think of “container images” as the only thing we can push to an OCI registry, but we can actually push any OCI-compliant artifact to an OCI registry, and over time we will see much more use of this.</p>
</blockquote>

<pre><code>export OCI_ARTIFACT_VERSION=latest
export OCI_REGISTRY=some_registry
export OCI_REPOSITORY=some_repo/falco-rules
export RULESET_FILE=custom_rules.yaml
falcoctl registry push \
    --config /dev/null \
    --type rulesfile \
    --version ${OCI_ARTIFACT_VERSION} \
    ${OCI_REGISTRY}/${OCI_REPOSITORY}:${OCI_ARTIFACT_VERSION} \
    ${RULESET_FILE}
</code></pre>

<ul>
  <li>Resulting output</li>
</ul>

<pre><code>root@falco:/etc/falcoctl# export OCI_ARTIFACT_VERSION=latest
export OCI_REGISTRY=some_registry
export OCI_REPOSITORY=some_repo/falco-rules
export RULESET_FILE=custom_rules.yaml
falcoctl registry push \
    --config /dev/null \
    --type rulesfile \
    --version ${OCI_ARTIFACT_VERSION} \
    ${OCI_REGISTRY}/${OCI_REPOSITORY}:${OCI_ARTIFACT_VERSION} \
    ${RULESET_FILE}
2024-11-01 20:15:19 INFO  Preparing to push artifact
                      ├ name: some_registry/some_repo/falco-rules:latest
                      └ type: rulesfile
2024-11-01 20:15:19 ERROR open custom_rules.yaml: no such file or directory 
root@falco:/etc/falcoctl# cd 
root@falco:~# falcoctl registry push     --config /dev/null     --type rulesfile     --version ${OCI_ARTIFACT_VERSION}     ${OCI_REGISTRY}/${OCI_REPOSITORY}:${OCI_ARTIFACT_VERSION}     ${RULESET_FILE}
2024-11-01 20:15:24 INFO  Preparing to push artifact
                      ├ name: some_registry/some_repo/falco-rules:latest
                      └ type: rulesfile
2024-11-01 20:15:24 INFO  Parsing dependencies from:  rulesfile: custom_rules.yaml
2024-11-01 20:15:24 WARN  No dependencies were provided by the user and none were found in the
                      │   rulesfile.
2024-11-01 20:15:24 INFO  Parsing requirements from:  rulesfile: custom_rules.yaml
2024-11-01 20:15:24 WARN  No requirements were provided by the user and none were found in the
                      │   rulesfile.
2024-11-01 20:15:24 INFO  Pushing layer d5c35695420a 
2024-11-01 20:15:26 INFO  97c38f4c17c8: layer already exists                                    
2024-11-01 20:15:26 INFO  Pushing layer c891d7815e0a 
2024-11-01 20:15:26 INFO  Artifact pushed                                                       
                      ├ name: some_registry/some_repo/falco-rules:latest
                      ├ type: rulesfile
                      └ digest: sha256:c891d7815e0a30a1a73e026aea4603503b0a12df9bc8b7efc38f61de2d77bd6b
</code></pre>

<ul>
  <li>Now follow the artifact by updating the falcoctl.yaml file</li>
</ul>

<blockquote>
  <p>NOTE: Below I added <code>some_registry/some_repo/falco-rules:latest</code> to the <code>refs</code> section.</p>
</blockquote>

<pre><code>root@falco:/etc/falcoctl# cat falcoctl.yaml
artifact:
    follow:
        every: 6h0m0s
        falcoversions: http://localhost:8765/versions
        refs:
            - falco-rules:3
            - some_registry/some_repo/falco-rules:latest
driver:
    hostroot: /
    name: falco
    repos:
        - https://download.falco.org/driver
    type:
        - modern_ebpf
    version: 7.3.0+driver
indexes:
    - name: falcosecurity
      url: https://falcosecurity.github.io/falcoctl/index.yaml
      backend: ""
</code></pre>

<ul>
  <li>Restart the falcoctl service</li>
</ul>

<pre><code>systemctl restart falcoctl
</code></pre>

<ul>
  <li>Check the status of the falcoctl service</li>
</ul>

<pre><code>systemctl status falcoctl
</code></pre>

<ul>
  <li>And check if the new rules file was installed</li>
</ul>

<pre><code>root@falco:~# cat /etc/falco/custom_rules.yaml 
- list: falco_binaries
  items: [falcoctl]
</code></pre>

<p>It was!</p>

<ul>
  <li>There are falcoctl logs as well</li>
</ul>

<pre><code>root@falco:~# grep some_repo /var/log/syslog
2024-11-01T20:16:57.198277+00:00 falco falcoctl[5819]:                       ├ artifact: some_registry/some_repo/falco-rules:latest
2024-11-01T20:16:57.200084+00:00 falco falcoctl[5819]:                       └ artifact: some_registry/some_repo/falco-rules:latest
2024-11-01T20:16:57.433872+00:00 falco falcoctl[5819]:                       ├ followerName: some_registry/some_repo/falco-rules:latest
2024-11-01T20:16:59.154603+00:00 falco falcoctl[5819]:                       ├ followerName: some_registry/some_repo/falco-rules:latest
2024-11-01T20:16:59.154713+00:00 falco falcoctl[5819]:                       ├ artifactName: some_registry/some_repo/falco-rules:latest
</code></pre>

<p>Easy peasy. Well, at least getting it started. Now you need to automate the creation of the artifact, and of course write your own rules, which is the hard part once you have the technology.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In theory, you could deploy falcoctl as a daemon to every host you have, and configure it to check for your new rules basically on a cronjob-like schedule. Need every host to have a new rules file? No problem, just push the new rules file to the OCI registry and the falcoctl daemon will pull it down and install it.</p>

<p>Will that scale? I don’t know. Maybe…maybe not.</p>

<p>However, even once you have Falco installed, and then use falcoctl to have a distribution mechanism in place, you still need to customize your rules. Technology only gets us so far.</p>

<h2 id="ps-sysdig">PS. Sysdig</h2>

<p>I work at <a href="https://sysdig.com">Sysdig</a>, and while we use and support Falco, we’ve also built our enterprise product to have a much wider use case (see <a href="https://www.sysdig.com/cnapp">CNAPP</a> which includes CSPM, CIEM, vulnerability management, and more), and to be considerably more scalable than vanilla open source Falco. Sysdig does all the heavy lifting, rule distribution, etc., etc. And most importantly, in the context of threat detection, we write the rules for you as well.</p>

<h2 id="further-reading">Further Reading</h2>

<ul>
  <li><a href="https://falco.org/blog/gitops-falco-rules/">Gitops Your Falco Rules</a></li>
  <li><a href="https://github.com/falcosecurity/falcoctl">falcoctl</a></li>
  <li><a href="https://falco.org/docs/getting-started/falco-linux-quickstart/">Try Falcoctl</a></li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[tl;dr: falcoctl is an attempt to make it easier to distribute and upgrade Falco artifacts, such as rules and plugins.]]></summary></entry><entry><title type="html">The Numerous Pains of Programming: Death by a Thousand Cuts</title><link href="https://serverascode.com/2024/10/25/pain-of-programming.html" rel="alternate" type="text/html" title="The Numerous Pains of Programming: Death by a Thousand Cuts" /><published>2024-10-25T00:00:00-04:00</published><updated>2024-10-25T00:00:00-04:00</updated><id>https://serverascode.com/2024/10/25/pain-of-programming</id><content type="html" xml:base="https://serverascode.com/2024/10/25/pain-of-programming.html"><![CDATA[<p><img src="/img/posts/deathcuts.png" alt="/img/posts/deathcuts.png" /></p>

<p>I recently started building a small Python application, less than 1000 lines of code. However, it’s been a real challenge to get it to the point where it can be deployed and run in a production environment. I’ve done this before, but it’s always difficult, and each time I’m reminded of how much effort it takes, in part because I always have to start anew.</p>

<p>As a part-time developer…which of these extra things do I actually have to do? And to what extent? The code, sure, but all the other stuff?</p>

<h2 id="the-pain-of-programming">The Pain of Programming</h2>

<p>For most applications, writing the code is relatively straightforward, though it can still be challenging. However, there are tons of other things around the code, all the extra work you have to do to make it a quote unquote “real application”.</p>

<p>I’m not even including things like choosing a language or framework.</p>

<blockquote>
  <p>NOTE: This is not a perfect, comprehensive list–it’s a brainstorm of things I can think of that I’ve had to do, or might have to do to make an application “real” and somewhat professional.</p>
</blockquote>

<ol>
  <li>Writing and maintaining comprehensive test suites, unit tests, integration tests, etc.</li>
  <li>Setting up testing infrastructure and testing frameworks</li>
  <li>Designing and implementing CI/CD pipelines</li>
  <li>Creating and maintaining distributable packages</li>
  <li>Managing production environment deployment and monitoring</li>
  <li>Implementing release management</li>
  <li>Writing and maintaining documentation</li>
  <li>Creating and updating system architecture diagrams</li>
  <li>Managing version control workflows and git complexity</li>
  <li>Supporting and responding to user needs and feedback</li>
  <li>Implementing basic security best practices</li>
  <li>Conducting stress testing and performance optimization</li>
  <li>Containerizing the application and creating Kubernetes manifests</li>
  <li>Setting up vulnerability scanning</li>
  <li>Managing security issues in dependencies</li>
  <li>Managing databases, data models, and migrations</li>
  <li>Configuring IDE settings and extensions</li>
  <li>Setting up development environments (e.g., virtual environments)</li>
  <li>Managing dependency trees and version conflicts</li>
  <li>Configuring and maintaining linters and formatters</li>
  <li>Setting up logging and monitoring infrastructure</li>
  <li>What kind of logging…structured, unstructured, etc.</li>
  <li>Dealing with debugging tools…more than print statements?</li>
  <li>Testing with SSL certificates</li>
  <li>Deploying on different operating systems and user environments</li>
  <li>Dealing with authentication, perhaps RBAC too</li>
  <li>Do I need different environments for development, testing, staging, and production?</li>
  <li>Keeping track of issues, tickets, and other project management tasks</li>
  <li>How to get secrets and configuration into the application safely and securely</li>
  <li>Dealing with updating the software, from configuration to removing features and functionality</li>
  <li>Understanding resource requirements, memory, cpu, etc. and what to set limits to in production</li>
  <li>Threat modelling–how would an attacker get in, what would be the impact, etc.</li>
  <li>What license should I use for the software? What does it mean if I choose the wrong one?</li>
  <li>What metrics should it output, and how?</li>
  <li>Configuring…the config file. What should it look like? How to validate it? What’s a good layout?</li>
  <li>Finding dead and unused code</li>
</ol>

<p>I could go on, and I’m sure there are many more.</p>

<p>All of the above is an enormous amount of work, work that goes far beyond the few hundred or thousand lines of code you write for the core of a simple application. For a small application, it doesn’t seem worth it. There doesn’t seem to be much in the way of easy-to-use automation or other tools to help with all this–it’s just a big old mess of tasks that add up to a lot of work.</p>

<h2 id="table-formatted-pain">Table Formatted Pain</h2>

<p>Here’s a spreadsheet view of the list above.</p>

<p>Again, this is not a comprehensive list, but a brainstorm of things I can think of that I’ve had to do, or might have to do, to make an application “real” and somewhat professional. I’m sure there are better lists out there somewhere…</p>

<table>
  <thead>
    <tr>
      <th>Category</th>
      <th>Tasks</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Testing &amp; Quality</strong></td>
      <td>• Writing and maintaining test suites (unit, integration)<br />• Setting up testing infrastructure and frameworks<br />• Conducting stress testing and performance optimization<br />• Testing with SSL certificates<br />• Detecting dead and unused code</td>
    </tr>
    <tr>
      <td><strong>Infrastructure &amp; Deployment</strong></td>
      <td>• Designing and implementing CI/CD pipelines<br />• Managing production environment deployment and monitoring<br />• Managing different environments (dev, test, staging, prod)<br />• Containerizing applications and Kubernetes manifests<br />• Deploying on different operating systems/environments</td>
    </tr>
    <tr>
      <td><strong>Security</strong></td>
      <td>• Implementing basic security best practices<br />• Setting up vulnerability scanning<br />• Managing security issues in dependencies<br />• Dealing with authentication and RBAC<br />• Threat modeling<br />• Managing secrets and configuration securely</td>
    </tr>
    <tr>
      <td><strong>Development Environment</strong></td>
      <td>• Configuring IDE settings and extensions<br />• Setting up development environments<br />• Managing dependency trees and conflicts<br />• Configuring and maintaining linters/formatters<br />• Managing version control workflows</td>
    </tr>
    <tr>
      <td><strong>Documentation &amp; Architecture</strong></td>
      <td>• Writing and maintaining documentation<br />• Creating/updating system architecture diagrams<br />• Managing licenses and compliance<br />• Maintaining configuration file structure and validation</td>
    </tr>
    <tr>
      <td><strong>Operations &amp; Monitoring</strong></td>
      <td>• Setting up logging infrastructure (structured/unstructured)<br />• Managing debugging tools<br />• Understanding resource requirements (CPU, memory)<br />• Output metrics configuration and management</td>
    </tr>
    <tr>
      <td><strong>Data Management</strong></td>
      <td>• Managing databases, data models, and migrations</td>
    </tr>
    <tr>
      <td><strong>Distribution and Upgrades</strong></td>
      <td>• Creating and maintaining distributable packages<br />• Managing software updates and feature deprecation</td>
    </tr>
    <tr>
      <td><strong>Project Management</strong></td>
      <td>• Supporting and responding to user needs/feedback<br />• Keeping track of issues, tickets, and tasks<br />• Implementing release management</td>
    </tr>
  </tbody>
</table>

<h2 id="what-is-a-real-application">What is a real application?</h2>

<p>I keep using the term “real application”, but what does it mean? What makes an application real? I suppose what I mean is an enterprise, production-grade application of some kind. But that doesn’t mean much–I’m not sure anyone knows what makes an application production-grade. It’s kind of a vague term, and I think we could probably do better. The reality is that programming is more of an art, a dedicated craft, with hundreds of things that need to be done and hundreds of other tools, technologies, and techniques that can be used to do them. Some of this is optional, some of it is required, and some of it is just good practice. It’s not always clear which is which.</p>

<h2 id="tedious-work">Tedious work</h2>

<p>This list is a decomposition of what a developer has to do. I find all this extra work absolutely exhausting and, frankly, quite tedious. For every single one of these steps, it seems to me that there must be a better way. And for some of them, I’m sure there is, maybe I just don’t know about it yet. But I don’t think there’s a single tool, technique, or technology that can help with all of them.</p>

<p>Ultimately, I would love to have a magic wand that I could wave that would do all of that, so I could just focus on the code that delivers the value, but I don’t think that’s possible. Generative AI isn’t going to help that much, you can imagine it tying it all together somehow, but the problems are still there, they’re just hidden, like a river of lava under the surface.</p>

<p>Imagine what we could do if we just got rid of all that boredom?</p>

<h2 id="an-application-can-never-be-done-but-it-must-be-run">An application can never be done, but it must be run.</h2>

<p>I think the reality is that an application can never be finished. There is always more work to be done. It can never be 100% finished - there is always something missing, which is an incredible place to be when you consider that one of the main outputs of humanity at this point in time, one of our main economic drivers, is the writing and running of software. What a bizarre situation.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Incus Installation and Use - Setup Storage Pools and Bridged Networking</title><link href="https://serverascode.com/2024/10/19/incus-installation-and-use.html" rel="alternate" type="text/html" title="Incus Installation and Use - Setup Storage Pools and Bridged Networking" /><published>2024-10-19T00:00:00-04:00</published><updated>2024-10-19T00:00:00-04:00</updated><id>https://serverascode.com/2024/10/19/incus-installation-and-use</id><content type="html" xml:base="https://serverascode.com/2024/10/19/incus-installation-and-use.html"><![CDATA[<p>In this post I’ll show you how to install and setup <a href="https://linuxcontainers.org/incus/docs/main/">Incus</a> on a physical host running Ubuntu 24.04. I’ll setup a storage pool and a bridge network, then launch a VM. Once this is all done, I’ll have a nice homelab server that can spin up many virtual machines and do it quickly, putting them on the right storage pool, on a separate network.</p>

<h2 id="what-is-incus">What is Incus?</h2>

<blockquote>
  <p>Incus is a next-generation system container, application container, and virtual machine manager. It provides a user experience similar to that of a public cloud. With it, you can easily mix and match both containers and virtual machines, sharing the same underlying storage and network. - <a href="https://linuxcontainers.org/incus/docs/main/">Incus Docs</a></p>
</blockquote>

<p>Basically, once you install Incus you can ask it for virtual machines or system containers (not Docker containers, but system conatiners) and it will go and build them for you.</p>

<h2 id="physical-host">Physical Host</h2>

<p>I’m using Ubuntu 24.04 on my homelab server.</p>

<pre><code>$ cat /etc/lsb-release 
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=24.04
DISTRIB_CODENAME=noble
DISTRIB_DESCRIPTION="Ubuntu 24.04.1 LTS"
</code></pre>

<p>It’s an older server, but it’s got a lot of memory.</p>

<pre><code>$ free -h
               total        used        free      shared  buff/cache   available
Mem:           188Gi       3.1Gi       185Gi       673Mi       2.5Gi       185Gi
Swap:          8.0Gi          0B       8.0Gi
</code></pre>

<p>And lots of room for disks and such, including a 2TB NVMe drive, which I’ll use for my main storage pool.</p>

<h2 id="install-incus">Install Incus</h2>

<p>I’ll be following the Incus docs - <a href="https://linuxcontainers.org/incus/docs/main/installing/#installing">https://linuxcontainers.org/incus/docs/main/installing/#installing</a></p>

<p>First, install the incus and qemu packages; need qemu for the VM support.</p>

<pre><code>apt install incus qemu-system
</code></pre>

<p>Incus is a small set of packages, qemu is a fair bit larger.</p>

<p>Add your user to the incus group.</p>

<pre><code>$ sudo adduser curtis incus-admin
info: Adding user `curtis' to group `incus-admin' ...
</code></pre>

<p>Log out and log back in to get the new group, or use newgrp, whatever you want.</p>

<pre><code>$ incus ls
+------+-------+------+------+------+-----------+
| NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS |
+------+-------+------+------+------+-----------+
</code></pre>

<h2 id="storage-pool">Storage Pool</h2>

<p>I have a NVMe drive mounted on /mnt/nvme0n1 and I want to use that to back my incus managed virtual machines.</p>

<pre><code>$ sudo mkdir -p /mnt/nvme0n1/incus
$ incus storage create p1 dir source=/mnt/nvme0n1/incus
Storage pool p1 created
$ incus storage ls
+------+--------+--------------------+-------------+---------+---------+
| NAME | DRIVER |       SOURCE       | DESCRIPTION | USED BY |  STATE  |
+------+--------+--------------------+-------------+---------+---------+
| p1   | dir    | /mnt/nvme0n1/incus |             | 0       | CREATED |
+------+--------+--------------------+-------------+---------+---------+
</code></pre>

<p>Some files and directories are created in /mnt/nvme0n1/incus.</p>

<pre><code>$ ls /mnt/nvme0n1/incus/
buckets     containers-snapshots  custom-snapshots  virtual-machines
containers  custom                images            virtual-machines-snapshots
</code></pre>

<p>Very slick and easy to create the storage pool.</p>

<p>Now to build a VM using that storage pool.</p>

<pre><code>$ incus launch images:ubuntu/24.04 test --vm --storage p1
Launching test
                                          
The instance you are starting doesn't have any network attached to it.
  To create a new network, use: incus network create
  To attach a network to an instance, use: incus network attach
</code></pre>

<p>Note that I don’t have a network configured, so this didn’t actually start the VM.</p>

<p>But, files are created for the VM in the storage pool.</p>

<pre><code class="language-bash">$ sudo tree /mnt/nvme0n1/incus/
/mnt/nvme0n1/incus/
├── buckets
├── containers
├── containers-snapshots
├── custom
├── custom-snapshots
├── images
├── virtual-machines
│   └── test
│       ├── agent-client.crt
│       ├── agent-client.key
│       ├── agent.crt
│       ├── agent.key
│       ├── backup.yaml
│       ├── config
│       │   ├── agent.conf
│       │   ├── agent.crt
│       │   ├── agent.key
│       │   ├── agent-mounts.json
│       │   ├── files
│       │   │   ├── hostname.tpl.out
│       │   │   ├── hosts.tpl.out
│       │   │   └── metadata.yaml
│       │   ├── incus-agent
│       │   ├── install.sh
│       │   ├── lxd-agent -&gt; incus-agent
│       │   ├── nics
│       │   ├── server.crt
│       │   ├── systemd
│       │   │   ├── incus-agent.service
│       │   │   └── incus-agent-setup
│       │   └── udev
│       │       └── 99-incus-agent.rules
│       ├── metadata.yaml
│       ├── OVMF_VARS_4M.ms.fd
│       ├── qemu.nvram -&gt; OVMF_VARS_4M.ms.fd
│       ├── root.img
│       └── templates
│           ├── hostname.tpl
│           └── hosts.tpl
└── virtual-machines-snapshots

16 directories, 25 files

</code></pre>

<h2 id="networking">Networking</h2>

<p>OK, I love networking, but it can also be a pain, especially when we’re dealing with bridges and virtual machines, etc, etc. I like to think of networking as moving packets as quickly as possible, not configuring bridges, but there’s just no avoiding it.</p>

<p>The physical host has the below netplan configuration, where I’ve added a VLAN to eth3.</p>

<pre><code>$ sudo cat 50-cloud-init.yaml 
network:
    ethernets:
        eno1: {}
        eth3: {}
    version: 2
    vlans:
        eno1.101:
            addresses:
            - 10.100.1.20/24
            id: 101
            link: eno1
            nameservers:
                addresses:
                - 10.100.1.3
                search: []
            routes:
            -   to: default
                via: 10.100.1.1
        eth3.105:
            id: 105
            link: eth3
        eth3.106:
            id: 106
            link: eth3
</code></pre>

<p>I’m going to tell incus to create a bridge on a network interface that has a VLAN tag on it, eth3.106.</p>

<pre><code>$ incus network create br106 \
  --type=bridge \
  bridge.external_interfaces=eth3.106 \
  ipv4.dhcp=false \
  ipv4.nat=false \
  ipv6.nat=false \
  ipv4.address=none \
  ipv6.address=none
</code></pre>

<p>That command creates this config:</p>

<pre><code>$ incus network show br106
config:
  bridge.external_interfaces: eth3.106
  ipv4.address: none
  ipv4.dhcp: "false"
  ipv4.nat: "false"
  ipv6.address: none
  ipv6.nat: "false"
description: ""
name: br106
type: bridge
used_by:
- /1.0/instances/test
managed: true
status: Created
locations:
- none
</code></pre>

<p>DHCP is actually provided by my physical switch, not incus. So when I launch a VM, it starts with DHCP, but that DHCP address is coming from the upstream switch, not incus. This is what I want.</p>

<p>I can launch a VM with this network on the previously configured storage pool.</p>

<pre><code>$ incus launch images:ubuntu/24.04 test --vm --storage p1 --network br106
</code></pre>

<p>And list the VMs to see the new one, note that we can see the IP address of the VM even though Incus isn’t doing the IP address management.</p>

<pre><code>$ incus ls
+------+---------+---------------------+------+-----------------+-----------+
| NAME |  STATE  |        IPV4         | IPV6 |      TYPE       | SNAPSHOTS |
+------+---------+---------------------+------+-----------------+-----------+
| test | RUNNING | 10.100.6.250 (enp5s0) |      | VIRTUAL-MACHINE | 0         |
+------+---------+---------------------+------+-----------------+-----------+
</code></pre>

<p>Hop onto that VM and ping 1.1.1.1:</p>

<pre><code>$ incus shell test
root@test:~# ping -c 3 1.1.1.1
PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data.
64 bytes from 1.1.1.1: icmp_seq=1 ttl=53 time=5.80 ms
64 bytes from 1.1.1.1: icmp_seq=2 ttl=53 time=3.43 ms
64 bytes from 1.1.1.1: icmp_seq=3 ttl=53 time=3.47 ms

--- 1.1.1.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 3.425/4.232/5.802/1.110 ms
</code></pre>

<p>Network is online!</p>

<p>For various reasons I use Mikrotik switches/routers in my homelab, so this interface might look different on your network. Obviously I don’t have a lot of DHCP going on. :)</p>

<pre><code>[admin@MikroTik] &gt; /ip dhcp-server lease print 
Flags: X - disabled, R - radius, D - dynamic, B - blocked 
 #   ADDRESS                                                                    MAC-ADDRESS       HOST-NAME                                 SERVER                                 RATE-LIMIT                                 STATUS 
 0 D 10.100.6.250                                                                 00:16:3E:4D:15:97 distrobuilder-705ecd65-121a-4b5b-8cdc-... dhcp1                                                                             bound  
[admin@MikroTik] &gt; 
</code></pre>

<p>And we can see the DHCP lease is for the VM.</p>

<h2 id="incus-profiles">Incus Profiles</h2>

<p>Finally, I’ll create a profile for the VM, or rather I’ll edit the default profile to use the bridge network and the storage pool.</p>

<pre><code class="language-bash">$ incus profile show default
config: {}
description: Default Incus profile
devices:
  eth0:
    network: br106
    type: nic
  root:
    path: /
    pool: p1
    type: disk
name: default
used_by:
- /1.0/instances/test
</code></pre>

<h2 id="conclusion">Conclusion</h2>

<p>And there you have it. Incus is now managing my virtual machines, putting them on my storage pool, and giving me a bridge network with IPs from my DHCP server.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[In this post I’ll show you how to install and setup Incus on a physical host running Ubuntu 24.04. I’ll setup a storage pool and a bridge network, then launch a VM. Once this is all done, I’ll have a nice homelab server that can spin up many virtual machines and do it quickly, putting them on the right storage pool, on a separate network.]]></summary></entry><entry><title type="html">Why Aren’t You Using Incus to Create Containers and Virtual Machines?</title><link href="https://serverascode.com/2024/10/12/incus.html" rel="alternate" type="text/html" title="Why Aren’t You Using Incus to Create Containers and Virtual Machines?" /><published>2024-10-12T00:00:00-04:00</published><updated>2024-10-12T00:00:00-04:00</updated><id>https://serverascode.com/2024/10/12/incus</id><content type="html" xml:base="https://serverascode.com/2024/10/12/incus.html"><![CDATA[<h1 id="incus">Incus</h1>

<p>Virtual machines remain the main building block of pretty much all infrastructure. We tend to forget about the technology and just how entrenched it is in our daily technical lives. Ok, that’s a pretty heavy statement for a blog post, but I do think we forget about virtual machines and just how valuable and secure the technology is–most public cloud services are loss leaders for the VM part of their business.</p>

<p>Anyways, what I want to talk about is <a href="https://linuxcontainers.org/incus/introduction/">Incus</a>, a way to easily create containers AND virtual machines.</p>

<blockquote>
  <p>NOTE: I use Incus to exclusively create virtual machines, and don’t use the container functionality that much. I would imagine that most people use the container functionality more. So while I’ll touch on the system container functionality, I use Incus for local VMs.</p>
</blockquote>

<h2 id="what-is-incus">What is Incus?</h2>

<blockquote>
  <p>When using Incus, you can manage your instances (containers and VMs) with a simple command line tool, directly through the REST API or by using third-party tools and integrations. Incus implements a single REST API for both local and remote access. The Incus project was created by Aleksa Sarai as a community driven alternative to Canonical’s LXD. Today, it’s led and maintained by many of the same people that once created LXD. - <a href="https://linuxcontainers.org/incus/introduction/">Incus Docs</a></p>
</blockquote>

<p>With Incus, you can easily create virtual machines and containers.</p>

<p>I don’t know all the history of the project, where it comes from in terms of LXD/LXC, etc, but I do know that I need a way to easily create virtual machines on my local computer, and that I really enjoy using Incus. So easy.</p>

<p>Example of creating a virtual machine:</p>

<pre><code class="language-bash">incus launch ubuntu:22.04 my-server --vm
</code></pre>

<p>It’s that easy. Especially on Ubuntu 24.04, where you can just install the <code>incus</code> package from the default repositories.</p>

<p>I also alias the incus command to this script because I always forget the incus command syntax, and I’m super lazy. So this would create a default sized VM with just <code>vm launch my-vm</code>.</p>

<pre><code class="language-bash">#!/bin/bash

launch_vm() {
  if [ -z "$1" ]; then
    echo "Usage: $0 launch &lt;vm-name&gt;"
    echo "Example: $0 launch my-ubuntu-vm"
    exit 1
  fi

  local name="$1"
  local cpu=2
  local memory="4GiB"
  local disk="40GiB"

  incus launch images:ubuntu/24.04 "$name" --vm \
    --device root,size="$disk" \
    -c limits.cpu="$cpu" \
    -c limits.memory="$memory"
}

list_vms() {
  incus ls
}

show_help() {
  echo "Usage: $0 &lt;command&gt; [options]"
  echo
  echo "Commands:"
  echo "  launch &lt;vm-name&gt;  Launch a new VM"
  echo "  ls                List all VMs"
  echo "  help              Show this help message"
  echo
  echo "For other commands, this script will pass them directly to incus."
}

# Main command handler
case "$1" in
  launch)
    launch_vm "$2"
    ;;
  ls)
    list_vms
    ;;
  help)
    show_help
    ;;
  *)
    if [ -z "$1" ]; then
      show_help
    else
      # If the command isn't recognized, pass it to incus
      incus "$@"
    fi
    ;;
esac
</code></pre>

<h2 id="using-incus">Using Incus</h2>

<p>As mentioned earlier, I almost exclusively use Incus to get a virtual machine.</p>

<p>E.g. with my script I just run:</p>

<pre><code class="language-bash">vm launch a-vm
</code></pre>

<p>Or with the bare Incus command it’s just as easy:</p>

<pre><code class="language-bash">incus launch images:ubuntu/24.04 a-vm --vm
</code></pre>

<p>Now I can shell into the VM very quickly.</p>

<pre><code class="language-bash">$ incus shell a-vm # or with my script, vm shell a-vm
root@a-vm:~# 
</code></pre>

<p>And you are in a nice little virtual machine that you can install anything you want into.</p>

<h2 id="getting-a-container">Getting a Container</h2>

<p>Writing this post was the first time I used the container functionality of Incus! Getting a container is the default mode of operation, and it’s super easy.</p>

<pre><code class="language-bash">$ incus launch images:ubuntu/22.04 ubuntu-container
# Image is downloaded, and the container is created
Launching ubuntu-container
$ incus ls | grep ubuntu-container
| ubuntu-container | RUNNING | 10.57.7.201 (eth0)           | fd42:af1f:b7c8:a36c:216:3eff:fee9:32e4 (eth0)   | CONTAINER       | 0         |
$ vm shell ubuntu-container
</code></pre>

<p>That is lightning fast. But again, important to note: this is a “system container” and not a “application container”, or in simpler terms, it’s not a docker container. If you have ever used LXC, then you will be right at home.</p>

<blockquote>
  <p>Application containers (as provided by, for example, Docker) package a single process or application. System containers, on the other hand, simulate a full operating system similar to what you would be running on a host or in a virtual machine. You can run Docker in an Incus system container, but you would not run Incus in a Docker application container. - <a href="https://linuxcontainers.org/incus/docs/main/explanation/containers_and_vms/">Incus Docs</a></p>
</blockquote>

<p>You may also want to understand the differences between a virtual machine and a system container:</p>

<blockquote>
  <p>Virtual machines create a virtual version of a physical machine, using hardware features of the host system. The boundaries between the host system and virtual machines is enforced by those hardware features. System containers, on the other hand, use the already running OS kernel of the host system instead of launching their own kernel. If you run several system containers, they all share the same kernel, which makes them faster and more lightweight than virtual machines. - <a href="https://linuxcontainers.org/incus/docs/main/explanation/containers_and_vms/">Incus Docs</a></p>
</blockquote>

<h2 id="why-use-incus">Why Use Incus?</h2>

<p>You can see a list of major features <a href="https://linuxcontainers.org/incus/introduction/#features">here</a> but what I like about it might not be the same as what you like about it.</p>

<ol>
  <li>It’s very fast - There is an agent in the image that makes getting a shell into the VM super fast. The images are also small and download like lightning, at least in my experience.</li>
  <li>It’s easy to manage - Incus has a simple syntax for launching VMs and containers</li>
  <li>Image based - Incus uses images, instead of futzing around with qemu backing files and such</li>
  <li>You use Linux as your workstation and need to easily get a VM, or a system container</li>
</ol>

<p>You can also try it online: <a href="https://linuxcontainers.org/incus/try-it/">https://linuxcontainers.org/incus/try-it/</a></p>

<h2 id="why-wouldnt-you-use-incus">Why Wouldn’t You Use Incus?</h2>

<ol>
  <li>It’s not Docker - it’s a different style of containerization, which many people are not used to.</li>
  <li>It’s relatively new, and a lot of work is being done on it - But on Ubuntu 24.04 it’s easy to install and get started.</li>
  <li>I do have some trouble with outbound access from the VMs and have futzed around with Iptables to get it working, but it’s not as easy as one would think–I expect I’m missing something obvious from the docs.</li>
</ol>

<p>That’s about all I can think of.</p>

<h2 id="pairing-incus-with-my-kubernetes-install-script">Pairing Incus with My Kubernetes Install Script</h2>

<p>FYI - If you create an 8GB VM with 4 CPUS, my <a href="https://github.com/ccollicutt/install-kubernetes">single node Kubernetes install script</a> pairs nicely with Incus, and is often what I use it for.</p>

<h2 id="incus-66-was-just-released">Incus 6.6 Was Just Released</h2>

<p>See <a href="https://discuss.linuxcontainers.org/t/incus-6-6-has-been-released/21762">here</a>. There is also a video overview of the new features <a href="https://www.youtube.com/watch?v=gGBEPtQiiQQ">here</a></p>

<p>Install it, try it out. Have fun easily creating VMs and containers!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Incus]]></summary></entry><entry><title type="html">Building an Insecure App…on Purpose (So That GenAI Can Fix It)</title><link href="https://serverascode.com/2024/10/02/building-an-insecure-app.html" rel="alternate" type="text/html" title="Building an Insecure App…on Purpose (So That GenAI Can Fix It)" /><published>2024-10-02T00:00:00-04:00</published><updated>2024-10-02T00:00:00-04:00</updated><id>https://serverascode.com/2024/10/02/building-an-insecure-app</id><content type="html" xml:base="https://serverascode.com/2024/10/02/building-an-insecure-app.html"><![CDATA[<h2 id="tldr">tldr;</h2>

<p>tldr; I built an insecure web application (on purpose) for testing LLMs and here it is: <a href="https://github.com/ccollicutt/insecure-nextjs-guestbook">https://github.com/ccollicutt/insecure-nextjs-guestbook</a>.</p>

<h2 id="dealing-with-technical-debt-using-genai">Dealing with Technical Debt using GenAI</h2>

<p>Is cybersecurity largely a technical issue? An engineering issue? It’s difficult to say. Certainly human psychology plays a big part of it, but, then again, we’re building (insecure) software things and putting them out into the world. We write billions of lines of code, and we can’t do that without making mistakes…so there are billions of mistakes too. That code has bugs, it gets worse over time, and is hard (read: expensive) to maintain. It ends up being a technical liability–a security liability. The reality of software development is an important part of the cybersecurity story. Not the whole story, but an important part.</p>

<p>For this line of thinking, the question is, can Generative Artificial Intelligence (GenAI) help us deal with all this overwhelming technical debt? I believe that GenAI can code, and code well enough to help us get rid of technical debt. And what’s more, this ability can be automated and has the potential to be fast–very fast–so it can potentially take care of a lot of technical debt in a short period of time. Now, not everyone may agree with me, but that’s my opinion, and I’m sticking to it!</p>

<p>So if you believe, or can suspend your disbelief, that GenAI can help you deal with technical debt, then you’ll want to test it, just like I do. But how do you test code that generates code?</p>

<p>I’m going to build an insecure web application and then use GenAI to try to fix it.</p>

<h2 id="what-does-insecure-mean">What Does “Insecure” Mean?</h2>

<p>However, building an insecure web application is a bit of a challenge. On the one hand, we have all kinds of technical debt that’s easy to accumulate in the real world, but on the other hand, when we write a new application, the frameworks, libraries, and tools we use are working behind the scenes to keep us as secure as possible, so in some ways it’s a challenge to build an insecure application, at least out of the gate.</p>

<p>And yet I managed to do it. At least partially.</p>

<p>So, what is an insecure web application? What are common examples of insecurity in a web application?</p>

<h2 id="owasp-top-10">OWASP Top 10</h2>

<p>One way to think about web app vulnerabilities is through the <a href="https://owasp.org/www-project-top-ten/">OWASP Top 10</a>.</p>

<p>Here’s the current OWASP Top 10, as of 2021:</p>

<ul>
  <li>A01:2021-Broken Access Control</li>
  <li>A02:2021-Cryptographic Failures</li>
  <li>A03:2021-Injection</li>
  <li>A04:2021-Insecure Design</li>
  <li>A05:2021-Security Misconfiguration</li>
  <li>A06:2021-Vulnerable and Outdated Components</li>
  <li>A07:2021-Identification and Authentication Failures</li>
  <li>A08:2021-Software and Data Integrity Failures</li>
  <li>A09:2021-Security Logging and Monitoring Failures</li>
  <li>A10:2021-Server-Side Request Forgery (SSRF)</li>
</ul>

<p>Let’s look at A01:2021-Broken Access Control, as defined by OWASP:</p>

<ul>
  <li>Violation of the principle of least privilege or deny by default, where access should only be granted for particular capabilities, roles, or users, but is available to anyone.</li>
  <li>Bypassing access control checks by modifying the URL (parameter tampering or force browsing), internal application state, or the HTML page, or by using an attack tool modifying API requests.</li>
  <li>Permitting viewing or editing someone else’s account, by providing its unique identifier (insecure direct object references)</li>
  <li>Accessing API with missing access controls for POST, PUT and DELETE.</li>
  <li>Elevation of privilege. Acting as a user without being logged in or acting as an admin when logged in as a user.</li>
  <li>Metadata manipulation, such as replaying or tampering with a JSON Web Token (JWT) access control token, or a cookie or hidden field manipulated to elevate privileges or abusing JWT invalidation.</li>
  <li>CORS misconfiguration allows API access from unauthorized/untrusted origins.</li>
  <li>Force browsing to authenticated pages as an unauthenticated user or to privileged pages as a standard user.</li>
</ul>

<p>Forced browsing, as an example, sounds fun and technical–but it’s really just about browsing pages you aren’t supposed to know exist, pages that just happen to have additional permissions or access that the average user doesn’t have.</p>

<blockquote>
  <p>Forced browsing is an attack where the aim is to enumerate and access resources that are not referenced by the application, but are still accessible. An attacker can use Brute Force techniques to search for unlinked contents in the domain directory, such as temporary directories and files, and old backup and configuration files. These resources may store sensitive information about web applications and operational systems, such as source code, credentials, internal network addressing, and so on, thus being considered a valuable resource for intruders. - <a href="https://owasp.org/www-community/attacks/Forced_browsing">https://owasp.org/www-community/attacks/Forced_browsing</a></p>
</blockquote>

<p>And, in a similar vein, (typically SQL) injection, as defined by OWASP:</p>

<ul>
  <li>User-supplied data is not validated, filtered, or sanitized by the application.</li>
  <li>Dynamic queries or non-parameterized calls without context-aware escaping are used directly in the interpreter.</li>
  <li>Hostile data is used within object-relational mapping (ORM) search parameters to extract additional, sensitive records.</li>
  <li>Hostile data is directly used or concatenated. The SQL or command contains the structure and malicious data in dynamic queries, commands, or stored procedures.</li>
</ul>

<p>Some of these are more interesting than others, and for some–it’s hard to believe that they are still happening in 2024.</p>

<h2 id="building-an-insecure-web-app">Building an Insecure Web App</h2>

<p><img src="/img/insecure-webapp-guestbook.jpg" alt="img" /></p>

<p>While there are a handful of “webgoat”-style applications that will help you learn about these vulnerabilities, I decided to build my own so that I would know exactly what problems I was introducing - and thus I would know exactly what problems I was trying to fix with GenAI.</p>

<p>I was working on learning NodeJS and NextJS, so I decided to build my insecure web application using those technologies.</p>

<p>A few points:</p>

<ul>
  <li>
    <p>I wanted to make a guestbook app of all things because it would be easy to build, and the fact that anyone should be able to post to it would make it easier to introduce vulnerabilities.</p>
  </li>
  <li>
    <p>I immediately put the clear text authentication into a SQLite database. However, in the real world, no one would put cleartext authentication in a SQLite database–or even use their own authentication system. There are many, many libraries and SaaS services that provide authentication as a service, which is much, much more secure, and that is what people will use. (That is, they’re not as easy to configure, and they’re error-prone, but they’re still much more secure than doing it yourself). I imagine most people building a new web application would either use a third party or <a href="https://next-auth.js.org/">NextAuth</a>.</p>
  </li>
  <li>
    <p>There is an admin user with a default password.</p>
  </li>
</ul>

<pre><code>// Insert admin user if not exists
db.get(`SELECT * FROM users WHERE username = 'admin'`, (err, row) =&gt; {
  if (!row) {
    db.run(`INSERT INTO users (username, password, admin) VALUES ('admin', 'admin', 1)`);
  }
});
</code></pre>

<ul>
  <li>I wanted it to be susceptible to SQL injection–but interestingly, SQLite does a lot of work to prevent that, so I had to do some work to make it vulnerable in terms of using raw queries. For the most part, SQLite just does the right thing, and you have to do some work to make it vulnerable.</li>
</ul>

<pre><code>// Vulnerable to SQL injection
const query = `SELECT * FROM users WHERE username = '${username}' AND password = '${password}'`;
</code></pre>

<ul>
  <li>
    <p>I also added an admin page that was supposed to be protected, but wasn’t.</p>
  </li>
  <li>
    <p>Originally the app didn’t use a sessionID in the URL, but I added that to make the webapp EVEN MORE VULNERABLE. But you don’t see sessionIDs in the wild, so I’m not sure if that’s realistic.</p>
  </li>
</ul>

<p>What I have so far is a webapp that is vulnerable to a number of attacks, including</p>

<ul>
  <li>SQL injection</li>
  <li>Forced browsing</li>
  <li>Session hijacking</li>
  <li>Probably Cross-Site Scripting (XSS)</li>
</ul>

<h2 id="testing-the-vulnerabilities">Testing the Vulnerabilities</h2>

<p>In addition to building the insecure application, we need to test for the presence of these vulnerabilities. So there is also a script to test for them. Please note that this is not an exhaustive list of vulnerabilities, but rather a set of examples meant to be illustrative, and in fact many of them do not work.</p>
<pre><code>$ ./tests.sh 
Usage: ./tests.sh [test_name]

Available tests:
  login                  Test common logins
  sql_injection          Run SQL Injection Test
  drop_table             Drop messages table with SQL Injection
  xss                    Run Cross-Site Scripting (XSS) Test
  insecure_auth          List all users and get admin password via SQL Injection
  sensitive_data         Run Sensitive Data Exposure Test
  security_misconfig     Run Security Misconfiguration Test
  known_vulnerabilities  Run Known Vulnerabilities Test
  insufficient_logging   Run Insufficient Logging &amp; Monitoring Test
  list_tables_and_entries List all tables and entries in the database
  help                   Display this help message
  list_users             List all users in the database
  list_nonexistent_users List all users in the database
  list_tables_and_entries List all tables and entries in the database
  list_nonexistent_users List all users in the database
</code></pre>

<p>Here’s an example of SQL injection:</p>

<pre><code>$ ./tests.sh sql_injection
###################################################
# Running SQL Injection Test to create admin user #
###################################################
Step 1: Attempting SQL injection to create admin user...
SQL Injection Response: {"message":"Login successful","sessionId":"d86976cace3f01e5ae248e037483d70d","isAdmin":true,"redirectUrl":"/?sessionId=d86976cace3f01e5ae248e037483d70d&amp;username=admin' --&amp;isAdmin=true"}

Step 2: Inserting hacker user with admin privileges...

Step 3: Attempting to login as the new admin user 'hacker'...
Login response: {"message":"Login successful","sessionId":"d2cfc602ff3fd33d201d69f0fac9bdd2","isAdmin":true,"redirectUrl":"/?sessionId=d2cfc602ff3fd33d201d69f0fac9bdd2&amp;username=hacker&amp;isAdmin=true"}
User 'hacker' logged in successfully with admin privileges. SQL Injection successful.

Step 4: Checking database for 'hacker' user...
19|hacker|hackpass|1

Step 5: Listing all users in the database...
1|admin|admin|1
2|test|stsdf|0
3|admin|admin123|0
19|hacker|hackpass|1
</code></pre>

<p>Or drop some tables:</p>

<pre><code>$ ./tests.sh drop_table
##############################################################################################################################
# Running SQL Injection to Drop Table. This will attempt to drop the 'messages' table from the database using SQL injection. #
##############################################################################################################################
Logging in as admin to perform SQL Injection to drop the messages table...
Logging in with admin:admin
SessionId: f86bec743b5e8cd60ec886b4a6e9e3b1
IsAdmin: true


./tests.sh: line 143: get_cookie: command not found
Dropping the messages table with SQL Injection...
Response: {"message":"Entry added successfully","result":{}}
Querying the database to check if the messages table still exists...
users
</code></pre>

<p>Super hacker stuff, I know.</p>

<h2 id="building-insecure-applications-is-a-lot-of-work">Building Insecure Applications is a Lot of Work</h2>

<p>After all, getting a bunch of vulnerabilities into an application is a lot of work. It’s not realistic to deal with every example that OWASP provides. Furthermore, real-world scenarios are often <em>much</em> more complicated and <em>much</em>  more technical and subtle. Most of what we focus on in cybersecurity is the problem of aging code and the vulnerabilities that come with it. There is less focus on the vulnerabilities that come from improper use of libraries and frameworks and their configuration, vulnerabilities that are subtle and harder to detect. The web application I’m building is more like using a sledgehammer instead of a scalpel, if you’ll pardon the mixed metaphors.</p>

<p>I also need to do more research on OWASP, other tools like the Atomic Red Team, and what other “webgoat”-style applications are out there and how they work, and what they do best.</p>

<p>Find the code, such as it is, <a href="https://github.com/ccollicutt/insecure-nextjs-guestbook">here</a>.</p>

<h2 id="next-up">Next Up</h2>

<p>In future posts, I’ll look more at this insecure webapp, how to test and execute the exploitable vulnerabilities, as well as how to fix it, if possible, using GenAI and tools like <a href="https://cursor.sh/">Cursor</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[tldr;]]></summary></entry><entry><title type="html">Easily Create a Single Node Kubernetes Cluster</title><link href="https://serverascode.com/2024/08/22/install-kubernetes-script.html" rel="alternate" type="text/html" title="Easily Create a Single Node Kubernetes Cluster" /><published>2024-08-22T00:00:00-04:00</published><updated>2024-08-22T00:00:00-04:00</updated><id>https://serverascode.com/2024/08/22/install-kubernetes-script</id><content type="html" xml:base="https://serverascode.com/2024/08/22/install-kubernetes-script.html"><![CDATA[<p>I’ve been working with Kubernetes for a long time. Too long, actually. So long, in fact, that I don’t really use it much anymore. Kubernetes has won in terms of being the default way to deploy modern applications. At this point, it’s kind of boring, which is great! We want boring infrastructure. Boring works. If you’re writing a new application today, the target is going to be a container, and that container is probably going to run in good old boring Kubernetes.</p>

<p>For quite a while, the last few years, I have had a bunch of Kubernetes clusters running in my basement. I have half a rack there that used to be filled with servers. Then that changed to just running one larger server with a couple hundred gigs of memory, and that one server was running a bunch of Kubernetes clusters. But recently I shut that down. Mainly because it’s summer here in Toronto and that one big server was heating up the basement, and I wasn’t using it that much. I may turn it on again in the winter. Not sure. Anyways…</p>

<p>Yesterday I needed a small k8s cluster. So I used my good old <code>install-kubernetes.sh</code> script to install it onto a VM running on my local workstation.</p>

<h2 id="tldr">tl;dr</h2>

<ul>
  <li>I have a 500 line bash script that installs Kubernetes on Ubuntu 22.04, usually a small VM, 8 gigs of ram, 2-4 CPUs, 40 gigs of disk.</li>
  <li>The script can create a single node “cluster”</li>
  <li>Or you can deploy a bunch of virtual machines and make one a control plane node and the other workers</li>
  <li>It only takes 2 or 3 minutes to get a k8s cluster. Below is a picture of the test I ran in a github action. Of course, github’s infrastructure is blazing fast–the speed of the installation will largely depend on how fast you can download packages to the host.</li>
</ul>

<p><img src="/img/install-k8s-action.png" alt="quick install in a github action" /></p>

<p>There are other single node k8s tools, but I like mine, of course :)</p>

<h2 id="install-kubernetes">Install Kubernetes</h2>

<p>For a year and a half or so–first commit was March of 2023–I’ve had a script that will deploy a Kubernetes cluster into a virtual machine.</p>

<p>That script can be found here:</p>

<ul>
  <li><a href="https://github.com/ccollicutt/install-kubernetes">https://github.com/ccollicutt/install-kubernetes</a></li>
</ul>

<p>I haven’t used it for a while, and so it was actually broken for the last bit because the upstream Kubernetes project changed where the packages for Ubuntu are located. So I just updated the script, like bumped it to Kubernetes 1.31, fixed a few other things, and now it’s good to go again to create either a cluster of Kubernetes instances or, perhaps more useful, a full Kubernetes deployment running in a single virtual machine instance (where the single node is both a control plane node and a worker node).</p>

<h2 id="building-a-single-node-kubernetescluster">Building a Single Node Kubernetes…“Cluster”</h2>

<p>First, get yourself an Ubuntu 22.04 virtual machine with at least 8 gigs of ram and around 40 gigs of disk. I’d probably also give it 4 CPUs.</p>

<pre><code>root@install-k8s-demo:~# source /etc/lsb-release; echo $DISTRIB_RELEASE
22.04
root@install-k8s-demo:~# nproc
4
root@install-k8s-demo:~# free -h
           	total    	used    	free  	shared  buff/cache   available
Mem:       	7.7Gi   	124Mi   	7.5Gi    	17Mi   	111Mi   	7.4Gi
Swap:         	0B      	0B      	0B
root@install-k8s-demo:~# lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sda  	8:0	0   40G  0 disk
├─sda1   8:1	0  100M  0 part /boot/efi
└─sda2   8:2	0 39.9G  0 part /
</code></pre>

<p>Then, login to that VM and get a root shell.</p>

<p>Next, grab the install-kubernetes script.</p>

<blockquote>
  <p>NOTE: You’ll need git installed.</p>
</blockquote>

<pre><code>root@install-k8s-demo:~# git clone https://github.com/ccollicutt/install-kubernetes
root@install-k8s-demo:~# cd install-kubernetes
</code></pre>

<p>There will be a few files there:</p>

<pre><code>root@install-k8s-demo:~/install-kubernetes# ls
install-kubernetes.sh  makefile  manifests  README.md
</code></pre>

<p>Then, we simply run the <code>install-kubernetes.sh </code>script BUT using the “-s” option to set it so that it deploys a single node control plane + worker node.</p>

<blockquote>
  <p>NOTE: If you forget the “-s”, it is probably best to recreate the virtual machine and reinstall it. This is not idempotent, or at least it hasn’t been tested that way.</p>
</blockquote>

<pre><code>./install-kubernetes.sh -s
</code></pre>

<p>The output of that will look like:</p>

<pre><code>root@install-k8s-demo:~/install-kubernetes# ./install-kubernetes.sh -s
Starting install...
==&gt; Logging all output to /tmp/install-kubernetes-NMxK9WTKim/install.log
Checking Linux distribution
Disabling swap
Removing packages
Installing required packages
Installing Kubernetes packages
Configuring system
Configuring crictl
Configuring kubelet
Configuring containerd
Installing containerd
Starting services
Configuring control plane node...
Initialising the Kubernetes cluster via Kubeadm
Configuring kubeconfig for root and ubuntu users
Installing Calico CNI
==&gt; Installing Calico tigera-operator
==&gt; Installing Calico custom-resources
Waiting for nodes to be ready...
==&gt; Nodes are ready
Checking Kubernetes version...
==&gt; Client version: v1.31.0
==&gt; Server Version: v1.31.0
==&gt; Requested KUBE_VERSION matches the server version.
Installing metrics server
Configuring as a single node cluster
Configuring as a single node cluster
Deploying test nginx pod
Waiting for all pods to be running...
Install complete!

### Command to add a worker node ###
kubeadm join localhost:6443 --token &lt;redact&gt; --discovery-token-ca-cert-hash sha256:&lt;redact&gt;
</code></pre>

<h2 id="now-you-have-a-kubernetes-cluster">Now You Have a Kubernetes Cluster</h2>

<p>At this point, you can run kubectl and access the local cluster.</p>

<p>There’s a kubeconfig in:</p>

<pre><code>root@install-k8s-demo:~# ls ~/.kube/
cache  config
</code></pre>

<p>And, if there is an ubuntu user on the host, the config will be there too.</p>

<pre><code>root@install-k8s-demo:~# ls /home/ubuntu/.kube/
config
</code></pre>

<p>And we can connect to the “cluster”.</p>

<pre><code>root@install-k8s-demo:~# kubectl get pods -A
NAMESPACE      	NAME                                   	READY   STATUS	RESTARTS   AGE
calico-apiserver   calico-apiserver-78d48b5579-j97lc      	1/1 	Running   0      	4m15s
calico-apiserver   calico-apiserver-78d48b5579-kmcvr      	1/1 	Running   0      	4m15s
calico-system  	calico-kube-controllers-7d868b8f66-fldb5   1/1 	Running   0      	4m45s
calico-system  	calico-node-pqfdn                      	1/1 	Running   0      	4m45s
calico-system  	calico-typha-899c7464d-9vqzg           	1/1 	Running   0      	4m45s
calico-system  	csi-node-driver-vqvnx                  	2/2 	Running   0      	4m36s
kube-system    	coredns-6f6b679f8f-tvtjs               	1/1 	Running   0      	4m49s
kube-system    	coredns-6f6b679f8f-zvcdl               	1/1 	Running   0      	4m49s
kube-system    	etcd-install-k8s-demo                  	1/1 	Running   0      	4m57s
kube-system    	kube-apiserver-install-k8s-demo        	1/1 	Running   0      	4m56s
kube-system    	kube-controller-manager-install-k8s-demo   1/1 	Running   0      	4m57s
kube-system    	kube-proxy-9snr9                       	1/1 	Running   0      	4m49s
kube-system    	kube-scheduler-install-k8s-demo        	1/1 	Running   0      	4m56s
kube-system    	metrics-server-5f94f4d4fd-sg2gh        	1/1 	Running   0      	4m35s
tigera-operator	tigera-operator-b974bcbbb-4sjjz        	1/1 	Running   0      	4m49s
</code></pre>

<h2 id="you-can-deploy-many-worker-nodes-if-you-want">You Can Deploy Many Worker Nodes if You Want</h2>

<p>You could also use this script to deploy a single control plane only node, the standard model for deploying K8s where the control plane is separated, and then create and add as many worker nodes to that control plane as makes sense. However, this script does not orchestrate all of this. You would have to log in to each VM and run the script, set it up as a worker node or a control plane node, and then join the worker nodes to the control plane node using the kubeadm join command. So this is not meant to be some kind of high-level k8s cluster creation orchestration mechanism, no magic here. Of course you can create as large a cluster as you want, you just have to set up each node individually.</p>

<h2 id="some-design-decisions">Some Design Decisions</h2>

<p>If you look at the script, here are some design decisions. It’s using:</p>

<ul>
  <li>Ubuntu Kubernetes packages</li>
  <li>containerd</li>
  <li>Calico as the CNI</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>If you need a throwaway Kubernetes cluster that can be created in an Ubuntu 22.04 VM in a few minutes (like two!) I think this is a nice way to do that. Certainly it works for me.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I’ve been working with Kubernetes for a long time. Too long, actually. So long, in fact, that I don’t really use it much anymore. Kubernetes has won in terms of being the default way to deploy modern applications. At this point, it’s kind of boring, which is great! We want boring infrastructure. Boring works. If you’re writing a new application today, the target is going to be a container, and that container is probably going to run in good old boring Kubernetes.]]></summary></entry><entry><title type="html">Leapfrogging: Switching From OpenAI to Claude, and Github Copilot to Cursor</title><link href="https://serverascode.com/2024/08/12/switching-to-claude-and-cursor.html" rel="alternate" type="text/html" title="Leapfrogging: Switching From OpenAI to Claude, and Github Copilot to Cursor" /><published>2024-08-12T00:00:00-04:00</published><updated>2024-08-12T00:00:00-04:00</updated><id>https://serverascode.com/2024/08/12/switching-to-claude-and-cursor</id><content type="html" xml:base="https://serverascode.com/2024/08/12/switching-to-claude-and-cursor.html"><![CDATA[<p>I’m a fan of the recent wave of AI–specifically generative AI, or GenAI for short. I think of GenAI, these large language models, as a kind of compression. They take huge amounts of text–programming code, for example–and they are able to regurgitate it. So we take terrabytes of code, compress/feed it into an LLM that ends up being only a few gigabytes in size, and we can then talk to that LLM in natural language, and it can return code and other text…effectively uncompressing it.</p>

<h2 id="leapfrogging">Leapfrogging</h2>

<p>What I want to focus on here, for this post, is that vendors are getting better and better at building LLMs for decompressing code and, as well, better at building out the user experience for coaxing code out of LLMs. These LLMs, this GenAI, combined with a chatbot interface or an integrated development environment, can do so, so much. We can just ask them for the code, or about the code, or <em>how to code</em>, and they will provide the code or help you figure out how to write it. It’s not always great code, or perfect code, but it’s usually good enough.</p>

<p>A few notes:</p>

<ol>
  <li>There are many companies that make LLMs. A few are building “frontier” LLMs, such as OpenAI and Claude.</li>
  <li>These companies are working to make LLMs and their interfaces better at interacting with humans and creating code.</li>
  <li>There are also companies working on how programmers can best use LLMs to write code. They are not building the LLMs directly, instead finding out how we can best use them.</li>
</ol>

<p>In each of the above situations, at some point, one company will leapfrog another. And that, as far as I’m concerned, is what has happened recently.</p>

<p>For the last few months I’ve been using OpenAI’s ChatGPT and Github’s Copilot via VSCode. But now I have almost completely switched from ChatGPT and CoPilot to Claude and Cursor. I used to pay monthly for ChatGPT and CoPilot, and I stopped paying for them and started paying for Claude and Cursor.</p>

<table>
  <thead>
    <tr>
      <th>What I Used Before</th>
      <th>What I Use Now</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>OpenAI ChatGPT</td>
      <td>Claude 3.5 Sonnet</td>
    </tr>
    <tr>
      <td>GitHub Copilot</td>
      <td>Cursor</td>
    </tr>
  </tbody>
</table>

<p>The difference is amazing…for now!</p>

<h2 id="claude">Claude</h2>

<p>For a long time, OpenAI was great at coding. Over time, in my opinion, it started to slide. Maybe it got nerfed, I don’t know. Then Claude 3.5 Sonnet came out…and it blew me away. It’s just very, very good at spitting out the codez.</p>

<p>But regurgitating code is one thing, doing it in an easy and intuitive way while programming is another. I don’t mind the chatbot style of interacting with LLMs, but it does get tedious. Claude 3.5 Sonnet has helped solve this UX/UI problem with a concept it calls artifacts.</p>

<blockquote>
  <p>…introducing Artifacts on Claude.ai, a new feature that expands how users can interact with Claude. When a user asks Claude to generate content like code snippets, text documents, or website designs, these Artifacts appear in a dedicated window alongside their conversation. This creates a dynamic workspace where they can see, edit, and build upon Claude’s creations in real-time, seamlessly integrating AI-generated content into their projects and workflows.</p>
</blockquote>

<p>Claud is absolutely the best LLM for coding right now. I pay for it. It will save you massive amounts of time.</p>

<p>Find Claude at <a href="https://claude.ai/">https://claude.ai/</a>.</p>

<h2 id="cursor">Cursor</h2>

<p>But no matter how much we tweak the chatbot-style interface, it’s never going to be good enough. We need AI built right into the IDE–the Integrated Developer Environment–which is really just a fancy text editor. AI has to be built in, and totally er…integrated…into the IDE.</p>

<p>This is what cursor is–AI built right into the IDE. It’s not perfect, but it’s certainly a good place to start. The best place right now.</p>

<blockquote>
  <p>Built to make you extraordinarily productive, Cursor is the best way to code with AI.</p>
</blockquote>

<p>Find Cursor at <a href="https://www.cursor.com/">https://www.cursor.com/</a>.</p>

<h2 id="competition">Competition</h2>

<p>I don’t feel bad for Github CoPilot or OpenAI: that’s how competition works. As a consumer, I am in a great position to make choices about what tools I think are best, and given the pace of change, those tools will likely change over time, and perhaps Claude and Cursor will be leapfrogged by other companies and projects. What a fun time!</p>

<h2 id="the-future-looks-fantastic">The Future Looks Fantastic</h2>

<p>I can’t describe where I think things are going with GenAI/LLMs and code better than this video. I heavily suggest watching it all, and perhaps even taking the time to watch the three hour video from which it came.</p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">On using Cursor + Claude 3.5 Sonnet + Tailwind to ship 20x faster (ft. <a href="https://twitter.com/Shpigford?ref_src=twsrc%5Etfw">@Shpigford</a>) <a href="https://t.co/lQ0yTjm8MF">pic.twitter.com/lQ0yTjm8MF</a></p>&mdash; Sahil Lavingia (@shl) <a href="https://twitter.com/shl/status/1821646287290110184?ref_src=twsrc%5Etfw">August 8, 2024</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>]]></content><author><name></name></author><summary type="html"><![CDATA[I’m a fan of the recent wave of AI–specifically generative AI, or GenAI for short. I think of GenAI, these large language models, as a kind of compression. They take huge amounts of text–programming code, for example–and they are able to regurgitate it. So we take terrabytes of code, compress/feed it into an LLM that ends up being only a few gigabytes in size, and we can then talk to that LLM in natural language, and it can return code and other text…effectively uncompressing it.]]></summary></entry><entry><title type="html">Fine Tuning LLMs: Part 1 - Just Getting Started</title><link href="https://serverascode.com/2024/05/16/fine-tuning-llms-part-one-getting-started.html" rel="alternate" type="text/html" title="Fine Tuning LLMs: Part 1 - Just Getting Started" /><published>2024-05-16T00:00:00-04:00</published><updated>2024-05-16T00:00:00-04:00</updated><id>https://serverascode.com/2024/05/16/fine-tuning-llms-part-one-getting-started</id><content type="html" xml:base="https://serverascode.com/2024/05/16/fine-tuning-llms-part-one-getting-started.html"><![CDATA[<p>There are a few ways we can customise a Large Language Model (LLM), and one of those ways is to fine-tune it.</p>

<p>But why fine-tune an LLM?</p>

<blockquote>
  <p>Large language models (LLMs) like GPT-3 and Llama have shown immense promise for natural language generation. With sufficient data and compute, these models can produce remarkably human-like text. However, off-the-shelf LLMs still have limitations. They may generate text that is bland, inconsistent, or not tailored to your specific needs.
This is where finetuning comes in. Finetuning is the process of taking a pre-trained LLM and customizing it for a specific task or dataset. With finetuning, you can steer the LLM towards producing the kind of text you want. - <a href="https://medium.com/@dave-shap/a-pros-guide-to-finetuning-llms-c6eb570001d3">https://medium.com/@dave-shap/a-pros-guide-to-finetuning-llms-c6eb570001d3</a></p>
</blockquote>

<p>As well I just want to make a little disclaimer here on decisions I’ve made. :)</p>

<p><strong>DISCLAIMER</strong></p>

<blockquote>
  <p>Please note that what I’ve done here is really a personal experiment in fine-tuning an LLM. There’s no particular rhyme or reason to the infrastructure and other choices I’ve made. I’m using a particular GPU supplier. I’m using a certain Python notebook. I’ve made some choices that might actually make things more difficult, or that might not make sense to an experienced fine-tuner. Also, in this post, I’m not tuning with a specific set of data or goal in mind. I’m just trying out a set of basic tools.</p>
</blockquote>

<p>Happy hacking!</p>

<h2 id="quick-fine-tuning-example">Quick Fine Tuning Example</h2>

<p>Steps:</p>

<ol>
  <li>Get a GPU from somewhere (I’m using brev.dev)</li>
  <li>Create an instance with proper CUDA and pytorch versioning</li>
  <li>Build a data set to fine-tune with (or use an existing one), NOTE: This step I will build on in later posts</li>
  <li>Use a <a href="https://github.com/unslothai/unsloth">Unsloth iPython notebook</a></li>
  <li>Step through the provided notebook and create a fine-tuned LLM</li>
</ol>

<h2 id="gpu-instance">GPU Instance</h2>

<p>First, we need a GPU.</p>

<blockquote>
  <p>NOTE: The easiest thing to do would just be to use Google Colab and the notebook that Unsloth links to; that would be super easy. Google Colab is a free cloud service to run Jupyter Notebooks and provides access to GPUs. But I’m not using Colab for…some reason. You might want to. Keep that in mind!</p>
</blockquote>

<p>I’m using <a href="https://brev.dev">brev.dev</a> to get access to a GPU instance, but there are tons of “GPU Brokers” out there.</p>

<blockquote>
  <p>NOTE: I have no relationship with brev.dev, I just randomly started using the service. I can’t tell you if it’s good or not, but the combination of the provider plus the docker image for CUDA + pytorch is working for me. Plus if you leave the GUI console for long enough, a cute DVD-style screen saver comes on. lol!</p>
</blockquote>

<p>Here I’m creating a small NVIDIA 4090 instance. Other much larger GPUs are available from brev.dev and other providers.</p>

<p><img src="/img/brev1.jpg" alt="brev.dev" /></p>

<p>Note that I’m using the “advanced container settings” and selecting the docker.io/pythorch/pytorch:2.2.0-cuda12.1-cudnn8-runtime image. This is key because I’ve had lots of problems matching up these versions, especially on my home workstation where I have a NVIDIA 3090.</p>

<p>I find versioning CUDA and Pytorch challenging so this is a really nice feature of brev.dev, though it’s really just about dialing in the right image/settings/etc.</p>

<p><img src="/img/brev2.jpg" alt="brev.dev" /></p>

<p>Once the instance is running there is an option to connect to a notebook.</p>

<p><img src="/img/brev3.jpg" alt="brev.dev" /></p>

<p>And now we can use the notebook.</p>

<p><img src="/img/brev4.jpg" alt="brev.dev" /></p>

<p>Or you can login with the the brev shell. Here my instance is brilliantly named “aaa”.</p>

<pre><code>$ brev shell aaa
⢿ waiting for SSH connection to be available Agent pid 9158
Warning: Permanently added '[provider.pdx.nb.akash.pub]:31314' (ED25519) to the list of known hosts.
Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.0-101-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

This system has been minimized by removing packages and content that are
not required on a system that users do not log into.

To restore this content, you can run the 'unminimize' command.
➜  verb-workspace 
Connection to provider.pdx.nb.akash.pub closed.
</code></pre>

<h2 id="unsloth-notebook">Unsloth Notebook</h2>

<p>Next I’ll upload the unsloth conversational notebook, which I obtained by opening the Colab notebook and downloading the file, then uploading it into the brev.dev instance’s notebook.</p>

<blockquote>
  <p>NOTE: There are a lot of notebooks for getting started training Llama3 out there. For example brev.dev has <a href="https://github.com/brevdev/notebooks/blob/main/README.md">some</a> too. As well, unsloth provides some via <a href="https://huggingface.co/datasets/unsloth/notebooks">huggingface</a>.</p>
</blockquote>

<p>Unsloth - <a href="https://github.com/unslothai/unsloth">https://github.com/unslothai/unsloth</a></p>

<blockquote>
  <p>Unsloth is a lightweight library for faster LLM fine-tuning which is fully compatible with the Hugging Face ecosystem (Hub, transformers, PEFT, TRL). The library is actively developed by the Unsloth team (Daniel and Michael) and the open source community. The library supports most NVIDIA GPUs–from GTX 1070 all the way up to H100s–, and can be used with the entire trainer suite from the TRL library (SFTTrainer, DPOTrainer, PPOTrainer). At the time of writing, Unsloth supports the Llama (CodeLlama, Yi, etc) and Mistral architectures. - <a href="https://huggingface.co/blog/unsloth-trl">https://huggingface.co/blog/unsloth-trl</a></p>
</blockquote>

<p>I’m using the <a href="https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing">conversational notebook</a> they link to in their README. That will bring you to a</p>

<p><img src="/img/unsloth1.jpg" alt="unsloth" /></p>

<h2 id="train-the-model">Train the Model</h2>

<p>Now we can simply step through the notebook and train an example model.</p>

<p><img src="/img/unsloth4.jpg" alt="unsloth" /></p>

<p>After stepping through the cells, we come to the training cell.</p>

<p><img src="/img/unsloth2.jpg" alt="unsloth" /></p>

<p>And we can see the memory usage.</p>

<p><img src="/img/unsloth3.jpg" alt="unsloth" /></p>

<p>If you continue through the notebook you can save the model in various ways, upload it to hugging face, etc.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The technical part of fine-tuning a model is fairly straightforward from a user perspective if you know a bit of Python and understand the concept of a Jupyter notebook and have one to follow through with. Really this blog post is just connecting some dots, like GPU providers and Python notebooks. However, navigating through a notebook and understanding exactly what it does are two different things. Also, once you start understanding the fine tuning process, it becomes a matter of what data we put in and what results we get out. That is the real work. This is just a basic skeleton, but everyone needs a first step!</p>

<h2 id="ps">PS.</h2>

<p>One of the things I love about LLMs right now is just how messy the technology landscape is. There is so much going on, so many niche technologies, libraries, chunks of code, websites, notebooks, on and on. It’s an amazing time.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[There are a few ways we can customise a Large Language Model (LLM), and one of those ways is to fine-tune it.]]></summary></entry><entry><title type="html">Ollama &amp;amp; Llama 3:8b Running Locally</title><link href="https://serverascode.com/2024/04/23/ollama-running-locally.html" rel="alternate" type="text/html" title="Ollama &amp;amp; Llama 3:8b Running Locally" /><published>2024-04-23T00:00:00-04:00</published><updated>2024-04-23T00:00:00-04:00</updated><id>https://serverascode.com/2024/04/23/ollama-running-locally</id><content type="html" xml:base="https://serverascode.com/2024/04/23/ollama-running-locally.html"><![CDATA[<p>First, we start up ollama.</p>

<pre><code>$ ollama serve
</code></pre>

<p>OK, it’s running.</p>

<pre><code>$ curl localhost:11434 &amp;&amp; echo
Ollama is running
</code></pre>

<p>Now, let’s run Meta’s Llama 3:8b.</p>

<blockquote>
  <p>NOTE: This is only the 8b model, the smallest one so far, AFAIK.</p>
</blockquote>

<pre><code>$ ollama run llama3:8b
pulling manifest 
pulling 00e1317cbf74... 100% ▕█████████████████████████████████████████▏ 4.7 GB                         
pulling 4fa551d4f938... 100% ▕█████████████████████████████████████████▏  12 KB                         
pulling 8ab4849b038c... 100% ▕█████████████████████████████████████████▏  254 B                         
pulling c0aac7c7f00d... 100% ▕█████████████████████████████████████████▏  128 B                         
pulling db46ef36ef0b... 100% ▕█████████████████████████████████████████▏  483 B                         
verifying sha256 digest 
writing manifest 
removing any unused layers 
success 
&gt;&gt;&gt; Send a message (/? for help)
</code></pre>

<p>Checkout memory usage. Not bad. Pretty small.</p>

<pre><code>$ nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits -i 0
5308
</code></pre>

<h2 id="intersecting-polygons">Intersecting Polygons</h2>

<p>Lets ask Llama 3:8b to solve a problem: write the code to determine if two polygons are intersecting. This is a problem that some LLMs can’t answer. (However, it is possible that Llama 3 has “learned” how to solve the problem from it being discussed recently on the internet, and given the fact that Llama 3 has new training data.)</p>

<p>Below, it looks like we’ve gotten a pretty good answer, as it uses the Python shapely library, which is the easiest thing to do, and what I’d expect to see.</p>

<pre><code class="language-python">&gt;&gt;&gt; Write a function that calculates the intersection of two polygons. The function is given two polygons
...  represented as a list of points. A third polygon representing the intersection of the two is returne
... d, or an empty list of points if the intersection is empty.
Here's a Python function to calculate the intersection of two polygons:

import shapely.geometry
from typing import List

def polygon_intersection(polygon1: List[List[float]], polygon2: List[List[float]]) -&gt; 
List[List[float]]:
    """
    Calculate the intersection of two polygons.

    Args:
    polygon1 (List[List[float]]): The first polygon represented as a list of points.
    polygon2 (List[List[float]]): The second polygon represented as a list of points.

    Returns:
    List[List[float]]: A list of points representing the intersection of the two polygons, or an 
empty list if the intersection is empty.
    """
    # Convert the input polygons to Shapely Polygon objects
    poly1 = shapely.geometry.Polygon(polygon1)
    poly2 = shapely.geometry.Polygon(polygon2)

    # Calculate the intersection polygon
    intersection = poly1.intersection(poly2)

    # If the intersection is not empty, return it as a list of points
    if not intersection.is_empty:
        return [(point.x, point.y) for point in intersection.exterior.coords]
    else:
        return []

This function uses the Shapely library to work with geometric shapes. It first converts the input 
polygons into Shapely Polygon objects. Then it calculates the intersection of these two polygons 
using the `intersection` method.

If the intersection is not empty, the function returns a list of points representing this polygon 
as a list of coordinates. If the intersection is empty, an empty list is returned.

Here's how you can use this function:

polygon1 = [(0, 0), (0, 2), (2, 2), (2, 0)]
polygon2 = [(1, 1), (1, 3), (3, 3), (3, 1)]

intersection_polygon = polygon_intersection(polygon1, polygon2)
print(intersection_polygon)

This will output the points that make up the intersection of the two polygons.
</code></pre>

<p>However, this code, as is, as provided, will not run.</p>

<pre><code>$ python polygons.py 
Polygon 1:
(0, 0)
(0, 2)
(2, 2)
(2, 0)

Polygon 2:
(1, 1)
(1, 3)
(3, 3)
(3, 1)
Traceback (most recent call last):
  File "/home/curtis/working/intersecting-polygons/polygons.py", line 43, in &lt;module&gt;
    intersection_polygon = polygon_intersection(polygon1, polygon2)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/curtis/working/intersecting-polygons/polygons.py", line 26, in polygon_intersection
    return [(point.x, point.y) for point in intersection.exterior.coords]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/curtis/working/intersecting-polygons/polygons.py", line 26, in &lt;listcomp&gt;
    return [(point.x, point.y) for point in intersection.exterior.coords]
             ^^^^^^^
AttributeError: 'tuple' object has no attribute 'x'
</code></pre>

<p>But I provide llama 3 the error and it gives me this response.</p>

<pre><code>&gt;&gt;&gt; I get this error: AttributeError: 'tuple' object has no attribute 'x'
That's a great error message!

It looks like the `point` variable is actually a tuple (a simple data structure in Python), not an 
object with an `x` attribute. This is because the `intersection.exterior.coords` returns a list of 
tuples, where each tuple represents a point on the polygon.

To fix this, you can simply change the line:

return [(point.x, point.y) for point in intersection.exterior.coords]

to:

return [tuple(point) for point in intersection.exterior.coords]

This will convert each tuple (representing a point) into a new tuple with the same elements.

Alternatively, you can also use a list comprehension to extract the `x` and `y` values from each 
tuple, like this:

return [[p[0], 
p[1][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p]p[1][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p]p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][
</code></pre>

<blockquote>
  <p>NOTE: Interesting additional output (the [p]s). I copied and pasted directly. Not sure what happened there…</p>
</blockquote>

<p>I replaced that line of code as it suggested, and the program ran. For an 8b model this capability seems perfectly fine, at least to me, and was better than what I found ChatGPT 3.5 was capable of a few months ago, in that Llama 3 used the Shapely module, whereas ChatGPT 3.5 would not. I would imagine Llama 3:8b would fail if I told it not to use Shapely.</p>

<p>Results of running the provided code:</p>

<pre><code>$ python polygons.py 
Polygon 1:
(0, 0)
(0, 2)
(2, 2)
(2, 0)

Polygon 2:
(1, 1)
(1, 3)
(3, 3)
(3, 1)

Intersection Polygon:
(2.0, 2.0)
(2.0, 1.0)
(1.0, 1.0)
(1.0, 2.0)
(2.0, 2.0)
</code></pre>

<h2 id="but-it-cant-graph-and-save-an-image">…But It Can’t Graph and Save an Image</h2>

<p>Unfortunately, Llama 3:8b was unable to provide the code to plot the polygons and save them as an image file, which would have looked something like the below image. It may have been able to do this with a different prompt.</p>

<p><img src="/img/intersection.png" alt="img" /></p>

<h2 id="overall-impressive">Overall: Impressive</h2>

<p>This was a very quick test. I only spent a handful of minutes on it.</p>

<p>Llama 3:8B. It’s fast. It can run locally. It’s pretty “smart”, although it would take a bit of manual configuration to get the code it output to work; I didn’t give it much of a chance to get things right. Overall, I’m impressed with this little LLM–its compressed a lot of information.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[First, we start up ollama.]]></summary></entry><entry><title type="html">My Cyberpunk Weekend - Part 3: Using Docker and GPUs</title><link href="https://serverascode.com/2023/12/18/cyberpunk-weekend-3.html" rel="alternate" type="text/html" title="My Cyberpunk Weekend - Part 3: Using Docker and GPUs" /><published>2023-12-18T00:00:00-05:00</published><updated>2023-12-18T00:00:00-05:00</updated><id>https://serverascode.com/2023/12/18/cyberpunk-weekend-3</id><content type="html" xml:base="https://serverascode.com/2023/12/18/cyberpunk-weekend-3.html"><![CDATA[<p>I’m working on running LocalAI. But I feel like running that out of Docker.</p>

<p>So how to use a GPU with Docker (on Linux).</p>

<p>First, need the <code>nvidia-docker2</code> driver. Otherwise you get an error like this:</p>

<pre><code>docker: Error response from daemon: could not select device driver "" with capabilities: [[gpu]].
</code></pre>

<p>So install that.</p>

<pre><code>sudo apt install nvidia-docker2
</code></pre>

<p>I had a fun thing to fix in that I had added some things to the “daemon.json” so had to fix that.</p>

<pre><code>$ sudo dpkg --configure -a
Setting up nvidia-docker2 (2.13.0-1) ...

Configuration file '/etc/docker/daemon.json'
 ==&gt; File on system created by you or by a script.
 ==&gt; File also in package provided by package maintainer.
   What would you like to do about it ?  Your options are:
    Y or I  : install the package maintainer's version
    N or O  : keep your currently-installed version
      D     : show the differences between the versions
      Z     : start a shell to examine the situation
 The default action is to keep your current version.
*** daemon.json (Y/I/N/O/D/Z) [default=N] ? D
--- /etc/docker/daemon.json     2023-04-10 15:23:11.735382489 -0400
+++ /etc/docker/daemon.json.dpkg-new    2023-03-31 09:10:49.000000000 -0400
@@ -1,4 +1,8 @@
 {
-  "registry-mirrors": ["http://10.8.24.123"],
-  "insecure-registries": ["https://some.registry"]
+    "runtimes": {
+        "nvidia": {
+            "path": "nvidia-container-runtime",
+            "runtimeArgs": []
+        }
+    }
 }
</code></pre>

<p>Next, I have two NVIDIA GPUS, one old one and one newer, better one, the 3090, which is what I want to be using for LLMs.</p>

<p>So, locally I have two, as shown below.</p>

<pre><code>$ nvidia-smi 
Mon Dec 18 11:35:39 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.223.02   Driver Version: 470.223.02   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:06:00.0 Off |                  N/A |
|  0%   32C    P8    12W / 350W |     10MiB / 24268MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce ...  Off  | 00000000:07:00.0 N/A |                  N/A |
| 44%   71C    P0    N/A /  N/A |   2574MiB /  3015MiB |     N/A      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1445      G   /usr/lib/xorg/Xorg                  4MiB |
|    0   N/A  N/A      3231      G   /usr/lib/xorg/Xorg                  4MiB |
+-----------------------------------------------------------------------------+
</code></pre>

<p>But we can specify to use “device=0” only in the container, so we should only see one GPU.</p>

<pre><code>$ docker run -it --gpus "device=0" nvidia/cuda:11.4.3-base-ubuntu20.04 nvidia-smi
Mon Dec 18 16:33:29 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.223.02   Driver Version: 470.223.02   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:06:00.0 Off |                  N/A |
|  0%   32C    P8    10W / 350W |     10MiB / 24268MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
</code></pre>

<p>BOOM!</p>

<p>One of the hard parts is figuring out what tag to use on the NVIDIA image. They are all listed here:</p>

<ul>
  <li><a href="https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/supported-tags.md">https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/supported-tags.md</a></li>
</ul>

<p>Examples:</p>

<pre><code>11.4.3-base-ubuntu20.04 (11.4.3/ubuntu20.04/base/Dockerfile)
11.4.3-cudnn8-devel-ubuntu20.04 (11.4.3/ubuntu20.04/devel/cudnn8/Dockerfile)
11.4.3-cudnn8-runtime-ubuntu20.04 (11.4.3/ubuntu20.04/runtime/cudnn8/Dockerfile)
11.4.3-devel-ubuntu20.04 (11.4.3/ubuntu20.04/devel/Dockerfile)
11.4.3-runtime-ubuntu20.04 (11.4.3/ubuntu20.04/runtime/Dockerfile)
</code></pre>

<p>Note that these will change over time, of course. But if Docker reports it can’t find the tag, it’s likely because the tag is wrong, or has changed.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I’m working on running LocalAI. But I feel like running that out of Docker.]]></summary></entry></feed>