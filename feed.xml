<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://serverascode.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://serverascode.com/" rel="alternate" type="text/html" /><updated>2024-11-23T14:18:35-05:00</updated><id>https://serverascode.com/feed.xml</id><title type="html">Server As Code Dot Com</title><subtitle>A techno-blog for our techno-times</subtitle><entry><title type="html">Tailscale, Mullvad, and More</title><link href="https://serverascode.com/2024/11/23/tailscale-mulladvpn.html" rel="alternate" type="text/html" title="Tailscale, Mullvad, and More" /><published>2024-11-23T00:00:00-05:00</published><updated>2024-11-23T00:00:00-05:00</updated><id>https://serverascode.com/2024/11/23/tailscale-mulladvpn</id><content type="html" xml:base="https://serverascode.com/2024/11/23/tailscale-mulladvpn.html"><![CDATA[<div style="display: flex; align-items: center; gap: 20px; margin-bottom: 20px;">
    <img src="/img/magazine-cards/tailscale-mullvad-200w.png" alt="Tailscale Mullvad" style="max-width: 200px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);" />
    <p><em>tldr; Tailscale allows you to create your own private, secure networks, like the LANs of yesteryear, and now they can have "exit nodes" that are Mullvad VPN servers.</em></p>
</div>

<hr />

<p>Iâ€™ve been a Tailscale user for a while now, though my use waned as I struggled to deal with using a VPN as an exit node. For real, that was my problem. I really struggled with the exit nodes and VPNs. For a while I had a tailscale exit node, as simple Linux VM running in Digital Ocean, which is still an option because Tailscale can use any server as an exit node, but it was a pain to setup and manage and updateâ€¦and, and, andâ€¦</p>

<p>Now Tailscale has a feature where you can use Mullvad VPN endpoints as exit nodes. This is got me right back into using Tailscale full time on all of my devices.</p>

<p>Check out the feature page here: <a href="https://tailscale.com/mullvad">https://tailscale.com/mullvad</a></p>

<blockquote>
  <p>NOTE: I have no affiliation with Tailscale or Mullvad. Iâ€™m a big fan of Tailscale because it is a CANADIAN ğŸ‡¨ğŸ‡¦ company!</p>
</blockquote>

<h2 id="tailscale-">Tailscale ğŸŒ</h2>

<p>As mentioned, Tailscale is a Canadian company that makes a VPN service that is very easy to use. But what is a VPN? Why have one at all? What are VPNs really good for? Do they work? There are still a lot of questions out there as to what a â€œvirtual private networkâ€ really is. Is it just something we login to at work in most enterprises?</p>

<blockquote>
  <p>Tailscale makes creating software-defined networks easy: securely connecting users, services, and devices. - <a href="https://tailscale.com">https://tailscale.com</a></p>
</blockquote>

<p>Hereâ€™s the CEO talking about the VPN/Internet problem in the Tailscale blog:</p>

<blockquote>
  <p>We looked at a lot of options, and talked to a lot of people, and there was an underlying cause for all the problems. The Internet. Things used to be simple. Remember the LAN? But then we connected our LANs to the Internet, and thereâ€™s been more and more firewalls and attackers everywhere, and things have slowly been degrading ever since. - <a href="https://tailscale.com/blog/new-internet">https://tailscale.com/blog/new-internet</a></p>
</blockquote>

<p>I donâ€™t want to get into the philosophical discussion here, but I think Tailscale is on to something, Iâ€™m just not exactly sure what it isâ€¦possibly something like how social networks may be changing into group chats. But that is for another blog post.</p>

<h2 id="mullvad-">Mullvad ğŸ¦Š</h2>

<p>I have used PIA, Private Internet Access, for a long time. Itâ€™s a great VPN service. Mullvad is similar, but with a few key differences in that they are heavily privacy focused and have a unique way of â€œlogging inâ€.</p>

<p>Prior to realizing that Tailscale could use Mullvad as an exit node, I had coincidentally been using Mullvad as my VPN for a month or so, wanting to try something new after my PIA subscription expired. Itâ€™s a bit more expensive, but the model is interesting in that you donâ€™t have a subscription really, instead you pay $5 USD per month and you donâ€™t have a username or password, only an account ID.</p>

<p>One thing to note is that Mullvad just doesnâ€™t have the same wide Internet pipes as PIA does, though they do have servers in Canada. However, I donâ€™t see that much of a difference in speed, though it is there. So that is something to consider if you are a heavy downloader.</p>

<p>Overall, a fascinating service that I need to learn more about.</p>

<h2 id="tailscale-and-mullvad-together-">Tailscale and Mullvad Together ğŸ¤</h2>

<p>Basically you enable Mullvad in Tailscale, it costs $5 USD per month for up to 5 devices. This 5 devices model is really useful, because thatâ€™s about what I have in terms of the number of devices that should be on Tailscale. Phones. Workstations. Laptops. Entertainment devices. Servers. Five devices is a good number for me, but if you need more, you just pay more.</p>

<p>First, enable Mullvad in Tailscale. Go to â€œSettingsâ€ and you can find it there.</p>

<p><img src="/img/tailscale-mullvad1.png" alt="Tailscale Mullvad" /></p>

<p>After adding devices you should see something like the below.</p>

<blockquote>
  <p>NOTE: Iâ€™ve removed my devices from the list of course.</p>
</blockquote>

<p><img src="/img/tailscale-mullvad2.png" alt="Tailscale Mullvad" /></p>

<p>Once you enable Mullvad in Tailscale, you can select it as an exit node on your Tailscale devices. Thatâ€™s it. No need to configure Tailscale, then configure Mullvad, then troubleshoot the inevitable configuration issues.</p>

<h2 id="using-mullvad-as-an-exit-node-">Using Mullvad as an Exit Node ğŸ”„</h2>

<p>If you want to see all the exit nodes:</p>

<pre><code class="language-bash">$ tailscale exit-node list

 IP                  HOSTNAME                         COUNTRY            CITY                   STATUS       
 100.91.198.95       al-tia-wg-001.mullvad.ts.net     Albania            Tirana                 -            
 100.65.216.68       au-adl-wg-301.mullvad.ts.net     Australia          Any                    -            
 100.65.216.68       au-adl-wg-301.mullvad.ts.net     Australia          Adelaide               -            
 100.70.240.117      au-bne-wg-301.mullvad.ts.net     Australia          Brisbane               -            
 100.117.126.96      au-mel-wg-301.mullvad.ts.net     Australia          Melbourne              -            
 100.88.22.25        au-per-wg-301.mullvad.ts.net     Australia          Perth                  -            
 100.100.169.122     au-syd-wg-001.mullvad.ts.net     Australia          Sydney                 -            
 100.79.65.118       at-vie-wg-001.mullvad.ts.net     Austria            Vienna                 -            
 100.120.7.76        be-bru-wg-101.mullvad.ts.net     Belgium            Brussels               -            
SNIP!
</code></pre>

<p>I use the <code>just</code> command runner to setup some easy commands to enable/disable Mullvad as an exit node.</p>

<pre><code class="language-bash">tailscale-use-vpn:
	sudo tailscale set \
		--exit-node-allow-lan-access \
		--exit-node=${MULLVAD_EXIT_NODE}

tailscale-no-vpn:
	# blank exit node
	sudo tailscale set --exit-node=
</code></pre>

<p>So if I need to I can turn off the VPN, for example Reddit blocks VPNs, not that I use Reddit, but it certainly comes up in Google searches.</p>

<h2 id="tailscale-access-control-list-">Tailscale Access Control List ğŸ”’</h2>

<p>While Tailscale does talk about being easy to use, it also has complex looking <a href="https://tailscale.com/kb/1018/acls">ACLs</a>. I hadnâ€™t used the ACLs previously, but now with more devices on Tailscale, I wanted to control what could access what, specifically I wanted a couple of devices only to be able to use the exit nodes, not access any other devices on the tailnet.</p>

<p>I struggled a bit with the ACLs, as any kind of RBAC is challenging to get right, but I had three main realizations that allowed me to get the right setup.</p>

<p>1. The only action is â€œacceptâ€â€¦this rattled my cage for a bit, as I was expecting deny or some other keywords.</p>

<blockquote>
  <p>Tailscale access rules deny access by default. As a result, the only possible action is accept. accept allows traffic from the source (src) to the destination (dst). - <a href="https://tailscale.com/kb/1337/acl-syntax">docs</a></p>
</blockquote>

<p>2. Users: You can control based on what user the device is logged in as on the tailnet. I didnâ€™t realize this initially and was just trying to configure by device. And then realizing my mistake, it was much easier, as my main devices are logged in as me, and the other Internet/VPN only devices have their own users that were invited to the tailnet. Duh!</p>

<p>3. Groups: There is an â€œautogroupâ€ for accessing the Internet which means you can set users to only have access to the Internet, and not any other devices on the tailnet. Perfect!</p>

<p><img src="/img/tailscale-mullvad3.png" alt="Tailscale ACLs" /></p>

<p>In the end my ACLs were maybe 10 lines of text, but it took a while to get there.</p>

<h2 id="conclusion-">Conclusion ğŸ¯</h2>

<p>Iâ€™m really happy to be back to using Tailscale full time. We all really need to have better operational security for our Internet, phones, and application useâ€¦which I may be violating by even mentioning my use of Tailscale and Mullvad. But I think itâ€™s a good thing to be talking about, and I hope to see more people using better tools to secure their Internet access.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[tldr; Tailscale allows you to create your own private, secure networks, like the LANs of yesteryear, and now they can have "exit nodes" that are Mullvad VPN servers.]]></summary></entry><entry><title type="html">falcoctl: Installation and Management of Falco Artifacts</title><link href="https://serverascode.com/2024/11/01/falcoctl.html" rel="alternate" type="text/html" title="falcoctl: Installation and Management of Falco Artifacts" /><published>2024-11-01T00:00:00-04:00</published><updated>2024-11-01T00:00:00-04:00</updated><id>https://serverascode.com/2024/11/01/falcoctl</id><content type="html" xml:base="https://serverascode.com/2024/11/01/falcoctl.html"><![CDATA[<div style="display: flex; align-items: center; gap: 20px; margin-bottom: 20px;">
    <img src="/img/magazine-cards/falcoctl-200w.png" alt="falcoctl" style="max-width: 200px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);" />
    <p><em>tl;dr: <a href="https://github.com/falcosecurity/falcoctl">falcoctl</a> is an attempt to make it easier to distribute and upgrade Falco artifacts, such as rules and plugins.</em></p>
</div>

<hr />

<h2 id="what-is-falco">What is Falco?</h2>

<blockquote>
  <p>Falco is a cloud native security tool that provides runtime security across hosts, containers, Kubernetes, and cloud environments. It leverages custom rules on Linux kernel events and other data sources through plugins, enriching event data with contextual metadata to deliver real-time alerts. Falco enables the detection of abnormal behavior, potential security threats, and compliance violations. - <a href="https://falco.org/">Falco</a></p>
</blockquote>

<p><a href="https://falco.org/">Falco</a> is like a security camera for your servers, your Kubernetes nodes, and your cloud environments. Itâ€™s a way to detect and respond to security threats in real time.</p>

<p>The underlying technology relies on rules, which are really a set of configuration files. The point of these security rules is that the world around us is changing, and so we need new and updated rules to help us detect and respond to new and emerging threats. If you simply deploy the standard open source Falco rules and never change them, or do not add your own custom rules based on your own unique needs, youâ€™re missing out on much of the power of Falco.</p>

<p>Once you have the base Falco technology, which is of course completely invaluable, the real work is in creating and managing the threat detection rules and then responding to alerts.</p>

<h2 id="distributed-systems">Distributed Systems</h2>

<p>Itâ€™s important to note that Falco deployments are like a single security camera. They are not, by default, a distributed collection of security cameras that all send their information to a central location, nor do they by default get their configuration from a central location.</p>

<p>This presents a challenge in that organizations often run many hundreds, thousands, or even tens of thousands of nodes, each with their own Falco installation. Additionally, Falco can also have plugins installed on each node, which further complicates the distribution of configuration.</p>

<ul>
  <li><strong>Configuration files</strong> - As mentioned, Falco is a rules based engineâ€“so it needs the rules to work, and thus we have to manage those rules, and distribute them to potentially tens of thousands of nodes.</li>
</ul>

<p>How do we distribute these rules?</p>

<ul>
  <li><strong>Plugins</strong> - Over the last couple of years Falco has added a plugin framework which allows you in many ways to vastly increase the capacity of what Falco can monitor, i.e. historically Falco has looked at system calls, events from the Linux kernel, but with plugins you can add in other data sources, such as audit logs.</li>
</ul>

<p>So we have rules <em>and</em> plugins to manage, but how?</p>

<p>That is where falcoctl comes in.</p>

<h2 id="what-is-falcoctl">What is falcoctl?</h2>

<p>Hereâ€™s a good <a href="https://falco.org/blog/falcoctl-install-manage-rules-plugins/">blog post</a> from the Falco team that explains it well:</p>

<blockquote>
  <p>Since the launch of the plugin framework in January 2022, our adopters have requested an out-of-the-box solution to manage the lifecycle of rules (installation, updates). We heard your request and also created a guide to help you smoothly install the plugins. The Falco maintainers proposed the following solution to help with these issues: falcoctl. Falcoctl is a CLI tool that performs several useful tasks for Falco.</p>
</blockquote>

<p>falcoctl effectively manages artifacts for Falco, including rules, plugins, and configuration.</p>

<ul>
  <li>Install the falcoctl binary CLI application</li>
</ul>

<pre><code>LATEST=$(curl -sI https://github.com/falcosecurity/falcoctl/releases/latest | awk '/location: /{gsub("\r","",$2);split($2,v,"/");print substr(v[8],2)}')
curl --fail -LS "https://github.com/falcosecurity/falcoctl/releases/download/v${LATEST}/falcoctl_${LATEST}_linux_amd64.tar.gz" | tar -xz
sudo install -o root -g root -m 0755 falcoctl /usr/local/bin/falcoctl
</code></pre>

<ul>
  <li>Run it</li>
</ul>

<pre><code>root@falco:~# which falcoctl
/usr/local/bin/falcoctl
root@falco:~# falcoctl

     __       _                _   _ 
    / _| __ _| | ___ ___   ___| |_| |
   | |_ / _  | |/ __/ _ \ / __| __| |
   |  _| (_| | | (_| (_) | (__| |_| |
   |_|  \__,_|_|\___\___/ \___|\__|_|
									 
	
The official CLI tool for working with Falco and its ecosystem components

Usage:
  falcoctl [command]

Available Commands:
  artifact    Interact with Falco artifacts
  completion  Generate the autocompletion script for the specified shell
  driver      Interact with falcosecurity driver
  help        Help about any command
  index       Interact with index
  registry    Interact with OCI registries
  tls         Generate and install TLS material for Falco
  version     Print the falcoctl version information

Flags:
      --config string       config file to be used for falcoctl (default "/etc/falcoctl/falcoctl.yaml")
  -h, --help                help for falcoctl
      --log-format string   Set formatting for logs (color, text, json) (default "color")
      --log-level string    Set level for logs (info, warn, debug, trace) (default "info")

Use "falcoctl [command] --help" for more information about a command.
root@falco:~# 
</code></pre>

<h2 id="manage-falco-artifacts-with-falcoctl">Manage Falco Artifacts with falcoctl</h2>

<p>What we want to do is use falcoctl to manage our Falco artifacts. There are several components and features of falcoctl that we can use to do this.</p>

<ul>
  <li>Add an â€œindexâ€</li>
</ul>

<pre><code>root@falco:~# sudo falcoctl index add falcosecurity https://falcosecurity.github.io/falcoctl/index.yaml
2024-11-01 19:30:11 INFO  Adding index
                      â”œ name: falcosecurity
                      â”” path: https://falcosecurity.github.io/falcoctl/index.yaml
2024-11-01 19:30:11 INFO  Index successfully added 
</code></pre>

<ul>
  <li>Review the config</li>
</ul>

<pre><code>root@falco:~# cat /etc/falcoctl/falcoctl.yaml 
artifact:
    follow:
        every: 6h0m0s
        falcoversions: http://localhost:8765/versions
        refs:
            - falco-rules:3
driver:
    hostroot: /
    name: falco
    repos:
        - https://download.falco.org/driver
    type:
        - modern_ebpf
    version: 7.3.0+driver
indexes:
    - name: falcosecurity
      url: https://falcosecurity.github.io/falcoctl/index.yaml
      backend: ""
</code></pre>

<ul>
  <li>Search for Falco artifacts</li>
</ul>

<pre><code>root@falco:~# falcoctl artifact search falco
INDEX        	ARTIFACT              	TYPE     	REGISTRY	REPOSITORY
falcosecurity	falco-incubating-rules	rulesfile	ghcr.io 	falcosecurity/rules/falco-incubating-rules
falcosecurity	falco-rules           	rulesfile	ghcr.io 	falcosecurity/rules/falco-rules
falcosecurity	falco-sandbox-rules   	rulesfile	ghcr.io 	falcosecurity/rules/falco-sandbox-rules
</code></pre>

<ul>
  <li>Search for Kubernetes artifacts</li>
</ul>

<pre><code>root@falco:~# falcoctl artifact search kubernetes
INDEX        	ARTIFACT          	TYPE     	REGISTRY	REPOSITORY
falcosecurity	k8saudit-eks      	plugin   	ghcr.io 	falcosecurity/plugins/plugin/k8saudit-eks
falcosecurity	k8saudit-gke      	plugin   	ghcr.io 	falcosecurity/plugins/plugin/k8saudit-gke
falcosecurity	k8saudit-gke-rules	rulesfile	ghcr.io 	falcosecurity/plugins/ruleset/k8saudit-gke
falcosecurity	k8saudit-rules    	rulesfile	ghcr.io 	falcosecurity/plugins/ruleset/k8saudit
falcosecurity	k8smeta           	plugin   	ghcr.io 	falcosecurity/plugins/plugin/k8smeta
falcosecurity	k8saudit          	plugin   	ghcr.io 	falcosecurity/plugins/plugin/k8saudit
</code></pre>

<ul>
  <li>Install the <code>falco-rules</code> artifact</li>
</ul>

<pre><code>root@falco:~# falcoctl artifact install falco-rules
2024-11-01 19:33:05 INFO  Resolving dependencies ... 
2024-11-01 19:33:05 INFO  Installing artifacts
                      â”” refs: [ghcr.io/falcosecurity/rules/falco-rules:latest]
2024-11-01 19:33:05 INFO  Preparing to pull artifact
                      â”” ref: ghcr.io/falcosecurity/rules/falco-rules:latest
2024-11-01 19:33:06 INFO  Pulling layer 8da145602705 
2024-11-01 19:33:06 INFO  Pulling layer b3990bf0209c                                            
2024-11-01 19:33:06 INFO  Pulling layer de2cd036fd7f                                            
2024-11-01 19:33:06 INFO  Verifying signature for artifact                                      
                      â”” digest: ghcr.io/falcosecurity/rules/falco-rules@sha256:de2cd036fd7f9bb87de5d62b36d0f35ff4fa8afbeb9a41aa9624e5f6f9a004e1
2024-11-01 19:33:07 INFO  Signature successfully verified! 
2024-11-01 19:33:07 INFO  Extracting and installing artifact
                      â”œ type: rulesfile
                      â”” file: falco_rules.yaml.tar.gz
2024-11-01 19:33:07 INFO  Artifact successfully installed                                       
                      â”œ name: ghcr.io/falcosecurity/rules/falco-rules:latest
                      â”œ type: rulesfile
                      â”œ digest: sha256:de2cd036fd7f9bb87de5d62b36d0f35ff4fa8afbeb9a41aa9624e5f6f9a004e1
                      â”” directory: /etc/falco
</code></pre>

<h2 id="following-as-a-daemon">Following as a Daemon</h2>

<blockquote>
  <p>A great feature of falcoctl is its ability to run as a daemon to periodically check the artifactsâ€™ repositories and automatically install new versions. - <a href="https://github.com/falcosecurity/falcoctl">falcoctl</a></p>
</blockquote>

<p>So this would be the key feature of falcoctl, because, again, itâ€™s great to have the technology, but what does it do to make our lives easier?</p>

<p>When falcoctl tracks (follows) an artifact, it will automatically install new versions of that artifact without any human intervention, which I think is a good thing. Of course, we want to make sure that what we deploy works as expected, but that is a whole other can of worms.</p>

<p>So, with all that in mind, letâ€™s create a service to run falcoctl as a daemon.</p>

<ul>
  <li>Create a service</li>
</ul>

<pre><code>cat &lt;&lt; 'EOF' | sudo tee /etc/systemd/system/falcoctl.service
[Unit]
Description=Falcoctl
After=network.target
StartLimitIntervalSec=0

[Service]
Type=simple
Restart=always
RestartSec=1
ExecStart=/usr/local/bin/falcoctl artifact follow
EOF
systemctl enable falcoctl
systemctl start falcoctl
</code></pre>

<ul>
  <li>Check the status now that it is running</li>
</ul>

<pre><code>root@falco:/etc/falcoctl# systemctl status falcoctl
â— falcoctl.service - Falcoctl
     Loaded: loaded (/etc/systemd/system/falcoctl.service; static)
     Active: active (running) since Fri 2024-11-01 19:37:32 UTC; 11s ago
   Main PID: 5544 (falcoctl)
      Tasks: 7 (limit: 2296)
     Memory: 25.4M (peak: 25.6M)
        CPU: 674ms
     CGroup: /system.slice/falcoctl.service
             â””â”€5544 /usr/local/bin/falcoctl artifact follow

Nov 01 19:37:33 falco falcoctl[5544]:                       â”” artifact: ghcr.io/falcosecuri&gt;
Nov 01 19:37:33 falco falcoctl[5544]: 2024-11-01 19:37:33 INFO  Found new artifact version
Nov 01 19:37:33 falco falcoctl[5544]:                       â”œ followerName: ghcr.io/falcosecuri&gt;
Nov 01 19:37:33 falco falcoctl[5544]:                       â”” tag: 3
Nov 01 19:37:35 falco falcoctl[5544]: 2024-11-01 19:37:35 INFO  Artifact correctly installed
Nov 01 19:37:35 falco falcoctl[5544]:                       â”œ followerName: ghcr.io/falcosecuri&gt;
Nov 01 19:37:35 falco falcoctl[5544]:                       â”œ artifactName: ghcr.io/falcosecuri&gt;
Nov 01 19:37:35 falco falcoctl[5544]:                       â”œ type: rulesfile
Nov 01 19:37:35 falco falcoctl[5544]:                       â”œ digest: sha256:de2cd036fd7f9bb87d&gt;
Nov 01 19:37:35 falco falcoctl[5544]:                       â”” directory: /etc/falco
root@falco:/etc/falcoctl# systemctl status falcoctl --no-pager
â— falcoctl.service - Falcoctl
     Loaded: loaded (/etc/systemd/system/falcoctl.service; static)
     Active: active (running) since Fri 2024-11-01 19:37:32 UTC; 15s ago
   Main PID: 5544 (falcoctl)
      Tasks: 7 (limit: 2296)
     Memory: 25.4M (peak: 25.6M)
        CPU: 674ms
     CGroup: /system.slice/falcoctl.service
             â””â”€5544 /usr/local/bin/falcoctl artifact follow

Nov 01 19:37:33 falco falcoctl[5544]:                       â”” artifact: ghcr.io/falcosecuâ€¦ules:3
Nov 01 19:37:33 falco falcoctl[5544]: 2024-11-01 19:37:33 INFO  Found new artifact version
Nov 01 19:37:33 falco falcoctl[5544]:                       â”œ followerName: ghcr.io/falcoâ€¦ules:3
Nov 01 19:37:33 falco falcoctl[5544]:                       â”” tag: 3
Nov 01 19:37:35 falco falcoctl[5544]: 2024-11-01 19:37:35 INFO  Artifact correctly installed
Nov 01 19:37:35 falco falcoctl[5544]:                       â”œ followerName: ghcr.io/falcoâ€¦ules:3
Nov 01 19:37:35 falco falcoctl[5544]:                       â”œ artifactName: ghcr.io/falcoâ€¦ules:3
Nov 01 19:37:35 falco falcoctl[5544]:                       â”œ type: rulesfile
Nov 01 19:37:35 falco falcoctl[5544]:                       â”œ digest: sha256:de2cd036fd7fâ€¦a004e1
Nov 01 19:37:35 falco falcoctl[5544]:                       â”” directory: /etc/falco
Hint: Some lines were ellipsized, use -l to show in full.
</code></pre>

<p>Looks good.</p>

<h2 id="build-your-own-falcoctl-artifact">Build Your Own Falcoctl Artifact</h2>

<p>Letâ€™s push a rules file into an OCI registry.</p>

<ul>
  <li>First create a rules file</li>
</ul>

<pre><code>cat &lt;&lt; 'EOF' | sudo tee ./custom_rules.yaml
- list: falco_binaries
  items: [falcoctl]
EOF
</code></pre>

<ul>
  <li>Login to the registry</li>
</ul>

<pre><code>root@falco:/etc/falcoctl# falcoctl registry auth basic some_registry -u 'your_user' -p 'some_password'
2024-11-01 19:52:43 INFO  Login succeeded registry: some_registry user: your_user
</code></pre>

<ul>
  <li>Now push that single rules file to the registry</li>
</ul>

<blockquote>
  <p>NOTE: We often think of â€œcontainer imagesâ€ as the only thing we can push to an OCI registry, but we can actually push any OCI-compliant artifact to an OCI registry, and over time we will see much more use of this.</p>
</blockquote>

<pre><code>export OCI_ARTIFACT_VERSION=latest
export OCI_REGISTRY=some_registry
export OCI_REPOSITORY=some_repo/falco-rules
export RULESET_FILE=custom_rules.yaml
falcoctl registry push \
    --config /dev/null \
    --type rulesfile \
    --version ${OCI_ARTIFACT_VERSION} \
    ${OCI_REGISTRY}/${OCI_REPOSITORY}:${OCI_ARTIFACT_VERSION} \
    ${RULESET_FILE}
</code></pre>

<ul>
  <li>Resulting output</li>
</ul>

<pre><code>root@falco:/etc/falcoctl# export OCI_ARTIFACT_VERSION=latest
export OCI_REGISTRY=some_registry
export OCI_REPOSITORY=some_repo/falco-rules
export RULESET_FILE=custom_rules.yaml
falcoctl registry push \
    --config /dev/null \
    --type rulesfile \
    --version ${OCI_ARTIFACT_VERSION} \
    ${OCI_REGISTRY}/${OCI_REPOSITORY}:${OCI_ARTIFACT_VERSION} \
    ${RULESET_FILE}
2024-11-01 20:15:19 INFO  Preparing to push artifact
                      â”œ name: some_registry/some_repo/falco-rules:latest
                      â”” type: rulesfile
2024-11-01 20:15:19 ERROR open custom_rules.yaml: no such file or directory 
root@falco:/etc/falcoctl# cd 
root@falco:~# falcoctl registry push     --config /dev/null     --type rulesfile     --version ${OCI_ARTIFACT_VERSION}     ${OCI_REGISTRY}/${OCI_REPOSITORY}:${OCI_ARTIFACT_VERSION}     ${RULESET_FILE}
2024-11-01 20:15:24 INFO  Preparing to push artifact
                      â”œ name: some_registry/some_repo/falco-rules:latest
                      â”” type: rulesfile
2024-11-01 20:15:24 INFO  Parsing dependencies from:  rulesfile: custom_rules.yaml
2024-11-01 20:15:24 WARN  No dependencies were provided by the user and none were found in the
                      â”‚   rulesfile.
2024-11-01 20:15:24 INFO  Parsing requirements from:  rulesfile: custom_rules.yaml
2024-11-01 20:15:24 WARN  No requirements were provided by the user and none were found in the
                      â”‚   rulesfile.
2024-11-01 20:15:24 INFO  Pushing layer d5c35695420a 
2024-11-01 20:15:26 INFO  97c38f4c17c8: layer already exists                                    
2024-11-01 20:15:26 INFO  Pushing layer c891d7815e0a 
2024-11-01 20:15:26 INFO  Artifact pushed                                                       
                      â”œ name: some_registry/some_repo/falco-rules:latest
                      â”œ type: rulesfile
                      â”” digest: sha256:c891d7815e0a30a1a73e026aea4603503b0a12df9bc8b7efc38f61de2d77bd6b
</code></pre>

<ul>
  <li>Now follow the artifact by updating the falcoctl.yaml file</li>
</ul>

<blockquote>
  <p>NOTE: Below I added <code>some_registry/some_repo/falco-rules:latest</code> to the <code>refs</code> section.</p>
</blockquote>

<pre><code>root@falco:/etc/falcoctl# cat falcoctl.yaml
artifact:
    follow:
        every: 6h0m0s
        falcoversions: http://localhost:8765/versions
        refs:
            - falco-rules:3
            - some_registry/some_repo/falco-rules:latest
driver:
    hostroot: /
    name: falco
    repos:
        - https://download.falco.org/driver
    type:
        - modern_ebpf
    version: 7.3.0+driver
indexes:
    - name: falcosecurity
      url: https://falcosecurity.github.io/falcoctl/index.yaml
      backend: ""
</code></pre>

<ul>
  <li>Restart the falcoctl service</li>
</ul>

<pre><code>systemctl restart falcoctl
</code></pre>

<ul>
  <li>Check the status of the falcoctl service</li>
</ul>

<pre><code>systemctl status falcoctl
</code></pre>

<ul>
  <li>And check if the new rules file was installed</li>
</ul>

<pre><code>root@falco:~# cat /etc/falco/custom_rules.yaml 
- list: falco_binaries
  items: [falcoctl]
</code></pre>

<p>It was!</p>

<ul>
  <li>There are falcoctl logs as well</li>
</ul>

<pre><code>root@falco:~# grep some_repo /var/log/syslog
2024-11-01T20:16:57.198277+00:00 falco falcoctl[5819]:                       â”œ artifact: some_registry/some_repo/falco-rules:latest
2024-11-01T20:16:57.200084+00:00 falco falcoctl[5819]:                       â”” artifact: some_registry/some_repo/falco-rules:latest
2024-11-01T20:16:57.433872+00:00 falco falcoctl[5819]:                       â”œ followerName: some_registry/some_repo/falco-rules:latest
2024-11-01T20:16:59.154603+00:00 falco falcoctl[5819]:                       â”œ followerName: some_registry/some_repo/falco-rules:latest
2024-11-01T20:16:59.154713+00:00 falco falcoctl[5819]:                       â”œ artifactName: some_registry/some_repo/falco-rules:latest
</code></pre>

<p>Easy peasy. Well, at least getting it started. Now you need to automate the creation of the artifact, and of course write your own rules, which is the hard part once you have the technology.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In theory, you could deploy falcoctl as a daemon to every host you have, and configure it to check for your new rules basically on a cronjob-like schedule. Need every host to have a new rules file? No problem, just push the new rules file to the OCI registry and the falcoctl daemon will pull it down and install it.</p>

<p>Will that scale? I donâ€™t know. Maybeâ€¦maybe not.</p>

<p>However, even once you have Falco installed, and then use falcoctl to have a distribution mechanism in place, you still need to customize your rules. Technology only gets us so far.</p>

<h2 id="ps-sysdig">PS. Sysdig</h2>

<p>I work at <a href="https://sysdig.com">Sysdig</a>, and while we use and support Falco, weâ€™ve also built our enterprise product to have a much wider use case (see <a href="https://www.sysdig.com/cnapp">CNAPP</a> which includes CSPM, CIEM, vulnerability management, and more), and to be considerably more scalable than vanilla open source Falco. Sysdig does all the heavy lifting, rule distribution, etc., etc. And most importantly, in the context of threat detection, we write the rules for you as well.</p>

<h2 id="further-reading">Further Reading</h2>

<ul>
  <li><a href="https://falco.org/blog/gitops-your-falco-rules/">Gitops Your Falco Rules</a></li>
  <li><a href="https://github.com/falcosecurity/falcoctl">falcoctl</a></li>
  <li><a href="https://falco.org/docs/getting-started/falco-linux-quickstart/">Try Falcoctl</a></li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[tl;dr: falcoctl is an attempt to make it easier to distribute and upgrade Falco artifacts, such as rules and plugins.]]></summary></entry><entry><title type="html">The Numerous Pains of Programming: Death by a Thousand Cuts</title><link href="https://serverascode.com/2024/10/25/pain-of-programming.html" rel="alternate" type="text/html" title="The Numerous Pains of Programming: Death by a Thousand Cuts" /><published>2024-10-25T00:00:00-04:00</published><updated>2024-10-25T00:00:00-04:00</updated><id>https://serverascode.com/2024/10/25/pain-of-programming</id><content type="html" xml:base="https://serverascode.com/2024/10/25/pain-of-programming.html"><![CDATA[<p><img src="/img/posts/deathcuts.png" alt="/img/posts/deathcuts.png" /></p>

<p>I recently started building a small Python application, less than 1000 lines of code. However, itâ€™s been a real challenge to get it to the point where it can be deployed and run in a production environment. Iâ€™ve done this before, but itâ€™s always difficult, and each time Iâ€™m reminded of how much effort it takes, in part because I always have to start anew.</p>

<p>As a part-time developerâ€¦which of these extra things do I actually have to do? And to what extent? The code, sure, but all the other stuff?</p>

<h2 id="the-pain-of-programming">The Pain of Programming</h2>

<p>For most applications, writing the code is relatively straightforward, though it can still be challenging. However, there are tons of other things around the code, all the extra work you have to do to make it a quote unquote â€œreal applicationâ€.</p>

<p>Iâ€™m not even including things like choosing a language or framework.</p>

<blockquote>
  <p>NOTE: This is not a perfect, comprehensive listâ€“itâ€™s a brainstorm of things I can think of that Iâ€™ve had to do, or might have to do to make an application â€œrealâ€ and somewhat professional.</p>
</blockquote>

<ol>
  <li>Writing and maintaining comprehensive test suites, unit tests, integration tests, etc.</li>
  <li>Setting up testing infrastructure and testing frameworks</li>
  <li>Designing and implementing CI/CD pipelines</li>
  <li>Creating and maintaining distributable packages</li>
  <li>Managing production environment deployment and monitoring</li>
  <li>Implementing release management</li>
  <li>Writing and maintaining documentation</li>
  <li>Creating and updating system architecture diagrams</li>
  <li>Managing version control workflows and git complexity</li>
  <li>Supporting and responding to user needs and feedback</li>
  <li>Implementing basic security best practices</li>
  <li>Conducting stress testing and performance optimization</li>
  <li>Containerizing the application and creating Kubernetes manifests</li>
  <li>Setting up vulnerability scanning</li>
  <li>Managing security issues in dependencies</li>
  <li>Managing databases, data models, and migrations</li>
  <li>Configuring IDE settings and extensions</li>
  <li>Setting up development environments (e.g., virtual environments)</li>
  <li>Managing dependency trees and version conflicts</li>
  <li>Configuring and maintaining linters and formatters</li>
  <li>Setting up logging and monitoring infrastructure</li>
  <li>What kind of loggingâ€¦structured, unstructured, etc.</li>
  <li>Dealing with debugging toolsâ€¦more than print statements?</li>
  <li>Testing with SSL certificates</li>
  <li>Deploying on different operating systems and user environments</li>
  <li>Dealing with authentication, perhaps RBAC too</li>
  <li>Do I need different environments for development, testing, staging, and production?</li>
  <li>Keeping track of issues, tickets, and other project management tasks</li>
  <li>How to get secrets and configuration into the application safely and securely</li>
  <li>Dealing with updating the software, from configuration to removing features and functionality</li>
  <li>Understanding resource requirements, memory, cpu, etc. and what to set limits to in production</li>
  <li>Threat modellingâ€“how would an attacker get in, what would be the impact, etc.</li>
  <li>What license should I use for the software? What does it mean if I choose the wrong one?</li>
  <li>What metrics should it output, and how?</li>
  <li>Configuringâ€¦the config file. What should it look like? How to validate it? Whatâ€™s a good layout?</li>
  <li>Finding dead and unused code</li>
</ol>

<p>I could go on, and Iâ€™m sure there are many more.</p>

<p>All of the above is an enormous amount of work, work that goes far beyond the few hundred or thousand lines of code you write for the core of a simple application. For a small application, it doesnâ€™t seem worth it. There doesnâ€™t seem to be much in the way of easy-to-use automation or other tools to help with all thisâ€“itâ€™s just a big old mess of tasks that add up to a lot of work.</p>

<h2 id="table-formatted-pain">Table Formatted Pain</h2>

<p>Hereâ€™s a table view of the list above.</p>

<p>Again, this is not a comprehensive list, but a brainstorm of things I can think of that Iâ€™ve had to do, or might have to do, to make an application â€œrealâ€ and somewhat professional. Iâ€™m sure there are better lists out there somewhereâ€¦</p>

<table>
  <thead>
    <tr>
      <th>Category</th>
      <th>Tasks</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Testing &amp; Quality</strong></td>
      <td>â€¢ Writing and maintaining test suites (unit, integration)<br />â€¢ Setting up testing infrastructure and frameworks<br />â€¢ Conducting stress testing and performance optimization<br />â€¢ Testing with SSL certificates<br />â€¢ Detecting dead and unused code</td>
    </tr>
    <tr>
      <td><strong>Infrastructure &amp; Deployment</strong></td>
      <td>â€¢ Designing and implementing CI/CD pipelines<br />â€¢ Managing production environment deployment and monitoring<br />â€¢ Managing different environments (dev, test, staging, prod)<br />â€¢ Containerizing applications and Kubernetes manifests<br />â€¢ Deploying on different operating systems/environments</td>
    </tr>
    <tr>
      <td><strong>Security</strong></td>
      <td>â€¢ Implementing basic security best practices<br />â€¢ Setting up vulnerability scanning<br />â€¢ Managing security issues in dependencies<br />â€¢ Dealing with authentication and RBAC<br />â€¢ Threat modeling<br />â€¢ Managing secrets and configuration securely</td>
    </tr>
    <tr>
      <td><strong>Development Environment</strong></td>
      <td>â€¢ Configuring IDE settings and extensions<br />â€¢ Setting up development environments<br />â€¢ Managing dependency trees and conflicts<br />â€¢ Configuring and maintaining linters/formatters<br />â€¢ Managing version control workflows</td>
    </tr>
    <tr>
      <td><strong>Documentation &amp; Architecture</strong></td>
      <td>â€¢ Writing and maintaining documentation<br />â€¢ Creating/updating system architecture diagrams<br />â€¢ Managing licenses and compliance<br />â€¢ Maintaining configuration file structure and validation</td>
    </tr>
    <tr>
      <td><strong>Operations &amp; Monitoring</strong></td>
      <td>â€¢ Setting up logging infrastructure (structured/unstructured)<br />â€¢ Managing debugging tools<br />â€¢ Understanding resource requirements (CPU, memory)<br />â€¢ Output metrics configuration and management</td>
    </tr>
    <tr>
      <td><strong>Data Management</strong></td>
      <td>â€¢ Managing databases, data models, and migrations</td>
    </tr>
    <tr>
      <td><strong>Distribution and Upgrades</strong></td>
      <td>â€¢ Creating and maintaining distributable packages<br />â€¢ Managing software updates and feature deprecation</td>
    </tr>
    <tr>
      <td><strong>Project Management</strong></td>
      <td>â€¢ Supporting and responding to user needs/feedback<br />â€¢ Keeping track of issues, tickets, and tasks<br />â€¢ Implementing release management</td>
    </tr>
  </tbody>
</table>

<h2 id="what-is-a-real-application">What is a real application?</h2>

<p>I keep using the term â€œreal applicationâ€, but what does it mean? What makes an application real? I suppose what I mean is an enterprise, production-grade application of some kind. But that doesnâ€™t mean muchâ€“Iâ€™m not sure anyone knows what makes an application production-grade. Itâ€™s kind of a vague term, and I think we could probably do better. The reality is that programming is more of an art, a dedicated craft, with hundreds of things that need to be done and hundreds of other tools, technologies, and techniques that can be used to do them. Some of this is optional, some of it is required, and some of it is just good practice. Itâ€™s not always clear which is which.</p>

<h2 id="tedious-work">Tedious work</h2>

<p>This list is a decomposition of what a developer has to do. I find all this extra work absolutely exhausting and, frankly, quite tedious. For every single one of these steps, it seems to me that there must be a better way. And for some of them, Iâ€™m sure there is, maybe I just donâ€™t know about it yet. But I donâ€™t think thereâ€™s a single tool, technique, or technology that can help with all of them.</p>

<p>Ultimately, I would love to have a magic wand that I could wave that would do all of that, so I could just focus on the code that delivers the value, but I donâ€™t think thatâ€™s possible. Generative AI isnâ€™t going to help that much, you can imagine it tying it all together somehow, but the problems are still there, theyâ€™re just hidden, like a river of lava under the surface.</p>

<p>Imagine what we could do if we just got rid of all that boredom?</p>

<h2 id="an-application-can-never-be-done-but-it-must-be-run">An application can never be done, but it must be run.</h2>

<p>I think the reality is that an application can never be finished. There is always more work to be done. It can never be 100% finished - there is always something missing, which is an incredible place to be when you consider that one of the main outputs of humanity at this point in time, one of our main economic drivers, is the writing and running of software. What a bizarre situation.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Incus Installation and Use - Setup Storage Pools and Bridged Networking</title><link href="https://serverascode.com/2024/10/19/incus-installation-and-use.html" rel="alternate" type="text/html" title="Incus Installation and Use - Setup Storage Pools and Bridged Networking" /><published>2024-10-19T00:00:00-04:00</published><updated>2024-10-19T00:00:00-04:00</updated><id>https://serverascode.com/2024/10/19/incus-installation-and-use</id><content type="html" xml:base="https://serverascode.com/2024/10/19/incus-installation-and-use.html"><![CDATA[<p>In this post Iâ€™ll show you how to install and setup <a href="https://linuxcontainers.org/incus/docs/main/">Incus</a> on a physical host running Ubuntu 24.04. Iâ€™ll setup a storage pool and a bridge network, then launch a VM. Once this is all done, Iâ€™ll have a nice homelab server that can spin up many virtual machines and do it quickly, putting them on the right storage pool, on a separate network.</p>

<h2 id="what-is-incus">What is Incus?</h2>

<blockquote>
  <p>Incus is a next-generation system container, application container, and virtual machine manager. It provides a user experience similar to that of a public cloud. With it, you can easily mix and match both containers and virtual machines, sharing the same underlying storage and network. - <a href="https://linuxcontainers.org/incus/docs/main/">Incus Docs</a></p>
</blockquote>

<p>Basically, once you install Incus you can ask it for virtual machines or system containers (not Docker containers, but system conatiners) and it will go and build them for you.</p>

<h2 id="physical-host">Physical Host</h2>

<p>Iâ€™m using Ubuntu 24.04 on my homelab server.</p>

<pre><code>$ cat /etc/lsb-release 
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=24.04
DISTRIB_CODENAME=noble
DISTRIB_DESCRIPTION="Ubuntu 24.04.1 LTS"
</code></pre>

<p>Itâ€™s an older server, but itâ€™s got a lot of memory.</p>

<pre><code>$ free -h
               total        used        free      shared  buff/cache   available
Mem:           188Gi       3.1Gi       185Gi       673Mi       2.5Gi       185Gi
Swap:          8.0Gi          0B       8.0Gi
</code></pre>

<p>And lots of room for disks and such, including a 2TB NVMe drive, which Iâ€™ll use for my main storage pool.</p>

<h2 id="install-incus">Install Incus</h2>

<p>Iâ€™ll be following the Incus docs - <a href="https://linuxcontainers.org/incus/docs/main/installing/#installing">https://linuxcontainers.org/incus/docs/main/installing/#installing</a></p>

<p>First, install the incus and qemu packages; need qemu for the VM support.</p>

<pre><code>apt install incus qemu-system
</code></pre>

<p>Incus is a small set of packages, qemu is a fair bit larger.</p>

<p>Add your user to the incus group.</p>

<pre><code>$ sudo adduser curtis incus-admin
info: Adding user `curtis' to group `incus-admin' ...
</code></pre>

<p>Log out and log back in to get the new group, or use newgrp, whatever you want.</p>

<pre><code>$ incus ls
+------+-------+------+------+------+-----------+
| NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS |
+------+-------+------+------+------+-----------+
</code></pre>

<h2 id="storage-pool">Storage Pool</h2>

<p>I have a NVMe drive mounted on /mnt/nvme0n1 and I want to use that to back my incus managed virtual machines.</p>

<pre><code>$ sudo mkdir -p /mnt/nvme0n1/incus
$ incus storage create p1 dir source=/mnt/nvme0n1/incus
Storage pool p1 created
$ incus storage ls
+------+--------+--------------------+-------------+---------+---------+
| NAME | DRIVER |       SOURCE       | DESCRIPTION | USED BY |  STATE  |
+------+--------+--------------------+-------------+---------+---------+
| p1   | dir    | /mnt/nvme0n1/incus |             | 0       | CREATED |
+------+--------+--------------------+-------------+---------+---------+
</code></pre>

<p>Some files and directories are created in /mnt/nvme0n1/incus.</p>

<pre><code>$ ls /mnt/nvme0n1/incus/
buckets     containers-snapshots  custom-snapshots  virtual-machines
containers  custom                images            virtual-machines-snapshots
</code></pre>

<p>Very slick and easy to create the storage pool.</p>

<p>Now to build a VM using that storage pool.</p>

<pre><code>$ incus launch images:ubuntu/24.04 test --vm --storage p1
Launching test
                                          
The instance you are starting doesn't have any network attached to it.
  To create a new network, use: incus network create
  To attach a network to an instance, use: incus network attach
</code></pre>

<p>Note that I donâ€™t have a network configured, so this didnâ€™t actually start the VM.</p>

<p>But, files are created for the VM in the storage pool.</p>

<pre><code class="language-bash">$ sudo tree /mnt/nvme0n1/incus/
/mnt/nvme0n1/incus/
â”œâ”€â”€ buckets
â”œâ”€â”€ containers
â”œâ”€â”€ containers-snapshots
â”œâ”€â”€ custom
â”œâ”€â”€ custom-snapshots
â”œâ”€â”€ images
â”œâ”€â”€ virtual-machines
â”‚Â Â  â””â”€â”€ test
â”‚Â Â      â”œâ”€â”€ agent-client.crt
â”‚Â Â      â”œâ”€â”€ agent-client.key
â”‚Â Â      â”œâ”€â”€ agent.crt
â”‚Â Â      â”œâ”€â”€ agent.key
â”‚Â Â      â”œâ”€â”€ backup.yaml
â”‚Â Â      â”œâ”€â”€ config
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ agent.conf
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ agent.crt
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ agent.key
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ agent-mounts.json
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ files
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ hostname.tpl.out
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ hosts.tpl.out
â”‚Â Â      â”‚Â Â  â”‚Â Â  â””â”€â”€ metadata.yaml
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ incus-agent
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ install.sh
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ lxd-agent -&gt; incus-agent
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ nics
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ server.crt
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ systemd
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ incus-agent.service
â”‚Â Â      â”‚Â Â  â”‚Â Â  â””â”€â”€ incus-agent-setup
â”‚Â Â      â”‚Â Â  â””â”€â”€ udev
â”‚Â Â      â”‚Â Â      â””â”€â”€ 99-incus-agent.rules
â”‚Â Â      â”œâ”€â”€ metadata.yaml
â”‚Â Â      â”œâ”€â”€ OVMF_VARS_4M.ms.fd
â”‚Â Â      â”œâ”€â”€ qemu.nvram -&gt; OVMF_VARS_4M.ms.fd
â”‚Â Â      â”œâ”€â”€ root.img
â”‚Â Â      â””â”€â”€ templates
â”‚Â Â          â”œâ”€â”€ hostname.tpl
â”‚Â Â          â””â”€â”€ hosts.tpl
â””â”€â”€ virtual-machines-snapshots

16 directories, 25 files

</code></pre>

<h2 id="networking">Networking</h2>

<p>OK, I love networking, but it can also be a pain, especially when weâ€™re dealing with bridges and virtual machines, etc, etc. I like to think of networking as moving packets as quickly as possible, not configuring bridges, but thereâ€™s just no avoiding it.</p>

<p>The physical host has the below netplan configuration, where Iâ€™ve added a VLAN to eth3.</p>

<pre><code>$ sudo cat 50-cloud-init.yaml 
network:
    ethernets:
        eno1: {}
        eth3: {}
    version: 2
    vlans:
        eno1.101:
            addresses:
            - 10.100.1.20/24
            id: 101
            link: eno1
            nameservers:
                addresses:
                - 10.100.1.3
                search: []
            routes:
            -   to: default
                via: 10.100.1.1
        eth3.105:
            id: 105
            link: eth3
        eth3.106:
            id: 106
            link: eth3
</code></pre>

<p>Iâ€™m going to tell incus to create a bridge on a network interface that has a VLAN tag on it, eth3.106.</p>

<pre><code>$ incus network create br106 \
  --type=bridge \
  bridge.external_interfaces=eth3.106 \
  ipv4.dhcp=false \
  ipv4.nat=false \
  ipv6.nat=false \
  ipv4.address=none \
  ipv6.address=none
</code></pre>

<p>That command creates this config:</p>

<pre><code>$ incus network show br106
config:
  bridge.external_interfaces: eth3.106
  ipv4.address: none
  ipv4.dhcp: "false"
  ipv4.nat: "false"
  ipv6.address: none
  ipv6.nat: "false"
description: ""
name: br106
type: bridge
used_by:
- /1.0/instances/test
managed: true
status: Created
locations:
- none
</code></pre>

<p>DHCP is actually provided by my physical switch, not incus. So when I launch a VM, it starts with DHCP, but that DHCP address is coming from the upstream switch, not incus. This is what I want.</p>

<p>I can launch a VM with this network on the previously configured storage pool.</p>

<pre><code>$ incus launch images:ubuntu/24.04 test --vm --storage p1 --network br106
</code></pre>

<p>And list the VMs to see the new one, note that we can see the IP address of the VM even though Incus isnâ€™t doing the IP address management.</p>

<pre><code>$ incus ls
+------+---------+---------------------+------+-----------------+-----------+
| NAME |  STATE  |        IPV4         | IPV6 |      TYPE       | SNAPSHOTS |
+------+---------+---------------------+------+-----------------+-----------+
| test | RUNNING | 10.100.6.250 (enp5s0) |      | VIRTUAL-MACHINE | 0         |
+------+---------+---------------------+------+-----------------+-----------+
</code></pre>

<p>Hop onto that VM and ping 1.1.1.1:</p>

<pre><code>$ incus shell test
root@test:~# ping -c 3 1.1.1.1
PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data.
64 bytes from 1.1.1.1: icmp_seq=1 ttl=53 time=5.80 ms
64 bytes from 1.1.1.1: icmp_seq=2 ttl=53 time=3.43 ms
64 bytes from 1.1.1.1: icmp_seq=3 ttl=53 time=3.47 ms

--- 1.1.1.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 3.425/4.232/5.802/1.110 ms
</code></pre>

<p>Network is online!</p>

<p>For various reasons I use Mikrotik switches/routers in my homelab, so this interface might look different on your network. Obviously I donâ€™t have a lot of DHCP going on. :)</p>

<pre><code>[admin@MikroTik] &gt; /ip dhcp-server lease print 
Flags: X - disabled, R - radius, D - dynamic, B - blocked 
 #   ADDRESS                                                                    MAC-ADDRESS       HOST-NAME                                 SERVER                                 RATE-LIMIT                                 STATUS 
 0 D 10.100.6.250                                                                 00:16:3E:4D:15:97 distrobuilder-705ecd65-121a-4b5b-8cdc-... dhcp1                                                                             bound  
[admin@MikroTik] &gt; 
</code></pre>

<p>And we can see the DHCP lease is for the VM.</p>

<h2 id="incus-profiles">Incus Profiles</h2>

<p>Finally, Iâ€™ll create a profile for the VM, or rather Iâ€™ll edit the default profile to use the bridge network and the storage pool.</p>

<pre><code class="language-bash">$ incus profile show default
config: {}
description: Default Incus profile
devices:
  eth0:
    network: br106
    type: nic
  root:
    path: /
    pool: p1
    type: disk
name: default
used_by:
- /1.0/instances/test
</code></pre>

<h2 id="conclusion">Conclusion</h2>

<p>And there you have it. Incus is now managing my virtual machines, putting them on my storage pool, and giving me a bridge network with IPs from my DHCP server.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[In this post Iâ€™ll show you how to install and setup Incus on a physical host running Ubuntu 24.04. Iâ€™ll setup a storage pool and a bridge network, then launch a VM. Once this is all done, Iâ€™ll have a nice homelab server that can spin up many virtual machines and do it quickly, putting them on the right storage pool, on a separate network.]]></summary></entry><entry><title type="html">Why Arenâ€™t You Using Incus to Create Containers and Virtual Machines?</title><link href="https://serverascode.com/2024/10/12/incus.html" rel="alternate" type="text/html" title="Why Arenâ€™t You Using Incus to Create Containers and Virtual Machines?" /><published>2024-10-12T00:00:00-04:00</published><updated>2024-10-12T00:00:00-04:00</updated><id>https://serverascode.com/2024/10/12/incus</id><content type="html" xml:base="https://serverascode.com/2024/10/12/incus.html"><![CDATA[<h1 id="incus">Incus</h1>

<p>Virtual machines remain the main building block of pretty much all infrastructure. We tend to forget about the technology and just how entrenched it is in our daily technical lives. Ok, thatâ€™s a pretty heavy statement for a blog post, but I do think we forget about virtual machines and just how valuable and secure the technology isâ€“most public cloud services are loss leaders for the VM part of their business.</p>

<p>Anyways, what I want to talk about is <a href="https://linuxcontainers.org/incus/introduction/">Incus</a>, a way to easily create containers AND virtual machines.</p>

<blockquote>
  <p>NOTE: I use Incus to exclusively create virtual machines, and donâ€™t use the container functionality that much. I would imagine that most people use the container functionality more. So while Iâ€™ll touch on the system container functionality, I use Incus for local VMs.</p>
</blockquote>

<h2 id="what-is-incus">What is Incus?</h2>

<blockquote>
  <p>When using Incus, you can manage your instances (containers and VMs) with a simple command line tool, directly through the REST API or by using third-party tools and integrations. Incus implements a single REST API for both local and remote access. The Incus project was created by Aleksa Sarai as a community driven alternative to Canonicalâ€™s LXD. Today, itâ€™s led and maintained by many of the same people that once created LXD. - <a href="https://linuxcontainers.org/incus/introduction/">Incus Docs</a></p>
</blockquote>

<p>With Incus, you can easily create virtual machines and containers.</p>

<p>I donâ€™t know all the history of the project, where it comes from in terms of LXD/LXC, etc, but I do know that I need a way to easily create virtual machines on my local computer, and that I really enjoy using Incus. So easy.</p>

<p>Example of creating a virtual machine:</p>

<pre><code class="language-bash">incus launch ubuntu:22.04 my-server --vm
</code></pre>

<p>Itâ€™s that easy. Especially on Ubuntu 24.04, where you can just install the <code>incus</code> package from the default repositories.</p>

<p>I also alias the incus command to this script because I always forget the incus command syntax, and Iâ€™m super lazy. So this would create a default sized VM with just <code>vm launch my-vm</code>.</p>

<pre><code class="language-bash">#!/bin/bash

launch_vm() {
  if [ -z "$1" ]; then
    echo "Usage: $0 launch &lt;vm-name&gt;"
    echo "Example: $0 launch my-ubuntu-vm"
    exit 1
  fi

  local name="$1"
  local cpu=2
  local memory="4GiB"
  local disk="40GiB"

  incus launch images:ubuntu/24.04 "$name" --vm \
    --device root,size="$disk" \
    -c limits.cpu="$cpu" \
    -c limits.memory="$memory"
}

list_vms() {
  incus ls
}

show_help() {
  echo "Usage: $0 &lt;command&gt; [options]"
  echo
  echo "Commands:"
  echo "  launch &lt;vm-name&gt;  Launch a new VM"
  echo "  ls                List all VMs"
  echo "  help              Show this help message"
  echo
  echo "For other commands, this script will pass them directly to incus."
}

# Main command handler
case "$1" in
  launch)
    launch_vm "$2"
    ;;
  ls)
    list_vms
    ;;
  help)
    show_help
    ;;
  *)
    if [ -z "$1" ]; then
      show_help
    else
      # If the command isn't recognized, pass it to incus
      incus "$@"
    fi
    ;;
esac
</code></pre>

<h2 id="using-incus">Using Incus</h2>

<p>As mentioned earlier, I almost exclusively use Incus to get a virtual machine.</p>

<p>E.g. with my script I just run:</p>

<pre><code class="language-bash">vm launch a-vm
</code></pre>

<p>Or with the bare Incus command itâ€™s just as easy:</p>

<pre><code class="language-bash">incus launch images:ubuntu/24.04 a-vm --vm
</code></pre>

<p>Now I can shell into the VM very quickly.</p>

<pre><code class="language-bash">$ incus shell a-vm # or with my script, vm shell a-vm
root@a-vm:~# 
</code></pre>

<p>And you are in a nice little virtual machine that you can install anything you want into.</p>

<h2 id="getting-a-container">Getting a Container</h2>

<p>Writing this post was the first time I used the container functionality of Incus! Getting a container is the default mode of operation, and itâ€™s super easy.</p>

<pre><code class="language-bash">$ incus launch images:ubuntu/22.04 ubuntu-container
# Image is downloaded, and the container is created
Launching ubuntu-container
$ incus ls | grep ubuntu-container
| ubuntu-container | RUNNING | 10.57.7.201 (eth0)           | fd42:af1f:b7c8:a36c:216:3eff:fee9:32e4 (eth0)   | CONTAINER       | 0         |
$ vm shell ubuntu-container
</code></pre>

<p>That is lightning fast. But again, important to note: this is a â€œsystem containerâ€ and not a â€œapplication containerâ€, or in simpler terms, itâ€™s not a docker container. If you have ever used LXC, then you will be right at home.</p>

<blockquote>
  <p>Application containers (as provided by, for example, Docker) package a single process or application. System containers, on the other hand, simulate a full operating system similar to what you would be running on a host or in a virtual machine. You can run Docker in an Incus system container, but you would not run Incus in a Docker application container. - <a href="https://linuxcontainers.org/incus/docs/main/explanation/containers_and_vms/">Incus Docs</a></p>
</blockquote>

<p>You may also want to understand the differences between a virtual machine and a system container:</p>

<blockquote>
  <p>Virtual machines create a virtual version of a physical machine, using hardware features of the host system. The boundaries between the host system and virtual machines is enforced by those hardware features. System containers, on the other hand, use the already running OS kernel of the host system instead of launching their own kernel. If you run several system containers, they all share the same kernel, which makes them faster and more lightweight than virtual machines. - <a href="https://linuxcontainers.org/incus/docs/main/explanation/containers_and_vms/">Incus Docs</a></p>
</blockquote>

<h2 id="why-use-incus">Why Use Incus?</h2>

<p>You can see a list of major features <a href="https://linuxcontainers.org/incus/introduction/#features">here</a> but what I like about it might not be the same as what you like about it.</p>

<ol>
  <li>Itâ€™s very fast - There is an agent in the image that makes getting a shell into the VM super fast. The images are also small and download like lightning, at least in my experience.</li>
  <li>Itâ€™s easy to manage - Incus has a simple syntax for launching VMs and containers</li>
  <li>Image based - Incus uses images, instead of futzing around with qemu backing files and such</li>
  <li>You use Linux as your workstation and need to easily get a VM, or a system container</li>
</ol>

<p>You can also try it online: <a href="https://linuxcontainers.org/incus/try-it/">https://linuxcontainers.org/incus/try-it/</a></p>

<h2 id="why-wouldnt-you-use-incus">Why Wouldnâ€™t You Use Incus?</h2>

<ol>
  <li>Itâ€™s not Docker - itâ€™s a different style of containerization, which many people are not used to.</li>
  <li>Itâ€™s relatively new, and a lot of work is being done on it - But on Ubuntu 24.04 itâ€™s easy to install and get started.</li>
  <li>I do have some trouble with outbound access from the VMs and have futzed around with Iptables to get it working, but itâ€™s not as easy as one would thinkâ€“I expect Iâ€™m missing something obvious from the docs.</li>
</ol>

<p>Thatâ€™s about all I can think of.</p>

<h2 id="pairing-incus-with-my-kubernetes-install-script">Pairing Incus with My Kubernetes Install Script</h2>

<p>FYI - If you create an 8GB VM with 4 CPUS, my <a href="https://github.com/ccollicutt/install-kubernetes">single node Kubernetes install script</a> pairs nicely with Incus, and is often what I use it for.</p>

<h2 id="incus-66-was-just-released">Incus 6.6 Was Just Released</h2>

<p>See <a href="https://discuss.linuxcontainers.org/t/incus-6-6-has-been-released/21762">here</a>. There is also a video overview of the new features <a href="https://www.youtube.com/watch?v=gGBEPtQiiQQ">here</a></p>

<p>Install it, try it out. Have fun easily creating VMs and containers!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Incus]]></summary></entry><entry><title type="html">Building an Insecure Appâ€¦on Purpose (So That GenAI Can Fix It)</title><link href="https://serverascode.com/2024/10/02/building-an-insecure-app.html" rel="alternate" type="text/html" title="Building an Insecure Appâ€¦on Purpose (So That GenAI Can Fix It)" /><published>2024-10-02T00:00:00-04:00</published><updated>2024-10-02T00:00:00-04:00</updated><id>https://serverascode.com/2024/10/02/building-an-insecure-app</id><content type="html" xml:base="https://serverascode.com/2024/10/02/building-an-insecure-app.html"><![CDATA[<h2 id="tldr">tldr;</h2>

<p>tldr; I built an insecure web application (on purpose) for testing LLMs and here it is: <a href="https://github.com/ccollicutt/insecure-nextjs-guestbook">https://github.com/ccollicutt/insecure-nextjs-guestbook</a>.</p>

<h2 id="dealing-with-technical-debt-using-genai">Dealing with Technical Debt using GenAI</h2>

<p>Is cybersecurity largely a technical issue? An engineering issue? Itâ€™s difficult to say. Certainly human psychology plays a big part of it, but, then again, weâ€™re building (insecure) software things and putting them out into the world. We write billions of lines of code, and we canâ€™t do that without making mistakesâ€¦so there are billions of mistakes too. That code has bugs, it gets worse over time, and is hard (read: expensive) to maintain. It ends up being a technical liabilityâ€“a security liability. The reality of software development is an important part of the cybersecurity story. Not the whole story, but an important part.</p>

<p>For this line of thinking, the question is, can Generative Artificial Intelligence (GenAI) help us deal with all this overwhelming technical debt? I believe that GenAI can code, and code well enough to help us get rid of technical debt. And whatâ€™s more, this ability can be automated and has the potential to be fastâ€“very fastâ€“so it can potentially take care of a lot of technical debt in a short period of time. Now, not everyone may agree with me, but thatâ€™s my opinion, and Iâ€™m sticking to it!</p>

<p>So if you believe, or can suspend your disbelief, that GenAI can help you deal with technical debt, then youâ€™ll want to test it, just like I do. But how do you test code that generates code?</p>

<p>Iâ€™m going to build an insecure web application and then use GenAI to try to fix it.</p>

<h2 id="what-does-insecure-mean">What Does â€œInsecureâ€ Mean?</h2>

<p>However, building an insecure web application is a bit of a challenge. On the one hand, we have all kinds of technical debt thatâ€™s easy to accumulate in the real world, but on the other hand, when we write a new application, the frameworks, libraries, and tools we use are working behind the scenes to keep us as secure as possible, so in some ways itâ€™s a challenge to build an insecure application, at least out of the gate.</p>

<p>And yet I managed to do it. At least partially.</p>

<p>So, what is an insecure web application? What are common examples of insecurity in a web application?</p>

<h2 id="owasp-top-10">OWASP Top 10</h2>

<p>One way to think about web app vulnerabilities is through the <a href="https://owasp.org/www-project-top-ten/">OWASP Top 10</a>.</p>

<p>Hereâ€™s the current OWASP Top 10, as of 2021:</p>

<ul>
  <li>A01:2021-Broken Access Control</li>
  <li>A02:2021-Cryptographic Failures</li>
  <li>A03:2021-Injection</li>
  <li>A04:2021-Insecure Design</li>
  <li>A05:2021-Security Misconfiguration</li>
  <li>A06:2021-Vulnerable and Outdated Components</li>
  <li>A07:2021-Identification and Authentication Failures</li>
  <li>A08:2021-Software and Data Integrity Failures</li>
  <li>A09:2021-Security Logging and Monitoring Failures</li>
  <li>A10:2021-Server-Side Request Forgery (SSRF)</li>
</ul>

<p>Letâ€™s look at A01:2021-Broken Access Control, as defined by OWASP:</p>

<ul>
  <li>Violation of the principle of least privilege or deny by default, where access should only be granted for particular capabilities, roles, or users, but is available to anyone.</li>
  <li>Bypassing access control checks by modifying the URL (parameter tampering or force browsing), internal application state, or the HTML page, or by using an attack tool modifying API requests.</li>
  <li>Permitting viewing or editing someone elseâ€™s account, by providing its unique identifier (insecure direct object references)</li>
  <li>Accessing API with missing access controls for POST, PUT and DELETE.</li>
  <li>Elevation of privilege. Acting as a user without being logged in or acting as an admin when logged in as a user.</li>
  <li>Metadata manipulation, such as replaying or tampering with a JSON Web Token (JWT) access control token, or a cookie or hidden field manipulated to elevate privileges or abusing JWT invalidation.</li>
  <li>CORS misconfiguration allows API access from unauthorized/untrusted origins.</li>
  <li>Force browsing to authenticated pages as an unauthenticated user or to privileged pages as a standard user.</li>
</ul>

<p>Forced browsing, as an example, sounds fun and technicalâ€“but itâ€™s really just about browsing pages you arenâ€™t supposed to know exist, pages that just happen to have additional permissions or access that the average user doesnâ€™t have.</p>

<blockquote>
  <p>Forced browsing is an attack where the aim is to enumerate and access resources that are not referenced by the application, but are still accessible. An attacker can use Brute Force techniques to search for unlinked contents in the domain directory, such as temporary directories and files, and old backup and configuration files. These resources may store sensitive information about web applications and operational systems, such as source code, credentials, internal network addressing, and so on, thus being considered a valuable resource for intruders. - <a href="https://owasp.org/www-community/attacks/Forced_browsing">https://owasp.org/www-community/attacks/Forced_browsing</a></p>
</blockquote>

<p>And, in a similar vein, (typically SQL) injection, as defined by OWASP:</p>

<ul>
  <li>User-supplied data is not validated, filtered, or sanitized by the application.</li>
  <li>Dynamic queries or non-parameterized calls without context-aware escaping are used directly in the interpreter.</li>
  <li>Hostile data is used within object-relational mapping (ORM) search parameters to extract additional, sensitive records.</li>
  <li>Hostile data is directly used or concatenated. The SQL or command contains the structure and malicious data in dynamic queries, commands, or stored procedures.</li>
</ul>

<p>Some of these are more interesting than others, and for someâ€“itâ€™s hard to believe that they are still happening in 2024.</p>

<h2 id="building-an-insecure-web-app">Building an Insecure Web App</h2>

<p><img src="/img/insecure-webapp-guestbook.jpg" alt="img" /></p>

<p>While there are a handful of â€œwebgoatâ€-style applications that will help you learn about these vulnerabilities, I decided to build my own so that I would know exactly what problems I was introducing - and thus I would know exactly what problems I was trying to fix with GenAI.</p>

<p>I was working on learning NodeJS and NextJS, so I decided to build my insecure web application using those technologies.</p>

<p>A few points:</p>

<ul>
  <li>
    <p>I wanted to make a guestbook app of all things because it would be easy to build, and the fact that anyone should be able to post to it would make it easier to introduce vulnerabilities.</p>
  </li>
  <li>
    <p>I immediately put the clear text authentication into a SQLite database. However, in the real world, no one would put cleartext authentication in a SQLite databaseâ€“or even use their own authentication system. There are many, many libraries and SaaS services that provide authentication as a service, which is much, much more secure, and that is what people will use. (That is, theyâ€™re not as easy to configure, and theyâ€™re error-prone, but theyâ€™re still much more secure than doing it yourself). I imagine most people building a new web application would either use a third party or <a href="https://next-auth.js.org/">NextAuth</a>.</p>
  </li>
  <li>
    <p>There is an admin user with a default password.</p>
  </li>
</ul>

<pre><code>// Insert admin user if not exists
db.get(`SELECT * FROM users WHERE username = 'admin'`, (err, row) =&gt; {
  if (!row) {
    db.run(`INSERT INTO users (username, password, admin) VALUES ('admin', 'admin', 1)`);
  }
});
</code></pre>

<ul>
  <li>I wanted it to be susceptible to SQL injectionâ€“but interestingly, SQLite does a lot of work to prevent that, so I had to do some work to make it vulnerable in terms of using raw queries. For the most part, SQLite just does the right thing, and you have to do some work to make it vulnerable.</li>
</ul>

<pre><code>// Vulnerable to SQL injection
const query = `SELECT * FROM users WHERE username = '${username}' AND password = '${password}'`;
</code></pre>

<ul>
  <li>
    <p>I also added an admin page that was supposed to be protected, but wasnâ€™t.</p>
  </li>
  <li>
    <p>Originally the app didnâ€™t use a sessionID in the URL, but I added that to make the webapp EVEN MORE VULNERABLE. But you donâ€™t see sessionIDs in the wild, so Iâ€™m not sure if thatâ€™s realistic.</p>
  </li>
</ul>

<p>What I have so far is a webapp that is vulnerable to a number of attacks, including</p>

<ul>
  <li>SQL injection</li>
  <li>Forced browsing</li>
  <li>Session hijacking</li>
  <li>Probably Cross-Site Scripting (XSS)</li>
</ul>

<h2 id="testing-the-vulnerabilities">Testing the Vulnerabilities</h2>

<p>In addition to building the insecure application, we need to test for the presence of these vulnerabilities. So there is also a script to test for them. Please note that this is not an exhaustive list of vulnerabilities, but rather a set of examples meant to be illustrative, and in fact many of them do not work.</p>
<pre><code>$ ./tests.sh 
Usage: ./tests.sh [test_name]

Available tests:
  login                  Test common logins
  sql_injection          Run SQL Injection Test
  drop_table             Drop messages table with SQL Injection
  xss                    Run Cross-Site Scripting (XSS) Test
  insecure_auth          List all users and get admin password via SQL Injection
  sensitive_data         Run Sensitive Data Exposure Test
  security_misconfig     Run Security Misconfiguration Test
  known_vulnerabilities  Run Known Vulnerabilities Test
  insufficient_logging   Run Insufficient Logging &amp; Monitoring Test
  list_tables_and_entries List all tables and entries in the database
  help                   Display this help message
  list_users             List all users in the database
  list_nonexistent_users List all users in the database
  list_tables_and_entries List all tables and entries in the database
  list_nonexistent_users List all users in the database
</code></pre>

<p>Hereâ€™s an example of SQL injection:</p>

<pre><code>$ ./tests.sh sql_injection
###################################################
# Running SQL Injection Test to create admin user #
###################################################
Step 1: Attempting SQL injection to create admin user...
SQL Injection Response: {"message":"Login successful","sessionId":"d86976cace3f01e5ae248e037483d70d","isAdmin":true,"redirectUrl":"/?sessionId=d86976cace3f01e5ae248e037483d70d&amp;username=admin' --&amp;isAdmin=true"}

Step 2: Inserting hacker user with admin privileges...

Step 3: Attempting to login as the new admin user 'hacker'...
Login response: {"message":"Login successful","sessionId":"d2cfc602ff3fd33d201d69f0fac9bdd2","isAdmin":true,"redirectUrl":"/?sessionId=d2cfc602ff3fd33d201d69f0fac9bdd2&amp;username=hacker&amp;isAdmin=true"}
User 'hacker' logged in successfully with admin privileges. SQL Injection successful.

Step 4: Checking database for 'hacker' user...
19|hacker|hackpass|1

Step 5: Listing all users in the database...
1|admin|admin|1
2|test|stsdf|0
3|admin|admin123|0
19|hacker|hackpass|1
</code></pre>

<p>Or drop some tables:</p>

<pre><code>$ ./tests.sh drop_table
##############################################################################################################################
# Running SQL Injection to Drop Table. This will attempt to drop the 'messages' table from the database using SQL injection. #
##############################################################################################################################
Logging in as admin to perform SQL Injection to drop the messages table...
Logging in with admin:admin
SessionId: f86bec743b5e8cd60ec886b4a6e9e3b1
IsAdmin: true


./tests.sh: line 143: get_cookie: command not found
Dropping the messages table with SQL Injection...
Response: {"message":"Entry added successfully","result":{}}
Querying the database to check if the messages table still exists...
users
</code></pre>

<p>Super hacker stuff, I know.</p>

<h2 id="building-insecure-applications-is-a-lot-of-work">Building Insecure Applications is a Lot of Work</h2>

<p>After all, getting a bunch of vulnerabilities into an application is a lot of work. Itâ€™s not realistic to deal with every example that OWASP provides. Furthermore, real-world scenarios are often <em>much</em> more complicated and <em>much</em>  more technical and subtle. Most of what we focus on in cybersecurity is the problem of aging code and the vulnerabilities that come with it. There is less focus on the vulnerabilities that come from improper use of libraries and frameworks and their configuration, vulnerabilities that are subtle and harder to detect. The web application Iâ€™m building is more like using a sledgehammer instead of a scalpel, if youâ€™ll pardon the mixed metaphors.</p>

<p>I also need to do more research on OWASP, other tools like the Atomic Red Team, and what other â€œwebgoatâ€-style applications are out there and how they work, and what they do best.</p>

<p>Find the code, such as it is, <a href="https://github.com/ccollicutt/insecure-nextjs-guestbook">here</a>.</p>

<h2 id="next-up">Next Up</h2>

<p>In future posts, Iâ€™ll look more at this insecure webapp, how to test and execute the exploitable vulnerabilities, as well as how to fix it, if possible, using GenAI and tools like <a href="https://cursor.sh/">Cursor</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[tldr;]]></summary></entry><entry><title type="html">Easily Create a Single Node Kubernetes Cluster</title><link href="https://serverascode.com/2024/08/22/install-kubernetes-script.html" rel="alternate" type="text/html" title="Easily Create a Single Node Kubernetes Cluster" /><published>2024-08-22T00:00:00-04:00</published><updated>2024-08-22T00:00:00-04:00</updated><id>https://serverascode.com/2024/08/22/install-kubernetes-script</id><content type="html" xml:base="https://serverascode.com/2024/08/22/install-kubernetes-script.html"><![CDATA[<p>Iâ€™ve been working with Kubernetes for a long time. Too long, actually. So long, in fact, that I donâ€™t really use it much anymore. Kubernetes has won in terms of being the default way to deploy modern applications. At this point, itâ€™s kind of boring, which is great! We want boring infrastructure. Boring works. If youâ€™re writing a new application today, the target is going to be a container, and that container is probably going to run in good old boring Kubernetes.</p>

<p>For quite a while, the last few years, I have had a bunch of Kubernetes clusters running in my basement. I have half a rack there that used to be filled with servers. Then that changed to just running one larger server with a couple hundred gigs of memory, and that one server was running a bunch of Kubernetes clusters. But recently I shut that down. Mainly because itâ€™s summer here in Toronto and that one big server was heating up the basement, and I wasnâ€™t using it that much. I may turn it on again in the winter. Not sure. Anywaysâ€¦</p>

<p>Yesterday I needed a small k8s cluster. So I used my good old <code>install-kubernetes.sh</code> script to install it onto a VM running on my local workstation.</p>

<h2 id="tldr">tl;dr</h2>

<ul>
  <li>I have a 500 line bash script that installs Kubernetes on Ubuntu 22.04, usually a small VM, 8 gigs of ram, 2-4 CPUs, 40 gigs of disk.</li>
  <li>The script can create a single node â€œclusterâ€</li>
  <li>Or you can deploy a bunch of virtual machines and make one a control plane node and the other workers</li>
  <li>It only takes 2 or 3 minutes to get a k8s cluster. Below is a picture of the test I ran in a github action. Of course, githubâ€™s infrastructure is blazing fastâ€“the speed of the installation will largely depend on how fast you can download packages to the host.</li>
</ul>

<p><img src="/img/install-k8s-action.png" alt="quick install in a github action" /></p>

<p>There are other single node k8s tools, but I like mine, of course :)</p>

<h2 id="install-kubernetes">Install Kubernetes</h2>

<p>For a year and a half or soâ€“first commit was March of 2023â€“Iâ€™ve had a script that will deploy a Kubernetes cluster into a virtual machine.</p>

<p>That script can be found here:</p>

<ul>
  <li><a href="https://github.com/ccollicutt/install-kubernetes">https://github.com/ccollicutt/install-kubernetes</a></li>
</ul>

<p>I havenâ€™t used it for a while, and so it was actually broken for the last bit because the upstream Kubernetes project changed where the packages for Ubuntu are located. So I just updated the script, like bumped it to Kubernetes 1.31, fixed a few other things, and now itâ€™s good to go again to create either a cluster of Kubernetes instances or, perhaps more useful, a full Kubernetes deployment running in a single virtual machine instance (where the single node is both a control plane node and a worker node).</p>

<h2 id="building-a-single-node-kubernetescluster">Building a Single Node Kubernetesâ€¦â€œClusterâ€</h2>

<p>First, get yourself an Ubuntu 22.04 virtual machine with at least 8 gigs of ram and around 40 gigs of disk. Iâ€™d probably also give it 4 CPUs.</p>

<pre><code>root@install-k8s-demo:~# source /etc/lsb-release; echo $DISTRIB_RELEASE
22.04
root@install-k8s-demo:~# nproc
4
root@install-k8s-demo:~# free -h
           	total    	used    	free  	shared  buff/cache   available
Mem:       	7.7Gi   	124Mi   	7.5Gi    	17Mi   	111Mi   	7.4Gi
Swap:         	0B      	0B      	0B
root@install-k8s-demo:~# lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sda  	8:0	0   40G  0 disk
â”œâ”€sda1   8:1	0  100M  0 part /boot/efi
â””â”€sda2   8:2	0 39.9G  0 part /
</code></pre>

<p>Then, login to that VM and get a root shell.</p>

<p>Next, grab the install-kubernetes script.</p>

<blockquote>
  <p>NOTE: Youâ€™ll need git installed.</p>
</blockquote>

<pre><code>root@install-k8s-demo:~# git clone https://github.com/ccollicutt/install-kubernetes
root@install-k8s-demo:~# cd install-kubernetes
</code></pre>

<p>There will be a few files there:</p>

<pre><code>root@install-k8s-demo:~/install-kubernetes# ls
install-kubernetes.sh  makefile  manifests  README.md
</code></pre>

<p>Then, we simply run the <code>install-kubernetes.sh </code>script BUT using the â€œ-sâ€ option to set it so that it deploys a single node control plane + worker node.</p>

<blockquote>
  <p>NOTE: If you forget the â€œ-sâ€, it is probably best to recreate the virtual machine and reinstall it. This is not idempotent, or at least it hasnâ€™t been tested that way.</p>
</blockquote>

<pre><code>./install-kubernetes.sh -s
</code></pre>

<p>The output of that will look like:</p>

<pre><code>root@install-k8s-demo:~/install-kubernetes# ./install-kubernetes.sh -s
Starting install...
==&gt; Logging all output to /tmp/install-kubernetes-NMxK9WTKim/install.log
Checking Linux distribution
Disabling swap
Removing packages
Installing required packages
Installing Kubernetes packages
Configuring system
Configuring crictl
Configuring kubelet
Configuring containerd
Installing containerd
Starting services
Configuring control plane node...
Initialising the Kubernetes cluster via Kubeadm
Configuring kubeconfig for root and ubuntu users
Installing Calico CNI
==&gt; Installing Calico tigera-operator
==&gt; Installing Calico custom-resources
Waiting for nodes to be ready...
==&gt; Nodes are ready
Checking Kubernetes version...
==&gt; Client version: v1.31.0
==&gt; Server Version: v1.31.0
==&gt; Requested KUBE_VERSION matches the server version.
Installing metrics server
Configuring as a single node cluster
Configuring as a single node cluster
Deploying test nginx pod
Waiting for all pods to be running...
Install complete!

### Command to add a worker node ###
kubeadm join localhost:6443 --token &lt;redact&gt; --discovery-token-ca-cert-hash sha256:&lt;redact&gt;
</code></pre>

<h2 id="now-you-have-a-kubernetes-cluster">Now You Have a Kubernetes Cluster</h2>

<p>At this point, you can run kubectl and access the local cluster.</p>

<p>Thereâ€™s a kubeconfig in:</p>

<pre><code>root@install-k8s-demo:~# ls ~/.kube/
cache  config
</code></pre>

<p>And, if there is an ubuntu user on the host, the config will be there too.</p>

<pre><code>root@install-k8s-demo:~# ls /home/ubuntu/.kube/
config
</code></pre>

<p>And we can connect to the â€œclusterâ€.</p>

<pre><code>root@install-k8s-demo:~# kubectl get pods -A
NAMESPACE      	NAME                                   	READY   STATUS	RESTARTS   AGE
calico-apiserver   calico-apiserver-78d48b5579-j97lc      	1/1 	Running   0      	4m15s
calico-apiserver   calico-apiserver-78d48b5579-kmcvr      	1/1 	Running   0      	4m15s
calico-system  	calico-kube-controllers-7d868b8f66-fldb5   1/1 	Running   0      	4m45s
calico-system  	calico-node-pqfdn                      	1/1 	Running   0      	4m45s
calico-system  	calico-typha-899c7464d-9vqzg           	1/1 	Running   0      	4m45s
calico-system  	csi-node-driver-vqvnx                  	2/2 	Running   0      	4m36s
kube-system    	coredns-6f6b679f8f-tvtjs               	1/1 	Running   0      	4m49s
kube-system    	coredns-6f6b679f8f-zvcdl               	1/1 	Running   0      	4m49s
kube-system    	etcd-install-k8s-demo                  	1/1 	Running   0      	4m57s
kube-system    	kube-apiserver-install-k8s-demo        	1/1 	Running   0      	4m56s
kube-system    	kube-controller-manager-install-k8s-demo   1/1 	Running   0      	4m57s
kube-system    	kube-proxy-9snr9                       	1/1 	Running   0      	4m49s
kube-system    	kube-scheduler-install-k8s-demo        	1/1 	Running   0      	4m56s
kube-system    	metrics-server-5f94f4d4fd-sg2gh        	1/1 	Running   0      	4m35s
tigera-operator	tigera-operator-b974bcbbb-4sjjz        	1/1 	Running   0      	4m49s
</code></pre>

<h2 id="you-can-deploy-many-worker-nodes-if-you-want">You Can Deploy Many Worker Nodes if You Want</h2>

<p>You could also use this script to deploy a single control plane only node, the standard model for deploying K8s where the control plane is separated, and then create and add as many worker nodes to that control plane as makes sense. However, this script does not orchestrate all of this. You would have to log in to each VM and run the script, set it up as a worker node or a control plane node, and then join the worker nodes to the control plane node using the kubeadm join command. So this is not meant to be some kind of high-level k8s cluster creation orchestration mechanism, no magic here. Of course you can create as large a cluster as you want, you just have to set up each node individually.</p>

<h2 id="some-design-decisions">Some Design Decisions</h2>

<p>If you look at the script, here are some design decisions. Itâ€™s using:</p>

<ul>
  <li>Ubuntu Kubernetes packages</li>
  <li>containerd</li>
  <li>Calico as the CNI</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>If you need a throwaway Kubernetes cluster that can be created in an Ubuntu 22.04 VM in a few minutes (like two!) I think this is a nice way to do that. Certainly it works for me.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Iâ€™ve been working with Kubernetes for a long time. Too long, actually. So long, in fact, that I donâ€™t really use it much anymore. Kubernetes has won in terms of being the default way to deploy modern applications. At this point, itâ€™s kind of boring, which is great! We want boring infrastructure. Boring works. If youâ€™re writing a new application today, the target is going to be a container, and that container is probably going to run in good old boring Kubernetes.]]></summary></entry><entry><title type="html">Leapfrogging: Switching From OpenAI to Claude, and Github Copilot to Cursor</title><link href="https://serverascode.com/2024/08/12/switching-to-claude-and-cursor.html" rel="alternate" type="text/html" title="Leapfrogging: Switching From OpenAI to Claude, and Github Copilot to Cursor" /><published>2024-08-12T00:00:00-04:00</published><updated>2024-08-12T00:00:00-04:00</updated><id>https://serverascode.com/2024/08/12/switching-to-claude-and-cursor</id><content type="html" xml:base="https://serverascode.com/2024/08/12/switching-to-claude-and-cursor.html"><![CDATA[<p>Iâ€™m a fan of the recent wave of AIâ€“specifically generative AI, or GenAI for short. I think of GenAI, these large language models, as a kind of compression. They take huge amounts of textâ€“programming code, for exampleâ€“and they are able to regurgitate it. So we take terrabytes of code, compress/feed it into an LLM that ends up being only a few gigabytes in size, and we can then talk to that LLM in natural language, and it can return code and other textâ€¦effectively uncompressing it.</p>

<h2 id="leapfrogging">Leapfrogging</h2>

<p>What I want to focus on here, for this post, is that vendors are getting better and better at building LLMs for decompressing code and, as well, better at building out the user experience for coaxing code out of LLMs. These LLMs, this GenAI, combined with a chatbot interface or an integrated development environment, can do so, so much. We can just ask them for the code, or about the code, or <em>how to code</em>, and they will provide the code or help you figure out how to write it. Itâ€™s not always great code, or perfect code, but itâ€™s usually good enough.</p>

<p>A few notes:</p>

<ol>
  <li>There are many companies that make LLMs. A few are building â€œfrontierâ€ LLMs, such as OpenAI and Claude.</li>
  <li>These companies are working to make LLMs and their interfaces better at interacting with humans and creating code.</li>
  <li>There are also companies working on how programmers can best use LLMs to write code. They are not building the LLMs directly, instead finding out how we can best use them.</li>
</ol>

<p>In each of the above situations, at some point, one company will leapfrog another. And that, as far as Iâ€™m concerned, is what has happened recently.</p>

<p>For the last few months Iâ€™ve been using OpenAIâ€™s ChatGPT and Githubâ€™s Copilot via VSCode. But now I have almost completely switched from ChatGPT and CoPilot to Claude and Cursor. I used to pay monthly for ChatGPT and CoPilot, and I stopped paying for them and started paying for Claude and Cursor.</p>

<table>
  <thead>
    <tr>
      <th>What I Used Before</th>
      <th>What I Use Now</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>OpenAI ChatGPT</td>
      <td>Claude 3.5 Sonnet</td>
    </tr>
    <tr>
      <td>GitHub Copilot</td>
      <td>Cursor</td>
    </tr>
  </tbody>
</table>

<p>The difference is amazingâ€¦for now!</p>

<h2 id="claude">Claude</h2>

<p>For a long time, OpenAI was great at coding. Over time, in my opinion, it started to slide. Maybe it got nerfed, I donâ€™t know. Then Claude 3.5 Sonnet came outâ€¦and it blew me away. Itâ€™s just very, very good at spitting out the codez.</p>

<p>But regurgitating code is one thing, doing it in an easy and intuitive way while programming is another. I donâ€™t mind the chatbot style of interacting with LLMs, but it does get tedious. Claude 3.5 Sonnet has helped solve this UX/UI problem with a concept it calls artifacts.</p>

<blockquote>
  <p>â€¦introducing Artifacts on Claude.ai, a new feature that expands how users can interact with Claude. When a user asks Claude to generate content like code snippets, text documents, or website designs, these Artifacts appear in a dedicated window alongside their conversation. This creates a dynamic workspace where they can see, edit, and build upon Claudeâ€™s creations in real-time, seamlessly integrating AI-generated content into their projects and workflows.</p>
</blockquote>

<p>Claud is absolutely the best LLM for coding right now. I pay for it. It will save you massive amounts of time.</p>

<p>Find Claude at <a href="https://claude.ai/">https://claude.ai/</a>.</p>

<h2 id="cursor">Cursor</h2>

<p>But no matter how much we tweak the chatbot-style interface, itâ€™s never going to be good enough. We need AI built right into the IDEâ€“the Integrated Developer Environmentâ€“which is really just a fancy text editor. AI has to be built in, and totally erâ€¦integratedâ€¦into the IDE.</p>

<p>This is what cursor isâ€“AI built right into the IDE. Itâ€™s not perfect, but itâ€™s certainly a good place to start. The best place right now.</p>

<blockquote>
  <p>Built to make you extraordinarily productive, Cursor is the best way to code with AI.</p>
</blockquote>

<p>Find Cursor at <a href="https://www.cursor.com/">https://www.cursor.com/</a>.</p>

<h2 id="competition">Competition</h2>

<p>I donâ€™t feel bad for Github CoPilot or OpenAI: thatâ€™s how competition works. As a consumer, I am in a great position to make choices about what tools I think are best, and given the pace of change, those tools will likely change over time, and perhaps Claude and Cursor will be leapfrogged by other companies and projects. What a fun time!</p>

<h2 id="the-future-looks-fantastic">The Future Looks Fantastic</h2>

<p>I canâ€™t describe where I think things are going with GenAI/LLMs and code better than this video. I heavily suggest watching it all, and perhaps even taking the time to watch the three hour video from which it came.</p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">On using Cursor + Claude 3.5 Sonnet + Tailwind to ship 20x faster (ft. <a href="https://twitter.com/Shpigford?ref_src=twsrc%5Etfw">@Shpigford</a>) <a href="https://t.co/lQ0yTjm8MF">pic.twitter.com/lQ0yTjm8MF</a></p>&mdash; Sahil Lavingia (@shl) <a href="https://twitter.com/shl/status/1821646287290110184?ref_src=twsrc%5Etfw">August 8, 2024</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>]]></content><author><name></name></author><summary type="html"><![CDATA[Iâ€™m a fan of the recent wave of AIâ€“specifically generative AI, or GenAI for short. I think of GenAI, these large language models, as a kind of compression. They take huge amounts of textâ€“programming code, for exampleâ€“and they are able to regurgitate it. So we take terrabytes of code, compress/feed it into an LLM that ends up being only a few gigabytes in size, and we can then talk to that LLM in natural language, and it can return code and other textâ€¦effectively uncompressing it.]]></summary></entry><entry><title type="html">Fine Tuning LLMs: Part 1 - Just Getting Started</title><link href="https://serverascode.com/2024/05/16/fine-tuning-llms-part-one-getting-started.html" rel="alternate" type="text/html" title="Fine Tuning LLMs: Part 1 - Just Getting Started" /><published>2024-05-16T00:00:00-04:00</published><updated>2024-05-16T00:00:00-04:00</updated><id>https://serverascode.com/2024/05/16/fine-tuning-llms-part-one-getting-started</id><content type="html" xml:base="https://serverascode.com/2024/05/16/fine-tuning-llms-part-one-getting-started.html"><![CDATA[<p>There are a few ways we can customise a Large Language Model (LLM), and one of those ways is to fine-tune it.</p>

<p>But why fine-tune an LLM?</p>

<blockquote>
  <p>Large language models (LLMs) like GPT-3 and Llama have shown immense promise for natural language generation. With sufficient data and compute, these models can produce remarkably human-like text. However, off-the-shelf LLMs still have limitations. They may generate text that is bland, inconsistent, or not tailored to your specific needs.
This is where finetuning comes in. Finetuning is the process of taking a pre-trained LLM and customizing it for a specific task or dataset. With finetuning, you can steer the LLM towards producing the kind of text you want. - <a href="https://medium.com/@dave-shap/a-pros-guide-to-finetuning-llms-c6eb570001d3">https://medium.com/@dave-shap/a-pros-guide-to-finetuning-llms-c6eb570001d3</a></p>
</blockquote>

<p>As well I just want to make a little disclaimer here on decisions Iâ€™ve made. :)</p>

<p><strong>DISCLAIMER</strong></p>

<blockquote>
  <p>Please note that what Iâ€™ve done here is really a personal experiment in fine-tuning an LLM. Thereâ€™s no particular rhyme or reason to the infrastructure and other choices Iâ€™ve made. Iâ€™m using a particular GPU supplier. Iâ€™m using a certain Python notebook. Iâ€™ve made some choices that might actually make things more difficult, or that might not make sense to an experienced fine-tuner. Also, in this post, Iâ€™m not tuning with a specific set of data or goal in mind. Iâ€™m just trying out a set of basic tools.</p>
</blockquote>

<p>Happy hacking!</p>

<h2 id="quick-fine-tuning-example">Quick Fine Tuning Example</h2>

<p>Steps:</p>

<ol>
  <li>Get a GPU from somewhere (Iâ€™m using brev.dev)</li>
  <li>Create an instance with proper CUDA and pytorch versioning</li>
  <li>Build a data set to fine-tune with (or use an existing one), NOTE: This step I will build on in later posts</li>
  <li>Use a <a href="https://github.com/unslothai/unsloth">Unsloth iPython notebook</a></li>
  <li>Step through the provided notebook and create a fine-tuned LLM</li>
</ol>

<h2 id="gpu-instance">GPU Instance</h2>

<p>First, we need a GPU.</p>

<blockquote>
  <p>NOTE: The easiest thing to do would just be to use Google Colab and the notebook that Unsloth links to; that would be super easy. Google Colab is a free cloud service to run Jupyter Notebooks and provides access to GPUs. But Iâ€™m not using Colab forâ€¦some reason. You might want to. Keep that in mind!</p>
</blockquote>

<p>Iâ€™m using <a href="https://brev.dev">brev.dev</a> to get access to a GPU instance, but there are tons of â€œGPU Brokersâ€ out there.</p>

<blockquote>
  <p>NOTE: I have no relationship with brev.dev, I just randomly started using the service. I canâ€™t tell you if itâ€™s good or not, but the combination of the provider plus the docker image for CUDA + pytorch is working for me. Plus if you leave the GUI console for long enough, a cute DVD-style screen saver comes on. lol!</p>
</blockquote>

<p>Here Iâ€™m creating a small NVIDIA 4090 instance. Other much larger GPUs are available from brev.dev and other providers.</p>

<p><img src="/img/brev1.jpg" alt="brev.dev" /></p>

<p>Note that Iâ€™m using the â€œadvanced container settingsâ€ and selecting the docker.io/pythorch/pytorch:2.2.0-cuda12.1-cudnn8-runtime image. This is key because Iâ€™ve had lots of problems matching up these versions, especially on my home workstation where I have a NVIDIA 3090.</p>

<p>I find versioning CUDA and Pytorch challenging so this is a really nice feature of brev.dev, though itâ€™s really just about dialing in the right image/settings/etc.</p>

<p><img src="/img/brev2.jpg" alt="brev.dev" /></p>

<p>Once the instance is running there is an option to connect to a notebook.</p>

<p><img src="/img/brev3.jpg" alt="brev.dev" /></p>

<p>And now we can use the notebook.</p>

<p><img src="/img/brev4.jpg" alt="brev.dev" /></p>

<p>Or you can login with the the brev shell. Here my instance is brilliantly named â€œaaaâ€.</p>

<pre><code>$ brev shell aaa
â¢¿ waiting for SSH connection to be available Agent pid 9158
Warning: Permanently added '[provider.pdx.nb.akash.pub]:31314' (ED25519) to the list of known hosts.
Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.0-101-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

This system has been minimized by removing packages and content that are
not required on a system that users do not log into.

To restore this content, you can run the 'unminimize' command.
âœ  verb-workspace 
Connection to provider.pdx.nb.akash.pub closed.
</code></pre>

<h2 id="unsloth-notebook">Unsloth Notebook</h2>

<p>Next Iâ€™ll upload the unsloth conversational notebook, which I obtained by opening the Colab notebook and downloading the file, then uploading it into the brev.dev instanceâ€™s notebook.</p>

<blockquote>
  <p>NOTE: There are a lot of notebooks for getting started training Llama3 out there. For example brev.dev has <a href="https://github.com/brevdev/notebooks/blob/main/README.md">some</a> too. As well, unsloth provides some via <a href="https://huggingface.co/datasets/unsloth/notebooks">huggingface</a>.</p>
</blockquote>

<p>Unsloth - <a href="https://github.com/unslothai/unsloth">https://github.com/unslothai/unsloth</a></p>

<blockquote>
  <p>Unsloth is a lightweight library for faster LLM fine-tuning which is fully compatible with the Hugging Face ecosystem (Hub, transformers, PEFT, TRL). The library is actively developed by the Unsloth team (Daniel and Michael) and the open source community. The library supports most NVIDIA GPUsâ€“from GTX 1070 all the way up to H100sâ€“, and can be used with the entire trainer suite from the TRL library (SFTTrainer, DPOTrainer, PPOTrainer). At the time of writing, Unsloth supports the Llama (CodeLlama, Yi, etc) and Mistral architectures. - <a href="https://huggingface.co/blog/unsloth-trl">https://huggingface.co/blog/unsloth-trl</a></p>
</blockquote>

<p>Iâ€™m using the <a href="https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing">conversational notebook</a> they link to in their README. That will bring you to a</p>

<p><img src="/img/unsloth1.jpg" alt="unsloth" /></p>

<h2 id="train-the-model">Train the Model</h2>

<p>Now we can simply step through the notebook and train an example model.</p>

<p><img src="/img/unsloth4.jpg" alt="unsloth" /></p>

<p>After stepping through the cells, we come to the training cell.</p>

<p><img src="/img/unsloth2.jpg" alt="unsloth" /></p>

<p>And we can see the memory usage.</p>

<p><img src="/img/unsloth3.jpg" alt="unsloth" /></p>

<p>If you continue through the notebook you can save the model in various ways, upload it to hugging face, etc.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The technical part of fine-tuning a model is fairly straightforward from a user perspective if you know a bit of Python and understand the concept of a Jupyter notebook and have one to follow through with. Really this blog post is just connecting some dots, like GPU providers and Python notebooks. However, navigating through a notebook and understanding exactly what it does are two different things. Also, once you start understanding the fine tuning process, it becomes a matter of what data we put in and what results we get out. That is the real work. This is just a basic skeleton, but everyone needs a first step!</p>

<h2 id="ps">PS.</h2>

<p>One of the things I love about LLMs right now is just how messy the technology landscape is. There is so much going on, so many niche technologies, libraries, chunks of code, websites, notebooks, on and on. Itâ€™s an amazing time.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[There are a few ways we can customise a Large Language Model (LLM), and one of those ways is to fine-tune it.]]></summary></entry><entry><title type="html">Ollama &amp;amp; Llama 3:8b Running Locally</title><link href="https://serverascode.com/2024/04/23/ollama-running-locally.html" rel="alternate" type="text/html" title="Ollama &amp;amp; Llama 3:8b Running Locally" /><published>2024-04-23T00:00:00-04:00</published><updated>2024-04-23T00:00:00-04:00</updated><id>https://serverascode.com/2024/04/23/ollama-running-locally</id><content type="html" xml:base="https://serverascode.com/2024/04/23/ollama-running-locally.html"><![CDATA[<p>First, we start up ollama.</p>

<pre><code>$ ollama serve
</code></pre>

<p>OK, itâ€™s running.</p>

<pre><code>$ curl localhost:11434 &amp;&amp; echo
Ollama is running
</code></pre>

<p>Now, letâ€™s run Metaâ€™s Llama 3:8b.</p>

<blockquote>
  <p>NOTE: This is only the 8b model, the smallest one so far, AFAIK.</p>
</blockquote>

<pre><code>$ ollama run llama3:8b
pulling manifest 
pulling 00e1317cbf74... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 4.7 GB                         
pulling 4fa551d4f938... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  12 KB                         
pulling 8ab4849b038c... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  254 B                         
pulling c0aac7c7f00d... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  128 B                         
pulling db46ef36ef0b... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  483 B                         
verifying sha256 digest 
writing manifest 
removing any unused layers 
success 
&gt;&gt;&gt; Send a message (/? for help)
</code></pre>

<p>Checkout memory usage. Not bad. Pretty small.</p>

<pre><code>$ nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits -i 0
5308
</code></pre>

<h2 id="intersecting-polygons">Intersecting Polygons</h2>

<p>Lets ask Llama 3:8b to solve a problem: write the code to determine if two polygons are intersecting. This is a problem that some LLMs canâ€™t answer. (However, it is possible that Llama 3 has â€œlearnedâ€ how to solve the problem from it being discussed recently on the internet, and given the fact that Llama 3 has new training data.)</p>

<p>Below, it looks like weâ€™ve gotten a pretty good answer, as it uses the Python shapely library, which is the easiest thing to do, and what Iâ€™d expect to see.</p>

<pre><code class="language-python">&gt;&gt;&gt; Write a function that calculates the intersection of two polygons. The function is given two polygons
...  represented as a list of points. A third polygon representing the intersection of the two is returne
... d, or an empty list of points if the intersection is empty.
Here's a Python function to calculate the intersection of two polygons:

import shapely.geometry
from typing import List

def polygon_intersection(polygon1: List[List[float]], polygon2: List[List[float]]) -&gt; 
List[List[float]]:
    """
    Calculate the intersection of two polygons.

    Args:
    polygon1 (List[List[float]]): The first polygon represented as a list of points.
    polygon2 (List[List[float]]): The second polygon represented as a list of points.

    Returns:
    List[List[float]]: A list of points representing the intersection of the two polygons, or an 
empty list if the intersection is empty.
    """
    # Convert the input polygons to Shapely Polygon objects
    poly1 = shapely.geometry.Polygon(polygon1)
    poly2 = shapely.geometry.Polygon(polygon2)

    # Calculate the intersection polygon
    intersection = poly1.intersection(poly2)

    # If the intersection is not empty, return it as a list of points
    if not intersection.is_empty:
        return [(point.x, point.y) for point in intersection.exterior.coords]
    else:
        return []

This function uses the Shapely library to work with geometric shapes. It first converts the input 
polygons into Shapely Polygon objects. Then it calculates the intersection of these two polygons 
using the `intersection` method.

If the intersection is not empty, the function returns a list of points representing this polygon 
as a list of coordinates. If the intersection is empty, an empty list is returned.

Here's how you can use this function:

polygon1 = [(0, 0), (0, 2), (2, 2), (2, 0)]
polygon2 = [(1, 1), (1, 3), (3, 3), (3, 1)]

intersection_polygon = polygon_intersection(polygon1, polygon2)
print(intersection_polygon)

This will output the points that make up the intersection of the two polygons.
</code></pre>

<p>However, this code, as is, as provided, will not run.</p>

<pre><code>$ python polygons.py 
Polygon 1:
(0, 0)
(0, 2)
(2, 2)
(2, 0)

Polygon 2:
(1, 1)
(1, 3)
(3, 3)
(3, 1)
Traceback (most recent call last):
  File "/home/curtis/working/intersecting-polygons/polygons.py", line 43, in &lt;module&gt;
    intersection_polygon = polygon_intersection(polygon1, polygon2)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/curtis/working/intersecting-polygons/polygons.py", line 26, in polygon_intersection
    return [(point.x, point.y) for point in intersection.exterior.coords]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/curtis/working/intersecting-polygons/polygons.py", line 26, in &lt;listcomp&gt;
    return [(point.x, point.y) for point in intersection.exterior.coords]
             ^^^^^^^
AttributeError: 'tuple' object has no attribute 'x'
</code></pre>

<p>But I provide llama 3 the error and it gives me this response.</p>

<pre><code>&gt;&gt;&gt; I get this error: AttributeError: 'tuple' object has no attribute 'x'
That's a great error message!

It looks like the `point` variable is actually a tuple (a simple data structure in Python), not an 
object with an `x` attribute. This is because the `intersection.exterior.coords` returns a list of 
tuples, where each tuple represents a point on the polygon.

To fix this, you can simply change the line:

return [(point.x, point.y) for point in intersection.exterior.coords]

to:

return [tuple(point) for point in intersection.exterior.coords]

This will convert each tuple (representing a point) into a new tuple with the same elements.

Alternatively, you can also use a list comprehension to extract the `x` and `y` values from each 
tuple, like this:

return [[p[0], 
p[1][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p]p[1][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p]p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][p][
</code></pre>

<blockquote>
  <p>NOTE: Interesting additional output (the [p]s). I copied and pasted directly. Not sure what happened thereâ€¦</p>
</blockquote>

<p>I replaced that line of code as it suggested, and the program ran. For an 8b model this capability seems perfectly fine, at least to me, and was better than what I found ChatGPT 3.5 was capable of a few months ago, in that Llama 3 used the Shapely module, whereas ChatGPT 3.5 would not. I would imagine Llama 3:8b would fail if I told it not to use Shapely.</p>

<p>Results of running the provided code:</p>

<pre><code>$ python polygons.py 
Polygon 1:
(0, 0)
(0, 2)
(2, 2)
(2, 0)

Polygon 2:
(1, 1)
(1, 3)
(3, 3)
(3, 1)

Intersection Polygon:
(2.0, 2.0)
(2.0, 1.0)
(1.0, 1.0)
(1.0, 2.0)
(2.0, 2.0)
</code></pre>

<h2 id="but-it-cant-graph-and-save-an-image">â€¦But It Canâ€™t Graph and Save an Image</h2>

<p>Unfortunately, Llama 3:8b was unable to provide the code to plot the polygons and save them as an image file, which would have looked something like the below image. It may have been able to do this with a different prompt.</p>

<p><img src="/img/intersection.png" alt="img" /></p>

<h2 id="overall-impressive">Overall: Impressive</h2>

<p>This was a very quick test. I only spent a handful of minutes on it.</p>

<p>Llama 3:8B. Itâ€™s fast. It can run locally. Itâ€™s pretty â€œsmartâ€, although it would take a bit of manual configuration to get the code it output to work; I didnâ€™t give it much of a chance to get things right. Overall, Iâ€™m impressed with this little LLMâ€“its compressed a lot of information.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[First, we start up ollama.]]></summary></entry></feed>